<us-patent-grant lang="EN" dtd-version="v4.2 2006-08-23" file="US07298913-20071120.XML" status="PRODUCTION" id="us-patent-grant" country="US" date-produced="20071106" date-publ="20071120">
<us-bibliographic-data-grant>
<publication-reference>
<document-id>
<country>US</country>
<doc-number>07298913</doc-number>
<kind>B2</kind>
<date>20071120</date>
</document-id>
</publication-reference>
<application-reference appl-type="utility">
<document-id>
<country>US</country>
<doc-number>10396437</doc-number>
<date>20030326</date>
</document-id>
</application-reference>
<us-application-series-code>10</us-application-series-code>
<priority-claims>
<priority-claim sequence="01" kind="national">
<country>JP</country>
<doc-number>2002-097892</doc-number>
<date>20020329</date>
</priority-claim>
</priority-claims>
<us-term-of-grant>
<us-term-extension>756</us-term-extension>
</us-term-of-grant>
<classifications-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>G</section>
<class>06</class>
<subclass>K</subclass>
<main-group>9</main-group>
<subgroup>36</subgroup>
<symbol-position>F</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20071120</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>G</section>
<class>06</class>
<subclass>K</subclass>
<main-group>9</main-group>
<subgroup>46</subgroup>
<symbol-position>L</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20071120</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
</classifications-ipcr>
<classification-national>
<country>US</country>
<main-classification>382239</main-classification>
<further-classification>382236</further-classification>
<further-classification>382238</further-classification>
</classification-national>
<invention-title id="d0e71">Video encoding method and apparatus employing motion compensated prediction interframe encoding, and corresponding video decoding method and apparatus</invention-title>
<references-cited>
<citation>
<patcit num="00001">
<document-id>
<country>US</country>
<doc-number>6037987</doc-number>
<kind>A</kind>
<name>Sethuraman</name>
<date>20000300</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>37524003</main-classification></classification-national>
</citation>
<citation>
<patcit num="00002">
<document-id>
<country>US</country>
<doc-number>6052150</doc-number>
<kind>A</kind>
<name>Kikuchi et al.</name>
<date>20000400</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>37524016</main-classification></classification-national>
</citation>
<citation>
<patcit num="00003">
<document-id>
<country>US</country>
<doc-number>6097842</doc-number>
<kind>A</kind>
<name>Suzuki et al.</name>
<date>20000800</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>382232</main-classification></classification-national>
</citation>
<citation>
<patcit num="00004">
<document-id>
<country>US</country>
<doc-number>6167158</doc-number>
<kind>A</kind>
<name>Boon</name>
<date>20001200</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>382238</main-classification></classification-national>
</citation>
<citation>
<patcit num="00005">
<document-id>
<country>US</country>
<doc-number>6343156</doc-number>
<kind>B1</kind>
<name>Yamaguchi et al.</name>
<date>20020100</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>382243</main-classification></classification-national>
</citation>
<citation>
<patcit num="00006">
<document-id>
<country>US</country>
<doc-number>6510249</doc-number>
<kind>B1</kind>
<name>Boon</name>
<date>20030100</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>382238</main-classification></classification-national>
</citation>
<citation>
<patcit num="00007">
<document-id>
<country>JP</country>
<doc-number>7-95589</doc-number>
<date>19950400</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<patcit num="00008">
<document-id>
<country>JP</country>
<doc-number>10-136372</doc-number>
<date>19980500</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<patcit num="00009">
<document-id>
<country>JP</country>
<doc-number>2000-32446</doc-number>
<date>20000100</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<patcit num="00010">
<document-id>
<country>JP</country>
<doc-number>2001-28757</doc-number>
<date>20010100</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<patcit num="00011">
<document-id>
<country>WO</country>
<doc-number>WO 02/05563</doc-number>
<kind>A1</kind>
<date>20020100</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<nplcit num="00012">
<othercit>International Standard ISO/IEC 13818-2 : 2000 (E), Dec. 2000, pp. 99-102.</othercit>
</nplcit>
<category>cited by other</category>
</citation>
<citation>
<nplcit num="00013">
<othercit>“Recommendation H.263: Video Coding For Low Bit Rate Communication”, ITU-T Draft Recommendation H.263, XP-002176560, Feb. 1998, pp. 1-167.</othercit>
</nplcit>
<category>cited by other</category>
</citation>
<citation>
<nplcit num="00014">
<othercit>Stephan Wenger, “Temporal Scalability Using P-Pictures For Low-Latency Applications”, Multimedia Signal Processing, XP-010318327, Dec. 7, 1998, pp. 559-564.</othercit>
</nplcit>
<category>cited by other</category>
</citation>
<citation>
<nplcit num="00015">
<othercit>Marta Karczewicz, et al., “A Proposal for SP-frames”, ITU-Telecommunications Standardization Sector VCEG-L27, XP-002287038, Jan. 9-12, 2001, pp. 1-9.</othercit>
</nplcit>
<category>cited by other</category>
</citation>
<citation>
<nplcit num="00016">
<othercit>“H. 26L Test Model Long Term No. 5 (TML-5) draft0.”, ITU-Telecommunications Standardization Sector of ITU, XP-001086628, Aug. 22-25, 200, pp. 1-31.</othercit>
</nplcit>
<category>cited by other</category>
</citation>
<citation>
<nplcit num="00017">
<othercit>“Motion Image Encoding Technique in Internet/Mobile Communication,” Triceps, Aug. 30, 2000, pp. 74 to 78, “PTYPE” and “ELNUM” in “1.4.1 Picture Layer”.</othercit>
</nplcit>
<category>cited by other</category>
</citation>
</references-cited>
<number-of-claims>4</number-of-claims>
<us-exemplary-claim>1</us-exemplary-claim>
<us-field-of-classification-search>
<classification-national>
<country>US</country>
<main-classification>382239</main-classification>
</classification-national>
</us-field-of-classification-search>
<figures>
<number-of-drawing-sheets>8</number-of-drawing-sheets>
<number-of-figures>13</number-of-figures>
</figures>
<us-related-documents>
<related-publication>
<document-id>
<country>US</country>
<doc-number>20040017951</doc-number>
<kind>A1</kind>
<date>20040129</date>
</document-id>
</related-publication>
</us-related-documents>
<parties>
<applicants>
<applicant sequence="001" app-type="applicant-inventor" designation="us-only">
<addressbook>
<last-name>Koto</last-name>
<first-name>Shinichiro</first-name>
<address>
<city>Machida</city>
<country>JP</country>
</address>
</addressbook>
<nationality>
<country>JP</country>
</nationality>
<residence>
<country>JP</country>
</residence>
</applicant>
<applicant sequence="002" app-type="applicant-inventor" designation="us-only">
<addressbook>
<last-name>Chujoh</last-name>
<first-name>Takeshi</first-name>
<address>
<city>Tokyo</city>
<country>JP</country>
</address>
</addressbook>
<nationality>
<country>JP</country>
</nationality>
<residence>
<country>JP</country>
</residence>
</applicant>
<applicant sequence="003" app-type="applicant-inventor" designation="us-only">
<addressbook>
<last-name>Kikuchi</last-name>
<first-name>Yoshihiro</first-name>
<address>
<city>Yokohama</city>
<country>JP</country>
</address>
</addressbook>
<nationality>
<country>JP</country>
</nationality>
<residence>
<country>JP</country>
</residence>
</applicant>
</applicants>
<agents>
<agent sequence="01" rep-type="attorney">
<addressbook>
<orgname>Oblon, Spivak, McClelland, Maier &amp; Neustadt, P.C.</orgname>
<address>
<country>unknown</country>
</address>
</addressbook>
</agent>
</agents>
</parties>
<assignees>
<assignee>
<addressbook>
<orgname>Kabushiki Kaisha Toshiba</orgname>
<role>03</role>
<address>
<city>Tokyo</city>
<country>JP</country>
</address>
</addressbook>
</assignee>
</assignees>
<examiners>
<primary-examiner>
<last-name>Chen</last-name>
<first-name>Wenpeng</first-name>
<department>2624</department>
</primary-examiner>
<assistant-examiner>
<last-name>Ge</last-name>
<first-name>Yuzhen</first-name>
</assistant-examiner>
</examiners>
</us-bibliographic-data-grant>
<abstract id="abstract">
<p id="p-0001" num="0000">A method for encoding a video block using reference blocks comprises assigning the video block to one of first and second prediction groups, and encoding the video block according to a motion compensated prediction encoding mode, using the reference blocks depending on the one of the first and second prediction groups to which the video block is assigned, one of the reference blocks being a decoded block, wherein a first prediction group is obtained by a prediction using the reference blocks belonging to a first prediction group, and a second prediction group is obtained by a prediction using the reference blocks belonging to at least one of the second prediction group and the first prediction group.</p>
</abstract>
<drawings id="DRAWINGS">
<figure id="Fig-EMI-D00000" num="00000">
<img id="EMI-D00000" he="120.57mm" wi="140.04mm" file="US07298913-20071120-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00001" num="00001">
<img id="EMI-D00001" he="231.39mm" wi="149.35mm" file="US07298913-20071120-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00002" num="00002">
<img id="EMI-D00002" he="200.41mm" wi="158.58mm" file="US07298913-20071120-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00003" num="00003">
<img id="EMI-D00003" he="229.62mm" wi="157.40mm" file="US07298913-20071120-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00004" num="00004">
<img id="EMI-D00004" he="240.62mm" wi="155.28mm" file="US07298913-20071120-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00005" num="00005">
<img id="EMI-D00005" he="231.06mm" wi="174.92mm" file="US07298913-20071120-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00006" num="00006">
<img id="EMI-D00006" he="248.07mm" wi="166.45mm" file="US07298913-20071120-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00007" num="00007">
<img id="EMI-D00007" he="235.54mm" wi="160.10mm" file="US07298913-20071120-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00008" num="00008">
<img id="EMI-D00008" he="180.09mm" wi="164.68mm" file="US07298913-20071120-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
</drawings>
<description id="description">
<?BRFSUM description="Brief Summary" end="lead"?>
<heading id="h-0001" level="1">CROSS-REFERENCE TO RELATED APPLICATIONS</heading>
<p id="p-0002" num="0001">This application is based upon and claims the benefit of priority from the prior Japanese Patent Application No. 2002-97892, filed Mar. 29, 2002, the entire contents of which are incorporated herein by reference.</p>
<heading id="h-0002" level="1">BACKGROUND OF THE INVENTION</heading>
<p id="p-0003" num="0002">1. Field of the Invention</p>
<p id="p-0004" num="0003">The present invention relates to a video encoding method and apparatus and a video decoding method and apparatus with the use of a motion compensated prediction intra frame encoding.</p>
<p id="p-0005" num="0004">2. Description of the Related Art</p>
<p id="p-0006" num="0005">As a video compression encoding technique, MPEG 1 (ISO/IEC 11172-2), MPEG2 (ISO/IEC 13818-2), MPEG 4 (ISO/IEC 14496-2) are put to practical use broadly. These video encoding modes are performed by a combination of an intra frame encoding, a forward prediction intra frame encoding, an encoding and a bi-directional prediction intra frame encoding. The frames encoded by these encoding modes are called I picture, P picture and B picture. P picture is encoded using as a reference frame P or I picture just before the former P picture. B picture is encoded using as reference frame P or I picture just before and after the B picture. The forward prediction interframe encoding and bi-directional prediction interframe encoding are referred to as a motion compensated prediction interframe encoding.</p>
<p id="p-0007" num="0006">When the video encoding data based on the MPEG mode is played back in fast-forward, a method that only I picture that the reference frame is not required is played back or a method that only I and P pictures is decoded while skipping B picture using a nature that B picture cannot be used as a reference frame is conventional. However, when only I picture is played back, if the period of I picture is long, a high-speed fast-forward playback can be carried out but a smooth fast-forward playback cannot be carried out. In a fast-forward playback with the use of I and P pictures, since P picture is encoded by an interframe prediction encoding, all I and P pictures must be decoded. For this reasons, it becomes difficult to change a fast-forward speed freely.</p>
<p id="p-0008" num="0007">In the video encoding of the conventional MPEG mode, B picture is not used as a reference frame. Therefore, in case of the prediction configuration that plural B pictures continue, B picture must be encoded using P picture separating from B picture with respect to a time as a reference frame. This results in a problem that the encoding efficiency of B picture deteriorates. On the other hand, when the decoded B picture is used as a reference frame in P picture, it is necessary to decode all frames including B picture in the fast-forward playback while skipping B picture. As a result, it becomes difficult to perform the fast-forward playback effectively.</p>
<p id="p-0009" num="0008">As described above, when the video encoded data obtained by the encoding including a motion compensated prediction interframe encoding such as MPEG is played back with a fast-forward, it is difficult to perform a smooth fast-forward playback at a free playback speed in playing back only I picture. When the fast-forward playback is performed with skipping B picture without decoding it, it is difficult to use the decoded B picture as a reference frame. For this reason, there is a problem that the encoding efficiency deteriorates in a prediction configuration that the B pictures continue.</p>
<heading id="h-0003" level="1">BRIEF SUMMARY OF THE INVENTION</heading>
<p id="p-0010" num="0009">It is an object of the invention is to provide a video encoding and decoding method and apparatus using a motion compensated prediction interframe encoding, that enable a fast-forward playback at a high encoding efficiency and a high degree of freedom in the decoding side.</p>
<p id="p-0011" num="0010">According to an aspect of the invention, there is provided a method for encoding a video block using reference blocks, comprising assigning the video block to one of a plurality of prediction groups including at least first and second prediction groups; and encoding the video block according to a motion compensated prediction encoding mode, using the reference blocks depending on the one of the prediction groups to which the video block is assigned, one of the reference blocks being a decoded block, wherein the first prediction group is obtained by a prediction using the reference blocks belonging to the first prediction group, and the second prediction group is obtained by a prediction using the reference blocks belonging to at least one of the second prediction group and the first prediction group.</p>
<?BRFSUM description="Brief Summary" end="tail"?>
<?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?>
<description-of-drawings>
<heading id="h-0004" level="1">BRIEF DESCRIPTION OF THE SEVERAL VIEWS OF THE DRAWING</heading>
<p id="p-0012" num="0011"><figref idref="DRAWINGS">FIG. 1</figref> shows a block diagram of a video encoding apparatus according to one embodiment of the present invention;</p>
<p id="p-0013" num="0012"><figref idref="DRAWINGS">FIG. 2</figref> is a diagram showing a flow of a main process concerning a motion compensated prediction interframe encoding in a video encoding;</p>
<p id="p-0014" num="0013"><figref idref="DRAWINGS">FIG. 3</figref> shows a block diagram of a moving image decoding apparatus according to one embodiment of the present invention;</p>
<p id="p-0015" num="0014"><figref idref="DRAWINGS">FIG. 4</figref> shows a flow of a main process for decoding a result of a motion compensated prediction interframe encoding;</p>
<p id="p-0016" num="0015"><figref idref="DRAWINGS">FIG. 5</figref> is a block diagram of a motion compensated prediction unit used for a video encoding apparatus and a video decoding apparatus according to the above embodiment;</p>
<p id="p-0017" num="0016"><figref idref="DRAWINGS">FIG. 6</figref> is a diagram showing an example of an interframe prediction configuration and reference frame control according to one embodiment of the present invention;</p>
<p id="p-0018" num="0017"><figref idref="DRAWINGS">FIG. 7</figref> is a diagram showing an example of an interframe prediction configuration and reference frame control according to one embodiment of the present invention;</p>
<p id="p-0019" num="0018"><figref idref="DRAWINGS">FIG. 8</figref> is a diagram showing an example of an interframe prediction configuration and reference memory control according to one embodiment of the present invention;</p>
<p id="p-0020" num="0019"><figref idref="DRAWINGS">FIG. 9</figref> is a diagram showing an example of an interframe prediction configuration and reference memory control according to one embodiment of the present invention;</p>
<p id="p-0021" num="0020"><figref idref="DRAWINGS">FIG. 10</figref> is a diagram showing an example of an interframe prediction configuration and reference memory control according to one embodiment of the present invention;</p>
<p id="p-0022" num="0021"><figref idref="DRAWINGS">FIG. 11</figref> is a diagram showing an example of an interframe prediction configuration and reference memory control according to one embodiment of the present invention;</p>
<p id="p-0023" num="0022"><figref idref="DRAWINGS">FIG. 12</figref> shows a block diagram of a video encoding apparatus according to a modification of the embodiment of the present invention; and</p>
<p id="p-0024" num="0023"><figref idref="DRAWINGS">FIG. 13</figref> shows a block diagram of a moving image decoding apparatus according to a modification of the embodiment of the present invention.</p>
</description-of-drawings>
<?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?>
<?DETDESC description="Detailed Description" end="lead"?>
<heading id="h-0005" level="1">DETAILED DESCRIPTION OF THE INVENTION</heading>
<p id="p-0025" num="0024">An embodiment of the present invention will be described with reference to accompanying drawings.</p>
<p id="h-0006" num="0000">(Encoding)</p>
<p id="p-0026" num="0025"><figref idref="DRAWINGS">FIG. 1</figref> is a block diagram of a video encoding apparatus according to the present embodiment. <figref idref="DRAWINGS">FIG. 2</figref> is a flow chart indicating steps of a process executed by the motion compensated prediction interframe encoding. The video encoding apparatus shown in <figref idref="DRAWINGS">FIG. 1</figref> may be realized by hardware, and may be executed by software by means of a computer. A part of the process is executed by the hardware and the remaining part thereof may be executed by the software.</p>
<p id="p-0027" num="0026">The present embodiment is based on a video encoding which is a combination of a motion compensated prediction, an orthogonal transformation and a variable-length coding, the video encoding being represented by a conventional MPEG scheme. There will now be described a video encoding method based on prediction groups including two hierarchical layers.</p>
<p id="p-0028" num="0027">A video signal <b>100</b> (video frame) is input to a video encoding apparatus every frame. At first the video frame of the video signal <b>100</b> is assigned to either of prediction groups of two hierarchical layers by a motion compensation prediction unit <b>111</b> (step S<b>11</b>). The video frame is encoded by a motion compensated prediction interframe encoding, using at least one reference frame belonging to a prediction group of at least one hierarchical layer lower than the hierarchical layer of the prediction group to which the video frame is assigned (step S<b>12</b>). In this embodiment, the reference frame stored in the frame memory set <b>118</b> is used.</p>
<p id="p-0029" num="0028">The assignment of the video frame to the prediction group of each hierarchical layer is changed between frames with time. For example, the even numbered frame is assigned to the prediction group of the first hierarchical layer, and the odd numbered frame to the prediction group of the second hierarchical layer. The reference frame belonging to the prediction group of each hierarchical layer is determined according to the prediction group belonging to the video frame corresponding to the encoded frame used as a reference frame. In other words, if a video frame is assigned to a prediction group of a hierarchical layer, the encoded frame obtained by encoding and local-decoding the video frame belongs to the prediction group of the same hierarchical layer. The process of steps S<b>11</b> and S<b>12</b> is explained in detail.</p>
<p id="p-0030" num="0029">As described above, a plurality of encoded frames belong to the prediction groups of the first and second hierarchical layers as reference frames. Two reference memory sets <b>118</b> and <b>119</b> are prepared for temporarily storing the encoded frames as the reference frames. The encoded frames belonging to the prediction group of the first hierarchical layer (i.e., the lowest hierarchical layer) are temporarily stored as reference frames in the first reference memory set <b>118</b>. The encoded frames belonging to the prediction group of the second hierarchical layer (i.e., the higher hierarchical layer) are temporarily stored as the reference frames in the second reference memory set <b>119</b>.</p>
<p id="p-0031" num="0030">The video frame assigned to the prediction group of the first hierarchical layer is subjected to the motion compensated prediction interframe encoding, using the reference frame belonging to the prediction group of the first hierarchical layer and stored in the first reference memory set <b>118</b>. On the other hand, the video frame assigned to the prediction group of the second hierarchical layer is subjected to the motion compensated prediction interframe encoding, using the reference frames belonging to both prediction groups of the first and the second hierarchical layers and stored in the first and second reference memory sets <b>118</b> and <b>119</b>.</p>
<p id="p-0032" num="0031">The motion compensated prediction frame encoding will be concretely explained. When the video frame corresponding to the video signal <b>100</b> belongs to the prediction group of the first hierarchical layer, one or more reference frames temporarily stored in the first reference memory set <b>118</b> are read out therefrom and input to the motion compensation prediction unit <b>111</b>. In this time, the switch <b>120</b> is OFF, so that the reference frame from the first reference memory set <b>119</b> is not input to the motion compensation prediction unit <b>111</b>. The motion compensation prediction unit <b>111</b> executes the motion compensated prediction using one or more reference frames read out from the reference memory set <b>118</b> to generate a prediction picture signal <b>104</b>. The prediction picture signal <b>104</b> is input to the subtracter <b>110</b> to generate a predictive error signal <b>101</b> that is an error signal of the prediction picture signal <b>104</b> with respect to the input video signal <b>100</b>.</p>
<p id="p-0033" num="0032">When the video frame corresponding to the input video signal <b>100</b> belongs to the prediction group of the second hierarchical layer, the switch <b>120</b> is ON. In this time, one or more reference frames temporarily stored in the first and second reference memory sets <b>118</b> and <b>119</b> are read out therefrom, and input to the motion compensation prediction unit <b>111</b>. The motion compensation prediction unit <b>111</b> generates the prediction picture signal <b>104</b> and supplies to the subtracter <b>110</b> similarly to the above. The subtracter <b>110</b> generates the predictive error signal <b>101</b>.</p>
<p id="p-0034" num="0033">The predictive error signal <b>101</b> is subjected to a discrete cosine transformation with the DCT transformer <b>112</b>. The DCT coefficient from the DCT transformer <b>112</b> is quantized with the quantizer <b>113</b>. The quantized DCT coefficient data <b>102</b> is divided in two routes, and encoded by the variable-length encoder <b>114</b> in one route. The DCT coefficient data <b>102</b> is reproduced as a predictive error signal by the dequantizer <b>115</b> and inverse DCT transformer <b>116</b> in the other route. This reproduced predictive error signal is added to the prediction picture signal <b>104</b> to generate a local decoded picture signal <b>103</b>.</p>
<p id="p-0035" num="0034">The encoded frame corresponding to the local decoded picture signal <b>103</b> is temporarily stored in either of the first and second reference memory sets <b>118</b> and <b>119</b> according to the prediction group of the hierarchical layer to which the video frame corresponding to the input video signal <b>100</b> is assigned (step S<b>13</b>). In other words, when the video frame belongs to the prediction group of the first hierarchical layer, the encoded frame is temporarily stored in the first reference memory set <b>118</b>. When the video frame belongs to the prediction group of the second hierarchical layer, the encoded frame is temporarily stored in the second reference memory set <b>119</b>.</p>
<p id="p-0036" num="0035">From the motion compensation prediction unit <b>111</b> is output so-called side information <b>105</b> including a motion vector used for a motion compensated prediction, an index (first identification information) for identifying the prediction group to which the video frame belongs and an index (second identification information) which specifies the reference frame used for the motion compensated prediction interframe encoding. The side information is encoded by the variable-length encoding unit <b>114</b> (step S<b>14</b>). In this case, the index for identifying the prediction group is encoded as a picture type representing, for example, a prediction configuration. The index specifying the reference frame is encoded every macroblock.</p>
<p id="p-0037" num="0036">These side information are output as variable-length coded data <b>106</b> along with the quantized DCT coefficient data which is a result of the motion compensated prediction interframe encoding (step S<b>15</b>). For example, the side information is encoded as header information to encoded data <b>106</b>. Further, if a second reference frame number setting method is adopted, information indicating the maximum number of frames is encoded as header information to the encoded data <b>106</b>. The second reference frame number setting method is a method of setting the maximum number of reference frames assigned to the prediction group of each hierarchical layer by predefining the total number of reference frames belonging to the prediction group of each hierarchical layer. The encoded data <b>106</b> is sent to a storage medium or a transmission medium (not shown).</p>
<p id="p-0038" num="0037">The new decoded frames are sequentially written in the reference memory sets <b>118</b> and <b>119</b> as reference frames. So-called FIFO (First-In First-Out) type control that the stored frames are sequentially deleted from the oldest reference frame is performed in units of a frame. However, when the reference frame is read out, a random access is done to an arbitrary reference frame in each of the reference memory sets <b>118</b> and <b>119</b>.</p>
<p id="p-0039" num="0038">The number of reference frames temporarily stored in the reference memory sets <b>118</b> and <b>119</b> respectively, in other words, the number of reference memories included in each of the reference memory sets <b>118</b> and <b>119</b> is determined by either of the following two methods.</p>
<p id="p-0040" num="0039">In the first reference frame number setting method, the maximum number of reference frames belonging to the prediction group of each hierarchical layer is previously established according to an encoding method or an encoding specification such as a profile and a level. In the video encoding apparatus and the video decoding apparatus, the maximum number of the reference frames determined as described above is assured every prediction group, and encoding and decoding are done. In this case, the necessary number of reference frames can be assured automatically, by making the encoding specification coincide between the video encoding apparatus and the video decoding apparatus.</p>
<p id="p-0041" num="0040">In the second reference frame number setting method, the total number of reference frames belonging to the prediction group of each hierarchical layer is predefined according to an encoding method or an encoding specification such as a profile and a level, and information on how many reference frames are assigned to the prediction group of each hierarchical layer, that is, information indicating the maximum number of frames is encoded as header information to the encoded data <b>106</b>.</p>
<p id="p-0042" num="0041">As thus described, in the second reference frame number setting method, the maximum number of reference frames which are most suitable for the prediction group of each hierarchical layer is dynamically assigned to the prediction group in the encoding side.</p>
<p id="p-0043" num="0042">By encoding information indicating the assigned maximum number of frames, it is possible to make the maximum number of reference frames belonging to the prediction group of each hierarchical layer coincide between the encoding side and the decoding side. Therefore, a ratio of the maximum number of reference frames belonging to the prediction group of each hierarchical layer with respect to the total number of reference frames is suitably changed according to the change of the image nature of the input video signal <b>100</b>. As a result, the encoding efficiency is improved.</p>
<p id="p-0044" num="0043">In the above explanation, the encoding is performed in units of frames. The encoding is performed in units of blocks (macroblocks). In other words, the video block is assigned to one of a plurality of prediction groups including at least first and second prediction groups. The video block is encoded according to a motion compensated prediction encoding mode, using the reference blocks depending on the one of the prediction groups to which the video block is assigned, one of the reference blocks being a decoded block. The first prediction group is obtained by a prediction using the reference blocks belonging to the first prediction group. The second prediction group is obtained by a prediction using the reference blocks belonging to at least one of the second prediction group and the first prediction group.</p>
<p id="p-0045" num="0044">The video block is encoded by each of an intraframe encoding mode, a forward prediction interframe encoding mode and a bi-directional prediction interframe encoding mode. The first video blocks encoded by the intraframe encoding mode and the forward prediction interframe encoding mode and the reference blocks corresponding to the first video blocks are assigned to the first prediction group. The second video blocks encoded by the bi-directional prediction interframe encoding mode and the reference blocks corresponding to the second video blocks are assigned to at least one of the first and second prediction groups.</p>
<p id="h-0007" num="0000">(Decoding)</p>
<p id="p-0046" num="0045"><figref idref="DRAWINGS">FIG. 3</figref> is a block diagram of a video decoding apparatus corresponding to the video encoding apparatus shown in <figref idref="DRAWINGS">FIG. 1</figref>. <figref idref="DRAWINGS">FIG. 4</figref> is a flow chart indicating steps of a process concerning the decoding corresponding to the motion compensated prediction interframe encoding. The video decoding apparatus shown in <figref idref="DRAWINGS">FIG. 3</figref> may be realized by hardware, and may carry out by software. Alternately, a part of the process is executed by the hardware and the remaining part may be executed by the software.</p>
<p id="p-0047" num="0046">The encoded data <b>106</b> output from the video encoding apparatus shown in <figref idref="DRAWINGS">FIG. 1</figref> is input to the video decoding apparatus shown in <figref idref="DRAWINGS">FIG. 3</figref> through the storage medium or transmission medium. The input encoded data <b>200</b> is subjected to a variable-length decoding by a variable-length decoder <b>214</b>, so that quantized DCT coefficient data <b>201</b> and side information <b>202</b> are output. The quantized DCT coefficient data <b>201</b> is decoded via the dequantizer <b>215</b> and inverse DCT transformer <b>216</b> so that a predictive error signal is reproduced.</p>
<p id="p-0048" num="0047">On the other hand, side information <b>202</b> including a motion vector encoded every macroblock, an index (first identification information) identifying the prediction group belonging to each video frame and an index (second identification information) specifying a reference frame is decoded (step <b>21</b>). The selection of reference frame and motion compensation is performed according to the side information similarly to the encoding to generate a prediction picture signal <b>203</b>. In other words, the reference frame is selected according to the first identification information and the second identification information (step S<b>22</b>). The result of the motion compensated prediction interframe encoding is decoded by the selected reference frame (step S<b>23</b>). The prediction picture signal <b>203</b> and the predictive error signal from the inverse DCT transformer <b>216</b> are added to generate a decoded picture signal <b>204</b>.</p>
<p id="p-0049" num="0048">The decoded frame corresponding to the decoded picture signal <b>204</b> is temporarily stored in either of the first and second reference memory sets <b>218</b> and <b>219</b> according to the prediction group to which the encoded frame corresponding to the decoded frame belongs (step S<b>24</b>). The decoded frame is used as the reference frame. These reference memory sets <b>218</b> and <b>219</b> are controlled in FIFO type similarly to the video encoding apparatus. The number of reference frames belonging to the prediction group of each hierarchical layer is set according to the first and second reference frame number setting methods described in the video encoding apparatus.</p>
<p id="p-0050" num="0049">In other words, when the maximum number of reference frames belonging to the prediction group of each hierarchical layer is predefined according to the first reference frame number setting method and the encoding specification, the number of reference frames belonging to the prediction group of each hierarchical layer is set to a fixed value every encoding specification. When the total number of reference frames belonging to the prediction group of each hierarchical layer is predefined according to the second reference frame number setting method and the encoding specification, and the maximum number of reference frames is assigned to the prediction group of each hierarchical layer. Only the total number of reference frames is fixed, and the number of reference frames belonging to the prediction group of each hierarchical layer is dynamically controlled based on information indicating the maximum number of reference frames decoded according to the header information of encoded data.</p>
<p id="p-0051" num="0050"><figref idref="DRAWINGS">FIG. 5</figref> shows a configuration of the motion compensation prediction unit <b>111</b> in the video encoding apparatus shown in <figref idref="DRAWINGS">FIG. 1</figref> or the motion compensation prediction unit <b>211</b> in the video decoding apparatus shown in <figref idref="DRAWINGS">FIG. 3</figref>.</p>
<p id="p-0052" num="0051">As mentioned above, available reference frames differ according to the prediction group of the hierarchical layer to which the frame to be encoded or the frame to be decoded belongs. Assuming that frame memories <b>302</b> to <b>304</b> in <figref idref="DRAWINGS">FIG. 5</figref> store reference frames available as a reference frame for the encoded frame belonging to the prediction group of one hierarchical layer.</p>
<p id="p-0053" num="0052">The motion compensation prediction unit selects one from among the available reference frames every macroblock or calculates a linear sum of the available reference frames by the linear predictor <b>301</b> to predict a reference frame based on the linear sum, whereby a motion compensation is performed to generate a prediction macroblock.</p>
<p id="p-0054" num="0053">The video encoding apparatus selects the reference frame and the motion vector every macroblock so that the prediction macroblock with a small prediction error and a highest encoding efficiency is selected. The information of the selected reference frame and the information of the motion vector are encoded every macroblock.</p>
<p id="p-0055" num="0054">In the video decoding apparatus, the motion compensation unit generates and decodes a prediction macroblock according to the received motion vector and information of the reference frame. When the prediction is performed based on the linear sum, information concerning the linear prediction coefficient is encoded as header information of the encoded data to make the linear predictor coefficient coincide between encoding and decoding.</p>
<p id="p-0056" num="0055"><figref idref="DRAWINGS">FIGS. 6 to 11</figref> show diagrams for explaining an interframe prediction configuration and a reference memory control in the present embodiment.</p>
<p id="p-0057" num="0056"><figref idref="DRAWINGS">FIG. 1</figref> shows an example configured by I and P pictures and switching each frame alternatively between a prediction group a and a prediction group b. Assuming that the prediction group b is a higher hierarchical layer than the prediction group a. Also, it is assumed that the reference memory of each of the prediction group a and the prediction group b is one frame.</p>
<p id="p-0058" num="0057">A picture with a suffix a such as Ia<b>0</b>, Pa<b>2</b> or Pa<b>4</b> belongs to the prediction group a, and a picture with a suffix b such as Pb<b>1</b>, Pb<b>3</b> or Pb<b>5</b> belongs to the prediction group b. The attributes of these prediction groups are encoded as an extension of a picture type or an independent index and are used as header information of the video frame. The video frame belonging to the prediction group a can use only the frame belonging to the prediction frame a and already decoded as a reference frame.</p>
<p id="p-0059" num="0058">As for the prediction frame b of the higher hierarchical layer, a prediction picture is generated using one frame belonging to either of the prediction group a and the prediction group b and already decoded or a linear sum of both decoded frames.</p>
<p id="p-0060" num="0059">The prediction group of each hierarchical layer has a reference memory corresponding to one frame. Thus, the number of reference frame for the video frame of the prediction group a is 1 in maximum. Two reference frames in maximum can be used for the video frame of the prediction group b. The frame Pa<b>2</b> belonging to, for example, the prediction group a uses only the decoded frame Ia<b>0</b> as the reference frame. The frame Pb<b>3</b> belonging to the prediction group b uses two frames, i.e., the decoded frame Pa<b>2</b> belonging to the prediction group a and the decoded frame Pb<b>1</b> belonging to the prediction group b as the reference frame.</p>
<p id="p-0061" num="0060">In <figref idref="DRAWINGS">FIG. 6</figref>, FM<b>1</b>, FM<b>2</b> and FM<b>3</b> show physical reference memories. DEC, REFa and REFb show logical reference memories respectively. In other words, DEC, REFa and REFb are the frame memories expressed by virtual addresses. FM<b>1</b>, FM<b>2</b> and FM<b>3</b> are the frame memories expressed by physical addresses. In virtual address expression, DEC is a frame memory for temporarily storing a currently decoded frame. REFa and REFb show reference memories of the prediction groups a and b, respectively. Therefore, the decoded frames belonging to the prediction group a are sequentially and temporarily stored in the reference memory REFa. The decoded frames belonging to the prediction group b are sequentially and temporality stored in the reference memory REBb.</p>
<p id="p-0062" num="0061">In the example of <figref idref="DRAWINGS">FIG. 6</figref>, it is possible to discard the video frame belonging to the prediction group b of the higher hierarchical layer and decode only a frame belonging to the prediction group a. In this case, if there are two reference memories, e.g., a frame memory DEC for temporarily storing a currently decoded frame and a reference memory REFa of the prediction group a, decoding is possible.</p>
<p id="p-0063" num="0062">It is possible to decode the frame at a half frame period without breaking down a prediction configuration by decoding only the frame belonging to the prediction group a. A smooth fast-forward playback can be performed by playing back the decoded frame belonging to the prediction group a at a frame rate of 2 times, for example. Also, when the bandwidth of a transmission channel fluctuates along with time in a video streaming, all encoded data are transmitted in normal cases. When the effective bandwidth of the transmission channel decreases, the encoded data belonging to the prediction group b is discarded and only the encoded data belonging to the prediction group a of the lower hierarchical layer is sent. In this case, the decoded frame can be reproduced without failure on the receiving side.</p>
<p id="p-0064" num="0063"><figref idref="DRAWINGS">FIG. 7</figref> shows a modification of <figref idref="DRAWINGS">FIG. 6</figref>, and illustrates a prediction configuration that two frames belonging to the prediction group b are inserted between the frames belonging to the prediction group a. The reference memory of the prediction group of each hierarchical layer is one frame. For this case, decoding is possible by using a frame memory for three frames similarly to <figref idref="DRAWINGS">FIG. 6</figref>. In the example of <figref idref="DRAWINGS">FIG. 7</figref>, by decoding only frames of, for example, the prediction group a and playing back the encoded frames at an original frame rate, it is possible to perform a smooth three-times fast-forward playback.</p>
<p id="p-0065" num="0064"><figref idref="DRAWINGS">FIG. 8</figref> shows a prediction configuration which is configured by I and P pictures and whose prediction group includes three hierarchical layers a, b and c. The frames of the prediction group a are assigned every four input frames. One frame of the prediction group b and two frames of the prediction group c are inserted between the frames of the prediction group a.</p>
<p id="p-0066" num="0065">The reference frame of the prediction groups a, b and c of respective hierarchical layers is one frame. The hierarchy increase in an order of a, b and c. In other words, the frame belonging to the prediction group a can use only one frame of the decoded prediction group a as a reference frame. The frame belonging to the prediction group b can use two frames of the decoded prediction groups a and b as reference frames. The frame belonging to the prediction group c can use three frames of the decoded prediction groups a, b and c as reference frames.</p>
<p id="p-0067" num="0066">In <figref idref="DRAWINGS">FIG. 8</figref>, DEC, REFa, REFb and REFc show a frame memory for temporarily storing a decoded frame, and logical frame memories for storing the reference frames of the prediction group a, the reference frames of the prediction group b and the reference frames of the prediction group c, respectively. FM<b>1</b>, FM<b>2</b>, FM<b>3</b> and FM<b>4</b> show physical frame memories for the above four frames, respectively. One frame that has been decoded just before the current frame is temporarily stored in the reference memories REFa, REFb and REFc. The currently decoded frame is written in the decoded frame memory DEC.</p>
<p id="p-0068" num="0067">In the configuration of <figref idref="DRAWINGS">FIG. 8</figref>, the prediction group includes three hierarchical layers. Therefore, when all encoded frames not more than the prediction group c are decoded, a normal playback is performed. When the encoded frames not more than the prediction group b are decoded, the frames ½ of the normal number of frames are decoded. When the encoded frames not more than the prediction group a are decoded, the frames ¼ of the normal number of frames are decoded. In either decoding, the normally decoded picture can be generated without failure of the prediction configuration. The fast-forward playback at a smoothly adjustable speed can be realized by dynamically controlling the hierarchical layer to be decoded. Alternatively, a transmission bit rate is dynamically changed by dynamically controlling the hierarchical layer to be transmitted.</p>
<p id="p-0069" num="0068">In <figref idref="DRAWINGS">FIG. 9</figref>, the prediction configuration comprises I, P and B pictures, I and P pictures are assigned to the prediction group a and B picture to the prediction group b. The prediction group b assumes a higher hierarchical layer that that of the prediction group a. The prediction group a includes two frames, i.e., two reference memories, and the prediction group b includes one frame, i.e., one reference memory.</p>
<p id="p-0070" num="0069">In the example of <figref idref="DRAWINGS">FIG. 9</figref>, the number of reference memories of I and P pictures of the prediction group a is 2. Therefore, it is possible to use two frames as reference frames, one of the two frames being I or P picture that is encoded or decoded just before a current P picture and the other being I or P pictures at two frames before the current P picture. In B picture, the prediction group b has one reference frame. Therefore, one frame of B picture encoded or decoded just before the current frame is used as a reference frame. Further, it is possible to use the reference frames of three frames that are formed from B picture and I and P pictures of two past frames included in a prediction group corresponding to the lower hierarchical layer.</p>
<p id="p-0071" num="0070">Similarly to <figref idref="DRAWINGS">FIGS. 6 to 8</figref>, FM<b>1</b>, FM<b>2</b>, FM<b>3</b> and FM<b>4</b> show physical frame memories, and DEC, REFa<b>1</b>, REFa<b>2</b> and REFb show logical frame memories. DEC shows a frame memory for temporarily storing a frame during decoding. REFa<b>1</b> and REFa<b>2</b> show reference memories corresponding to two frames of the prediction group a. REFb shows a reference memory corresponding to one frame of the prediction group b.</p>
<p id="p-0072" num="0071">Idx<b>0</b> and Idx<b>1</b> in <figref idref="DRAWINGS">FIG. 9</figref> show indexes to specify the reference frames for a frame during decoding. In decoding, for example, a frame Pa<b>6</b>, two frames Pa<b>3</b> and Ia<b>0</b> just before the frame Pa<b>6</b> and belonging to the prediction group a are candidates of the reference frame. The indexes of the reference frames are assigned in sequence to the frames that are time-closer to the video frame. The index indicating the reference frame is encoded every macroblock and the reference frame is selected every macroblock. With respect to the macroblock of the index <b>0</b>, the prediction image is generated by I or P picture just before the picture corresponding to the macroblock. With respect to the macroblock of the index <b>1</b>, the prediction image is generated by I or P picture at two frames before the picture corresponding to the macroblock. When the prediction image is generated by a linear sum of I or P picture just before the current picture and I or P picture at two frames before the current picture, an index identifying a pair of indexes <b>0</b> and <b>1</b> is encoded as header information of a macro book.</p>
<p id="p-0073" num="0072">BWref in <figref idref="DRAWINGS">FIG. 9</figref> shows a reference frame for the backward prediction of B picture. In the example of <figref idref="DRAWINGS">FIG. 9</figref>, the backward reference frame for pictures Bb<b>1</b> and Bb<b>2</b> is a picture Pa<b>3</b>, and the backward reference frame for pictures Bb<b>4</b> and Bb<b>5</b> is a picture Pa<b>6</b>. The reference frame of the backward prediction is limited to I or P picture encoded or decoded just before due to constraint of sorting of frames. Thus, the reference frame is uniquely determined. Therefore, the reference frame BWref of the backward prediction should not be encoded as header information.</p>
<p id="p-0074" num="0073">The forward prediction of B picture can be performed by two frames selectable in maximum in the example of <figref idref="DRAWINGS">FIG. 9</figref>. In encoding and decoding of, for example, the picture Bb<b>4</b>, the picture Pa<b>3</b> that is a frame just before the picture Bb<b>4</b> in time and belongs to the prediction group a and the picture Bb<b>2</b> that is a frame at two frames before the picture Bb<b>4</b> and belongs to the prediction group b can be used as the reference frames. An index indicating which reference frame is selected every macroblock or whether a prediction is performed by the linear sum of both reference frames is encoded. Two kinds of pictures Bb<b>4</b> and Pa<b>3</b> are used as the reference frames similarly to the picture Bb<b>5</b>.</p>
<p id="p-0075" num="0074">As for the indexes of the reference frames, numbering is added to the reference frames every video frame in a sequence to be time-closer to the reference frame for the forward prediction. In the example of <figref idref="DRAWINGS">FIG. 9</figref>, in encoding and decoding of P picture, I or P pictures stored in the reference memory are arranged in a time order and numbered. In encoding and decoding of B picture, all reference frames stored in the reference memory, except for I or P picture that is encoded or decoded just before that it is used as the reference frame for the backward prediction, are arranged in a time order and numbered. Idx<b>0</b> and Idx<b>1</b> in <figref idref="DRAWINGS">FIG. 9</figref> indicate indexes generated according to the above rule.</p>
<p id="p-0076" num="0075"><figref idref="DRAWINGS">FIG. 10</figref> is a modification of <figref idref="DRAWINGS">FIG. 9</figref>, and shows a case that sets the number of reference frame to 2 and the total number of frame memories to 5 with respect to the prediction group b, that is, B picture, too. FM<b>1</b> to FM<b>5</b> show physical reference frames. DEC shows a buffer that temporarily stores a picture in decoding. REFa<b>1</b> and REFa<b>2</b> show the prediction group a, namely, reference memories for I and P pictures. REFb<b>1</b> and REFb<b>2</b> show the prediction group b, namely, logical reference memories for B picture respectively. Idx<b>0</b>, Idx<b>1</b> and Idx<b>2</b> indicate reference frame indexes allocated in the forward prediction. BWref shows a reference frame for the backward prediction of B picture. The reference frame index in the forward prediction is encoded as header information every macroblock similarly to the example of <figref idref="DRAWINGS">FIG. 9</figref>.</p>
<p id="p-0077" num="0076">In the examples of <figref idref="DRAWINGS">FIGS. 6 to 10</figref>, the number of reference memories of the prediction group of each hierarchical layer is fixed. However, the number of reference frames of the prediction group of each hierarchical layer may be dynamically changed under the constant total number of reference frames. In the configuration of, for example, <figref idref="DRAWINGS">FIG. 6</figref>, the number of reference memories of the prediction group b is set to 0, and at the same time the number of reference memories of the prediction group a is set to 2. Such a change may be notified with header information of encoded data from the encoding side to the decoding side. On that occasion, the selection of motion compensated prediction is controlled so that the prediction from two past frames of the prediction group a can be employed in the encoding side, and the prediction from the past frame of the prediction group b is prohibited whereas the prediction from two past frames of the prediction group a is performed.</p>
<p id="p-0078" num="0077">In the above explanation, the decoding is performed in units of frames. The decoding is performed in units of blocks (macroblocks). In other words, the coded data includes encoded video block data, first encoded identification information indicating first and second prediction groups to which the video block data is assigned and second encoded identification information indicating reference block data used in the motion compensated prediction interframe encoding. The first encoded identification information and the second encoded identification information are decoded to generate first decoded identification information and second decoded identification information. The video block data is decoded using the reference block data belonging to the first prediction group and the reference block data belonging to at least one of the first and second prediction groups according to the first decoded identification information and the second decoded identification information.</p>
<p id="p-0079" num="0078"><figref idref="DRAWINGS">FIG. 11</figref> shows a prediction configuration and how to use the frame memory when allocation of the reference memories is changed to the example of <figref idref="DRAWINGS">FIG. 6</figref> as described above.</p>
<p id="p-0080" num="0079">The above way enables dynamically to set an optimum prediction configuration suitable for an input video image in the limited number of reference frames. Also, the way enables a high efficiency encoding with improved prediction efficiency.</p>
<p id="p-0081" num="0080"><figref idref="DRAWINGS">FIGS. 12 and 13</figref> show a video encoding apparatus and a video decoding apparatus using prediction groups not less than three hierarchical layers, respectively. According to this, the reference frame set <b>118</b> or <b>218</b> belong to the lowest hierarchical layer. Two or more reference frame sets <b>119</b> belonging to higher hierarchical layers and two or more switches <b>117</b> and <b>120</b> are provided in the video encoding apparatus. Two or more reference frame sets <b>219</b> belonging to higher hierarchical layers and two or more switches <b>217</b> and <b>220</b> are provided in the video decoding apparatus. When the switches <b>117</b> and <b>120</b> or the switches <b>217</b> and <b>220</b> are closed according to the number of hierarchical layers, the number of the reference frames is increased. In other words, the switches <b>117</b> and <b>120</b> or the switches <b>217</b> and <b>220</b> are sequentially closed according to incrementation of a hierarchy. More specifically, a plurality of video frames assigned to a plurality of prediction groups sequentially layered from a prediction group of a lowest hierarchical layer to at least one prediction group of a higher hierarchical layer than the lowest hierarchical layer. The video frames are subjected to a motion compensated prediction interframe encoding, using reference frames belonging to the prediction group of the lowest hierarchical layer and the prediction group of the hierarchical layer lower than that of the prediction group to which the video frames are assigned.</p>
<p id="p-0082" num="0081">As described above, an interframe prediction configuration is made up as a layered prediction group configuration. An interframe prediction from the reference frame of the prediction group of a higher hierarchical layer is prohibited. In addition, the number of reference frames of the prediction group of each hierarchical layer is dynamically changed under the constant total number of reference frames, resulting in that the encoding efficiency is improved and the fast-forward playback can be realized with a high degree of freedom.</p>
<p id="p-0083" num="0082">When the hierarchy is increased, a gentle playback can be done in the fast-forward playback. Also, since a period of frame, i.e., a frame frequency increases, a picture quality is improved in the fast-forward playback.</p>
<p id="p-0084" num="0083">When the multi-hierarchical layer video image described above is played back with a home television, all hierarchical layers can be played back. When the multi-hierarchical layer video image is played back with a cellular phone, the multi-hierarchical layer video image can be played back with being appropriately skipped in order to lighten a burden of a hardware. That is to say, the hierarchical layers can be selected according to the hardware of the receiver side.</p>
<p id="p-0085" num="0084">Additional advantages and modifications will readily occur to those skilled in the art. Therefore, the invention in its broader aspects is not limited to the specific details and representative embodiments shown and described herein. Accordingly, various modifications may be made without departing from the spirit or scope of the general inventive concept as defined by the appended claims and their equivalents.</p>
<?DETDESC description="Detailed Description" end="tail"?>
</description>
<us-claim-statement>What is claimed is:</us-claim-statement>
<claims id="claims">
<claim id="CLM-00001" num="00001">
<claim-text>1. A method for encoding a video picture using reference pictures, comprising:
<claim-text>assigning a video picture to one of a plurality of prediction groups of hierarchical layers including at least a first hierarchical layer and a second hierarchical layer higher than the first hierarchical layer;</claim-text>
<claim-text>encoding the video picture assigned to a prediction group of the first hierarchical layer according to a motion compensated prediction interframe encoding mode, using a reference picture belonging to the prediction group of the first hierarchical layer;</claim-text>
<claim-text>encoding the video picture assigned to a prediction group of the second hierarchical layer according to the motion compensated prediction interframe encoding mode, using a reference picture belonging to either of the prediction group of the first hierarchical layer and the prediction group of the second hierarchical layer;</claim-text>
<claim-text>encoding first identification information indicating the hierarchical layer of the group to which the video picture belongs and second identification information indicating a prediction mode of the motion compensated prediction interframe encoding mode to generate side information;</claim-text>
<claim-text>outputting the side information together with the video picture encoded according to the motion compensated prediction interframe encoding mode;</claim-text>
<claim-text>setting a sum of the reference pictures assigned to the prediction groups to a constant value; and</claim-text>
<claim-text>encoding reference picture number information indicating the number of reference pictures assigned to each of the prediction groups and including the coded reference picture number information in the side information.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00002" num="00002">
<claim-text>2. A video encoding method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein assigning the video picture includes encoding the video picture by each of an intraframe encoding mode, a forward prediction interframe encoding mode and a bi-directional prediction interframe encoding mode, and assigning the video picture includes assigning first video pictures encoded by the intraframe encoding mode and the forward prediction interframe encoding mode and the reference pictures corresponding to the first video pictures to the prediction group of the first hierarchical layer, and assigning second video pictures encoded by the bi-directional prediction interframe encoding mode and the reference pictures corresponding to the second video pictures to at least one of the prediction group of the first hierarchical layer and the prediction group of the second hierarchical layer.</claim-text>
</claim>
<claim id="CLM-00003" num="00003">
<claim-text>3. A video encoding apparatus which encodes a video picture using reference pictures, comprising:
<claim-text>an assigning unit configured to assign the video picture to one of a plurality of groups of hierarchical layers including at least first and second hierarchical layers;</claim-text>
<claim-text>an encoding unit configured to encode the video picture assigned to a group of the first hierarchical layer according to a motion compensated prediction interframe encoding mode, using a reference picture belonging to the group of the first hierarchical layer, and encode the video picture assigned to a group of the second hierarchical layer according to the motion compensated prediction interframe encoding mode, using the reference pictures belonging to the groups of the first and the second hierarchical layers, wherein</claim-text>
<claim-text>the encoding unit encodes first identification information indicating the group to which the video pictures belongs and second identification information indicating the reference picture used in the motion compensated prediction interframe encoding to generate side information, and outputs the side information together with the video pictures encoded according to the motion compensated prediction interframe encoding mode; and</claim-text>
<claim-text>a setting unit configured to set a sum of the reference pictures assigned to the prediction groups to a constant value,</claim-text>
<claim-text>wherein the encoder encodes reference picture number information indicating the number of reference pictures assigned to each of the prediction groups and outputs the side information including the coded reference picture number information.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00004" num="00004">
<claim-text>4. A video encoding apparatus according to <claim-ref idref="CLM-00003">claim 3</claim-ref>, wherein the encoding unit includes an encoder which encodes the video picture by each of an intraframe encoding mode, a forward prediction interframe encoding mode and a bi-directional prediction interframe encoding mode, and the assigning unit assigns first video pictures encoded by the intraframe encoding mode and the forward prediction interframe encoding mode and the reference pictures corresponding to the first video pictures to the group of the first hierarchical layer, and assigns second video pictures encoded by the bi-directional prediction interframe encoding mode and the reference pictures corresponding to the second video pictures to at least one of the group of first hierarchical layer and the group of the second.</claim-text>
</claim>
</claims>
</us-patent-grant>
