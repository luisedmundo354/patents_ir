<us-patent-grant lang="EN" dtd-version="v4.2 2006-08-23" file="US07298414-20071120.XML" status="PRODUCTION" id="us-patent-grant" country="US" date-produced="20071106" date-publ="20071120">
<us-bibliographic-data-grant>
<publication-reference>
<document-id>
<country>US</country>
<doc-number>07298414</doc-number>
<kind>B2</kind>
<date>20071120</date>
</document-id>
</publication-reference>
<application-reference appl-type="utility">
<document-id>
<country>US</country>
<doc-number>10353675</doc-number>
<date>20030129</date>
</document-id>
</application-reference>
<us-application-series-code>10</us-application-series-code>
<us-term-of-grant>
<us-term-extension>864</us-term-extension>
</us-term-of-grant>
<classifications-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>G</section>
<class>03</class>
<subclass>B</subclass>
<main-group>13</main-group>
<subgroup>00</subgroup>
<symbol-position>F</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20071120</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
</classifications-ipcr>
<classification-national>
<country>US</country>
<main-classification>348350</main-classification>
<further-classification>348350</further-classification>
</classification-national>
<invention-title id="d0e53">Digital camera autofocus using eye focus measurement</invention-title>
<references-cited>
<citation>
<patcit num="00001">
<document-id>
<country>US</country>
<doc-number>5155516</doc-number>
<kind>A</kind>
<name>Shindo</name>
<date>19921000</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<patcit num="00002">
<document-id>
<country>US</country>
<doc-number>5327191</doc-number>
<kind>A</kind>
<name>Shindo et al.</name>
<date>19940700</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<patcit num="00003">
<document-id>
<country>US</country>
<doc-number>5331149</doc-number>
<kind>A</kind>
<name>Spitzer et al.</name>
<date>19940700</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<patcit num="00004">
<document-id>
<country>US</country>
<doc-number>5486892</doc-number>
<kind>A</kind>
<name>Suzuki et al.</name>
<date>19960100</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>396 51</main-classification></classification-national>
</citation>
<citation>
<patcit num="00005">
<document-id>
<country>US</country>
<doc-number>5557364</doc-number>
<kind>A</kind>
<name>Shindo et al.</name>
<date>19960900</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<patcit num="00006">
<document-id>
<country>US</country>
<doc-number>5608489</doc-number>
<kind>A</kind>
<name>Ozaki</name>
<date>19970300</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<patcit num="00007">
<document-id>
<country>US</country>
<doc-number>5610681</doc-number>
<kind>A</kind>
<name>Nagano et al.</name>
<date>19970300</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<patcit num="00008">
<document-id>
<country>US</country>
<doc-number>5614985</doc-number>
<kind>A</kind>
<name>Odaka</name>
<date>19970300</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<patcit num="00009">
<document-id>
<country>US</country>
<doc-number>5857120</doc-number>
<kind>A</kind>
<name>Konishi</name>
<date>19990100</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<patcit num="00010">
<document-id>
<country>US</country>
<doc-number>5882301</doc-number>
<kind>A</kind>
<name>Yoshida</name>
<date>19990300</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<patcit num="00011">
<document-id>
<country>US</country>
<doc-number>RE36237</doc-number>
<kind>E</kind>
<name>Shindo</name>
<date>19990600</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<patcit num="00012">
<document-id>
<country>US</country>
<doc-number>6014524</doc-number>
<kind>A</kind>
<name>Suzuki et al.</name>
<date>20000100</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<patcit num="00013">
<document-id>
<country>US</country>
<doc-number>6027216</doc-number>
<kind>A</kind>
<name>Guyton et al.</name>
<date>20000200</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<patcit num="00014">
<document-id>
<country>US</country>
<doc-number>6072443</doc-number>
<kind>A</kind>
<name>Nasserbakht et al.</name>
<date>20000600</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<patcit num="00015">
<document-id>
<country>US</country>
<doc-number>6394602</doc-number>
<kind>B1</kind>
<name>Morrison et al.</name>
<date>20020500</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<patcit num="00016">
<document-id>
<country>US</country>
<doc-number>6522360</doc-number>
<kind>B1</kind>
<name>Miyawaki et al.</name>
<date>20030200</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>348347</main-classification></classification-national>
</citation>
<citation>
<patcit num="00017">
<document-id>
<country>JP</country>
<doc-number>63-40112</doc-number>
<kind>A</kind>
<date>19880200</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<patcit num="00018">
<document-id>
<country>JP</country>
<doc-number>63-40525</doc-number>
<kind>A</kind>
<date>19880200</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<patcit num="00019">
<document-id>
<country>JP</country>
<doc-number>63-40625</doc-number>
<kind>A</kind>
<date>19880200</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
</references-cited>
<number-of-claims>9</number-of-claims>
<us-exemplary-claim>1</us-exemplary-claim>
<us-field-of-classification-search>
<classification-national>
<country>US</country>
<main-classification>None</main-classification>
</classification-national>
</us-field-of-classification-search>
<figures>
<number-of-drawing-sheets>3</number-of-drawing-sheets>
<number-of-figures>5</number-of-figures>
</figures>
<us-related-documents>
<related-publication>
<document-id>
<country>US</country>
<doc-number>20040165099</doc-number>
<kind>A1</kind>
<date>20040826</date>
</document-id>
</related-publication>
</us-related-documents>
<parties>
<applicants>
<applicant sequence="001" app-type="applicant-inventor" designation="us-only">
<addressbook>
<last-name>Stavely</last-name>
<first-name>Donald J.</first-name>
<address>
<city>Windsor</city>
<state>CO</state>
<country>US</country>
</address>
</addressbook>
<nationality>
<country>omitted</country>
</nationality>
<residence>
<country>US</country>
</residence>
</applicant>
<applicant sequence="002" app-type="applicant-inventor" designation="us-only">
<addressbook>
<last-name>Campbell</last-name>
<first-name>David K.</first-name>
<address>
<city>Loveland</city>
<state>CA</state>
<country>US</country>
</address>
</addressbook>
<nationality>
<country>omitted</country>
</nationality>
<residence>
<country>US</country>
</residence>
</applicant>
<applicant sequence="003" app-type="applicant-inventor" designation="us-only">
<addressbook>
<last-name>Aas</last-name>
<first-name>Eric F.</first-name>
<address>
<city>Windsor</city>
<state>CO</state>
<country>US</country>
</address>
</addressbook>
<nationality>
<country>omitted</country>
</nationality>
<residence>
<country>US</country>
</residence>
</applicant>
<applicant sequence="004" app-type="applicant-inventor" designation="us-only">
<addressbook>
<last-name>Hofer</last-name>
<first-name>Gregory V.</first-name>
<address>
<city>Loveland</city>
<state>OR</state>
<country>US</country>
</address>
</addressbook>
<nationality>
<country>omitted</country>
</nationality>
<residence>
<country>US</country>
</residence>
</applicant>
<applicant sequence="005" app-type="applicant-inventor" designation="us-only">
<addressbook>
<last-name>Woods</last-name>
<first-name>Scott A.</first-name>
<address>
<city>Bellvue</city>
<state>CO</state>
<country>US</country>
</address>
</addressbook>
<nationality>
<country>omitted</country>
</nationality>
<residence>
<country>US</country>
</residence>
</applicant>
<applicant sequence="006" app-type="applicant-inventor" designation="us-only">
<addressbook>
<last-name>Dalton</last-name>
<first-name>Dan L.</first-name>
<address>
<city>Greeley</city>
<state>CO</state>
<country>US</country>
</address>
</addressbook>
<nationality>
<country>omitted</country>
</nationality>
<residence>
<country>US</country>
</residence>
</applicant>
<applicant sequence="007" app-type="applicant-inventor" designation="us-only">
<addressbook>
<last-name>Spears</last-name>
<first-name>Kurt E.</first-name>
<address>
<city>Fort Collins</city>
<state>CO</state>
<country>US</country>
</address>
</addressbook>
<nationality>
<country>omitted</country>
</nationality>
<residence>
<country>US</country>
</residence>
</applicant>
</applicants>
</parties>
<assignees>
<assignee>
<addressbook>
<orgname>Hewlett-Packard Development Company, L.P.</orgname>
<role>02</role>
<address>
<city>Houston</city>
<state>TX</state>
<country>US</country>
</address>
</addressbook>
</assignee>
</assignees>
<examiners>
<primary-examiner>
<last-name>Srivastava</last-name>
<first-name>Vivek</first-name>
<department>2622</department>
</primary-examiner>
<assistant-examiner>
<last-name>Daniels</last-name>
<first-name>Anthony J.</first-name>
</assistant-examiner>
</examiners>
</us-bibliographic-data-grant>
<abstract id="abstract">
<p id="p-0001" num="0000">Viewfinder apparatus, methods, and digital cameras that provide autofocus using retroreflected eye focus measurements. When a user looks at a part of a scene that is the intended subject of the image, his or her eye is correctly focused. The present invention measures the focus distance of the eye when the eye is focused on the desired location in the scene, and then uses the measured distance to set the focus of the camera.</p>
</abstract>
<drawings id="DRAWINGS">
<figure id="Fig-EMI-D00000" num="00000">
<img id="EMI-D00000" he="93.56mm" wi="151.13mm" file="US07298414-20071120-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00001" num="00001">
<img id="EMI-D00001" he="230.80mm" wi="151.64mm" file="US07298414-20071120-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00002" num="00002">
<img id="EMI-D00002" he="235.97mm" wi="153.08mm" file="US07298414-20071120-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00003" num="00003">
<img id="EMI-D00003" he="154.69mm" wi="145.46mm" file="US07298414-20071120-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
</drawings>
<description id="description">
<?BRFSUM description="Brief Summary" end="lead"?>
<heading id="h-0001" level="1">TECHNICAL FIELD</heading>
<p id="p-0002" num="0001">The present invention relates generally to digital cameras, and more specifically, to the use of retroreflected eye focus measurements to autofocus a digital camera.</p>
<heading id="h-0002" level="1">BACKGROUND</heading>
<p id="p-0003" num="0002">Autofocus has become a standard feature on both film and digital cameras. Despite years of innovation and development, autofocus still has several problems that often result in out-of-focus images.</p>
<p id="p-0004" num="0003">The biggest issue is user intent. In a scene with objects at multiple distances, the camera must choose which part of the image the user would like to be in focus. The conventional solution is to focus on center region of the image, assuming the subject is in the center. This technique fails in the classic case of two people standing side-by-side, with the camera focusing on the background between them.</p>
<p id="p-0005" num="0004">In addition, conventional autofocus techniques on digital cameras are also slow, power-hungry, and inaccurate in low contrast or low light conditions.</p>
<p id="p-0006" num="0005">One sophisticated approach to the problem of user intent for autofocus is the use of eye tracking. As a user looks into the viewfinder, an IR emitter and detector array track where the user is looking while framing the shot. The camera then uses an autofocus zone or spot at that position in the scene. An autofocus algorithm then proceeds conventionally, using the image as seen through the camera lens. However, this solution still suffers from the speed, power, and accuracy issues of conventional autofocus.</p>
<p id="p-0007" num="0006">Prior art relating to the present invention is as follows.</p>
<p id="p-0008" num="0007">U.S. Pat. No. 6,072,443 discloses an “ocular projection display (<b>12</b>) projects an image directly to the eye (<b>26</b>) of the user (<b>10</b>). The focus of the image may be varied to allow for different user profiles or to relieve the stress of maintaining a fixed focus over a prolonged period of time. Optionally, the ocular projection display (<b>12</b>) can include a location and distance sensor (<b>46</b>) for identifying the location of the user's eyes for proper aiming of the image to the eyes of the user and focus detection circuitry (<b>54</b>) to correct for the user's focusing abilities.”</p>
<p id="p-0009" num="0008">U.S. Pat. No. 5,857,120 discloses an “eye axis detector for detecting to which position the eye axis of a user is directed in a picture, comprising means for dividing and specifying the picture into a plurality of pictures, and means for extracting the watching point of the user using a plurality of types of eye axis information concerning the eye axis in each area within such divided picture.”</p>
<p id="p-0010" num="0009">U.S. Pat. No. 5,155,516 discloses a “view finder optical system including a window of an eye piece portion comprising an eye cup having a contact surface, a shape of the contact surface being made so as to incline an axis of the lens with respect to an axis of the eye piece. The shape of the contact surface is curved corresponding to the lens's shape. The contact surface is made so as to conform to a lens of glasses.”</p>
<p id="p-0011" num="0010">U.S. Pat. No. 5,608,489 discloses an “anamorphic lens system in an eye direction detection device, applied to an observing optical system, modifies illumination light from a light source to be parallel in a lateral direction of a user's eye positioned to look through the observing optical system, dispersed in a vertical direction of the position of the user's eye, and substantially centered on the position of the user's eye. The anamorphic lens system transmits the illumination light from the light source to an illumination light exit surface positioned away from the optical axis of the observing optical system, and the illumination light is projected toward the position of the user's eye from the illumination light exit surface to separate signals from the image of the eye and an image formed by eyeglasses, if worn. The illumination light exit surface is a rectangular window having a longitudinal side parallel to longitudinal side of a field of view of the observing optical system, and is positioned below the position of a user's eye.”</p>
<p id="p-0012" num="0011">U.S. Reissue Pat. No. RE36,237 (which corresponds to U.S. Pat. No. 5,155,516) discloses a “view finder optical system including a window of an eye piece portion comprising an eye cup having a contact surface, a shape of the contact surface being made so as to incline an axis of the lens with respect to an axis of the eye piece. The shape of the contact surface is curved corresponding to the lens's shape. The contact surface is made so as to conform to a lens of glasses.”</p>
<p id="p-0013" num="0012">U.S. Pat. Nos. 5,327,191 and 5,557,364 disclose an “eye direction detecting apparatus for a camera having a light transferring system for guiding a beam of parallel light rays to an eye of a photographer includes a light receiving system having a light receiving portion on which a first Purkinje image based on specular reflection of a cornea of the eye and reflecting light from a retina of the eye is formed, the light receiving portion generating a light receiving output. The apparatus further includes a processing circuit for detecting the eye direction of the eye based on the light receiving output of the light receiving portion. Further, according to the teachings of the present invention, including an optical member having certain identically inclined surfaces prevents refracted light from forming a ghost image within the light receiving system of an eye direction detecting apparatus.”</p>
<p id="p-0014" num="0013">U.S. Pat. No. 5,331,149 discloses an “eye tracking system is disclosed which is comprised of an eye tracking module formed of a display joined to a photodetector array. Each pixel in the display is aligned with a corresponding photodetector. An image generated by the display is projected onto a viewing screen or toward a viewer. Axial light rays from the display pixels are reflected by the eye and detected by a respective photodetector which generates an electrical signal indicative of eye position.”</p>
<p id="p-0015" num="0014">U.S. Pat. No. 5,614,985 discloses a “camera having sight line detection means for detecting the line of sight of an observer, focus detection means having plural focus detection areas and capable of focus detection in each focus detecting area, and selection means for selecting at least one of the plural focus detecting areas, based on the information of the line of sight detected by the sight line detection means, wherein the selection means is adapted to select the focus detecting area based on the result of focus detection by the focus detection means, according to the state of detection of the line of sight by the sight line detection means.”</p>
<p id="p-0016" num="0015">U.S. Pat. No. 5,882,301 discloses that “Relative directions of excitation and photoreceiving optical systems are so set that an angle formed by optical axes thereof in the air is 14°, and an eyeball is fixed in such a direction that its ocular axis divides the angle formed by the optical axes into two equal parts. On a light incidence side of a one-dimensional solid-state image pickup device of the photoreceiving optical system, a slit is arranged for inputting measuring light components generated from portions of the eyeball having different depth positions on an excitation light beam in photoelectric conversion elements of different positions of the image pickup device. The measuring light components generated from the respective portions of the eyeball parallel to the optical axis are incident upon the one-dimensional solid-state image pickup device through the slit and simultaneously detected, so that the positions of the photoelectric conversion elements and measuring light component generating positions at the eyeball correspond to each other.”</p>
<p id="p-0017" num="0016">U.S. Pat. No. 5,610,681 discloses an “apparatus having an irradiation device for irradiating the eye of an observer; a sensor having a number of pixels with a set pitch; an image forming optical unit for imaging light reflected by the eye onto the sensor; and an electronic circuit for making a signal denoting the direction of the line of sight of the eye in accordance with an output from the sensor, wherein the relationship expressed by Pitch X/β&lt;0.41 mm is satisfied when an assumption is made that the image forming magnification of the image forming unit is β and the pitch of the pixels of the sensor is Pitch X so that accuracy in detecting the line of sight of the eye is improved.”</p>
<p id="p-0018" num="0017">U.S. Pat. No. 6,014,524 discloses a “camera comprising: finder means for inspecting an object, illumination means for illuminating an operator's eye by which the operator looks in at the finder means, a condensing optical system for condensing a reflected light from the operator's eye, photoelectric converter means for receiving the condensed reflected light, calculation means for calculating a direction of a visual axis of the operator's eye on the basis of an output of the photoelectric converter means, and condition setting means operable in response to the result of the calculation of the calculation means, for setting taking conditions of the camera”</p>
<p id="p-0019" num="0018">U.S. Pat. No. 6,394,602 discloses an “optical instrument, such as a microscope or a camera, is provided for forming a viewable image of an object. The optical instrument comprises an objective lens for forming the viewable image at an image plane, an eye sensor for sensing the direction of gaze of a user viewing the viewable image and means for controlling a controllable function of the optical instrument in dependence upon the sensed direction of gaze. The eye sensor comprises a sensor lens for focusing light reflected from the retina of the user, an imaging transducer located at a plane which is conjugate to the image plane with respect to the sensor lens, for generating an electrical image sign of a portion of the user's retina, a memory for storing retinal image information of the user and circuitry for comparing the retinal image signal generated by the imaging transducer with the stored retinal image information to generate gaze information indicative of the direction of gaze of the user”</p>
<p id="p-0020" num="0019">U.S. Pat. No. 6,027,216 discloses “Apparatus and method are provided for assessing the direction of fixation of an eye by detecting polarization-related changes in light retroreflected from the fundus of the eye. Nerve fibers in the retina of the eye are birefringent and alter the polarization state of light traversing them as a function of their orientation. The nerve fibers are arrayed in a characteristic pattern in the retina, specifically radiating outward from the fovea and converging to the optic nerve head. By assessment of polarization-related changes in retroreflected light from multiple retinal areas either sequentially or simultaneously, characteristic birefringence signatures of portions of the retina can be identified which are used to assess the direction of fixation of the eye. In addition, interference from the corneal birefringence is reduced by using incident light having a polarization state that is substantially independent of meridional direction. Circularly polarized light or non-polarized light is used for the assessment. Interference from the corneal birefringence is reduced still further by detecting polarization-related changes that are substantially independent of the meridional direction of the corneal birefringence. This is accomplished by detecting changes in ellipticity by measuring solely the Stokes parameter S<sub>3 </sub>or by measuring any two Stokes parameters. An alternative is measuring the overall intensity of the retroreflected light when the dichroism of the lutein pigment particles in the vicinity of the fovea is used for the assessment.”</p>
<p id="p-0021" num="0020">However, none of the prior art references discloses or suggests using retroreflected eye focus measurements to measure the focus distance of the eye, and use this distance to set the focus of the camera.</p>
<p id="p-0022" num="0021">It is an objective of the present invention to provide for a digital camera, viewfinder apparatus and methods that use retroreflected eye focus measurements to provide autofocus, and that improves over conventional implementations.</p>
<heading id="h-0003" level="1">SUMMARY OF THE INVENTION</heading>
<p id="p-0023" num="0022">To accomplish the above and other objectives, the present invention provides for viewfinder apparatus, digital cameras and methods that provide autofocus using retroreflected eye focus measurements. When a user looks at a part of a scene that is the intended subject of the image, the eye is correctly focused on that point. Rather than tracking the position of the eye, the present invention measures the focus distance of the eye, and then uses this distance to set the focus of the camera. The camera does not detect the focus of the scene to determine focus.</p>
<p id="p-0024" num="0023">Like the eye-tracking technique described in the Background section, the problem of user intent is solved. In addition, the present invention takes advantage of the superior autofocus capability of the human eye. The present invention is faster, more accurate, and more robust in low contrast and low light conditions than conventional electronic autofocus systems.</p>
<p id="p-0025" num="0024">In addition, the present invention can be used advantageously for video recording. Other techniques which use the camera's lens and sensor for focusing require the lens to “hunt around” the point of best focus, resulting in video images which are periodically out of focus. The present invention avoids this problem by “hunting around” to find the focus distance of the user's eye, without disrupting the focus of the camera lens.</p>
<?BRFSUM description="Brief Summary" end="tail"?>
<?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?>
<description-of-drawings>
<heading id="h-0004" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading>
<p id="p-0026" num="0025">The various features and advantages of embodiments of the present invention may be more readily understood with reference to the following detailed description taken in conjunction with the accompanying drawings, wherein like reference numerals designate like structural elements, and in which:</p>
<p id="p-0027" num="0026"><figref idref="DRAWINGS">FIG. 1</figref> illustrates a first exemplary embodiment of digital camera autofocus apparatus in accordance with the principles of the present invention;</p>
<p id="p-0028" num="0027"><figref idref="DRAWINGS">FIG. 2</figref> illustrates a second exemplary embodiment of digital camera autofocus apparatus in accordance with the principles of the present invention;</p>
<p id="p-0029" num="0028"><figref idref="DRAWINGS">FIG. 3</figref> illustrates a third exemplary embodiment of digital camera autofocus apparatus in accordance with the principles of the present invention;</p>
<p id="p-0030" num="0029"><figref idref="DRAWINGS">FIG. 4</figref> illustrates a fourth exemplary embodiment of digital camera autofocus apparatus in accordance with the principles of the present invention; and</p>
<p id="p-0031" num="0030"><figref idref="DRAWINGS">FIG. 5</figref> illustrates an exemplary method in accordance with the principles of the present invention.</p>
</description-of-drawings>
<?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?>
<?DETDESC description="Detailed Description" end="lead"?>
<heading id="h-0005" level="1">DETAILED DESCRIPTION</heading>
<p id="p-0032" num="0031">Referring to the sole drawing figures, <figref idref="DRAWINGS">FIG. 1</figref> illustrates a first exemplary embodiment of autofocus apparatus <b>10</b> (digital camera autofocus apparatus <b>10</b>) in accordance with the principles of the present invention for use with a digital camera <b>20</b>. The first exemplary embodiment of the digital camera autofocus apparatus <b>10</b> focuses a retroreflected image of a scene <b>15</b>.</p>
<p id="p-0033" num="0032">The first exemplary embodiment of the digital camera autofocus apparatus <b>10</b> comprises a viewfinder <b>11</b> through which a user looks using his or her eye <b>16</b>. The viewfinder <b>11</b> comprises a selectively movable area array image sensor <b>12</b>, a selectively movable optical element <b>13</b> or lens <b>13</b>, and a beamsplitter <b>14</b> or angled partially reflective, partially transmissive mirror <b>14</b> disposed along an optical axis. A user views an image scene or target <b>15</b> through the beamsplitter <b>14</b> or angled partially reflective, partially transmissive mirror <b>14</b>.</p>
<p id="p-0034" num="0033">When a user looks at a part of a scene that is the intended subject of the image, the user's eye <b>16</b> is correctly focused on that point. Rather than tracking the position of the eye <b>16</b>, the present autofocus apparatus <b>10</b> measures the focus distance of the eye <b>16</b>, and uses this distance to set the focus of the camera <b>20</b>. The camera <b>20</b> does not detect the focus of the scene.</p>
<p id="p-0035" num="0034">Eye-focus sensing elements comprising the sensor <b>12</b>, movable optical element <b>13</b> or lens <b>13</b>, and beamsplitter <b>14</b> or angled partially reflective, partially transmissive mirror <b>14</b> are built into the viewfinder <b>11</b>. It is simplest to imagine a reflex viewfinder <b>11</b> that does not contain any magnifying or focusing elements that would modify the user's view of the scene <b>15</b>. The user views the scene <b>15</b> normally, and his or her eye <b>16</b> naturally focuses on the subject of interest.</p>
<p id="p-0036" num="0035">The first exemplary embodiment of the autofocus apparatus <b>10</b> measures the focus distance of the eye <b>16</b> using a retroreflected image of the scene <b>15</b>. As is shown in <figref idref="DRAWINGS">FIG. 1</figref>, the lens <b>13</b> and/or the image sensor <b>12</b> are moved until the image of the scene <b>15</b> projected on the retina <b>16</b><i>a </i>of the eye <b>16</b> is in focus on the sensor <b>12</b>. The displacement of the lens <b>13</b> and/or sensor <b>12</b> provides a measure of the focus distance of the eye <b>16</b>. The camera <b>20</b> is focused at a distance that is related to the displacement of the lens <b>13</b> and/or sensor <b>12</b> when the image on the retina <b>16</b><i>a </i>is in focus.</p>
<p id="p-0037" num="0036">Setting the focus distance of the camera lens from the autofocus data derived from the measured focus distance of the eye may be done by several ways. One exemplary method is to use a pre-stored table, which maps displacements of the movable elements in the viewfinder apparatus described above to the appropriate displacements of the camera lens. This table may be generated as part of a factory calibration procedure, thus compensating for manufacturing variations in both the viewfinder and camera elements. Alternatively, the displacement of the camera lens can be derived from the displacement of the viewfinder elements by processing <b>35</b> via a mathematical formula.</p>
<p id="p-0038" num="0037"><figref idref="DRAWINGS">FIG. 2</figref> illustrates a second exemplary embodiment of digital camera autofocus apparatus <b>10</b><i>a </i>in accordance with the principles of the present invention. The second exemplary embodiment of the autofocus apparatus <b>10</b><i>a </i>comprises a selectively movable area array image sensor <b>12</b>, a selectively movable optical element <b>13</b> or lens <b>13</b>, a first beamsplitter <b>14</b> or angled partially reflective, partially transmissive mirror <b>14</b>, and a second beamsplitter <b>14</b><i>a </i>or angled partially reflective, partially transmissive mirror <b>14</b><i>a, </i>each of which are disposed along a first optical axis. An infrared light emitting diode illuminator <b>17</b> is disposed along a second optical axis that intersects the second beamsplitter <b>14</b><i>a </i>or angled partially reflective, partially transmissive mirror.</p>
<p id="p-0039" num="0038">The second exemplary embodiment of digital camera autofocus apparatus <b>10</b><i>a </i>works in low-light situations and projects a near-infrared image of an image <b>15</b> or target <b>15</b> into the eye <b>16</b> through the second beamsplitter <b>14</b><i>a </i>as is shown in <figref idref="DRAWINGS">FIG. 2</figref>. The retroreflected image of the target <b>15</b> is sensed by the infrared image sensor <b>12</b>. As discussed with reference to <figref idref="DRAWINGS">FIG. 1</figref>, the lens <b>13</b> and/or sensor <b>12</b> are moved until the image of the target <b>15</b> projected on the retina <b>16</b><i>a </i>is in focus, and the displacement of the lens <b>13</b> and/or sensor <b>12</b> is a measure of the focus distance of the eye <b>16</b>.</p>
<p id="p-0040" num="0039"><figref idref="DRAWINGS">FIG. 3</figref> illustrates a third exemplary embodiment of digital camera autofocus apparatus <b>10</b><i>b </i>in accordance with the principles of the present invention. The third exemplary embodiment of the autofocus apparatus <b>10</b><i>b </i>comprises a selectively movable area array image sensor <b>12</b>, a selectively movable optical element <b>13</b> or lens <b>13</b>, a first beamsplitter <b>14</b> or angled partially reflective, partially transmissive mirror <b>14</b>, a second beamsplitter <b>14</b><i>a </i>or angled partially reflective, partially transmissive mirror <b>14</b><i>a</i>, each of which are disposed along a first optical axis.</p>
<p id="p-0041" num="0040">An infrared light emitting diode illuminator <b>17</b> is disposed along a second optical axis that intersects the second beamsplitter <b>14</b><i>a </i>or angled partially reflective, partially transmissive mirror. An astigmatic element <b>18</b> is interposed along the second optical axis between the infrared light emitting diode illuminator <b>17</b> and the second beamsplitter <b>14</b><i>a </i>or angled partially reflective, partially transmissive mirror <b>14</b><i>a. </i></p>
<p id="p-0042" num="0041">The third exemplary autofocus apparatus <b>10</b><i>b </i>is a variation that borrows from focus tracking techniques used on optical discs. The astigmatic element <b>18</b> is introduced in the infrared optical path as is shown in <figref idref="DRAWINGS">FIG. 3</figref>. As a result, the shape of a retroreflected spot or target <b>15</b> depends on the focus distance of the eye <b>16</b>. This is used to generate a bipolar focus signal that is produced by the sensor <b>12</b>. Bipolar focus signals have the advantage that the position of best focus is known (when balanced) and so tracking focus in real-time is easier and more robust.</p>
<p id="p-0043" num="0042"><figref idref="DRAWINGS">FIG. 4</figref> illustrates a fourth and perhaps best exemplary embodiment of digital camera autofocus apparatus <b>10</b><i>c </i>in accordance with the principles of the present invention. The fourth exemplary embodiment of the autofocus apparatus <b>10</b><i>c </i>comprises a tilted area array image sensor <b>12</b>, an optical element <b>13</b> or lens <b>13</b>, a first beamsplitter <b>14</b> or angled partially reflective, partially transmissive mirror <b>14</b>, a second beamsplitter <b>14</b><i>a </i>or angled partially reflective, partially transmissive mirror <b>14</b><i>a</i>, each of which are disposed along a first optical axis. An infrared light emitting diode illuminator <b>17</b> is disposed along a second optical axis that intersects the second beamsplitter <b>14</b><i>a </i>or angled partially reflective, partially transmissive mirror <b>14</b><i>a. </i></p>
<p id="p-0044" num="0043">The fourth exemplary autofocus apparatus <b>10</b><i>c </i>produces a retroreflected pattern that contains a series of targets <b>15</b> at various focus depths. One way this may be accomplished is by inclining a target reticule <b>19</b> disposed between the illuminator <b>17</b> and the second beamsplitter <b>14</b><i>a </i>or angled partially reflective, partially transmissive mirror <b>14</b><i>a</i>, and inclining the sensor <b>12</b> to match the change in conjugate distance as is shown in <figref idref="DRAWINGS">FIG. 4</figref>. The retroreflected image has one area of the inclined target <b>15</b> in best focus. This generates near instantaneous focus measurement data and eliminates the need for moving the lens <b>13</b> or sensor <b>12</b>.</p>
<p id="p-0045" num="0044">Again it is assumed that the reflex viewfinder <b>11</b> that does not contain any optical elements that would modify the user's focus on the scene. It is possible to use any of these above-described techniques with convention optical viewfinders <b>11</b>, including zooming viewfinders <b>11</b>. Care must be taken to compensate for changes in focus distance introduced by the optical power on the viewfinder <b>11</b>.</p>
<p id="p-0046" num="0045"><figref idref="DRAWINGS">FIG. 5</figref> illustrates an exemplary method <b>30</b> in accordance with the principles of the present invention. The exemplary method <b>30</b> comprises the following steps.</p>
<p id="p-0047" num="0046">A user of a digital camera <b>20</b> views <b>31</b> an image scene <b>15</b> through a viewfinder <b>11</b>. The image scene <b>15</b> is retroreflected <b>32</b> from the user's eye <b>16</b>. The retroreflected image scene is focused <b>33</b> onto an image sensor <b>12</b>. Data relating to the focused retroreflected image is processed <b>34</b> to determine the focus distance of the eye <b>16</b>. Data associated with the focus distance of the eye <b>16</b> is processed <b>35</b> to focus the digital camera <b>20</b> on the image scene <b>15</b>.</p>
<p id="p-0048" num="0047">Additional detailed methods address operation of the various components of the above-described apparatus relating to movement of lens <b>13</b> or sensor <b>12</b>. These methods should be readily apparent from reading the first part of this Detailed Description and will not be discussed in detail.</p>
<p id="p-0049" num="0048">Thus, digital cameras and viewfinder apparatus that use retroreflected eye focus measurements to provide autofocus have been disclosed. It is to be understood that the above-described embodiments are merely illustrative of some of the many specific embodiments that represent applications of the principles of the present invention. Clearly, numerous and other arrangements can be readily devised by those skilled in the art without departing from the scope of the invention.</p>
<?DETDESC description="Detailed Description" end="tail"?>
</description>
<us-claim-statement>What is claimed is:</us-claim-statement>
<claims id="claims">
<claim id="CLM-00001" num="00001">
<claim-text>1. A digital camera autofocus apparatus comprising:
<claim-text>a viewfinder through which a user looks with an eye that produces a focused image from light that is retroreflected from the eye, and associated data that are indicative of the focus distance of the eye, which data is used to focus the digital camera;</claim-text>
<claim-text>the viewfinder comprising an area array image sensor, an optical element, and a beamsplitter disposed along an optical axis, at least one of which is selectively movable with respect to the others; and</claim-text>
<claim-text>wherein data corresponding to the relative distance between the optical element and the sensor when the image on the retina is in focus are indicative of the focus distance of the eye, which data are used to focus the camera.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00002" num="00002">
<claim-text>2. The apparatus recited in <claim-ref idref="CLM-00001">claim 1</claim-ref> wherein the viewfinder comprises an area array image sensor, an optical element, a first beamsplitter, and a second beamsplitter disposed along a first optical axis, and an infrared light emitting diode illuminator disposed along a second optical axis that intersects the second beamsplitter at least one of which is selectively movable with respect to the others.</claim-text>
</claim>
<claim id="CLM-00003" num="00003">
<claim-text>3. The apparatus recited in <claim-ref idref="CLM-00002">claim 2</claim-ref> wherein data corresponding to the relative distance between the optical element and the sensor when the image on the retina is in focus are indicative of the focus distance of the eye, which data are used to focus the camera.</claim-text>
</claim>
<claim id="CLM-00004" num="00004">
<claim-text>4. The apparatus recited in <claim-ref idref="CLM-00001">claim 1</claim-ref> wherein the viewfinder comprises an area array image sensor, an optical element, a first beamsplitter, and a second beamsplitter disposed along a first optical axis, an infrared light emitting diode illuminator disposed along a second optical axis tat intersects the second beamsplitter, and an astigmatic element interposed along the second optical axis between the infrared light emitting diode illuminator and the second beamsplitter.</claim-text>
</claim>
<claim id="CLM-00005" num="00005">
<claim-text>5. The apparatus recited in <claim-ref idref="CLM-00004">claim 4</claim-ref> wherein data corresponding to the relative distance between the optical element and the sensor when the image on the retina is in focus are indicative of the focus distance of the eye, which data are used to focus the camera.</claim-text>
</claim>
<claim id="CLM-00006" num="00006">
<claim-text>6. The apparatus recited in <claim-ref idref="CLM-00001">claim 1</claim-ref> wherein the viewfinder comprises a tilted area array image sensor, an optical element, a first beamsplitter, and a second beamsplitter disposed along a first optical axis, and an infrared light emitting diode illuminator disposed along a second optical axis that intersects the second beamsplitter and a target reticule disposed between the illuminator and the second beamsplitter.</claim-text>
</claim>
<claim id="CLM-00007" num="00007">
<claim-text>7. The apparatus recited in <claim-ref idref="CLM-00006">claim 6</claim-ref> wherein a retroreflected pattern is produced by the sensor that contains a series of inclined targets at various focus depths, and wherein the target that is in best focus has data associated therewith that is used to focus the camera.</claim-text>
</claim>
<claim id="CLM-00008" num="00008">
<claim-text>8. An autofocusing method for use with a digital camera, comprising the steps of:
<claim-text>viewing an image scene through a viewfinder of a digital camera;</claim-text>
<claim-text>retroreflecting the image scene from a user's eye;</claim-text>
<claim-text>focusing the retroreflected image scene onto an image sensor;</claim-text>
<claim-text>processing data relating to the focused image to determine the focus distance of the eye;and</claim-text>
<claim-text>processing data associated with the focus distance of the eye to focus the digital camera on the image scene.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00009" num="00009">
<claim-text>9. The autofocusing method recited in <claim-ref idref="CLM-00008">claim 8</claim-ref> wherein the viewfinder comprises an array image sensor, an optical element, and a beamsplitter disposed along an optical axis, and wherein the method comprises the step of:
<claim-text>selectively moving the image sensor or optical element to focus the retroreflected image scene onto the image sensor.</claim-text>
</claim-text>
</claim>
</claims>
</us-patent-grant>
