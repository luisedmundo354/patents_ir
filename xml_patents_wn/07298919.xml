<us-patent-grant lang="EN" dtd-version="v4.2 2006-08-23" file="US07298919-20071120.XML" status="PRODUCTION" id="us-patent-grant" country="US" date-produced="20071106" date-publ="20071120">
<us-bibliographic-data-grant>
<publication-reference>
<document-id>
<country>US</country>
<doc-number>07298919</doc-number>
<kind>B2</kind>
<date>20071120</date>
</document-id>
</publication-reference>
<application-reference appl-type="utility">
<document-id>
<country>US</country>
<doc-number>10860145</doc-number>
<date>20040604</date>
</document-id>
</application-reference>
<us-application-series-code>10</us-application-series-code>
<priority-claims>
<priority-claim sequence="01" kind="national">
<country>JP</country>
<doc-number>9-245522</doc-number>
<date>19970910</date>
</priority-claim>
</priority-claims>
<classifications-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>G</section>
<class>06</class>
<subclass>K</subclass>
<main-group>9</main-group>
<subgroup>36</subgroup>
<symbol-position>F</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20071120</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
</classifications-ipcr>
<classification-national>
<country>US</country>
<main-classification>382284</main-classification>
<further-classification>382151</further-classification>
<further-classification>382184</further-classification>
<further-classification>382294</further-classification>
<further-classification>348 94</further-classification>
<further-classification>348 95</further-classification>
<further-classification>358540</further-classification>
<further-classification>358450</further-classification>
</classification-national>
<invention-title id="d0e61">System and method for displaying an image indicating a positional relation between partially overlapping images</invention-title>
<references-cited>
<citation>
<patcit num="00001">
<document-id>
<country>US</country>
<doc-number>4541010</doc-number>
<kind>A</kind>
<name>Alston</name>
<date>19850900</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<patcit num="00002">
<document-id>
<country>US</country>
<doc-number>5133020</doc-number>
<kind>A</kind>
<name>Giger et al.</name>
<date>19920700</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<patcit num="00003">
<document-id>
<country>US</country>
<doc-number>5140647</doc-number>
<kind>A</kind>
<name>Ise et al.</name>
<date>19920800</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<patcit num="00004">
<document-id>
<country>US</country>
<doc-number>5499146</doc-number>
<kind>A</kind>
<name>Donahue et al.</name>
<date>19960300</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>386117</main-classification></classification-national>
</citation>
<citation>
<patcit num="00005">
<document-id>
<country>US</country>
<doc-number>5659823</doc-number>
<kind>A</kind>
<name>Mukai et al.</name>
<date>19970800</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<patcit num="00006">
<document-id>
<country>US</country>
<doc-number>5682197</doc-number>
<kind>A</kind>
<name>Moghadam et al.</name>
<date>19971000</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>348 36</main-classification></classification-national>
</citation>
<citation>
<patcit num="00007">
<document-id>
<country>US</country>
<doc-number>5682198</doc-number>
<kind>A</kind>
<name>Katyayama et al.</name>
<date>19971000</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<patcit num="00008">
<document-id>
<country>US</country>
<doc-number>5721624</doc-number>
<kind>A</kind>
<name>Kumashiro et al.</name>
<date>19980200</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<patcit num="00009">
<document-id>
<country>US</country>
<doc-number>5731870</doc-number>
<kind>A</kind>
<name>Bartko et al.</name>
<date>19980300</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>35613909</main-classification></classification-national>
</citation>
<citation>
<patcit num="00010">
<document-id>
<country>US</country>
<doc-number>5742329</doc-number>
<kind>A</kind>
<name>Masunaga et al.</name>
<date>19980400</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>348 1407</main-classification></classification-national>
</citation>
<citation>
<patcit num="00011">
<document-id>
<country>US</country>
<doc-number>5764236</doc-number>
<kind>A</kind>
<name>Tanaka et al.</name>
<date>19980600</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>345606</main-classification></classification-national>
</citation>
<citation>
<patcit num="00012">
<document-id>
<country>US</country>
<doc-number>5774754</doc-number>
<kind>A</kind>
<name>Ootsuka</name>
<date>19980600</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>396380</main-classification></classification-national>
</citation>
<citation>
<patcit num="00013">
<document-id>
<country>US</country>
<doc-number>5825044</doc-number>
<kind>A</kind>
<name>Allen et al.</name>
<date>19981000</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>250557</main-classification></classification-national>
</citation>
<citation>
<patcit num="00014">
<document-id>
<country>US</country>
<doc-number>5845166</doc-number>
<kind>A</kind>
<name>Fellegara et al.</name>
<date>19981200</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<patcit num="00015">
<document-id>
<country>US</country>
<doc-number>5930411</doc-number>
<kind>A</kind>
<name>Kojima et al.</name>
<date>19990700</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>382312</main-classification></classification-national>
</citation>
<citation>
<patcit num="00016">
<document-id>
<country>US</country>
<doc-number>5940640</doc-number>
<kind>A</kind>
<name>Karasawa</name>
<date>19990800</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>396301</main-classification></classification-national>
</citation>
<citation>
<patcit num="00017">
<document-id>
<country>US</country>
<doc-number>5940641</doc-number>
<kind>A</kind>
<name>McIntyre et al.</name>
<date>19990800</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<patcit num="00018">
<document-id>
<country>US</country>
<doc-number>5963664</doc-number>
<kind>A</kind>
<name>Kumar et al.</name>
<date>19991000</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<patcit num="00019">
<document-id>
<country>US</country>
<doc-number>5978016</doc-number>
<kind>A</kind>
<name>Lourette et al.</name>
<date>19991100</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<patcit num="00020">
<document-id>
<country>US</country>
<doc-number>5982941</doc-number>
<kind>A</kind>
<name>Loveridge et al.</name>
<date>19991100</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<patcit num="00021">
<document-id>
<country>US</country>
<doc-number>5982951</doc-number>
<kind>A</kind>
<name>Katayama et al.</name>
<date>19991100</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<patcit num="00022">
<document-id>
<country>US</country>
<doc-number>5999211</doc-number>
<kind>A</kind>
<name>Hedges et al.</name>
<date>19991200</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>348144</main-classification></classification-national>
</citation>
<citation>
<patcit num="00023">
<document-id>
<country>US</country>
<doc-number>6074111</doc-number>
<kind>A</kind>
<name>Kasahara</name>
<date>20000600</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<patcit num="00024">
<document-id>
<country>US</country>
<doc-number>6078701</doc-number>
<kind>A</kind>
<name>Hsu et al.</name>
<date>20000600</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>382294</main-classification></classification-national>
</citation>
<citation>
<patcit num="00025">
<document-id>
<country>US</country>
<doc-number>6144403</doc-number>
<kind>A</kind>
<name>Otani</name>
<date>20001100</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>348 1412</main-classification></classification-national>
</citation>
<citation>
<patcit num="00026">
<document-id>
<country>US</country>
<doc-number>6222583</doc-number>
<kind>B1</kind>
<name>Matsumura et al.</name>
<date>20010400</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>348113</main-classification></classification-national>
</citation>
<citation>
<patcit num="00027">
<document-id>
<country>US</country>
<doc-number>6229565</doc-number>
<kind>B1</kind>
<name>Bobry</name>
<date>20010500</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<patcit num="00028">
<document-id>
<country>US</country>
<doc-number>6243103</doc-number>
<kind>B1</kind>
<name>Takiguchi et al.</name>
<date>20010600</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>345634</main-classification></classification-national>
</citation>
<citation>
<patcit num="00029">
<document-id>
<country>US</country>
<doc-number>6249360</doc-number>
<kind>B1</kind>
<name>Pollard et al.</name>
<date>20010600</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>358473</main-classification></classification-national>
</citation>
<citation>
<patcit num="00030">
<document-id>
<country>US</country>
<doc-number>6417936</doc-number>
<kind>B1</kind>
<name>Leberl et al.</name>
<date>20020700</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>358474</main-classification></classification-national>
</citation>
<citation>
<patcit num="00031">
<document-id>
<country>US</country>
<doc-number>6429968</doc-number>
<kind>B1</kind>
<name>Carver</name>
<date>20020800</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>359385</main-classification></classification-national>
</citation>
<citation>
<patcit num="00032">
<document-id>
<country>US</country>
<doc-number>6459819</doc-number>
<kind>B1</kind>
<name>Nakao</name>
<date>20021000</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>382284</main-classification></classification-national>
</citation>
<citation>
<patcit num="00033">
<document-id>
<country>US</country>
<doc-number>6466701</doc-number>
<kind>B1</kind>
<name>Ejiri et al.</name>
<date>20021000</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>382284</main-classification></classification-national>
</citation>
<citation>
<patcit num="00034">
<document-id>
<country>US</country>
<doc-number>6504943</doc-number>
<kind>B1</kind>
<name>Sweatt et al.</name>
<date>20030100</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>382103</main-classification></classification-national>
</citation>
<citation>
<patcit num="00035">
<document-id>
<country>US</country>
<doc-number>6549681</doc-number>
<kind>B1</kind>
<name>Takiguchi et al.</name>
<date>20030400</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>382294</main-classification></classification-national>
</citation>
<citation>
<patcit num="00036">
<document-id>
<country>US</country>
<doc-number>6640002</doc-number>
<kind>B1</kind>
<name>Kawada</name>
<date>20031000</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>382141</main-classification></classification-national>
</citation>
<citation>
<patcit num="00037">
<document-id>
<country>JP</country>
<doc-number>57-017273</doc-number>
<date>19820100</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<patcit num="00038">
<document-id>
<country>JP</country>
<doc-number>05-037856</doc-number>
<date>19930200</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<patcit num="00039">
<document-id>
<country>JP</country>
<doc-number>5-161050</doc-number>
<date>19930600</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<patcit num="00040">
<document-id>
<country>JP</country>
<doc-number>08-018857</doc-number>
<date>19960100</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<patcit num="00041">
<document-id>
<country>JP</country>
<doc-number>8-4783</doc-number>
<date>19960200</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<patcit num="00042">
<document-id>
<country>JP</country>
<doc-number>09-116799</doc-number>
<date>19970500</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<patcit num="00043">
<document-id>
<country>JP</country>
<doc-number>9-266561</doc-number>
<date>19971000</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
</references-cited>
<number-of-claims>19</number-of-claims>
<us-exemplary-claim>1</us-exemplary-claim>
<us-field-of-classification-search>
<classification-national>
<country>US</country>
<main-classification>382282</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>382284</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>382287</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>382294</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>382291</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>382151</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>382184</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>382295</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>395326</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>395332</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>395333</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>358538</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>358540</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>358450</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>3482084</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>3482119</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>348 94</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>348 95</main-classification>
</classification-national>
</us-field-of-classification-search>
<figures>
<number-of-drawing-sheets>8</number-of-drawing-sheets>
<number-of-figures>10</number-of-figures>
</figures>
<us-related-documents>
<division>
<relation>
<parent-doc>
<document-id>
<country>US</country>
<doc-number>10232516</doc-number>
<kind>00</kind>
<date>20020903</date>
</document-id>
<parent-grant-document>
<document-id>
<country>US</country>
<doc-number>6798924</doc-number>
<kind>A </kind>
</document-id>
</parent-grant-document>
</parent-doc>
<child-doc>
<document-id>
<country>US</country>
<doc-number>10860145</doc-number>
</document-id>
</child-doc>
</relation>
</division>
<division>
<relation>
<parent-doc>
<document-id>
<country>US</country>
<doc-number>09150288</doc-number>
<kind>00</kind>
<date>19980909</date>
</document-id>
<parent-grant-document>
<document-id>
<country>US</country>
<doc-number>6466701</doc-number>
<kind>A </kind>
</document-id>
</parent-grant-document>
</parent-doc>
<child-doc>
<document-id>
<country>US</country>
<doc-number>10232516</doc-number>
</document-id>
</child-doc>
</relation>
</division>
<related-publication>
<document-id>
<country>US</country>
<doc-number>20040218833</doc-number>
<kind>A1</kind>
<date>20041104</date>
</document-id>
</related-publication>
</us-related-documents>
<parties>
<applicants>
<applicant sequence="001" app-type="applicant-inventor" designation="us-only">
<addressbook>
<last-name>Ejiri</last-name>
<first-name>Koichi</first-name>
<address>
<city>Chiba</city>
<country>JP</country>
</address>
</addressbook>
<nationality>
<country>JP</country>
</nationality>
<residence>
<country>JP</country>
</residence>
</applicant>
<applicant sequence="002" app-type="applicant-inventor" designation="us-only">
<addressbook>
<last-name>Aoki</last-name>
<first-name>Shin</first-name>
<address>
<city>Kanagawa</city>
<country>JP</country>
</address>
</addressbook>
<nationality>
<country>JP</country>
</nationality>
<residence>
<country>JP</country>
</residence>
</applicant>
<applicant sequence="003" app-type="applicant-inventor" designation="us-only">
<addressbook>
<last-name>Saitoh</last-name>
<first-name>Takashi</first-name>
<address>
<city>Kanagawa</city>
<country>JP</country>
</address>
</addressbook>
<nationality>
<country>JP</country>
</nationality>
<residence>
<country>JP</country>
</residence>
</applicant>
<applicant sequence="004" app-type="applicant-inventor" designation="us-only">
<addressbook>
<last-name>Haike</last-name>
<first-name>Guan</first-name>
<address>
<city>Kanagawa</city>
<country>JP</country>
</address>
</addressbook>
<nationality>
<country>JP</country>
</nationality>
<residence>
<country>JP</country>
</residence>
</applicant>
<applicant sequence="005" app-type="applicant-inventor" designation="us-only">
<addressbook>
<last-name>Sakamoto</last-name>
<first-name>Takuji</first-name>
<address>
<city>Tokyo</city>
<country>JP</country>
</address>
</addressbook>
<nationality>
<country>JP</country>
</nationality>
<residence>
<country>JP</country>
</residence>
</applicant>
</applicants>
<agents>
<agent sequence="01" rep-type="attorney">
<addressbook>
<orgname>Oblon, Spivak, McClelland, Maier &amp; Neustadt, P.C.</orgname>
<address>
<country>unknown</country>
</address>
</addressbook>
</agent>
</agents>
</parties>
<assignees>
<assignee>
<addressbook>
<orgname>Ricoh Company, Ltd.</orgname>
<role>03</role>
<address>
<city>Tokyo</city>
<country>JP</country>
</address>
</addressbook>
</assignee>
</assignees>
<examiners>
<primary-examiner>
<last-name>Kassa</last-name>
<first-name>Yosef</first-name>
<department>2624</department>
</primary-examiner>
</examiners>
</us-bibliographic-data-grant>
<abstract id="abstract">
<p id="p-0001" num="0000">A camera system includes a display monitor which displays an image of an object, taken by an optical unit, on a screen of the monitor. A reading unit reads a preceding image and a current image among a plurality of partially overlapping images, from a memory device, the preceding image and the current image containing a common element. A determining unit determines a positional relation between the preceding image and the current image based on a common pattern derived from the common element in the two adjacent images read by the reading unit. A displaying unit displays an image indicating a boundary of the preceding image on the screen of the monitor at a shifted position according to the positional relation determined by the determining unit, with the current image concurrently displayed on the screen of the monitor.</p>
</abstract>
<drawings id="DRAWINGS">
<figure id="Fig-EMI-D00000" num="00000">
<img id="EMI-D00000" he="174.67mm" wi="135.89mm" file="US07298919-20071120-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00001" num="00001">
<img id="EMI-D00001" he="188.64mm" wi="150.96mm" file="US07298919-20071120-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00002" num="00002">
<img id="EMI-D00002" he="191.35mm" wi="138.85mm" file="US07298919-20071120-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00003" num="00003">
<img id="EMI-D00003" he="193.72mm" wi="154.01mm" file="US07298919-20071120-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00004" num="00004">
<img id="EMI-D00004" he="187.45mm" wi="143.59mm" file="US07298919-20071120-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00005" num="00005">
<img id="EMI-D00005" he="190.16mm" wi="146.64mm" file="US07298919-20071120-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00006" num="00006">
<img id="EMI-D00006" he="176.19mm" wi="139.02mm" file="US07298919-20071120-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00007" num="00007">
<img id="EMI-D00007" he="131.32mm" wi="118.62mm" file="US07298919-20071120-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00008" num="00008">
<img id="EMI-D00008" he="229.11mm" wi="145.29mm" file="US07298919-20071120-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
</drawings>
<description id="description">
<?RELAPP description="Other Patent Relations" end="lead"?>
<heading id="h-0001" level="1">CROSS-REFERENCE TO RELATED APPLICATIONS</heading>
<p id="p-0002" num="0001">This application is a division of application Serial No. 10/232,516, filed on Sep. 3, 2002, now U.S. Pat. No. 6,798,924 which is a Division of application Ser. No.: 09/150,288, filed on Sep. 9, 1998 (now U.S. Pat. No. 6,466,701), the contents of each of which is incorporated herein by reference.</p>
<?RELAPP description="Other Patent Relations" end="tail"?>
<?BRFSUM description="Brief Summary" end="lead"?>
<heading id="h-0002" level="1">BACKGROUND OF THE INVENTION</heading>
<p id="p-0003" num="0002">(1) Field of the Invention</p>
<p id="p-0004" num="0003">The present invention relates to a camera system which electronically stores an image of an object and displays the image on a display monitor.</p>
<p id="p-0005" num="0004">(2) Description of the Related Art</p>
<p id="p-0006" num="0005">Generally, to achieve an adequately high level of resolution of an image captured by using a digital camera or a video camera, it is necessary to use a zoom-up function of the camera or move the camera close to an object to be imaged. This makes it difficult to obtain an image covering a wide angle related to the object. To capture an image covering a wide angle related to the object, it is necessary to use a zoom-down function of the camera or move the camera away from the object. However, this makes it difficult to obtain an image with a high level of resolution.</p>
<p id="p-0007" num="0006">In order to obtain a wide-angle image with a high resolution from an object, a divisional shooting method has been proposed. In the divisional shooting method, a plurality of partially overlapping images are successively shot so as to cover a wide angle related to the object, and they are synthesized to create a composite image with an adequate level of resolution.</p>
<p id="p-0008" num="0007">As disclosed in Japanese Published Utility Model Application No. 8-4783, an image processing device which is capable of combining a plurality of partially overlapping images together to create a composite image is known.</p>
<p id="p-0009" num="0008">To effectively carry out the divisional shooting method, it is necessary that, after a preceding image is taken and before a current image is taken, the user stop movement of an optical axis of the camera at an appropriate position where an overlapping portion of the two adjacent images is appropriate for subsequently producing a composite image from the images. However, in order to meet this requirement, a conventional digital camera requires a special adapter. If such an adapter is not used, it is difficult for the conventional digital camera to effectively carry out the divisional shooting method. In a case of the conventional digital camera with no special adapter, there is a possibility that no overlapping portion exists between the two adjacent images or a too large overlapping portion be produced between the two adjacent images. If the overlapping images with undesired overlapping portions are obtained through the divisional shooting method, it is difficult to effectively combine or synthesize the images together to create a composite image.</p>
<heading id="h-0003" level="1">SUMMARY OF THE INVENTION</heading>
<p id="p-0010" num="0009">An object of the present invention is to provide a camera system which displays an image indicating a positional relation among partially overlapping images, and enables an operator to easily and effectively carry out a divisional shooting process.</p>
<p id="p-0011" num="0010">Another object of the present invention is to provide a divisional shooting method which displays an image indicating a positional relation among partially overlapping images on a screen of a monitor during a divisional shooting mode of a camera system.</p>
<p id="p-0012" num="0011">The above-mentioned objects of the present invention are achieved by a camera system which comprises: a display monitor which displays an image of an object, taken by an optical unit, on a screen of the monitor; a reading unit which reads a preceding image and a current image among a plurality of partially overlapping images, from a memory device, the preceding image and the current image containing a common element; a determining unit which determines a positional relation between the preceding image and the current image based on a common pattern derived from the common element in the two adjacent images read by the reading unit; and a displaying unit which displays an image indicating a boundary of the preceding image on the screen of the monitor at a shifted position according to the positional relation determined by the determining unit, with the current image concurrently displayed on the screen of the monitor.</p>
<p id="p-0013" num="0012">The above-mentioned objects of the present invention are achieved by a divisional shooting method for a camera system in which at least two of partially overlapping images of an object, taken by an optical unit, are displayed, comprising the steps of: reading a preceding image and a current image among the partially overlapping images, from a memory device, the preceding image and the current image containing a common element; determining a positional relation between the preceding image and the current image based on a common pattern derived from the common element in the two adjacent images; and displaying an image, indicating a boundary of the preceding image, on a screen of a display monitor at a shifted position according to the positional relation determined by the determining step, with the current image concurrently displayed on the screen of the monitor.</p>
<p id="p-0014" num="0013">In the camera system of the present invention, a positional relation between the preceding image and the current image is determined based on a common pattern derived from the common element in the two adjacent images. The operator can easily carry out a divisional shooting mode of the camera system by viewing both the current image and the image indicating the positional relation between the partially overlapping images on the screen of the monitor. The positional relation between the preceding image and the current image is clearly noticeable to the operator by viewing the positional relation image on the screen of the monitor together with the current image while the camera is panned in a desired direction. Therefore, the operator easily stops the movement of the optical axis of the camera at an appropriate position by viewing the positional relation image on the screen of the monitor, and turns ON a shutter switch to store the current image.</p>
<?BRFSUM description="Brief Summary" end="tail"?>
<?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?>
<description-of-drawings>
<heading id="h-0004" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading>
<p id="p-0015" num="0014">Other objects, features and advantages of the present invention will become more apparent from the following detailed description when read in conjunction with the accompanying drawings in which:</p>
<p id="p-0016" num="0015"><figref idref="DRAWINGS">FIG. 1</figref> is a block diagram of a preferred embodiment of a camera system of the present invention;</p>
<p id="p-0017" num="0016"><figref idref="DRAWINGS">FIG. 2</figref> is a flowchart for explaining a first example of a divisional shooting process performed by a processor of the camera system;</p>
<p id="p-0018" num="0017"><figref idref="DRAWINGS">FIG. 3A</figref> and <figref idref="DRAWINGS">FIG. 3B</figref> are diagrams showing an image which is displayed on a screen of a display monitor when the camera is moved in a given direction;</p>
<p id="p-0019" num="0018"/>
<p id="p-0020" num="0019"><figref idref="DRAWINGS">FIG. 4</figref> is a flowchart for explaining a second example of the divisional shooting process performed by the processor of the camera system;</p>
<p id="p-0021" num="0020"><figref idref="DRAWINGS">FIG. 5</figref> is a flowchart for explaining a third example of the divisional shooting process performed by the processor of the camera system;</p>
<p id="p-0022" num="0021"><figref idref="DRAWINGS">FIG. 6</figref> is a flowchart for explaining a fourth example of the divisional shooting process performed by the processor of the camera system;</p>
<p id="p-0023" num="0022"><figref idref="DRAWINGS">FIG. 7</figref> is a flowchart for explaining an image storage process performed by the processor of the camera system when a shutter switch is turned ON; and</p>
<p id="p-0024" num="0023"><figref idref="DRAWINGS">FIG. 8A</figref> and <figref idref="DRAWINGS">FIG. 8B</figref> are diagrams for explaining a determination of a positional relation between partially overlapping images in the divisional shooting process according to the present invention.</p>
</description-of-drawings>
<?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?>
<?DETDESC description="Detailed Description" end="lead"?>
<heading id="h-0005" level="1">DETAILED DESCRIPTION OF THE PREFERRED EMBODIMENTS</heading>
<p id="p-0025" num="0024">A description will now be given of the preferred embodiments of the present invention with reference to the accompanying drawings.</p>
<p id="p-0026" num="0025">In order to carry out a divisional shooting process, the present invention utilizes a method and a system for determining a positional relation between partially overlapping images based upon a common pattern in an overlapping portion of the images. The method and the system are disclosed, for example, in U.S. patent application Ser. No. 08/807,571 filed on Feb. 27, 1997 and U.S. patent application Ser. No. 08/966,889 filed on Nov. 10, 1997, both assigned to the applicant of the present application. The contents of these co-pending applications are hereby incorporated by reference.</p>
<p id="p-0027" num="0026"><figref idref="DRAWINGS">FIG. 1</figref> shows a preferred embodiment of a camera system of the present invention. One example of the camera system of the present invention is a digital camera.</p>
<p id="p-0028" num="0027">As shown in <figref idref="DRAWINGS">FIG. 1</figref>, the camera system of the present embodiment includes an optical unit <b>10</b>. The optical unit <b>10</b> has an image pickup device <b>12</b>, a lens (not shown), and a lens positioner (not shown). The image pickup device <b>12</b> is comprised of a charge-coupled device (CCD). The image pickup device <b>12</b> converts light incident from an object into an electrical signal, or an image signal indicative of an input image of the object or the scene. The lens positioner mechanically positions the lens of the optical unit <b>10</b> at a desired distance from the object along an optical axis of the lens. Hereinafter, the lens of the optical unit <b>10</b> will be referred to as the camera.</p>
<p id="p-0029" num="0028">In the camera system of the present embodiment, a lens positioner actuator <b>14</b> actuates the lens positioner of the optical unit <b>10</b> so that the lens is positioned at a desired distance from the object along the optical axis of the lens. An operation part <b>16</b> is an operation part of the camera system of <figref idref="DRAWINGS">FIG. 1</figref>, which includes a mode selection switch <b>18</b>, a shutter switch <b>20</b>, and other control switches (not shown). An operator can manipulate one of such switches of the operation part <b>16</b> so as to select one of operational modes of the camera system or to release the shutter of the camera system.</p>
<p id="p-0030" num="0029">In the camera system of the present embodiment, a video control unit <b>24</b> converts the signal from the image pickup device <b>12</b> into a digital signal, processes the digital signal to produce a frame of the input image, and stores the frame in a frame buffer <b>25</b>. The frame or image defined in the frame buffer <b>25</b> is a pixel map that has an array of pixel data, each indicating an intensity (and/or a color value) for a position of a corresponding one of the picture elements, or pixels, in the image. The video control unit <b>24</b> displays the image defined in the frame buffer <b>25</b> on a liquid-crystal display (LCD) monitor <b>27</b>, accessing the frame buffer <b>25</b> as frequently as a scan rate of the monitor <b>27</b>. The monitor <b>27</b> has a display screen <b>27</b>A, and the image defined in the frame buffer <b>25</b> is displayed on the screen <b>27</b>A of the monitor <b>27</b> by the video control unit <b>24</b>.</p>
<p id="p-0031" num="0030">The video control unit <b>24</b> further includes a frame buffer <b>26</b> in addition to the frame buffer <b>25</b>. The frame buffer <b>26</b> stores auxiliary data indicative of a peripheral boundary <b>27</b>B (which will be described later) corresponding to the image defined in the frame buffer <b>25</b>. The video control unit <b>24</b> displays the peripheral boundary <b>27</b>B, indicated by the auxiliary data defined in the frame buffer <b>26</b>, on the screen <b>27</b>A of the monitor <b>27</b>, accessing the frame buffer <b>26</b> at the same time as the frame buffer <b>25</b>. Hence, the image defined in the frame buffer <b>25</b> and the auxiliary data defined in the frame buffer <b>26</b> are synthesized so that the image with the peripheral boundary <b>27</b>B is displayed on the screen <b>27</b>A of the monitor <b>27</b> in an overlaid manner. The auxiliary data defined in the frame buffer <b>26</b> includes a frame number to identify a captured image among a plurality of partially overlapping images, which will be described later. Further, the auxiliary data may further include image data of a displacement vector or a direction of the optical axis of the camera, which will be described later.</p>
<p id="p-0032" num="0031">In the camera system of the present embodiment, an image memory <b>28</b> is a storage device which stores an image captured by the video control unit <b>24</b>. The image memory <b>28</b> may be any image storage device, for example, one of semiconductor memories including flash memories, or one of magnetic disks including floppy disks and mini-disks (MD).</p>
<p id="p-0033" num="0032">In the camera system of the present embodiment, a processor <b>30</b> controls the overall operation of the camera system and carries out a divisional shooting process including determination of a positional relation between partially overlapping images based upon a common pattern in an overlapping portion of the images. The processor <b>30</b> includes an arithmetic control unit <b>32</b>, a read-only memory (ROM) <b>33</b>, and a random access memory (RAM) <b>36</b>. The ROM <b>33</b> stores a number of programs <b>34</b>A through <b>34</b>N, and fixed information, such as character fonts. The arithmetic control unit <b>32</b> carries out individual control operations for the elements of the camera system when one of the programs <b>34</b>A through <b>34</b>N in the ROM <b>33</b> is executed by the processor <b>30</b>. The RAM <b>36</b> is a main memory of the processor <b>30</b> which is available to any of the programs when it is executed. The RAM <b>36</b> serves as a work memory available to the arithmetic control unit <b>32</b>. Further, the processor <b>30</b> includes a power supply circuit (not shown) which supplies power to the camera system, and an interface (not shown) which connects the camera system with an external host computer.</p>
<p id="p-0034" num="0033">In the camera system of <figref idref="DRAWINGS">FIG. 1</figref>, the operator can select one of the operational modes by using the mode selection switch <b>18</b>. In the present embodiment, the operational modes of the camera system include a normal shooting mode and a divisional shooting mode.</p>
<p id="p-0035" num="0034">When the normal shooting mode is selected by the mode selection switch <b>18</b>, a single image of an object or a scene is captured through the image pickup device <b>12</b>, the image displayed on the screen <b>27</b>A of the monitor <b>27</b> is viewed, and the shutter switch <b>20</b> is turned ON by the operator so that the image defined in the frame memory <b>25</b> is stored in the image memory <b>28</b>.</p>
<p id="p-0036" num="0035">When the divisional shooting mode is selected in the camera system of the present embodiment, a plurality of partially overlapping images are successively shot so as to cover a wide angle related to an object to be imaged, and they are synthesized to create a composite image with an adequate level of resolution. The divisional shooting mode is useful to obtain a panoramic image or a high-resolution image through image composition. The camera system of the present invention is particularly relevant to the divisional shooting mode, and the following description will be given of an operation of the camera system of the present embodiment when the divisional shooting mode is performed.</p>
<p id="p-0037" num="0036">In the camera system of the present embodiment, when the divisional shooting mode is selected by the mode selection switch <b>18</b>, the processor <b>30</b> starts the execution of a divisional shooting processing program <b>34</b>I among the programs <b>34</b>A through <b>34</b>N in the ROM <b>33</b>. A divisional shooting process is performed by the processor <b>30</b> according to the divisional shooting processing program <b>34</b>I.</p>
<p id="p-0038" num="0037">In order to take a first one of partially overlapping images when the divisional shooting process is started, the operator directs the optical axis of the camera (or the lens of the optical unit <b>10</b>) to an object to be imaged. In accordance with the signal from the image pickup device <b>12</b>, the video control unit <b>24</b> stores a corresponding frame in the frame memory <b>25</b>, and displays the image on the screen <b>27</b>A of the monitor <b>27</b>. The operator turns ON the shutter switch <b>20</b> of the operation part <b>16</b> while viewing the image on the screen <b>27</b>A of the monitor <b>27</b>. A shutter signal from the operation part <b>16</b> is sent to the processor <b>30</b> immediately after the shutter switch <b>20</b> is turned ON. In response to the shutter signal, the processor <b>30</b> stores the image, defined in the frame memory <b>25</b> of the video control unit <b>24</b>, in the image memory <b>28</b>.</p>
<p id="p-0039" num="0038">The above-mentioned image storage process is performed by the processor <b>30</b> of the camera system in accordance with an image storage processing program <b>34</b>N among the programs <b>34</b>A through <b>34</b>N stored in the ROM <b>33</b>. The execution of the image storage processing program <b>34</b>N is started by the processor <b>30</b> in response to the shutter signal. During the image storage process, all the image data corresponding to the entire screen <b>27</b>A of the monitor <b>27</b> is not stored in the image memory <b>28</b>, but only a portion of the image data corresponding to an internal portion of the screen <b>27</b>A of the monitor <b>27</b> within the peripheral boundary <b>27</b>B is stored in the image memory <b>28</b>. The processor <b>30</b> adds a frame number to the auxiliary data of the frame buffer <b>26</b> and stores such data defined in the frame buffer <b>26</b>, in the image memory <b>28</b>, together with the image defined in the frame buffer <b>25</b>, during the image storage process. The data being stored in the image memory <b>28</b> may be compressed in a compact form or may not be compressed in the original form. During the image storage process, the writing of image data to the frame buffer <b>25</b> is inhibited and the image displayed on the screen <b>27</b>A of the monitor <b>27</b> is fixed. Before the image storage process ends, the writing of image data to the frame buffer <b>25</b> is allowed. Hence, after the image storage process is performed, the image defined in the frame buffer <b>25</b> can be variably updated according to the movement of the optical axis of the camera, and the resulting image is displayed on the screen <b>27</b>A of the monitor <b>27</b>.</p>
<p id="p-0040" num="0039"><figref idref="DRAWINGS">FIG. 7</figref> shows an image storage process performed by the processor <b>30</b> of the camera system of the present embodiment. The image storage processing program <b>34</b>N among the programs <b>34</b>A through <b>34</b>N in the ROM <b>33</b> is loaded to the RAM <b>36</b> and executed by the processor <b>30</b> immediately after the shutter switch <b>20</b> is turned ON by the operator. Then, the image storage process of <figref idref="DRAWINGS">FIG. 7</figref> is performed by the processor <b>30</b> according to the image storage processing program <b>34</b>N.</p>
<p id="p-0041" num="0040">As shown in <figref idref="DRAWINGS">FIG. 7</figref>, at the start of the image storage process, the processor <b>30</b> at step S<b>500</b> inhibits the writing of image data to the frame buffer <b>25</b> by the video control unit <b>24</b>. Hence, during the image storage process, the image displayed on the screen <b>27</b>A of the monitor <b>27</b> is fixed.</p>
<p id="p-0042" num="0041">The processor <b>30</b> at step S<b>502</b> combines the auxiliary data of the frame buffer <b>26</b> with the image of the frame buffer <b>25</b> to create a synthesized image, and stores the synthesized image in the image memory <b>28</b>. As described above, the auxiliary data of the frame buffer <b>26</b> includes a frame number to identify a captured image among the partially overlapping images. The auxiliary data of the frame buffer <b>26</b> may include other parameter values (which will be described later). However, when the image storage process with respect to a first one of partially overlapping images is performed, the auxiliary data of the frame buffer <b>26</b> is null or vacant, and only the image of the frame buffer <b>25</b> is stored in the image memory <b>28</b> at the step S<b>502</b>.</p>
<p id="p-0043" num="0042">The processor <b>30</b> at step S<b>504</b> allows the writing of image data to the frame buffer <b>25</b> by the video control unit <b>24</b>. After the step S<b>504</b> is performed, the image storage process of <figref idref="DRAWINGS">FIG. 7</figref> ends. Hence, after the image storage process is performed, the image defined in the frame buffer <b>25</b> is displayed on the screen <b>27</b>A of the monitor <b>27</b>.</p>
<p id="p-0044" num="0043">After the first one of the partially overlapping images is taken, the operator pans the camera in a desired direction in order to take a following one of the partially overlapping images during the divisional shooting mode. By viewing the preceding image with the peripheral boundary on the screen <b>27</b>A of the monitor <b>27</b>, the operator stops the movement of the optical axis of the camera at an appropriate position where an overlapping portion of the two adjacent images is appropriate for subsequently producing a composite image from the images. Then, the current image is captured and stored in the image memory <b>28</b> in a similar manner. The above-described procedure is repeated until all the partially overlapping images for the object to be imaged are captured and stored. In this manner, the partially overlapping images are successively shot so as to cover a wide angle related to the object, and they are synthesized to create a composite image with an adequate level of resolution by using the technology as disclosed in the above-mentioned U.S. patent applications.</p>
<p id="p-0045" num="0044">According to the camera system of the present invention, the operator can easily carry out the divisional shooting process by viewing both the current image and the peripheral boundary <b>27</b>B (or the preceding image) on the screen <b>27</b>A of the monitor <b>27</b>. A positional relation between the preceding image and the current image is clearly noticeable to the operator by viewing the peripheral boundary <b>27</b>B on the screen <b>27</b>A of the monitor <b>27</b> and the current image while the camera is panned in the desired direction. Therefore, the operator easily stops the movement of the optical axis of the camera at an appropriate position by viewing an image of the peripheral boundary <b>27</b>B, and turns ON the shutter switch <b>20</b> to store the current image.</p>
<p id="p-0046" num="0045"><figref idref="DRAWINGS">FIG. 2</figref> shows a first example of the divisional shooting process performed by the processor <b>30</b> in accordance with the divisional shooting processing program <b>34</b>I.</p>
<p id="p-0047" num="0046">As shown in <figref idref="DRAWINGS">FIG. 2</figref>, at the start of the divisional shooting process, the processor <b>30</b> at step S<b>100</b> detects whether the image storage process, shown in <figref idref="DRAWINGS">FIG. 7</figref>, with respect to a preceding one of the partially overlapping images ends. The end of the image storage process is notified to the arithmetic control unit <b>32</b> when the execution of the image storage processing program <b>34</b>N has normally ended. When the result at the step S<b>100</b> is negative, the processor <b>30</b> repeats the step S<b>100</b>.</p>
<p id="p-0048" num="0047">When the result at the step S<b>100</b> is affirmative, the processor <b>30</b> at step S<b>104</b> reads out the pixel map of the preceding image from the image memory <b>28</b>, and reads out the pixel map of a currently-captured image from the frame buffer <b>25</b>. These pixel maps are temporarily stored in the RAM <b>36</b>. The pixel map of the preceding image is selected as a standard image. Each of the pixel data of the two adjacent images corresponding to an overlapping portion of the images is divided into blocks of a predetermined size, for example, 16 by 16 pixels.</p>
<p id="p-0049" num="0048">After the step S<b>104</b> is performed, the processor <b>30</b> at step S<b>106</b> performs a matching between corresponding blocks from an overlapping portion of the two adjacent images. During the step S<b>106</b>, a common pattern in the two adjacent images is identified if a certain similarity threshold is met. This matching may be performed by checking the intensities of individual pixels of the corresponding blocks. This is useful for reducing the amount of required calculations. Alternatively, the matching may be performed by checking the color values of individual pixels of the corresponding blocks, but this will increase the amount of required calculations. The above matching procedures are repeated until all the blocks are processed so that a maximum-similarity common pattern in the preceding image and the maximum-similarity common pattern in the current image are detected.</p>
<p id="p-0050" num="0049">A method and a system for determining a positional relation between partially overlapping images based upon a common pattern in an overlapping portion of the images are disclosed in the above-mentioned U.S. patent applications, and the divisional shooting process according to the present invention utilizes the method and the system.</p>
<p id="p-0051" num="0050">As previously described, during the step S<b>106</b> of the divisional shooting process of <figref idref="DRAWINGS">FIG. 2</figref>, a determination of a positional relation between partially overlapping images is carried out. By referring to <figref idref="DRAWINGS">FIG. 8A</figref> and <figref idref="DRAWINGS">FIG. 8B</figref>, a detailed procedure of the determination of the positional relation in the step S<b>106</b> will now be described.</p>
<p id="p-0052" num="0051">It is supposed that the pixel map of the preceding image from the image memory <b>28</b> and the pixel map of the current image from the frame buffer <b>25</b> have been read out as in the step S<b>104</b>. These pixel maps are temporarily stored in the RAM <b>36</b>. The pixel map of the preceding image is selected as the standard image. Each of the two adjacent images corresponding to an overlapping portion of the images is divided into blocks of a predetermined size.</p>
<p id="p-0053" num="0052">As shown in <figref idref="DRAWINGS">FIG. 8A</figref>, pixels A, B and C in the preceding image and pixels A, B and C in the current image correspond to the overlapping portion of the images. During the step S<b>106</b>, a matching between corresponding blocks from the overlapping portion of the two adjacent images is performed. A common pattern (such as the pixels A, B and C and the pixels A, B and C) in the two adjacent images is identified if a certain similarity threshold is met. This matching may be performed by checking the intensities of individual pixels of the corresponding blocks. The above matching procedures are repeated until all the blocks are processed, so that a maximum-similarity common pattern in the preceding image and the maximum-similarity common pattern in the current image are detected.</p>
<p id="p-0054" num="0053">As shown in <figref idref="DRAWINGS">FIG. 8B</figref>, the maximum-similarity common pattern in the two images is detected if the difference between the pixel values (or the intensities of the pixels A and A, the pixels B and B or the pixels C and C) of the corresponding blocks is found to be the minimum when the current image is moved relative to the preceding image by both a distance for a first number of pixels in the x-axis direction and a distance for a second number of pixels in the y-axis direction. Through the above pixel-based method, the processor <b>30</b> detects the maximum-similarity common pattern in the two images. That is, the processor <b>30</b> at the step S<b>106</b> carries out the determination of the positional relation between the partially overlapping images.</p>
<p id="p-0055" num="0054">In the above-described procedure, the maximum-similarity common pattern in the two images is detected by using the pixel-based method, in order to carry out the determination of the positional relation between the partially overlapping images. However, according to the present invention, it is also possible to achieve the determination of a positional relation between partially overlapping images at an accuracy higher than the accuracy of one pixel. As previously described, the determination of a positional relation between partially overlapping images based upon a common pattern in an overlapping portion of the images are disclosed in the above-mentioned U.S. patent applications, and, for that purpose, the divisional shooting process according to the present invention may utilize the method and the system.</p>
<p id="p-0056" num="0055">Referring back to <figref idref="DRAWINGS">FIG. 2</figref>, during the step S<b>106</b>, the processor <b>30</b> further determines both coordinates (I, J) of a central pixel of the maximum-similarity common pattern in the preceding image and coordinates (Im, Jm) of a central pixel of the maximum-similarity common pattern in the current image. The coordinates (I, J) and the coordinates (Im, Jm) based on a screen coordinate system of the screen <b>27</b>A of the monitor <b>27</b> are determined by the processor <b>30</b>.</p>
<p id="p-0057" num="0056">The processor <b>30</b> at step S<b>108</b> determines a displacement vector (I-Im, J-Jm), which indicates a positional relation between the preceding image and the current image, by the difference between the coordinates (I, J) and the coordinates (Im, Jm). In the step S<b>108</b>, after the contents of the frame buffer <b>26</b> are cleared, the processor <b>30</b> writes image data, indicative of the displacement vector, to the frame buffer <b>26</b> as part of the auxiliary data. Hence, the image of the displacement vector (or the auxiliary data defined in the frame buffer <b>26</b>) is displayed on the screen <b>27</b>A of the monitor <b>27</b>.</p>
<p id="p-0058" num="0057">The processor <b>30</b> at step S<b>11</b>O detects whether the operator stops the movement of the optical axis of the camera (or detects whether the operator turns ON the shutter switch <b>20</b>). When the result at the step S<b>110</b> is negative, the above steps S<b>106</b> and S<b>108</b> are repeated.</p>
<p id="p-0059" num="0058">When the step S<b>106</b> is performed for second or subsequent ones of the partially overlapping images, the coordinates (I, J) of the central pixel of the maximum-similarity common pattern in the preceding image and the direction of the displacement vector are known. The matching procedures in the step S<b>106</b> may be performed for only the blocks of the current image in the overlapping portion of the two images, indicated by the direction of the displacement vector and the coordinates (I, J). By using such a simplified matching, the common pattern in the two adjacent images may be identified, and coordinates (Im, Jm) of the central pixel of the maximum-similarity common pattern in the current image may be determined.</p>
<p id="p-0060" num="0059">The operator stops the panning of the camera at an appropriate position where an appropriate overlapping portion of the two adjacent images can be seen with the image of the displacement vector on the screen <b>27</b>A of the monitor <b>27</b>, and turns ON the shutter switch <b>20</b> to store the current image. Every time the steps S<b>106</b> and S<b>108</b> are performed, the processor <b>30</b> compares the currently obtained displacement vector and the previously obtained displacement vector (stored in an internal register of the processor <b>30</b> or the RAM <b>36</b>) so as to determine whether the operator stops the movement of the optical axis of the camera. If the difference between the two displacement vectors is larger than a threshold value, the result at the step S<b>110</b> is negative. If the difference between the two displacement vectors is less than the threshold value, the result at the step S<b>110</b> is affirmative.</p>
<p id="p-0061" num="0060">When the result at the step S<b>110</b> is affirmative, the processor <b>30</b> at step S<b>112</b> writes image data, indicative of the peripheral boundary <b>27</b>B of the preceding image, to the frame buffer <b>26</b> at a position shifted from the previous position. The shifted position is determined from the previous position based on the magnitude and direction of the displacement vector obtained in the step S<b>108</b>. Hence, the image of the peripheral boundary <b>27</b>B defined in the frame buffer <b>26</b> is displayed on the screen <b>27</b>A of the monitor <b>27</b> as if the peripheral boundary <b>27</b>B is shifted according to the movement of the optical axis of the camera.</p>
<p id="p-0062" num="0061">In the step S<b>112</b>, the image data of the displacement vector obtained in the step S<b>108</b> may be left in the frame buffer <b>26</b> without change. Alternatively, the image data of the displacement vector in the frame buffer <b>26</b> may be deleted, and then the image data of the shifted peripheral boundary <b>27</b>B may be defined in the frame buffer <b>26</b>. The image of the peripheral boundary <b>27</b>B being displayed on the screen <b>27</b>A of the monitor <b>27</b> may be a frame of the preceding image or a solid model of the preceding image with a certain color attached to the internal pixels.</p>
<p id="p-0063" num="0062">The operator can easily carry out the divisional shooting process with the camera system by viewing both the current image and the peripheral boundary <b>27</b>B (or the preceding image) on the screen <b>27</b>A of the monitor <b>27</b>. A positional relation between the preceding image and the current image is clearly noticeable to the operator by viewing the peripheral boundary <b>27</b>B on the screen <b>27</b>A of the monitor <b>27</b> and the current image while the camera is panned in a desired direction. Therefore, the operator easily stops the movement of the optical axis of the camera at an appropriate position by viewing an image of the peripheral boundary <b>27</b>A, and turns ON the shutter switch <b>20</b> to store the current image.</p>
<p id="p-0064" num="0063">After the step S<b>112</b> is performed, the control is transferred to the step S<b>100</b>. The processor <b>30</b> at the step S<b>100</b> waits for the end of the image storage process at which the currently captured image is further stored in the image memory <b>28</b>. As described above, during the image storage process, the frame number for the current image and the displacement vector for the current image are added to the auxiliary data of the frame buffer <b>26</b> and such data defined in the frame buffer <b>26</b> is stored in the image memory <b>28</b> together with the image defined in the frame buffer <b>25</b>. The frame number and the displacement data are used when synthesizing the partially overlapping images to create a composite image.</p>
<p id="p-0065" num="0064"><figref idref="DRAWINGS">FIG. 3A</figref> shows an image which is displayed on the screen <b>27</b>A of the monitor <b>27</b> when the camera is being moved in a given direction indicated in <figref idref="DRAWINGS">FIG. 3A</figref>. In <figref idref="DRAWINGS">FIG. 3A</figref>, a peripheral boundary of a preceding image is indicated by the dotted-line rectangle ABCD, and a peripheral boundary of a current image is indicated by the solid-line rectangle ABCD. A displacement between the preceding image and the current image proportional to the movement of the optical axis of the camera is defined by the displacement vector. In the case of <figref idref="DRAWINGS">FIG. 3A</figref>, the displacement vector is directed to the left and has a length proportional to the movement of the optical axis of the camera. An image <b>50</b> of the displacement vector is displaced on the screen <b>27</b>A of the monitor <b>27</b> as indicated in <figref idref="DRAWINGS">FIG. 3A</figref>. Although the contents of the preceding image are not displayed, the operator can easily notice a positional relation between the preceding image and the current image on the screen <b>27</b>A of the monitor <b>27</b> with the image <b>50</b>.</p>
<p id="p-0066" num="0065"><figref idref="DRAWINGS">FIG. 3B</figref> shows an image which is displayed on the screen <b>27</b>A of the monitor <b>27</b> when the movement of the optical axis of the camera is stopped and the shutter switch <b>20</b> is turned ON by the operator. In <figref idref="DRAWINGS">FIG. 3B</figref>, an image <b>52</b> of the peripheral boundary <b>27</b>B, which is displayed on the screen <b>27</b>A of the monitor <b>27</b>, is indicated by the rectangle ABCD. The rectangle ABCD corresponds to an overlapping portion of the two adjacent images. As described above, the image data, indicative of the peripheral boundary <b>27</b>B of the preceding image, is written to the frame buffer <b>26</b> at positions shifted from the previous positions according to the movement of the optical axis of the camera. The image <b>50</b> of the displacement vector corresponding to the magnitude and direction of the displacement vector is displayed on the screen <b>27</b>A of the monitor <b>27</b>. The operator can clearly notice an appropriate overlapping portion of the two images by the image <b>50</b> of the displacement vector and the image <b>52</b> of the peripheral boundary <b>27</b>B. The image <b>50</b> of the displacement vector, at the time the movement of the optical axis of the camera is stopped, may be displayed on the screen <b>27</b>A of the monitor <b>27</b>. Alternatively, the display of the image <b>50</b> of the displacement vector may be omitted.</p>
<p id="p-0067" num="0066"><figref idref="DRAWINGS">FIG. 4</figref> shows a second example of the divisional shooting process performed by the processor <b>30</b> in accordance with the divisional shooting processing program <b>34</b>I.</p>
<p id="p-0068" num="0067">As shown in <figref idref="DRAWINGS">FIG. 4</figref>, at the start of the divisional shooting process in the present embodiment, the processor <b>30</b> at step S<b>200</b> detects whether the image storage process with respect to a preceding one of the partially overlapping images ends. The end of the image storage process is notified to the arithmetic control unit <b>32</b> when the execution of the image storage processing program <b>34</b>N has normally ended. When the result at the step S<b>200</b> is negative, the processor <b>30</b> repeats the step S<b>200</b>.</p>
<p id="p-0069" num="0068">When the result at the step S<b>200</b> is affirmative, the processor <b>30</b> at step S<b>204</b> reads out the pixel map of the preceding image from the image memory <b>28</b>, and reads out the pixel map of the currently-captured image from the frame buffer <b>25</b>. The pixel maps are temporarily stored in the RAM <b>36</b>. The pixel map of the preceding image is selected as a standard image. Each of the pixel data of the two adjacent images corresponding to the overlapping portion of the images is divided into blocks of a predetermined size, for example, 16 by 16 pixels.</p>
<p id="p-0070" num="0069">After the step S<b>204</b> is performed, the processor <b>30</b> at step S<b>206</b> performs a matching between corresponding blocks from the two adjacent images. During the step S<b>206</b>, a common pattern in the two adjacent images is identified if a certain similarity threshold is met. The matching procedures are repeated for every block until all the blocks are processed so that the common pattern in the preceding image and the common pattern in the current image are identified.</p>
<p id="p-0071" num="0070">Further, during the step S<b>206</b>, the processor <b>30</b> determines both coordinates (I, J) of a central pixel of a maximum-similarity common pattern in the preceding image and coordinates (Im, Jm) of a central pixel of the maximum-similarity common pattern in the current image. The coordinates (I, J) and the coordinates (Im, Jm) based on a screen coordinate system of the screen <b>27</b>A of the monitor <b>27</b> are determined by the processor <b>30</b>.</p>
<p id="p-0072" num="0071">The steps S<b>200</b>-S<b>206</b> in the present embodiment are essentially the same as the steps S<b>100</b>-S<b>106</b> in the embodiment of <figref idref="DRAWINGS">FIG. 2</figref>.</p>
<p id="p-0073" num="0072">The processor at step S<b>208</b> determines a displacement vector (I-Im, J-Jm), which indicates a positional relation between the preceding image and the current image, by the difference between the coordinates (I, J) and the coordinates (Im, Jm). In the present embodiment, during the step S<b>208</b>, the processor <b>30</b> writes image data, indicative of the peripheral boundary <b>27</b>B of the preceding image, to the frame buffer <b>26</b> at positions shifted from the previous positions. The shifted positions are indicated by the magnitude and direction of the displacement vector. Hence, the image of the peripheral boundary <b>27</b>B defined in the frame buffer <b>26</b> is displayed on the screen <b>27</b>A of the monitor <b>27</b>.</p>
<p id="p-0074" num="0073">Unlike the embodiment of <figref idref="DRAWINGS">FIG. 2</figref>, during the step S<b>208</b> in the present embodiment, the processor <b>30</b> does not write the image data of the displacement vector to the frame buffer <b>26</b> as part of the auxiliary data. Hence, the image of the displacement vector is not displayed on the screen <b>27</b>A of the monitor <b>27</b>.</p>
<p id="p-0075" num="0074">The processor <b>30</b> at step S<b>210</b> detects whether the operator stops the movement of the optical axis of the camera (or detects whether the operator turns ON the shutter switch <b>20</b>). When the result at the step S<b>210</b> is negative, the above steps S<b>206</b> and S<b>208</b> are repeated.</p>
<p id="p-0076" num="0075">The operator stops the panning of the camera at an appropriate position where an appropriate overlapping portion of the two adjacent images can be seen with the image of the displacement vector on the screen <b>27</b>A of the monitor <b>27</b>, and turns ON the shutter switch <b>20</b> to store the current image. Every time the steps S<b>206</b> and S<b>208</b> are performed, the processor <b>30</b> compares the currently obtained displacement vector and the previously obtained displacement vector (stored in the internal register or the RAM <b>36</b>) so as to determine whether the operator stops the panning of the camera. If the difference between the two displacement vectors is larger than a threshold value, the result at the step S<b>210</b> is negative. If the difference between the two displacement vectors is less than the threshold value, the result at the step S<b>210</b> is affirmative.</p>
<p id="p-0077" num="0076">When the result at the step S<b>210</b> is affirmative, the control is transferred to the step S<b>200</b>. The processor <b>30</b> at the step S<b>200</b> waits for the end of the image storage process at which the currently captured image is further stored in the image memory <b>28</b>.</p>
<p id="p-0078" num="0077">In the present embodiment, the operator can view a peripheral boundary image indicating a positional relation between the current image and the preceding image before the movement of the optical axis of the camera is stopped or the shutter switch <b>20</b> is turned ON. The operator can easily carry out the divisional shooting process with the camera system, but the current image and the peripheral boundary image are always displayed on the screen <b>27</b>A of the monitor <b>27</b>. It is desirable that the intensity and/or color of the peripheral boundary image may be set at a suitable value so as to prevent the peripheral boundary image from hindering the check for the current image on the screen <b>27</b>A of the monitor <b>27</b>.</p>
<p id="p-0079" num="0078"><figref idref="DRAWINGS">FIG. 5</figref> shows a third example of the divisional shooting process performed by the processor <b>30</b> in accordance with the divisional shooting processing program <b>34</b>I.</p>
<p id="p-0080" num="0079">In the present embodiment, the camera system further includes a three-dimensional gyro sensor <b>40</b> connected to the arithmetic control unit <b>32</b> of the processor <b>30</b> as indicated by the dotted line in <figref idref="DRAWINGS">FIG. 1</figref>. The sensor <b>40</b> detects a three-dimensional direction of the optical axis of the optical unit <b>10</b> and outputs a signal indicating the optical axis direction to the arithmetic control unit <b>32</b> of the processor <b>30</b>. The sensor <b>40</b> may be a built-in type or an external-installation type for the camera system. Other elements of the camera system in the present embodiment are the same as corresponding elements of the camera system shown in <figref idref="DRAWINGS">FIG. 1</figref>, and a description thereof will be omitted.</p>
<p id="p-0081" num="0080">When the divisional shooting mode is selected by the mode selection switch <b>18</b>, the processor <b>30</b> starts the execution of the divisional shooting processing program <b>34</b>I in the ROM <b>33</b>. The present embodiment of the divisional shooting process is performed by the processor <b>30</b> according to the divisional shooting processing program <b>34</b>I.</p>
<p id="p-0082" num="0081">In order to take a first one of partially overlapping images at the start of the divisional shooting process is started, the operator directs the optical axis of the camera to an object to be imaged and turns ON the shutter switch <b>20</b>. A shutter signal from the operation part <b>16</b> is sent to the processor <b>30</b> immediately after the shutter switch <b>20</b> is turned ON. In response to the shutter signal, the processor <b>30</b> reads a signal output by the sensor <b>40</b> at that time, and temporarily stores the signal in an internal register of the processor <b>30</b> or the RAM <b>36</b>. In accordance with the signal from the image pickup device <b>12</b>, the video control unit <b>24</b> stores a corresponding frame in the frame memory <b>25</b>, and displays the image on the screen <b>27</b>A of the monitor <b>27</b>. In response to the shutter signal, the processor <b>30</b> stores the image, defined in the frame memory <b>25</b>, in the image memory <b>28</b>.</p>
<p id="p-0083" num="0082">The above-mentioned image storage process is performed by the processor <b>30</b> according to the image storage processing program <b>34</b>N in the ROM <b>33</b>. The execution of the image storage processing program <b>34</b>N is started by the processor <b>30</b> in response to the shutter signal. During the image storage process, the processor <b>30</b> adds both the frame number and the optical axis direction signal to the auxiliary data of the frame buffer <b>26</b>, and stores such data defined in the frame buffer <b>26</b>, in the image memory <b>28</b>, together with the image defined in the frame buffer <b>25</b>. During the image storage process, the writing of image data to the frame buffer <b>25</b> is inhibited and the image displayed on the screen <b>27</b>A of the monitor <b>27</b> is fixed. Before the image storage process ends, the writing of image data to the frame buffer <b>25</b> is allowed. Hence, after the image storage process is performed, the image defined in the frame buffer <b>25</b> can be variably updated according to the movement of the optical axis of the camera, and the resulting image is displayed on the screen <b>27</b>A of the monitor <b>27</b>.</p>
<p id="p-0084" num="0083">After the first one of the partially overlapping images is taken, the operator pans the camera in a desired direction in order to take a following one of the partially overlapping images during the divisional shooting mode. By viewing the preceding image with the peripheral boundary on the screen <b>27</b>A of the monitor <b>27</b>, the operator stops the movement of the optical axis of the camera such that the preceding image and the currently-captured image overlap each other with an appropriate overlapping portion of the images. Then, the current image is captured and stored in the image memory <b>28</b> together with the auxiliary data, including the frame number and the optical axis direction signal, in a similar manner. The above-described procedure is repeated until all the partially overlapping images for the object to be imaged are captured and stored.</p>
<p id="p-0085" num="0084">With reference to <figref idref="DRAWINGS">FIG. 5</figref>, a description will now be given of the third example of the divisional shooting process performed by the processor <b>30</b>.</p>
<p id="p-0086" num="0085">As shown in <figref idref="DRAWINGS">FIG. 5</figref>, at the start of the divisional shooting process in the present embodiment, the processor <b>30</b> at step S<b>300</b> detects whether the image storage process of <figref idref="DRAWINGS">FIG. 7</figref> with respect to a preceding one of the partially overlapping images ends. The end of the image storage process is notified to the processor <b>30</b>. When the result at the step S<b>300</b> is negative, the processor <b>30</b> repeats the step S<b>300</b>.</p>
<p id="p-0087" num="0086">When the result at the step S<b>300</b> is affirmative, the processor <b>30</b> at step S<b>304</b> reads an optical axis direction signal (related to the current image) output by the sensor <b>40</b> at that time, and reads the optical axis direction signal (related to the preceding image) from the internal register or the RAM <b>36</b>.</p>
<p id="p-0088" num="0087">After the step S<b>304</b> is performed, the processor <b>30</b> at step S<b>306</b> determines a displacement vector, which indicates a positional relation of the preceding image to the current image on the screen <b>27</b>A of the monitor <b>27</b>, by the difference between the optical axis direction signal related to the preceding image and the optical axis direction signal related to the current image.</p>
<p id="p-0089" num="0088">The processor <b>30</b> at step S<b>308</b> writes image data, indicative of the displacement vector, to the frame buffer <b>26</b> as part of the auxiliary data after the contents of the frame buffer <b>26</b> are cleared. Hence, an image of the displacement vector (or the auxiliary data defined in the frame buffer <b>26</b>) is displayed on the screen <b>27</b>A of the monitor <b>27</b>, similar to the image <b>50</b> shown in <figref idref="DRAWINGS">FIG. 3A</figref> and <figref idref="DRAWINGS">FIG. 3B</figref>.</p>
<p id="p-0090" num="0089">The processor <b>30</b> at step S<b>310</b> detects whether the operator stops the movement of the optical axis of the camera (or detects whether the operator turns ON the shutter switch <b>20</b>). When the result at the step S<b>310</b> is negative, the above steps S<b>304</b> through S<b>308</b> are repeated.</p>
<p id="p-0091" num="0090">The operator stops the panning of the camera at an appropriate position where an appropriate overlapping portion of the two adjacent images can be seen with the image of the displacement vector on the screen <b>27</b>A of the monitor <b>27</b>, and turns ON the shutter switch <b>20</b> to store the current image. Every time the steps S<b>304</b> through S<b>308</b> are performed, the processor <b>30</b> compares the currently obtained displacement vector and the previously obtained displacement vector (stored in the internal register or the RAM <b>36</b>) so as to determine whether the operator stops the movement of the optical axis of the camera. If the difference between the two displacement vectors is larger than a threshold value, the result at the step S<b>310</b> is negative. If the difference between the two displacement vectors is less than the threshold value, the result at the step S<b>310</b> is affirmative.</p>
<p id="p-0092" num="0091">When the result at the step S<b>310</b> is affirmative, the processor <b>30</b> at step S<b>312</b> writes image data, indicative of the peripheral boundary <b>27</b>B of the preceding image, to the frame buffer <b>26</b> at positions shifted from the previous positions. The shifted positions are indicated by the magnitude and direction of the displacement vector obtained in the step S<b>306</b>. Hence, the image of the peripheral boundary <b>27</b>B defined in the frame buffer <b>26</b> is displayed on the screen <b>27</b>A of the monitor <b>27</b>.</p>
<p id="p-0093" num="0092">In the step S<b>312</b>, the image data of the displacement vector obtained in the step S<b>306</b> may be left in the frame buffer <b>26</b> without change. Alternatively, the image data of the displacement vector in the frame buffer <b>26</b> may be deleted, and then the image data of the shifted peripheral boundary <b>27</b>B may be defined in the frame buffer <b>26</b>. The image of the peripheral boundary <b>27</b>B being displayed on the screen <b>27</b>A of the monitor <b>27</b> may be a frame of the preceding image or a solid model of the preceding image with a certain color attached to the internal pixels.</p>
<p id="p-0094" num="0093">The operator can easily carry out the divisional shooting process with the camera system by viewing both the current image and the peripheral boundary <b>27</b>B (or the preceding image) on the screen <b>27</b>A of the monitor <b>27</b>. A positional relation between the preceding image and the current image is clearly noticeable to the operator by viewing the peripheral boundary <b>27</b>B on the screen <b>27</b>A of the monitor <b>27</b> and the current image while the camera is panned in a desired direction. Therefore, the operator easily stops the movement of the optical axis of the camera at an appropriate position by viewing an image of the peripheral boundary <b>27</b>B, and turns ON the shutter switch <b>20</b> to store the current image.</p>
<p id="p-0095" num="0094">After the step S<b>312</b> is performed, the control is transferred to the step S<b>300</b>. The processor <b>30</b> at the step S<b>300</b> waits for the end of the image storage process at which the currently captured image is further stored in the image memory <b>28</b>. As described above, during the image storage process, the frame number for the current image and the displacement vector for the current image are added to the auxiliary data of the frame buffer <b>26</b> and such data defined in the frame buffer <b>26</b> is stored in the image memory <b>28</b> together with the image defined in the frame buffer <b>25</b>. The frame number and the displacement data are used when synthesizing the partially overlapping images to create a composite image.</p>
<p id="p-0096" num="0095"><figref idref="DRAWINGS">FIG. 6</figref> shows a fourth example of the divisional shooting process performed by the processor <b>30</b> in accordance with a divisional shooting processing program <b>34</b>I.</p>
<p id="p-0097" num="0096">As shown in <figref idref="DRAWINGS">FIG. 6</figref>, at the start of the divisional shooting process in the present embodiment, the processor <b>30</b> at step S<b>400</b> detects whether the image storage process of <figref idref="DRAWINGS">FIG. 7</figref> with respect to a preceding one of the partially overlapping images ends. The end of the image storage process is notified to the processor <b>30</b>. When the result at the step S<b>400</b> is negative, the processor <b>30</b> repeats the step S<b>400</b>.</p>
<p id="p-0098" num="0097">When the result at the step S<b>400</b> is affirmative, the processor <b>30</b> at step S<b>404</b> reads an optical axis direction signal (related to the current image) output by the sensor <b>40</b> at that time, and reads the optical axis direction signal (related to the preceding image) from the internal register or the RAM <b>36</b>.</p>
<p id="p-0099" num="0098">After the step S<b>404</b> is performed, the processor <b>30</b> at step S<b>406</b> determines a displacement vector, which indicates a positional relation of the preceding image to the current image on the screen <b>27</b>A of the monitor <b>27</b>, by the difference between the optical axis direction signal related to the preceding image and the optical axis direction signal related to the current image.</p>
<p id="p-0100" num="0099">The processor <b>30</b> at step S<b>408</b> writes image data, indicative of the peripheral boundary <b>27</b>B of the preceding image, to the frame buffer <b>26</b> at positions shifted from the previous positions. The shifted positions are indicated by the magnitude and direction of the displacement vector obtained in the step S<b>406</b>. Hence, an image of the peripheral boundary <b>27</b>B defined in the frame buffer <b>26</b> is displayed on the screen <b>27</b>A of the monitor <b>27</b>, similar to the image <b>52</b> shown in <figref idref="DRAWINGS">FIG. 3B</figref>.</p>
<p id="p-0101" num="0100">The processor <b>30</b> at step S<b>410</b> detects whether the operator stops the movement of the optical axis of the camera (or detects whether the operator turns ON the shutter switch <b>20</b>). When the result at the step S<b>410</b> is negative, the above steps S<b>404</b> through S<b>408</b> are repeated.</p>
<p id="p-0102" num="0101">The operator stops the panning of the camera at an appropriate position where an appropriate overlapping portion of the two adjacent images can be seen with the image of the peripheral boundary <b>27</b>B on the screen <b>27</b>A of the monitor <b>27</b>, and turns ON the shutter switch <b>20</b> to store the current image. Every time the steps S<b>404</b> through S<b>408</b> are performed, the processor <b>30</b> compares the currently obtained displacement vector and the previously obtained displacement vector (stored in the internal register or the RAM <b>36</b>) so as to determine whether the operator stops the movement of the optical axis of the camera. If the difference between the two displacement vectors is larger than a threshold value, the result at the step S<b>410</b> is negative. If the difference between the two displacement vectors is less than the threshold value, the result at the step S<b>410</b> is affirmative.</p>
<p id="p-0103" num="0102">The operator can easily carry out the divisional shooting process with the camera system by viewing both the current image and the peripheral boundary <b>27</b>B (or the preceding image) on the screen <b>27</b>A of the monitor <b>27</b>. A positional relation between the preceding image and the current image is clearly noticeable to the operator by viewing the peripheral boundary <b>27</b>B on the screen <b>27</b>A of the monitor <b>27</b> and the current image while the camera is panned in a desired direction. Therefore, the operator easily stops the movement of the optical axis of the camera at an appropriate position by viewing the image of the peripheral boundary <b>27</b>B, and turns ON the shutter switch <b>20</b> to store the current image.</p>
<p id="p-0104" num="0103">When the result at the step S<b>410</b> is affirmative, the control is transferred to the step S<b>400</b>. The processor <b>30</b> at the step S<b>400</b> waits for the end of the image storage process at which the currently captured image is further stored in the image memory <b>28</b>.</p>
<p id="p-0105" num="0104">The above-described embodiments of the present invention are applied to a digital camera. However, the present invention is not limited to the above-described embodiments. It is readily understood that the present invention is essentially applicable to a still-video camera and other camera systems which electronically store an image of an object and display the image on a display monitor. Further, variations and modifications of the above-described embodiments may be made without departing from the scope of the present invention.</p>
<p id="p-0106" num="0105">The present invention is based on Japanese priority application No. 9-245522, filed on Sep. 10, 1997, the entire contents of which are hereby incorporated by reference.</p>
<?DETDESC description="Detailed Description" end="tail"?>
</description>
<us-claim-statement>What is claimed is:</us-claim-statement>
<claims id="claims">
<claim id="CLM-00001" num="00001">
<claim-text>1. A device comprising:
<claim-text>a sensor configured to generate an optical axis direction signal representing an optical axis direction of an image capture device;</claim-text>
<claim-text>a video control unit configured to generate a digital image, and configured to store the digital image with corresponding auxiliary data; and</claim-text>
<claim-text>a processing unit configured to,
<claim-text>add the optical axis direction signal to the auxiliary data, and</claim-text>
<claim-text>determine a positional relationship between the digital image and a second digital image by comparing the optical axis direction signal to a second optical axis direction signal associated with the second digital image.</claim-text>
</claim-text>
</claim-text>
</claim>
<claim id="CLM-00002" num="00002">
<claim-text>2. The device of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the processing unit is configured to generate a synthesized image based on the digital image and the auxiliary data, and further comprising:
<claim-text>an image memory configured to store the synthesized image.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00003" num="00003">
<claim-text>3. The device of <claim-ref idref="CLM-00002">claim 2</claim-ref>, further comprising:
<claim-text>a display monitor including a screen; and</claim-text>
<claim-text>a control unit configured to display a part of the synthesized image on the screen when the second digital image is displayed on the screen, the displayed part of the synthesized image indicating the positional relationship between the second digital image and the synthesized image.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00004" num="00004">
<claim-text>4. The device of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the device is a camera.</claim-text>
</claim>
<claim id="CLM-00005" num="00005">
<claim-text>5. The device of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the optical axis direction is represented by a three-directional coordinate system.</claim-text>
</claim>
<claim id="CLM-00006" num="00006">
<claim-text>6. The device of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising:
<claim-text>a storing unit in which said second digital image is stored, and said digital image is currently captured.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00007" num="00007">
<claim-text>7. A method of capturing overlapping images, comprising:
<claim-text>capturing a first image when an optical axis is in a first position;</claim-text>
<claim-text>storing the first image;</claim-text>
<claim-text>displaying a part of the first image;</claim-text>
<claim-text>moving the optical axis from the first position;</claim-text>
<claim-text>stopping movement of the optical axis when the displayed part of the first image is overlapped with a corresponding portion of a second image such that a composite image can be subsequently generated from the first image and the second image; and</claim-text>
<claim-text>capturing the second image.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00008" num="00008">
<claim-text>8. The method of <claim-ref idref="CLM-00007">claim 7</claim-ref>, wherein the steps of capturing the first image, storing the first image, displaying the part of the first image, and capturing the second image are performed by a digital camera.</claim-text>
</claim>
<claim id="CLM-00009" num="00009">
<claim-text>9. The method of <claim-ref idref="CLM-00007">claim 7</claim-ref>, wherein a direction of the optical axis is represented by a three-directional coordinate system.</claim-text>
</claim>
<claim id="CLM-00010" num="00010">
<claim-text>10. The method of <claim-ref idref="CLM-00007">claim 7</claim-ref>, further comprising:
<claim-text>storing said second digital image and said composite image currently captured on a storing unit.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00011" num="00011">
<claim-text>11. A method for displaying an image comprising:
<claim-text>configuring a sensor for generating an optical axis direction signal representing an optical axis direction of an image capture device;</claim-text>
<claim-text>configuring a video control unit to generate a digital image and to store the digital image with corresponding auxiliary data; and</claim-text>
<claim-text>configuring a processing unit to add the optical axis direction signal to the auxiliary data and to determine a positional relationship between the digital image and the second digital image by comparing the optical axis direction signal to a second optical axis direction signal associated with the second digital image.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00012" num="00012">
<claim-text>12. The method of <claim-ref idref="CLM-00011">claim 11</claim-ref>, which comprises configuring the processing unit to generate a synthesized image based on the digital image and the auxiliary data; and
<claim-text>configuring an image memory to store the synthesized image.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00013" num="00013">
<claim-text>13. The method of <claim-ref idref="CLM-00012">claim 12</claim-ref>, further comprising:
<claim-text>providing a display monitor including a screen; and</claim-text>
<claim-text>configuring a control unit to display a part of the synthesized image on the screen where the second digital image is displayed on the screen, the displayed part of the synthesize image indicating the positional relationship between the second digital image and the synthesized image.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00014" num="00014">
<claim-text>14. The method of <claim-ref idref="CLM-00011">claim 11</claim-ref>, wherein the optical axis direction is represented by a three-directional coordinate system.</claim-text>
</claim>
<claim id="CLM-00015" num="00015">
<claim-text>15. The method of <claim-ref idref="CLM-00011">claim 11</claim-ref>, further comprising:
<claim-text>storing said second digital image and said digital image currently captured on a storing unit.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00016" num="00016">
<claim-text>16. A device configured for capturing overlapping images, comprising:
<claim-text>an image capture device configured to capture a first image when an optical axis is in a first position;</claim-text>
<claim-text>a storage unit configured to store the first image;</claim-text>
<claim-text>a display monitor configured to displaying a part of the first image;</claim-text>
<claim-text>a unit configured to move the optical axis from the first position;</claim-text>
<claim-text>a unit configured to stop movement of the optical axis when the display part of the first image is overlapped with a corresponding portion of a second image such that a composite image can be subsequently generated from the first image and the second image; and</claim-text>
<claim-text>a capturing unit configured to capture the second image.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00017" num="00017">
<claim-text>17. The device of <claim-ref idref="CLM-00016">claim 16</claim-ref>, wherein the image capture device comprises a digital camera.</claim-text>
</claim>
<claim id="CLM-00018" num="00018">
<claim-text>18. The device of <claim-ref idref="CLM-00016">claim 16</claim-ref>, wherein a direction of the optical axis is represented by a three-directional coordinate system.</claim-text>
</claim>
<claim id="CLM-00019" num="00019">
<claim-text>19. The device of <claim-ref idref="CLM-00016">claim 16</claim-ref>, further comprising:
<claim-text>a second storing unit in which said second image is stored, and said composite image is currently captured.</claim-text>
</claim-text>
</claim>
</claims>
</us-patent-grant>
