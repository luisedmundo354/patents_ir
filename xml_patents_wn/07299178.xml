<us-patent-grant lang="EN" dtd-version="v4.2 2006-08-23" file="US07299178-20071120.XML" status="PRODUCTION" id="us-patent-grant" country="US" date-produced="20071106" date-publ="20071120">
<us-bibliographic-data-grant>
<publication-reference>
<document-id>
<country>US</country>
<doc-number>07299178</doc-number>
<kind>B2</kind>
<date>20071120</date>
</document-id>
</publication-reference>
<application-reference appl-type="utility">
<document-id>
<country>US</country>
<doc-number>10784173</doc-number>
<date>20040224</date>
</document-id>
</application-reference>
<us-application-series-code>10</us-application-series-code>
<priority-claims>
<priority-claim sequence="01" kind="national">
<country>KR</country>
<doc-number>10-2003-0011345</doc-number>
<date>20030224</date>
</priority-claim>
</priority-claims>
<us-term-of-grant>
<us-term-extension>814</us-term-extension>
</us-term-of-grant>
<classifications-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>G</section>
<class>10</class>
<subclass>L</subclass>
<main-group>15</main-group>
<subgroup>00</subgroup>
<symbol-position>F</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20071120</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
</classifications-ipcr>
<classification-national>
<country>US</country>
<main-classification>704251</main-classification>
<further-classification>704254</further-classification>
</classification-national>
<invention-title id="d0e71">Continuous speech recognition method and system using inter-word phonetic information</invention-title>
<references-cited>
<citation>
<patcit num="00001">
<document-id>
<country>US</country>
<doc-number>5170432</doc-number>
<kind>A</kind>
<name>Hackbarth et al.</name>
<date>19921200</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>704254</main-classification></classification-national>
</citation>
<citation>
<patcit num="00002">
<document-id>
<country>US</country>
<doc-number>5536171</doc-number>
<kind>A</kind>
<name>Javkin et al.</name>
<date>19960700</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>434185</main-classification></classification-national>
</citation>
<citation>
<patcit num="00003">
<document-id>
<country>US</country>
<doc-number>5875426</doc-number>
<kind>A</kind>
<name>Bahl et al.</name>
<date>19990200</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>704255</main-classification></classification-national>
</citation>
<citation>
<patcit num="00004">
<document-id>
<country>US</country>
<doc-number>5995931</doc-number>
<kind>A</kind>
<name>Bahl et al.</name>
<date>19991100</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>704257</main-classification></classification-national>
</citation>
</references-cited>
<number-of-claims>8</number-of-claims>
<us-exemplary-claim>1</us-exemplary-claim>
<us-field-of-classification-search>
<classification-national>
<country>US</country>
<main-classification>704251</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>704254</main-classification>
</classification-national>
</us-field-of-classification-search>
<figures>
<number-of-drawing-sheets>6</number-of-drawing-sheets>
<number-of-figures>8</number-of-figures>
</figures>
<us-related-documents>
<related-publication>
<document-id>
<country>US</country>
<doc-number>20040172247</doc-number>
<kind>A1</kind>
<date>20040902</date>
</document-id>
</related-publication>
</us-related-documents>
<parties>
<applicants>
<applicant sequence="001" app-type="applicant-inventor" designation="us-only">
<addressbook>
<last-name>Yoon</last-name>
<first-name>Su-yeon</first-name>
<address>
<city>Seoul</city>
<country>KR</country>
</address>
</addressbook>
<nationality>
<country>KR</country>
</nationality>
<residence>
<country>KR</country>
</residence>
</applicant>
<applicant sequence="002" app-type="applicant-inventor" designation="us-only">
<addressbook>
<last-name>Choi</last-name>
<first-name>In-jeong</first-name>
<address>
<city>Gyeonggi-do</city>
<country>KR</country>
</address>
</addressbook>
<nationality>
<country>KR</country>
</nationality>
<residence>
<country>KR</country>
</residence>
</applicant>
<applicant sequence="003" app-type="applicant-inventor" designation="us-only">
<addressbook>
<last-name>Kim</last-name>
<first-name>Nam-hoon</first-name>
<address>
<city>Gyeonggi-do</city>
<country>KR</country>
</address>
</addressbook>
<nationality>
<country>KR</country>
</nationality>
<residence>
<country>KR</country>
</residence>
</applicant>
</applicants>
<agents>
<agent sequence="01" rep-type="attorney">
<addressbook>
<orgname>Buchanan Ingersoll &amp; Rooney PC</orgname>
<address>
<country>unknown</country>
</address>
</addressbook>
</agent>
</agents>
</parties>
<assignees>
<assignee>
<addressbook>
<orgname>Samsung Electronics Co., Ltd.</orgname>
<role>03</role>
<address>
<city>Suwon-si, Gyeonggi-do</city>
<country>KR</country>
</address>
</addressbook>
</assignee>
</assignees>
<examiners>
<primary-examiner>
<last-name>Abebe</last-name>
<first-name>Daniel</first-name>
<department>2626</department>
</primary-examiner>
</examiners>
</us-bibliographic-data-grant>
<abstract id="abstract">
<p id="p-0001" num="0000">A continuous speech recognition method and system are provided. The continuous speech recognition method includes constructing a pronunciation dictionary database including at least one pronunciation representation for each word which is influenced by applying phonological rules, wherein the pronunciation representation for the coda of a first word or the pronunciation representation for the onset of a second word following the first word is additionally indexed with an identifier if it does not match the phonetic pronunciation of its spelling, forming inter-word phonetic information in matrix form by combination of a number of all probable phonetic pairs, each of which is basically comprised of the coda of a first word and the onset of a second word following the first word, wherein the coda of the first word or the onset of the second word is indexed with an identifier if they undergo phonological changes and performing speech recognition on feature vectors extracted from an input speech signal with reference to the pronunciation dictionary database and the inter-word phonetic information.</p>
</abstract>
<drawings id="DRAWINGS">
<figure id="Fig-EMI-D00000" num="00000">
<img id="EMI-D00000" he="105.41mm" wi="238.25mm" file="US07299178-20071120-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00001" num="00001">
<img id="EMI-D00001" he="118.11mm" wi="162.56mm" file="US07299178-20071120-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00002" num="00002">
<img id="EMI-D00002" he="208.70mm" wi="92.37mm" file="US07299178-20071120-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00003" num="00003">
<img id="EMI-D00003" he="224.11mm" wi="118.53mm" orientation="landscape" file="US07299178-20071120-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00004" num="00004">
<img id="EMI-D00004" he="127.59mm" wi="161.88mm" file="US07299178-20071120-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00005" num="00005">
<img id="EMI-D00005" he="208.79mm" wi="90.34mm" file="US07299178-20071120-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00006" num="00006">
<img id="EMI-D00006" he="239.86mm" wi="124.54mm" orientation="landscape" file="US07299178-20071120-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
</drawings>
<description id="description">
<?BRFSUM description="Brief Summary" end="lead"?>
<heading id="h-0001" level="1">BACKGROUND OF THE INVENTION</heading>
<p id="p-0002" num="0001">This application claims priority from Korean Patent Application No. 2003-11345, filed on Feb. 24, 2003, in the Korean Intellectual Property Office, the disclosure of which is incorporated herein in its entirety by reference.</p>
<p id="p-0003" num="0002">1. Field of the Invention</p>
<p id="p-0004" num="0003">The present invention relates to speech recognition, and more particularly, to a speech recognition method and system using inter-word phonetic information.</p>
<p id="p-0005" num="0004">2. Description of the Related Art</p>
<p id="p-0006" num="0005">A general continuous speech recognition system has a structure as illustrated in <figref idref="DRAWINGS">FIG. 1</figref>. Referring to <figref idref="DRAWINGS">FIG. 1</figref>, a feature extraction unit <b>11</b> extracts feature vectors, which represents input speech data in a form suitable for speech recognition system. With an acoustic model database <b>13</b>, a pronunciation dictionary database <b>14</b>, and a language model database <b>15</b>, which are previously established through learning processes, a search unit <b>12</b> takes the feature vectors and computes which word sequences most likely produce. In Large Vocabulary Continuous Speech Recognition (LVCSR), words searched by the search unit <b>12</b> have a form as tree structure. A post-processing unit <b>16</b> removes the phonetic representations and tags from what has resulted in search unit <b>12</b>, then symbolize it in terms of syllable, and finally produces hypothesis in text form.</p>
<p id="p-0007" num="0006">Examples of Korean and English words and their possible pronunciation representations stored in the pronunciation dictionary database <b>13</b> are shown in <figref idref="DRAWINGS">FIGS. 2A and 2B</figref>. As shown in <figref idref="DRAWINGS">FIG. 2A</figref>, the word “<img id="CUSTOM-CHARACTER-00001" he="3.56mm" wi="4.91mm" file="US07299178-20071120-P00001.TIF" alt="custom character" img-content="character" img-format="tif"/>[dehak]” <b>21</b>, which means a university, may takes its pronunciation representation as either one of following; [dehak] <b>21</b><i>a</i>, [dehaη] <b>21</b><i>b</i>, and [dehag] <b>21</b><i>c</i>. When it comes to the word “<img id="CUSTOM-CHARACTER-00002" he="3.56mm" wi="4.23mm" file="US07299178-20071120-P00002.TIF" alt="custom character" img-content="character" img-format="tif"/>[dehaη]” <b>22</b>, as is another word example, it means an opposition and its pronunciation can be represented as [dehaη] <b>22</b><i>a</i>. But, it is almost impossible to distinguish pronunciation representations between [dehaη] <b>22</b><i>b </i>and “<img id="CUSTOM-CHARACTER-00003" he="3.56mm" wi="4.23mm" file="US07299178-20071120-P00002.TIF" alt="custom character" img-content="character" img-format="tif"/>[dehaη]” <b>22</b><i>a </i>in that the both of pronunciation representations are identical. Referring to <figref idref="DRAWINGS">FIG. 2B</figref>, the word “seat” <b>23</b> may take its pronunciation representation either [sit] <b>23</b><i>a </i>or [sip] <b>23</b><i>b</i>. However, the pronunciation representation [sip] <b>23</b><i>b </i>is substantially indistinguishable from the pronunciation representation [tip] <b>24</b><i>a </i>for the word “tip” <b>24</b>.</p>
<p id="p-0008" num="0007">An example of search process with the pronunciation dictionary database <b>14</b> in the search unit <b>12</b> will be described in <figref idref="DRAWINGS">FIG. 3</figref>. In order to recognize the word sequence, “<img id="CUSTOM-CHARACTER-00004" he="3.13mm" wi="9.91mm" file="US07299178-20071120-P00003.TIF" alt="custom character" img-content="character" img-format="tif"/>[hanguk dehak I]”, each of words, “hanguk”, “dehak”, and “i” is commonly fractionized into onset, which is initial consonants in syllable, nucleus, which is phonetically steady portion, and coda, which is final consonants in syllable. Here is an example for further understanding. First of all, when it comes to the word, “<img id="CUSTOM-CHARACTER-00005" he="3.56mm" wi="4.23mm" file="US07299178-20071120-P00004.TIF" alt="custom character" img-content="character" img-format="tif"/>[hanguk]”, a pronunciation sequence is generated with possible onset <b>31</b>, and coda <b>33</b> except nucleus, [angu] <b>32</b>. Next, in the case of word, “<img id="CUSTOM-CHARACTER-00006" he="3.56mm" wi="4.91mm" file="US07299178-20071120-P00001.TIF" alt="custom character" img-content="character" img-format="tif"/>[dehak]”, as a similar manner, a pronunciation sequence is generated with possible onset and coda <b>34</b> and <b>36</b> except nucleus, [eha] <b>35</b>. The pronunciation representation <b>37</b> for the word “<img id="CUSTOM-CHARACTER-00007" he="3.13mm" wi="2.12mm" file="US07299178-20071120-P00005.TIF" alt="custom character" img-content="character" img-format="tif"/>[i]” is generated. A subsequent searching process is performed on the generated presentation representations with the probability functions Pr(<img id="CUSTOM-CHARACTER-00008" he="3.56mm" wi="4.91mm" file="US07299178-20071120-P00001.TIF" alt="custom character" img-content="character" img-format="tif"/>[dehak]|<img id="CUSTOM-CHARACTER-00009" he="3.56mm" wi="4.23mm" file="US07299178-20071120-P00004.TIF" alt="custom character" img-content="character" img-format="tif"/>[hanguk]) and Pr(<img id="CUSTOM-CHARACTER-00010" he="3.13mm" wi="2.12mm" file="US07299178-20071120-P00005.TIF" alt="custom character" img-content="character" img-format="tif"/>[i]|<img id="CUSTOM-CHARACTER-00011" he="3.56mm" wi="4.91mm" file="US07299178-20071120-P00001.TIF" alt="custom character" img-content="character" img-format="tif"/>[dehak]). There are two combinations between the words “<img id="CUSTOM-CHARACTER-00012" he="3.56mm" wi="4.23mm" file="US07299178-20071120-P00004.TIF" alt="custom character" img-content="character" img-format="tif"/>” and “<img id="CUSTOM-CHARACTER-00013" he="3.56mm" wi="4.91mm" file="US07299178-20071120-P00001.TIF" alt="custom character" img-content="character" img-format="tif"/>”. In addition, there are three combinations between the words “<img id="CUSTOM-CHARACTER-00014" he="3.56mm" wi="4.91mm" file="US07299178-20071120-P00001.TIF" alt="custom character" img-content="character" img-format="tif"/>” and “<img id="CUSTOM-CHARACTER-00015" he="3.13mm" wi="2.12mm" file="US07299178-20071120-P00005.TIF" alt="custom character" img-content="character" img-format="tif"/>”. The word “<img id="CUSTOM-CHARACTER-00016" he="3.56mm" wi="4.23mm" file="US07299178-20071120-P00004.TIF" alt="custom character" img-content="character" img-format="tif"/>” means a Korean and “<img id="CUSTOM-CHARACTER-00017" he="3.13mm" wi="2.12mm" file="US07299178-20071120-P00005.TIF" alt="custom character" img-content="character" img-format="tif"/>” takes a role as an auxiliary word for a subjective case.</p>
<p id="p-0009" num="0008">For building up Large Vocabulary Continuous Speech Recognition (LVCSR) system, a pronunciation dictionary, which represents words of interest, should be defined ahead. In general, coarticulation effects frequently happen either between phonemes or between words. When coarticulatrion effects appear at the boundary of successive words, each of words cannot be correctly recognized, but it could also have variations of acoustic properties in line with neighborhood context. Accordingly, these phenomena must be considered in modeling pronunciation dictionary for speech.</p>
<p id="p-0010" num="0009">In particular, various phonological changes appear saliently in Korean spoken language depending on the phonemic context. Accordingly, there is a need to provide various pronunciation representations for each word based on such phonological changes. In general, intra-word pronunciation representations have substantially constant phonemic contexts, so that they can be easily modeled based on phonological rules through learning, for example, using triphone models. However, inter-word phonemic contexts vary depending on the surrounding words, so that more delicate modeling is required to reflect the complicated relevant phonological rules.</p>
<p id="p-0011" num="0010">To consider inter-word phonological changes, multiple pronunciation representations for each word, including all or major probable inter-word phonemic contexts, may be incorporated to build up a dictionary. Alternatively, a method of modeling inter-word phonological variations by use of more mixed Gaussian functions providing more state outputs to a HMM may be used. However, the former method expands the sizes of the dictionary and network. The latter method requires substantial computational processing and time and leads to a slow recognition rate. There is another method involving selecting more frequent inter-word phonological changes and applying language model-based, modified phonemic contexts to a recognition network using a crossword triphone model. In this method, multiple begin nodes are assigned to each word so as to consider its various phonemic contexts with the preceding word. As a result, this method drops sharing efficiency in tree-structured recognition networks and leads to extensive network size. Furthermore, in a method of using a tree-structure recognition network where the leading phonemic contexts of words are applied during recognition, not prior to recognition, when there are more than one alternative phonology rules which are applicable in a particular phonological environment, limitation to one of them is impossible. In addition, this method increases the computational load because pronunciation rules must be applied on a frame by frame basis and the recognition network must be continuously updated during recognition.</p>
<heading id="h-0002" level="1">SUMMARY OF THE INVENTION</heading>
<p id="p-0012" num="0011">The present invention provides a continuous speech recognition method, which eliminates phonologically inappropriate connections between pronunciation representations based on the inter-word phonetic information with an identifier, when a spelling of words is different from the representation of phonetic pronunciations.</p>
<p id="p-0013" num="0012">The present invention also provides a system which performs the continuous speech recognition method based on the inter-word phonetic information.</p>
<p id="p-0014" num="0013">In one aspect, the present invention provides a continuous speech recognition method comprising: (a) constructing a pronunciation dictionary database including at least one pronunciation representation for each word which is influenced by applying phonological rules, wherein the pronunciation representation for a coda of the last syllable of a first word or the pronunciation representation for an onset of the initial syllable of a second word following the first word is additionally indexed with an identifier if it does not match the phonetic pronunciation of its spelling; (b) forming inter-word phonetic information in matrix form by combination of a number of all probable phonetic pairs, each of which is basically comprised of the coda of a first word and the onset of a second word following the first word, wherein the coda of the first word or the onset of the second word is indexed with an identifier if they undergo phonological changes; and (c) performing speech recognition on feature vectors extracted from an input speech signal with reference to the pronunciation dictionary database and the inter-word phonetic information.</p>
<p id="p-0015" num="0014">In step (c), a pronunciation representation for the coda of a first word and a pronunciation representation for the onset of a second word following the first word, which do not comply with the phonological rules, may be constrained based on the inter-word phonetic information so as not to be linked to each other.</p>
<p id="p-0016" num="0015">In another aspect, the present invention provides a continuous speech recognition system having an acoustic model database and a language model database which are previously established through learning, the system including: an inter-word phonetic information storing unit which stores inter-word phonetic information by combination of all probable phonemic pairs, each of which is basically comprised of a coda of last syllable of a first word and an onset of initial syllable of a second word following the first word, wherein the coda of the first word or the onset of the second word is indexed with an identifier if it does not match the phonetic pronunciation of its spelling due to phonological interaction between the first and second words; a pronunciation dictionary database including at least one pronunciation representation for each word based on phonological rules, wherein the pronunciation representation for the coda of a first word or the pronunciation representation for the onset of a second word following the first word is additionally indexed with an identifier if it does not match the phonetic pronunciation of the spelling; a feature extraction unit which extracts information that is useful for recognition from an input speech signal and converts the extracted information into feature vectors; and a search unit which searches most likely word sequences among from the feature vectors obtained in the feature extraction unit using the inter-word phonetic information and with reference to the acoustic model database, the pronunciation dictionary database, and the language model database, and outputs the most likely word sequences in text form as a recognition result.</p>
<p id="p-0017" num="0016">The continuous speech recognition system may further include a post-processing unit which converts an intra-word biphone model into an inter-word triphone model, rescores the acoustic models for the most likely word sequences obtained in the search unit, recalculates the scores of candidate sentences, and selects the best candidate sentence as a recognition result.</p>
<?BRFSUM description="Brief Summary" end="tail"?>
<?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?>
<description-of-drawings>
<heading id="h-0003" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading>
<p id="p-0018" num="0017">The above and other features and advantages of the present invention will become more apparent by describing in detail exemplary embodiments thereof with reference to the attached drawings in which:</p>
<p id="p-0019" num="0018"><figref idref="DRAWINGS">FIG. 1</figref> is a block diagram of a general continuous speech recognition system;</p>
<p id="p-0020" num="0019"><figref idref="DRAWINGS">FIGS. 2A and 2B</figref> illustrate examples of pronunciation representations stored in a pronunciation dictionary in <figref idref="DRAWINGS">FIG. 1</figref>;</p>
<p id="p-0021" num="0020"><figref idref="DRAWINGS">FIG. 3</figref> is a diagram for explaining the operation of a search unit in <figref idref="DRAWINGS">FIG. 1</figref>;</p>
<p id="p-0022" num="0021"><figref idref="DRAWINGS">FIG. 4</figref> is a block diagram of a continuous speech recognition system according to an embodiment of the present invention;</p>
<p id="p-0023" num="0022"><figref idref="DRAWINGS">FIGS. 5A and 5B</figref> illustrate examples of pronunciation representations stored in a pronunciation dictionary in <figref idref="DRAWINGS">FIG. 4</figref>; and</p>
<p id="p-0024" num="0023"><figref idref="DRAWINGS">FIG. 6</figref> is a diagram for explaining the operation of a search unit in <figref idref="DRAWINGS">FIG. 4</figref>.</p>
</description-of-drawings>
<?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?>
<?DETDESC description="Detailed Description" end="lead"?>
<heading id="h-0004" level="1">DETAILED DESCRIPTION OF THE INVENTION</heading>
<p id="p-0025" num="0024">Referring to <figref idref="DRAWINGS">FIG. 4</figref>, a continuous speech recognition system according to an embodiment of the present invention includes a feature extraction unit <b>41</b>, a search unit <b>42</b>, an acoustic model database <b>43</b>, a pronunciation dictionary database <b>44</b>, an inter-word phonetic information storing unit <b>45</b>, a language model database <b>46</b>, and a post-processing unit <b>47</b>.</p>
<p id="p-0026" num="0025">The feature extraction unit <b>41</b> extracts effective feature vectors from a speech signal, which is digitized via pre-processing, such as environmental adaptation, end point detection, echo removal, or noise removal. General methods for the feature vector extraction include cepstrum extraction and a method using human's hearing recognition mechanism-based Mel-Frequency Cepstrum Coefficients (MFCC), the latter becoming more popular recently. Cepstrum extraction results in feature vectors having lower-order terms reflecting the features of a speaker's vocal tract upon speaking and higher-order terms reflecting the features of a speech excitation signal.</p>
<p id="p-0027" num="0026">The search unit <b>42</b> searches for most likelihood word sequences among from the feature vectors using Viterbi algorithms and with reference to inter-word phonetic information stored in the inter-word phonetic information storing unit <b>45</b>, which is described later, and the acoustic model database <b>43</b>, the pronunciation dictionary database <b>44</b>, and the language model database <b>46</b>, which are previously established through learning. Vocabularies searched by the search unit <b>42</b> have tree structures for recognition of a large vocabulary. The search unit <b>42</b> searches the trees of words by combination of the words in the pronunciation dictionary database <b>44</b> with reference to data on the frequency of use of words and data on the probability of use of words, both of which correspond to an acoustic difference obtained from the acoustic model database <b>43</b>, to obtain N best candidate word sequences.</p>
<p id="p-0028" num="0027">The acoustic model database <b>43</b> constructs acoustic models, such as Hidden Markov Models (HMM), using user's feature vectors extracted from acoustic data of a training vocal database. These acoustic models are used as reference models during speech recognition.</p>
<p id="p-0029" num="0028">The pronunciation dictionary database <b>44</b> includes all probable pronunciation representations for each word according to relevant phonological rules so as to enable inter-word phonological rule based recognition. An additional identifier is added to a pronunciation representation for a word having an onset or coda that does not match the spelling due to phonetic interaction with the coda of the last syllable of the preceding word or the onset of the initial syllable of the following word.</p>
<p id="p-0030" num="0029">The inter-word phonetic information storing unit <b>45</b> stores as inter-word phonetic information in a matrix form a series of phonemic pairs, which appear between words under phonological rules. Each phonemic pair is basically comprised of the coda of the last syllable of a first word and the onset of the initial syllable of a second word following the first word. The coda of the last syllable of the first word and the onset of the initial syllable of the second word, which form a phonemic pair, are indexed by identifiers when they undergo phonological changes. Such a phoneme with an identifier is referred to as “secondary phoneme”. In general, phonological changes occur in particular phonemic pairs, not all phonemic pairs, according to predetermined phonological rules. In other words, by understanding practical phonemic change rules between compound words or between syllables, inter-word phonological changes can be modeled with considerable accuracy. Typical phonological changes occurring between words include substitution, insertion, and deletion. Substitution refers to the replacement of both or one of adjacent consonants with another consonant due to nasaliation of an obstruent or liquid, change into a peripheral, glottalization, etc. Insertion is more frequent in words starting with a vowel. For example, “<img id="CUSTOM-CHARACTER-00018" he="3.56mm" wi="3.56mm" file="US07299178-20071120-P00006.TIF" alt="custom character" img-content="character" img-format="tif"/>[n]” is added in front of the second one of two words subject to liaison. Deletion is concurrent with insertion in most cases; the coda of a preceding word is dropped, and a phone is added to the onset of a following word. As described above, phonemes that undergo such phonological changes are indexed with identifiers, to discriminate them from phonemes that are not phonologically changed. Pairs of such phonemes with identifiers and phonemic pairs that are not phonologically changed, which are comprised of the coda of the last syllable of a first word and the onset of the initial syllable of a second word following the first word, are stored together as the inter-word phonetic information. The inter-word phonetic information stored in the inter-word phonetic information storing unit <b>45</b> is applied in searching in the search unit <b>42</b> and rescoring in the post-processing unit <b>47</b> to reduce the number of candidate words or sentences, thereby making the searched results more reliable.</p>
<p id="p-0031" num="0030">The language model database <b>46</b> stores data on the probabilities of occurrence of word sequences represented using a bigram or trigram model based on data on the frequency of occurrence of word sequences in sentences stored in a learning text database. The learning text database is comprised of sentences which are considered by a designer to more frequently appear in speech recognition. The number of sentences for the learning text database is limited by the designer.</p>
<p id="p-0032" num="0031">The post-processing unit <b>47</b> rescores the word sequences searched by the search unit <b>42</b> with reference to the inter-word phonetic information stored in the inter-word phonetic information storing unit <b>45</b>, the acoustic model database <b>43</b>, the pronunciation dictionary database <b>44</b>, and the language model database <b>46</b>, removes the phonetic representations and tags from the final rescored word sequences, and combines the resultant word sequences into syllable clusters to output the result in text form. In particular, an intra-word biphone model is converted into a more expanded inter-word triphone model in a boundary between words with respect to the most likely pronunciation representations obtained through backward searching, and the scores of candidate sentences are recalculated to select a recognition result. As such, by applying the inter-word triphone model, modified from the intra-word biphone model, to most probable candidate sentences, and recalculating the probabilities of acoustic models, recognition performance can be improved without degradation in efficiency.</p>
<p id="p-0033" num="0032">Each of the feature extraction unit <b>41</b>, the search unit <b>42</b>, and the post-processing unit <b>47</b> may be implemented using a computing processor, such as a central processing unit (CPU), and a speech recognition program which is executable by the computing processor. The acoustic model database <b>43</b>, the inter-word phonetic information storing unit <b>44</b>, the pronunciation dictionary database <b>45</b>, and the language model database <b>46</b> may be implemented using memory devices.</p>
<p id="p-0034" num="0033">Examples of inter-word phonetic information stored in the inter-word phonetic information storing unit <b>44</b> are shown in Table 1.</p>
<p id="p-0035" num="0034">
<tables id="TABLE-US-00001" num="00001">
<table frame="none" colsep="0" rowsep="0">
<tgroup align="left" colsep="0" rowsep="0" cols="3">
<colspec colname="1" colwidth="84pt" align="left"/>
<colspec colname="2" colwidth="84pt" align="left"/>
<colspec colname="3" colwidth="49pt" align="center"/>
<thead>
<row>
<entry namest="1" nameend="3" rowsep="1">TABLE 1</entry>
</row>
<row>
<entry namest="1" nameend="3" align="center" rowsep="1"/>
</row>
<row>
<entry/>
<entry/>
<entry>Crossword</entry>
</row>
<row>
<entry>Coda of the Last syllable</entry>
<entry>Onset of the Initial syllable</entry>
<entry>Information</entry>
</row>
<row>
<entry>of First word (C)</entry>
<entry>of Second word (D)</entry>
<entry>Value</entry>
</row>
<row>
<entry namest="1" nameend="3" align="center" rowsep="1"/>
</row>
</thead>
<tbody valign="top">
<row>
<entry>η(ο)</entry>
<entry>n(<img id="CUSTOM-CHARACTER-00019" he="2.46mm" wi="2.46mm" file="US07299178-20071120-P00007.TIF" alt="custom character" img-content="character" img-format="tif"/> )</entry>
<entry>0</entry>
</row>
<row>
<entry>η(ο)</entry>
<entry>n(<img id="CUSTOM-CHARACTER-00020" he="2.46mm" wi="2.46mm" file="US07299178-20071120-P00007.TIF" alt="custom character" img-content="character" img-format="tif"/> )</entry>
<entry>1</entry>
</row>
<row>
<entry>η(ο)</entry>
<entry>n(<img id="CUSTOM-CHARACTER-00021" he="2.46mm" wi="2.46mm" file="US07299178-20071120-P00007.TIF" alt="custom character" img-content="character" img-format="tif"/> )</entry>
<entry>2</entry>
</row>
<row>
<entry>η(ο)</entry>
<entry>m(<img id="CUSTOM-CHARACTER-00022" he="2.79mm" wi="2.46mm" file="US07299178-20071120-P00008.TIF" alt="custom character" img-content="character" img-format="tif"/> )</entry>
<entry>0</entry>
</row>
<row>
<entry>η(ο)</entry>
<entry>m(<img id="CUSTOM-CHARACTER-00023" he="2.79mm" wi="2.46mm" file="US07299178-20071120-P00008.TIF" alt="custom character" img-content="character" img-format="tif"/> )</entry>
<entry>1</entry>
</row>
<row>
<entry>η(ο)</entry>
<entry>m(<img id="CUSTOM-CHARACTER-00024" he="2.79mm" wi="2.46mm" file="US07299178-20071120-P00008.TIF" alt="custom character" img-content="character" img-format="tif"/> )</entry>
<entry>2</entry>
</row>
<row>
<entry>η(ο)</entry>
<entry>b(<img id="CUSTOM-CHARACTER-00025" he="2.46mm" wi="2.12mm" file="US07299178-20071120-P00009.TIF" alt="custom character" img-content="character" img-format="tif"/> )</entry>
<entry>0</entry>
</row>
<row>
<entry>η(ο)</entry>
<entry>b(<img id="CUSTOM-CHARACTER-00026" he="2.46mm" wi="2.12mm" file="US07299178-20071120-P00009.TIF" alt="custom character" img-content="character" img-format="tif"/> )</entry>
<entry>1</entry>
</row>
<row>
<entry>η(ο)</entry>
<entry>b(<img id="CUSTOM-CHARACTER-00027" he="2.46mm" wi="2.12mm" file="US07299178-20071120-P00009.TIF" alt="custom character" img-content="character" img-format="tif"/> )</entry>
<entry>2</entry>
</row>
<row>
<entry>η(ο)</entry>
<entry>s(<img id="CUSTOM-CHARACTER-00028" he="2.46mm" wi="2.12mm" file="US07299178-20071120-P00010.TIF" alt="custom character" img-content="character" img-format="tif"/> )</entry>
<entry>0</entry>
</row>
<row>
<entry>η(ο)</entry>
<entry>s(<img id="CUSTOM-CHARACTER-00029" he="2.46mm" wi="2.12mm" file="US07299178-20071120-P00010.TIF" alt="custom character" img-content="character" img-format="tif"/> )</entry>
<entry>1</entry>
</row>
<row>
<entry>η(ο)</entry>
<entry>s(<img id="CUSTOM-CHARACTER-00030" he="2.46mm" wi="2.12mm" file="US07299178-20071120-P00010.TIF" alt="custom character" img-content="character" img-format="tif"/> )</entry>
<entry>2</entry>
</row>
<row>
<entry>k(<img id="CUSTOM-CHARACTER-00031" he="2.46mm" wi="2.12mm" file="US07299178-20071120-P00011.TIF" alt="custom character" img-content="character" img-format="tif"/> )</entry>
<entry>b1(1)(<img id="CUSTOM-CHARACTER-00032" he="2.46mm" wi="2.12mm" file="US07299178-20071120-P00009.TIF" alt="custom character" img-content="character" img-format="tif"/>  → <img id="CUSTOM-CHARACTER-00033" he="2.79mm" wi="2.46mm" file="US07299178-20071120-P00012.TIF" alt="custom character" img-content="character" img-format="tif"/> )</entry>
<entry>0</entry>
</row>
<row>
<entry>k(<img id="CUSTOM-CHARACTER-00034" he="2.46mm" wi="2.12mm" file="US07299178-20071120-P00011.TIF" alt="custom character" img-content="character" img-format="tif"/> )</entry>
<entry>b1(1)(<img id="CUSTOM-CHARACTER-00035" he="2.46mm" wi="2.12mm" file="US07299178-20071120-P00009.TIF" alt="custom character" img-content="character" img-format="tif"/>  → <img id="CUSTOM-CHARACTER-00036" he="2.79mm" wi="2.46mm" file="US07299178-20071120-P00012.TIF" alt="custom character" img-content="character" img-format="tif"/> )</entry>
<entry>1</entry>
</row>
<row>
<entry>k(<img id="CUSTOM-CHARACTER-00037" he="2.46mm" wi="2.12mm" file="US07299178-20071120-P00011.TIF" alt="custom character" img-content="character" img-format="tif"/> )</entry>
<entry>b(<img id="CUSTOM-CHARACTER-00038" he="2.46mm" wi="2.12mm" file="US07299178-20071120-P00009.TIF" alt="custom character" img-content="character" img-format="tif"/> )</entry>
<entry>1</entry>
</row>
<row>
<entry>k(<img id="CUSTOM-CHARACTER-00039" he="2.46mm" wi="2.12mm" file="US07299178-20071120-P00011.TIF" alt="custom character" img-content="character" img-format="tif"/> )</entry>
<entry>b(<img id="CUSTOM-CHARACTER-00040" he="2.46mm" wi="2.12mm" file="US07299178-20071120-P00009.TIF" alt="custom character" img-content="character" img-format="tif"/> )</entry>
<entry>2</entry>
</row>
<row>
<entry>k(<img id="CUSTOM-CHARACTER-00041" he="2.46mm" wi="2.12mm" file="US07299178-20071120-P00011.TIF" alt="custom character" img-content="character" img-format="tif"/> )</entry>
<entry>s1(1)(<img id="CUSTOM-CHARACTER-00042" he="2.46mm" wi="2.12mm" file="US07299178-20071120-P00010.TIF" alt="custom character" img-content="character" img-format="tif"/>  → <img id="CUSTOM-CHARACTER-00043" he="2.46mm" wi="2.79mm" file="US07299178-20071120-P00013.TIF" alt="custom character" img-content="character" img-format="tif"/> )</entry>
<entry>0</entry>
</row>
<row>
<entry>k(<img id="CUSTOM-CHARACTER-00044" he="2.46mm" wi="2.12mm" file="US07299178-20071120-P00011.TIF" alt="custom character" img-content="character" img-format="tif"/> )</entry>
<entry>s1(1)(<img id="CUSTOM-CHARACTER-00045" he="2.46mm" wi="2.12mm" file="US07299178-20071120-P00010.TIF" alt="custom character" img-content="character" img-format="tif"/>  → <img id="CUSTOM-CHARACTER-00046" he="2.46mm" wi="2.79mm" file="US07299178-20071120-P00013.TIF" alt="custom character" img-content="character" img-format="tif"/> )</entry>
<entry>1</entry>
</row>
<row>
<entry>k(<img id="CUSTOM-CHARACTER-00047" he="2.46mm" wi="2.12mm" file="US07299178-20071120-P00011.TIF" alt="custom character" img-content="character" img-format="tif"/> )</entry>
<entry>s(<img id="CUSTOM-CHARACTER-00048" he="2.46mm" wi="2.12mm" file="US07299178-20071120-P00010.TIF" alt="custom character" img-content="character" img-format="tif"/> )</entry>
<entry>1</entry>
</row>
<row>
<entry>k(<img id="CUSTOM-CHARACTER-00049" he="2.46mm" wi="2.12mm" file="US07299178-20071120-P00011.TIF" alt="custom character" img-content="character" img-format="tif"/> )</entry>
<entry>s(<img id="CUSTOM-CHARACTER-00050" he="2.46mm" wi="2.12mm" file="US07299178-20071120-P00010.TIF" alt="custom character" img-content="character" img-format="tif"/> )</entry>
<entry>2</entry>
</row>
<row>
<entry>η (1)(<img id="CUSTOM-CHARACTER-00051" he="2.46mm" wi="2.12mm" file="US07299178-20071120-P00011.TIF" alt="custom character" img-content="character" img-format="tif"/>  → ο)</entry>
<entry>n(<img id="CUSTOM-CHARACTER-00052" he="2.46mm" wi="2.46mm" file="US07299178-20071120-P00007.TIF" alt="custom character" img-content="character" img-format="tif"/> )</entry>
<entry>0</entry>
</row>
<row>
<entry>η (1)(<img id="CUSTOM-CHARACTER-00053" he="2.46mm" wi="2.12mm" file="US07299178-20071120-P00011.TIF" alt="custom character" img-content="character" img-format="tif"/>  → ο)</entry>
<entry>m(<img id="CUSTOM-CHARACTER-00054" he="2.79mm" wi="2.46mm" file="US07299178-20071120-P00008.TIF" alt="custom character" img-content="character" img-format="tif"/> )</entry>
<entry>0</entry>
</row>
<row>
<entry namest="1" nameend="3" align="center" rowsep="1"/>
</row>
</tbody>
</tgroup>
</table>
</tables>
</p>
<p id="p-0036" num="0035">In Table 1, a number of phonemic pairs, each of which is comprised of the coda of the last syllable of a first word and the onset of the initial syllable of a second word and which are phonologically changed according to practical phonological rules and do not match the phonetic pronunciation corresponding to their spelling, are shown. The phonologically changed phonemic pairs are stored in the inter-word phonetic information storing unit <b>45</b> as inter-word phonetic information. For example, when the last syllable of a word having the coda “<img id="CUSTOM-CHARACTER-00055" he="3.13mm" wi="3.13mm" file="US07299178-20071120-P00014.TIF" alt="custom character" img-content="character" img-format="tif"/>[k]” is followed by a word having an initial syllable beginning with the onset “<img id="CUSTOM-CHARACTER-00056" he="3.56mm" wi="3.56mm" file="US07299178-20071120-P00006.TIF" alt="custom character" img-content="character" img-format="tif"/>[n]” or “<img id="CUSTOM-CHARACTER-00057" he="3.56mm" wi="3.13mm" file="US07299178-20071120-P00015.TIF" alt="custom character" img-content="character" img-format="tif"/>[m]”, the coda “<img id="CUSTOM-CHARACTER-00058" he="3.13mm" wi="3.13mm" file="US07299178-20071120-P00014.TIF" alt="custom character" img-content="character" img-format="tif"/>[k]” of the preceding word is practically pronounced as [η], which does not match the phonetic pronunciation corresponding to its spelling, so that the pronunciation representation [η(1)] is linked only to the pronunciation representation [n] or [m] for the onset of the initial syllable of the following word. In this case, only the pronunciation representation with an identifier, [η(1)n], not the combination form with the letters, i.e., “<img id="CUSTOM-CHARACTER-00059" he="3.13mm" wi="6.69mm" file="US07299178-20071120-P00016.TIF" alt="custom character" img-content="character" img-format="tif"/>→ [η(1)n]”, is stored as inter-word phonetic information. Since additional identifiers are also indexed in the pronunciation dictionary database <b>44</b>, impermissible pronunciation representations that do not comply with practical phonological rules can be constrained so as not to be linked into a sentence even through the inter-word phonetic information storing unit <b>45</b> has such a simple pronunciation representation format.</p>
<p id="p-0037" num="0036">The crossword information value appearing in Table 1 represents whether there is a pause between words in a spoken sentence. A phonological change does not occur between subsequent words when a speaker has paused between the words. Depending on the length of the pause, the crossword information value is represented as one of 0 for no pause, 1 for a short pause, and 2 for a long pause. The onset for the initial syllable of the following word varies depending on the crossword information value with the final syllable of the proceeding word. More than one crossword information values are assigned to each phonetic pair stored in the inter-word phonemic information storing unit <b>45</b>.</p>
<p id="p-0038" num="0037">Exemplary pronunciation representations for Korean and English words stored in the pronunciation database <b>44</b> will be described with reference to <figref idref="DRAWINGS">FIGS. 5A and 5B</figref>. Referring to <figref idref="DRAWINGS">FIG. 5A</figref>, there are three pronunciation representations [dehak] <b>51</b><i>a</i>, [dehaη(1)] <b>51</b><i>b</i>, and [dehag] <b>51</b><i>c </i>for the word “<img id="CUSTOM-CHARACTER-00060" he="3.56mm" wi="4.91mm" file="US07299178-20071120-P00001.TIF" alt="custom character" img-content="character" img-format="tif"/>[dehak]” <b>51</b> and one pronunciation representation [dehaη] <b>52</b><i>a </i>for the word “<img id="CUSTOM-CHARACTER-00061" he="3.56mm" wi="4.23mm" file="US07299178-20071120-P00002.TIF" alt="custom character" img-content="character" img-format="tif"/>[dehaη]” <b>52</b>. The (1) in the pronunciation representation [dehaη(1)] <b>51</b><i>b</i>, which is a pronunciation representation variation for the word “<img id="CUSTOM-CHARACTER-00062" he="3.56mm" wi="4.91mm" file="US07299178-20071120-P00001.TIF" alt="custom character" img-content="character" img-format="tif"/>[dehak]” <b>51</b> according to practical phonological rules, is an exemplary identifier added to discriminate it from the original pronunciation representation [dehaη] <b>52</b><i>a </i>for the word “<img id="CUSTOM-CHARACTER-00063" he="3.56mm" wi="4.23mm" file="US07299178-20071120-P00002.TIF" alt="custom character" img-content="character" img-format="tif"/>[dehaη]” <b>52</b>. Such a pronunciation representation variation with an identifier is allowed to precede only a particular pronunciation representation for the onset of the initial syllable of a subsequent word, as shown in Table 1.</p>
<p id="p-0039" num="0038">Referring to <figref idref="DRAWINGS">FIG. 5B</figref>, there are two pronunciation representations [sit] <b>53</b><i>a </i>and [sip] <b>53</b><i>b </i>for the word ‘seat’ <b>53</b> and one pronunciation representation [tip] <b>54</b><i>a </i>for the word “tip” <b>54</b>. Similar to the case of Korean words in <figref idref="DRAWINGS">FIG. 5B</figref>, the (1) in the pronunciation sequence [sip(1)] <b>53</b><i>b</i>, which is a pronunciation representation variation for the word ‘seat’ <b>53</b> according to practical phonological rules, is an exemplary identifier added to discriminate it from the original pronunciation representation [tip] <b>54</b><i>a </i>for the word “tip” <b>54</b>. The pronunciation representation [sip(1)] <b>53</b><i>b </i>can be linked to only to a subsequent word starting with the phoneme ‘b’, ‘p’, or ‘m’. For example, the pronunciation representation [sip(1)] <b>53</b><i>b </i>is not allowed to be linked to the word “down”. The pronunciation representation [tip] <b>54</b><i>a </i>can be linked to any subsequent word regardless of the onset of the initial syllable thereof.</p>
<p id="p-0040" num="0039"><figref idref="DRAWINGS">FIG. 6</figref> is for explaining an example of searching performed in the search unit <b>42</b> of <figref idref="DRAWINGS">FIG. 4</figref> using the pronunciation dictionary database <b>44</b> and the inter-word phonetic information storing unit <b>45</b>. Referring to <figref idref="DRAWINGS">FIG. 6</figref>, when it comes to the recognition of the word, “<img id="CUSTOM-CHARACTER-00064" he="3.13mm" wi="9.91mm" file="US07299178-20071120-P00003.TIF" alt="custom character" img-content="character" img-format="tif"/>[hanguk dehak i]” the word sequence is segmented in predetermined units of recognition, i.e., into separate words, “<img id="CUSTOM-CHARACTER-00065" he="3.56mm" wi="4.23mm" file="US07299178-20071120-P00004.TIF" alt="custom character" img-content="character" img-format="tif"/>[hanguk]”, “<img id="CUSTOM-CHARACTER-00066" he="3.56mm" wi="4.91mm" file="US07299178-20071120-P00001.TIF" alt="custom character" img-content="character" img-format="tif"/>[dehak]”, and “<img id="CUSTOM-CHARACTER-00067" he="3.13mm" wi="2.12mm" file="US07299178-20071120-P00005.TIF" alt="custom character" img-content="character" img-format="tif"/>[i]”, and a pronunciation representation network is operated to select the optimal pronunciation representation for each of the words. The pronunciation representation network consists of all probable pronunciation representations for words in a tree structure. Next, searching is performed on the selected pronunciation representations for the words with the probability functions Pr(<img id="CUSTOM-CHARACTER-00068" he="3.56mm" wi="4.91mm" file="US07299178-20071120-P00001.TIF" alt="custom character" img-content="character" img-format="tif"/>[dehak]|<img id="CUSTOM-CHARACTER-00069" he="3.56mm" wi="4.23mm" file="US07299178-20071120-P00004.TIF" alt="custom character" img-content="character" img-format="tif"/>[hanguk]) and Pr(<img id="CUSTOM-CHARACTER-00070" he="3.13mm" wi="2.12mm" file="US07299178-20071120-P00005.TIF" alt="custom character" img-content="character" img-format="tif"/>[i]|<img id="CUSTOM-CHARACTER-00071" he="3.13mm" wi="2.12mm" file="US07299178-20071120-P00005.TIF" alt="custom character" img-content="character" img-format="tif"/>[dehak]) with reference to the inter-word phonetic information provided by the inter-word phonetic information storing unit <b>45</b>. Based on the inter-word phonetic information, linking of the pronunciation representation [η(1)] for the coda of the last syllable of the word “<img id="CUSTOM-CHARACTER-00072" he="3.56mm" wi="4.23mm" file="US07299178-20071120-P00004.TIF" alt="custom character" img-content="character" img-format="tif"/>” to the pronunciation representation [d] for the onset of the initial syllable of the word “<img id="CUSTOM-CHARACTER-00073" he="3.56mm" wi="4.91mm" file="US07299178-20071120-P00001.TIF" alt="custom character" img-content="character" img-format="tif"/>”, linking of the pronunciation representation [η(1)] for the coda of the last syllable of the word “<img id="CUSTOM-CHARACTER-00074" he="3.56mm" wi="4.91mm" file="US07299178-20071120-P00001.TIF" alt="custom character" img-content="character" img-format="tif"/>” to the pronunciation representation [i] for the onset of the initial syllable of the word “<img id="CUSTOM-CHARACTER-00075" he="3.13mm" wi="2.12mm" file="US07299178-20071120-P00005.TIF" alt="custom character" img-content="character" img-format="tif"/>”, and linking of the pronunciation representation [k(1)] for the coda of the last syllable of the word “<img id="CUSTOM-CHARACTER-00076" he="3.56mm" wi="4.91mm" file="US07299178-20071120-P00001.TIF" alt="custom character" img-content="character" img-format="tif"/>” to the pronunciation representation [i] for the onset of the initial syllable of the word “<img id="CUSTOM-CHARACTER-00077" he="3.13mm" wi="2.12mm" file="US07299178-20071120-P00005.TIF" alt="custom character" img-content="character" img-format="tif"/>” are blocked, thereby simplifying the overall search network. In other words, when the search unit <b>42</b> performs forward and backward searching, whenever a node to node transition occurs, it is determined based on the inter-word phonetic information whether there is any pronunciation representation for the onset of the initial syllable of a word that cannot follow a pronunciation representation for the coda of the last syllable of a word according to practical phonological rules, wherein each word has at least one pronunciation representation for its onset and coda. As a result, such pronunciation representations for the coda and onset of successive words that do not comply with practical phonological rules are constrained so as not to be linked each other for a sentence. Backward searching is a process of constructing written sentences using the candidate words screened through forward searching. In this backward searching, a predetermined number, for example, <b>10</b>, of candidate written sentences where the candidate words are connected in a best way are constructed using language model probabilities. In a rescoring process, an inter-word biphone model is converted into an inter-word triphone model with respect to the <b>10</b> candidate written sentences screened through backward searching, and the probabilities for the <b>10</b> candidate written sentences are recalculated using the inter-word phonetic information and acoustic model to select the best written sentence for the input speech.</p>
<heading id="h-0005" level="1">EXPERIMENTAL EXAMPLE</heading>
<p id="p-0041" num="0040">A performance comparison test was performed using the speech recognition method according to the present invention and a conventional speech recognition method. In a quiet office environment, 600 speakers were asked to read 45000 Korean sentences as training data, and 80 speakers were asked to read 480 Korean sentences as test data. 12-dimension Mel-Frequency Cepstral Coefficients (MFCCs), and energy values and their difference coefficients were used as feature vectors for frames of data, and each frame was expressed as a 26-dimension vector. A 44-basephone, 4000-subword acoustic model of Phonetically-Tied Mixtures (PTMs) and a trigram language model were used for the experiment. 11,000 most frequently used words in political news were used and were recognized as pseudo-morphemes. The results of the performance test in the above-conditions are shown in Table 2.</p>
<p id="p-0042" num="0041">
<tables id="TABLE-US-00002" num="00002">
<table frame="none" colsep="0" rowsep="0">
<tgroup align="left" colsep="0" rowsep="0" cols="3">
<colspec colname="1" colwidth="140pt" align="center"/>
<colspec colname="2" colwidth="42pt" align="center"/>
<colspec colname="3" colwidth="35pt" align="center"/>
<thead>
<row>
<entry namest="1" nameend="3" rowsep="1">TABLE 2</entry>
</row>
</thead>
<tbody valign="top">
<row>
<entry namest="1" nameend="3" align="center" rowsep="1"/>
</row>
<row>
<entry>Speech Recognition Method</entry>
<entry/>
<entry>Sentence</entry>
</row>
</tbody>
</tgroup>
<tgroup align="left" colsep="0" rowsep="0" cols="5">
<colspec colname="1" colwidth="63pt" align="left"/>
<colspec colname="2" colwidth="35pt" align="center"/>
<colspec colname="3" colwidth="42pt" align="center"/>
<colspec colname="4" colwidth="42pt" align="center"/>
<colspec colname="5" colwidth="35pt" align="center"/>
<tbody valign="top">
<row>
<entry>Pronunciation</entry>
<entry/>
<entry>Crossword</entry>
<entry>Word Error</entry>
<entry>Error</entry>
</row>
<row>
<entry>Dictionary (PD)</entry>
<entry>Rescoring</entry>
<entry>Information</entry>
<entry>Rate (%)</entry>
<entry>Rate (%)</entry>
</row>
<row>
<entry namest="1" nameend="5" align="center" rowsep="1"/>
</row>
</tbody>
</tgroup>
<tgroup align="left" colsep="0" rowsep="0" cols="5">
<colspec colname="1" colwidth="63pt" align="left"/>
<colspec colname="2" colwidth="35pt" align="center"/>
<colspec colname="3" colwidth="42pt" align="center"/>
<colspec colname="4" colwidth="42pt" align="char" char="."/>
<colspec colname="5" colwidth="35pt" align="char" char="."/>
<tbody valign="top">
<row>
<entry>Conventional PD</entry>
<entry>X</entry>
<entry>X</entry>
<entry>10.74</entry>
<entry>53.96</entry>
</row>
<row>
<entry>Conventional PD</entry>
<entry>◯</entry>
<entry>X</entry>
<entry>9.03</entry>
<entry>48.33</entry>
</row>
<row>
<entry>PD according to the</entry>
<entry>X</entry>
<entry>X</entry>
<entry>8.87</entry>
<entry>47.50</entry>
</row>
<row>
<entry>present invention</entry>
</row>
<row>
<entry>PD according to the</entry>
<entry>◯</entry>
<entry>X</entry>
<entry>7.79</entry>
<entry>42.92</entry>
</row>
<row>
<entry>present invention</entry>
</row>
<row>
<entry>PD according to the</entry>
<entry>X</entry>
<entry>◯</entry>
<entry>7.58</entry>
<entry>41.27</entry>
</row>
<row>
<entry>present invention</entry>
</row>
<row>
<entry>PD according to the</entry>
<entry>◯</entry>
<entry>◯</entry>
<entry>6.53</entry>
<entry>40.00</entry>
</row>
<row>
<entry>present invention</entry>
</row>
<row>
<entry namest="1" nameend="5" align="center" rowsep="1"/>
</row>
</tbody>
</tgroup>
</table>
</tables>
</p>
<p id="p-0043" num="0042">Referring to Table 2 above, when using the pronunciation dictionary according to the present invention, the word error rate and sentence error rate decrease compared to using a conventional pronunciation dictionary, thereby improving speech recognition performance. It is also proved that rescoring and using crossword information values in combination with the pronunciation dictionary according to the present invention leads to best speech recognition performance.</p>
<p id="p-0044" num="0043">The invention may be embodied in a general purpose digital computer by running a program from a computer readable medium, including but not limited to storage media such as magnetic storage media (e.g., ROM's, floppy disks, hard disks, etc.), optically readable media (e.g., CD-ROMs, DVDs, etc.) and carrier waves (e.g., transmissions over the Internet). The present invention may be embodied as a computer readable medium having a computer readable program code unit embodied therein for causing a number of computer systems connected via a network to effect distributed processing. Functional programs, codes, and code segments for the implementation of the present invention can easily be inferred by programmers having skill in this field.</p>
<p id="p-0045" num="0044">As described above, a speech recognition method and system according to the present invention can be applied with more ease without increasing the number of sub-words or changing acoustic models and can improve the recognition rate by limiting unallowable word connection based on inter-word phonetic information. Also, there is no burden in decoding since practical phonological rules are not applied during recognition.</p>
<p id="p-0046" num="0045">Since context information is not constructed on a word-by-word basis, there is no increase in the sizes of a pronunciation dictionary database and a search network. According to the present invention, unlike language-dependent phonological rule applications, probable pronunciation representation variations are created for each word of a language vocabulary, and allowable pronunciation representations for successive words, which are pursuant to practical phonological rules, are automatically selected for word linking. Therefore, the prevent invention can be readily applied to various other languages in addition to Korean.</p>
<p id="p-0047" num="0046">In the drawings and specification, there have been disclosed typical preferred embodiments of the invention and, although specific terms are employed, they are used in a generic and descriptive sense only and not for purposes of limitation. Therefore, it will be understood by those of ordinary skill in the art that various changes in form and details equivalent to the above embodiments may be made therein without departing from the spirit and scope of the present invention as defined by the following claims.</p>
<?DETDESC description="Detailed Description" end="tail"?>
</description>
<us-claim-statement>What is claimed is:</us-claim-statement>
<claims id="claims">
<claim id="CLM-00001" num="00001">
<claim-text>1. A continuous speech recognition method comprising:
<claim-text>(a) constructing a pronunciation dictionary database including at least one pronunciation representation for each word which is influenced by applying phonological rules, wherein the pronunciation representation for the coda of a first word or the pronunciation representation for the onset of a second word following the first word is additionally indexed with an identifier if it does not match the phonetic pronunciation of its spelling;</claim-text>
<claim-text>(b) forming inter-word phonetic information in matrix form by combination of a number of all probable phonetic pairs, each of which is basically comprised of the coda of a first word and the onset of a second word following the first word, wherein the coda of the first word or the onset of the second word is indexed with an identifier if they undergo phonological changes; and</claim-text>
<claim-text>(c) performing speech recognition on feature vectors extracted from an input speech signal with reference to the pronunciation dictionary database and the inter-word phonetic information.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00002" num="00002">
<claim-text>2. The continuous speech recognition method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein in step (c), a pronunciation representation for the coda of a first word and a pronunciation representation for the onset of a second word following the first word, which do not comply with the phonological rules, are constrained based on the inter-word phonetic information so as not to be linked to each other.</claim-text>
</claim>
<claim id="CLM-00003" num="00003">
<claim-text>3. The continuous speech recognition method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein more than one crossword information values are assigned to each phonetic pair in a matrix.</claim-text>
</claim>
<claim id="CLM-00004" num="00004">
<claim-text>4. A computer readable medium having embodied thereon a computer program for the method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>.</claim-text>
</claim>
<claim id="CLM-00005" num="00005">
<claim-text>5. A continuous speech recognition system including an acoustic model database and a language model database which are previously established through learning, the system comprising:
<claim-text>an inter-word phonetic information storing unit which stores inter-word phonetic information by combination of all probable phonemic pairs, each of which is basically comprised of a coda of last syllable of a first word and an onset of initial syllable of a second word following the first word, wherein the coda of the first word or the onset of the second word is indexed with an identifier if it does not match the phonetic pronunciation of its spelling due to phonological interaction between the first and second words;</claim-text>
<claim-text>a pronunciation dictionary database including at least one pronunciation representation for each word based on phonological rules, wherein the pronunciation representation for the coda of a first word or the pronunciation representation for the onset of a second word following the first word is additionally indexed with an identifier if it does not match the phonetic pronunciation of the spelling;</claim-text>
<claim-text>a feature extraction unit which extracts information that is useful for recognition from an input speech signal and converts the extracted information into feature vectors; and</claim-text>
<claim-text>a search unit which searches most likely word sequences among from the feature vectors obtained in the feature extraction unit using the inter-word phonetic information and with reference to the acoustic model database, the pronunciation dictionary database, and the language model database, and outputs the most likely word sequences in text form as a recognition result.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00006" num="00006">
<claim-text>6. The continuous speech recognition system of <claim-ref idref="CLM-00005">claim 5</claim-ref>, wherein more than one crossword information values are assigned to each phonetic pair in a matrix.</claim-text>
</claim>
<claim id="CLM-00007" num="00007">
<claim-text>7. The continuous speech recognition system of <claim-ref idref="CLM-00005">claim 5</claim-ref>, wherein the search unit constrains linking between a pronunciation representation for the coda of a first word and a pronunciation representation for the onset of a second word following the first word, which do not comply with the phonological rules.</claim-text>
</claim>
<claim id="CLM-00008" num="00008">
<claim-text>8. The continuous speech recognition system of <claim-ref idref="CLM-00005">claim 5</claim-ref>, further comprising a post-processing unit which converts an intra-word biphone model into an inter-word triphone model, rescores the acoustic models for the most likely word sequences obtained in the search unit, recalculates the scores of candidate sentences, and selects the best candidate sentence as a recognition result.</claim-text>
</claim>
</claims>
</us-patent-grant>
