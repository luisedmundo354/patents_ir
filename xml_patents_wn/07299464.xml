<us-patent-grant lang="EN" dtd-version="v4.2 2006-08-23" file="US07299464-20071120.XML" status="PRODUCTION" id="us-patent-grant" country="US" date-produced="20071106" date-publ="20071120">
<us-bibliographic-data-grant>
<publication-reference>
<document-id>
<country>US</country>
<doc-number>07299464</doc-number>
<kind>B2</kind>
<date>20071120</date>
</document-id>
</publication-reference>
<application-reference appl-type="utility">
<document-id>
<country>US</country>
<doc-number>11559936</doc-number>
<date>20061115</date>
</document-id>
</application-reference>
<us-application-series-code>11</us-application-series-code>
<us-term-of-grant>
<disclaimer>
<text>This patent is subject to a terminal disclaimer.</text>
</disclaimer>
</us-term-of-grant>
<classifications-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>G</section>
<class>06</class>
<subclass>F</subclass>
<main-group>9</main-group>
<subgroup>455</subgroup>
<symbol-position>F</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20071120</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>G</section>
<class>06</class>
<subclass>F</subclass>
<main-group>9</main-group>
<subgroup>46</subgroup>
<symbol-position>L</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20071120</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>G</section>
<class>06</class>
<subclass>F</subclass>
<main-group>5</main-group>
<subgroup>00</subgroup>
<symbol-position>L</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20071120</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
</classifications-ipcr>
<classification-national>
<country>US</country>
<main-classification>718  1</main-classification>
<further-classification>718104</further-classification>
<further-classification>710 47</further-classification>
</classification-national>
<invention-title id="d0e51">System and method for transferring data between virtual machines or other computer entities</invention-title>
<references-cited>
<citation>
<patcit num="00001">
<document-id>
<country>US</country>
<doc-number>4812967</doc-number>
<kind>A</kind>
<name>Hirosawa et al.</name>
<date>19890300</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<patcit num="00002">
<document-id>
<country>US</country>
<doc-number>4835685</doc-number>
<kind>A</kind>
<name>Kun</name>
<date>19890500</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<patcit num="00003">
<document-id>
<country>US</country>
<doc-number>5446841</doc-number>
<kind>A</kind>
<name>Kitano et al.</name>
<date>19950800</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<patcit num="00004">
<document-id>
<country>US</country>
<doc-number>5953538</doc-number>
<kind>A</kind>
<name>Duncan et al.</name>
<date>19990900</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>710 22</main-classification></classification-national>
</citation>
<citation>
<patcit num="00005">
<document-id>
<country>US</country>
<doc-number>6115779</doc-number>
<kind>A</kind>
<name>Haubursin et al.</name>
<date>20000900</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<patcit num="00006">
<document-id>
<country>US</country>
<doc-number>6154832</doc-number>
<kind>A</kind>
<name>Maupin</name>
<date>20001100</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<patcit num="00007">
<document-id>
<country>US</country>
<doc-number>6332180</doc-number>
<kind>B1</kind>
<name>Kauffman et al.</name>
<date>20011200</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<patcit num="00008">
<document-id>
<country>US</country>
<doc-number>6347341</doc-number>
<kind>B1</kind>
<name>Glassen et al.</name>
<date>20020200</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<patcit num="00009">
<document-id>
<country>US</country>
<doc-number>6397350</doc-number>
<kind>B1</kind>
<name>Baskey et al.</name>
<date>20020500</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<patcit num="00010">
<document-id>
<country>US</country>
<doc-number>2002/0062401</doc-number>
<kind>A1</kind>
<name>Auslander et al.</name>
<date>20020500</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<patcit num="00011">
<document-id>
<country>US</country>
<doc-number>2003/0037178</doc-number>
<kind>A1</kind>
<name>Vessey et al.</name>
<date>20030200</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>709319</main-classification></classification-national>
</citation>
</references-cited>
<number-of-claims>9</number-of-claims>
<us-exemplary-claim>1</us-exemplary-claim>
<us-field-of-classification-search>
<classification-national>
<country>US</country>
<main-classification>718  1</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>718104</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>710 47</main-classification>
</classification-national>
</us-field-of-classification-search>
<figures>
<number-of-drawing-sheets>6</number-of-drawing-sheets>
<number-of-figures>6</number-of-figures>
</figures>
<us-related-documents>
<division>
<relation>
<parent-doc>
<document-id>
<country>US</country>
<doc-number>10280987</doc-number>
<kind>00</kind>
<date>20021024</date>
</document-id>
<parent-grant-document>
<document-id>
<country>US</country>
<doc-number>7181744</doc-number>
<kind>A </kind>
</document-id>
</parent-grant-document>
</parent-doc>
<child-doc>
<document-id>
<country>US</country>
<doc-number>11559936</doc-number>
</document-id>
</child-doc>
</relation>
</division>
<related-publication>
<document-id>
<country>US</country>
<doc-number>20070067775</doc-number>
<kind>A1</kind>
<date>20070322</date>
</document-id>
</related-publication>
</us-related-documents>
<parties>
<applicants>
<applicant sequence="001" app-type="applicant-inventor" designation="us-only">
<addressbook>
<last-name>Shultz</last-name>
<first-name>Steven S.</first-name>
<address>
<city>Endicott</city>
<state>NY</state>
<country>US</country>
</address>
</addressbook>
<nationality>
<country>omitted</country>
</nationality>
<residence>
<country>US</country>
</residence>
</applicant>
<applicant sequence="002" app-type="applicant-inventor" designation="us-only">
<addressbook>
<last-name>Tkatschow</last-name>
<first-name>Xenia</first-name>
<address>
<city>Jamesville</city>
<state>NY</state>
<country>US</country>
</address>
</addressbook>
<nationality>
<country>omitted</country>
</nationality>
<residence>
<country>US</country>
</residence>
</applicant>
</applicants>
<agents>
<agent sequence="01" rep-type="attorney">
<addressbook>
<last-name>Samodovitz</last-name>
<first-name>Arthur J.</first-name>
<address>
<country>unknown</country>
</address>
</addressbook>
</agent>
</agents>
</parties>
<assignees>
<assignee>
<addressbook>
<orgname>International Business Machines Corporation</orgname>
<role>02</role>
<address>
<city>Armonk</city>
<state>NY</state>
<country>US</country>
</address>
</addressbook>
</assignee>
</assignees>
<examiners>
<primary-examiner>
<last-name>An</last-name>
<first-name>Meng-Al T.</first-name>
<department>2195</department>
</primary-examiner>
<assistant-examiner>
<last-name>Truong</last-name>
<first-name>Camquy</first-name>
</assistant-examiner>
</examiners>
</us-bibliographic-data-grant>
<abstract id="abstract">
<p id="p-0001" num="0000">A method for communication between first and second computer programs having a shared memory. The first computer program has a first work dispatcher for a first work queue. The second computer program has a second work dispatcher for a second work queue. Without causing an interrupt, a message or data is written for the second program from the first program to the shared memory and the second work queue is updated with a work item indicating a message or data for the second program. In association with the updating step, it is determined if the second program is currently busy. If so, the second program is not interrupted regarding the message or data. When the second program subsequently becomes not busy, the second program receives, without an interrupt, and executes the work item to receive the message or data. If the second program was not currently busy, the second program is interrupted to process the message or data on its work queue. This imposes a minimal burden on the second program.</p>
</abstract>
<drawings id="DRAWINGS">
<figure id="Fig-EMI-D00000" num="00000">
<img id="EMI-D00000" he="168.15mm" wi="234.36mm" file="US07299464-20071120-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00001" num="00001">
<img id="EMI-D00001" he="237.91mm" wi="187.71mm" orientation="landscape" file="US07299464-20071120-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00002" num="00002">
<img id="EMI-D00002" he="197.27mm" wi="168.57mm" orientation="landscape" file="US07299464-20071120-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00003" num="00003">
<img id="EMI-D00003" he="201.93mm" wi="160.70mm" orientation="landscape" file="US07299464-20071120-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00004" num="00004">
<img id="EMI-D00004" he="233.93mm" wi="175.60mm" orientation="landscape" file="US07299464-20071120-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00005" num="00005">
<img id="EMI-D00005" he="202.18mm" wi="166.96mm" orientation="landscape" file="US07299464-20071120-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00006" num="00006">
<img id="EMI-D00006" he="201.17mm" wi="161.80mm" orientation="landscape" file="US07299464-20071120-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
</drawings>
<description id="description">
<?RELAPP description="Other Patent Relations" end="lead"?>
<p id="p-0002" num="0001">This is a divisional of U.S. patent application Ser. No. 10/280,987 filed on Oct. 24, 2002, now U.S. Pat. No. 7,181,744.</p>
<?RELAPP description="Other Patent Relations" end="tail"?>
<?BRFSUM description="Brief Summary" end="lead"?>
<heading id="h-0001" level="1">BACKGROUND OF THE INVENTION</heading>
<p id="p-0003" num="0002">The invention relates generally to computer systems and deals more particularly with an efficient technique for transferring messages and data between virtual machines, logical partitions or application programs.</p>
<p id="p-0004" num="0003">There are many computer environments in which two or more computer entities need to exchange messages or data. Such computer entities include virtual machines, logical partitions and applications running on a unitary operating system such as Unix or Windows NT.</p>
<p id="p-0005" num="0004">A virtual machine operating system is well known today and comprises a common base portion and separate user portions. In an IBM z/VM operating system, the common base portion is called the “Control Program” or “CP” and each user portion is called a “virtual machine” or “guest”. Many applications can run on each virtual machine. Each virtual machine has its own work dispatcher (and associated work queue) and appears to the user and his or her applications as a personal operating system. Each virtual machine executes commands on behalf of the applications they support. The different virtual machines can communicate with each other through the common base portion. The communications between the different virtual machines via CP may be in the form of messages conveyed by virtualized communication devices such as Guest Lan or IBM proprietary protocols such as IUCV. Though these communications are conveyed by a variety of protocols, all of these communication mechanisms have at least four common properties:
<ul id="ul0001" list-style="none">
    <li id="ul0001-0001" num="0000">
    <ul id="ul0002" list-style="none">
        <li id="ul0002-0001" num="0005">a) Message data is first written into the sender's virtual address space.</li>
        <li id="ul0002-0002" num="0006">b) An interrupt is generated for each message in each of the receivers' virtual machines.</li>
        <li id="ul0002-0003" num="0007">This invokes interrupt handling in each receiver virtual machine.</li>
        <li id="ul0002-0004" num="0008">c) CP must be invoked in order to accomplish the communication.</li>
        <li id="ul0002-0005" num="0009">d) CP copies message data from the sender's virtual address space to all of the receivers' virtual address spaces.
<br/>
With the foregoing communication methods there is significant overhead associated with invoking CP, generating interrupts, processing interrupts, and copying the message data from the sender's virtual address space to the virtual address space of each of the receivers.
</li>
    </ul>
    </li>
</ul>
</p>
<p id="p-0006" num="0010">The following is a more detailed description of IUCV. IUCV is an IBM proprietary point-to-point protocol. A point-to-point protocol transfers data from one sender to one receiver. To communicate via IUCV, a sender first invokes CP indicating the identity of the intended receiver of communication. CP generates an interrupt to the receiver and if the receiver agrees to communicate, CP provides the receiver with a communication path id. CP also then interrupts the sender and provides the sender with the communication path id. To send data, the sender invokes CP indicating the previously obtained path id and the data to be sent. CP uses the path id to identify the receiver and generates an interrupt to the receiver. The receiver responds to the interrupt by invoking CP to receive the data. CP then copies the data from the sender's virtual address space to the receiver's virtual address space and generates an interrupt to the sender indicating that the data has been transferred.</p>
<p id="p-0007" num="0011">The following is a more detailed description of Guest Lan. Guest Lan is a virtualized communication device using local area network (LAN) protocol. Lan protocol allows communication between a sender and multiple receivers simultaneously. To communicate via a Guest Lan, both sender and receivers invoke CP indicating that they wish to participate in the Guest Lan. To send data, the sender invokes CP indicating the data to be sent and which receivers should get the data. CP generates an interrupt for each identified receiver. The receivers each respond by invoking CP to receive the data. CP then copies the data from the sender's virtual address space to the virtual address spaces of each of the receivers. Once all receivers have received the data, CP generates an interrupt to the sender indicating that the data has been transferred to all receivers.</p>
<p id="p-0008" num="0012">A logical partition environment is also well known today. A logical partition is a logical division of resources of a single computer system, which division is accomplished by software and microcode. Each logical partition is defined by a respective configuration of CPU(s), memory and peripheral devices. An operating system running in a logical partition views its logical partition as nearly indistinguishable from a real computer, although the logical partition may provide some additional services not available on a real machine. Therefore, the operating system is largely unaware that it is running in a logical partition, and is largely unaware of other logical partitions of the same real computer. Each logical partition also has its own dispatcher, and uses interrupts to communicate messages/data from one logical partition to another as in the virtual machine environment.</p>
<p id="p-0009" num="0013">There are other known techniques for one application to communicate with another application when both applications are running on the same operating system, such as Windows NT or Unix. In this environment, the operating system utilizes the same dispatcher for both applications. According to these known communication techniques, when application “A” wants to communicate with application “B”, application A calls/notifies the supervisor within the operating system. The call includes the address of the message/data in memory accessible by application A. In response, the supervisor copies the message/data to a location that application B can access. Next, the supervisor puts a work element on the dispatch queue. The work element identifies application B as the recipient, and includes a command to fetch the message/data. Then, the dispatcher dispatches the work element to application B at a time consistent with the dispatching strategy of the operating system and the relative priorities of the work elements. The following are some of the possible, known dispatching strategies. If application B is not currently busy, then the message/data work element is dispatched to application B when the processor becomes free and/or is not occupied with processing higher priority work elements (for any application). If application B is currently busy with another, lower priority work item, then the dispatcher may substitute the message/data work item when the lower priority work item completes its allotted processor time slice or makes a call to the operating system. But, it would not be appropriate to “interrupt” the operating system to convey the message/data to application B because of the overhead involved. The sharing of the dispatcher makes this unnecessary. As noted above, virtual machine, logical partition and other environments do not have a common dispatcher.</p>
<p id="p-0010" num="0014">An object of the present invention is to provide an efficient method for communication/data transfer between (a) two different virtual machines running on the same base operating system, (b) two logical partitions of the same computer or (c) two applications running on the same computer but having different dispatchers.</p>
<p id="p-0011" num="0015">An object of the present invention is to provide an efficient method for communication/data transfer from (a) one virtual machine to two or more other virtual machines all running on the same base operating system, (b) from one logical partition to two or more other logical partitions of the same computer or (c) one application to two or more other applications running on the same computer but having different dispatchers.</p>
<heading id="h-0002" level="1">SUMMARY OF THE INVENTION</heading>
<p id="p-0012" num="0016">The invention resides in a method for communication between first and second computer programs having a shared memory. The first computer program has a first work dispatcher for a first work queue. The second computer program has a second work dispatcher for a second work queue. A message or data is written for the second program from the first program to the shared memory and the second work queue is updated with a work item indicating a message or data for the second program. In association with the updating step, it is determined if the second program is currently busy. If so, the second program is not interrupted regarding the message or data. When the second program subsequently becomes not busy, the second program receives, without an interrupt, and executes the work item to receive the message or data. If the second program was not currently busy, the second program is interrupted to process the message or data on its work queue. (This imposes a minimal burden on the second program.)</p>
<p id="p-0013" num="0017">According to another feature of the present invention, there is a method for communication between first and second virtual machines having a shared memory and a common base operating system. The first virtual machine has a first work dispatcher for a first work queue. The second virtual machine has a second work dispatcher for a second work queue. The first and second work queues reside in memory shared by both the first and second virtual machines. Without invoking the common base operating system, a message or data is written for the second virtual machine from the first virtual machine to the shared memory and the second work queue is updated with a work item indicating a message or data for the second virtual machine. Subsequently, the second virtual machine program reads the message or data from the shared memory.</p>
<?BRFSUM description="Brief Summary" end="tail"?>
<?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?>
<description-of-drawings>
<heading id="h-0003" level="1">BRIEF DESCRIPTION OF THE FIGURES</heading>
<p id="p-0014" num="0018"><figref idref="DRAWINGS">FIG. 1</figref> is a block diagram of a virtual machine operating system according to the present invention.</p>
<p id="p-0015" num="0019"><figref idref="DRAWINGS">FIG. 2</figref> is a flow chart illustrating a process implemented by a virtual machine of <figref idref="DRAWINGS">FIG. 1</figref> to receive a message or data from another virtual machine, according to the present invention.</p>
<p id="p-0016" num="0020"><figref idref="DRAWINGS">FIG. 3</figref> is a flow chart illustrating a process implemented by a virtual machine of <figref idref="DRAWINGS">FIG. 1</figref> to send a message or data to another virtual machine, according to the present invention.</p>
<p id="p-0017" num="0021"><figref idref="DRAWINGS">FIG. 4</figref> is a block diagram of a logically partitioned computer system according to the present invention.</p>
<p id="p-0018" num="0022"><figref idref="DRAWINGS">FIG. 5</figref> is a flow chart illustrating a process implemented by a logical partition of the computer system of <figref idref="DRAWINGS">FIG. 4</figref> to receive a message or data from another logical partition, according to the present invention.</p>
<p id="p-0019" num="0023"><figref idref="DRAWINGS">FIG. 6</figref> is a flow chart illustrating a process implemented by a logical partition of <figref idref="DRAWINGS">FIG. 5</figref> to send a message or data to another logical partition, according to the present invention.</p>
</description-of-drawings>
<?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?>
<?DETDESC description="Detailed Description" end="lead"?>
<heading id="h-0004" level="1">DETAILED DESCRIPTION OF THE PREFERRED EMBODIMENTS</heading>
<p id="p-0020" num="0024">Referring now to the figures in detail, wherein like reference numbers indicate like elements throughout, <figref idref="DRAWINGS">FIG. 1</figref> illustrates a virtual machine operating system generally designated <b>10</b> according to the present invention. By way of example, virtual machine operating system <b>10</b> can be IBM z/VM version 4.2.0 or 4.3.0 operating system although the present invention can be incorporated into other virtual machine and non virtual machine operating systems as well. The details of the z/VM 4.2.0 operating system are disclosed in IBM publication “z/VM 4.2.0 General Information” (Document Number: GC24-5991-03) which is available from International Business Machines Corp. at PO Box 29570, IBM Publications, Raleigh, N.C. 27626-0570 or on the WWW at www.IBM.com/shop/publications/order. This publication is hereby incorporated by reference as part of the present disclosure. Operating system <b>10</b> executes in a physical computer <b>11</b> such as an IBM zSeries mainframe although the present invention can be implemented in other server computers or personal computers as well. Operating system <b>10</b> comprises user portions <b>12</b>, <b>14</b>, <b>16</b> . . . (called “virtual machines” or “guest virtual machines” in the z/VM operating system) and common base portion <b>20</b> (called “CP” in the z/VM operating system). Each user portion <b>12</b> and <b>14</b> provides standard operating system functions such as I/O, communication, etc. Each user portion <b>12</b>, <b>14</b> and <b>16</b> is capable of concurrently executing a number of different applications such as applications <b>32</b>, <b>34</b> and <b>36</b> as shown. By way of examples, applications <b>32</b>, <b>34</b> and <b>36</b> can be TELNET, FTP and PING (and use the present invention instead of the prior art communication mechanisms). In the z/VM 4.2.0 and 4.3.0 operating systems, the Linux™ (of Linus Torvalds) operating system can also run on each virtual machine <b>12</b>, <b>14</b> and <b>16</b>, although some of the operating system functions of virtual machines <b>12</b>, <b>14</b> or <b>16</b> are not needed by the Linux operating system as they are currently provided by the Linux operating system. Although not shown, typically there are many other virtual machines and associated operating systems which also share common base portion <b>20</b>. Also, there can be multiple applications executing on each virtual machine. Base portion <b>20</b> includes known functions such as virtualized memory, virtualized devices, and virtualized CPUs.</p>
<p id="p-0021" num="0025">Computer <b>11</b> also includes memory area <b>21</b> which is shared by all of the virtual machines <b>12</b>, <b>14</b>, <b>16</b> etc. Being “shared” each virtual machine can directly address and access the shared memory area <b>21</b> to read data therefrom or write data thereto. For data requested by an application or generated by an application, the application makes the read or write request to the respective virtual machine on which it is running. This respective virtual machines accesses the shared memory on behalf of the application as explained below with reference to <figref idref="DRAWINGS">FIGS. 2 and 3</figref>. In one (of many) embodiments of the present invention, the shared memory <b>21</b> is part of a Discontiguous Saved Segment (“DCSS”) portion of the base portion <b>20</b>. DCSS is a special form of shared memory that can be dynamically loaded and unloaded. It can survive virtual machine termination and even CP termination, and can contain executable code. However, functions other than shared memory within DCSS are not needed for the present invention, so the present invention is not limited to implementations involving DCSS or its equivalents.</p>
<p id="p-0022" num="0026">Each virtual machine <b>12</b>, <b>14</b>, and <b>16</b> includes a respective read function <b>42</b><i>a</i>, <b>42</b><i>b</i>, and <b>42</b><i>c</i>, a respective write function <b>33</b><i>a</i>, <b>33</b><i>b </i>and <b>33</b><i>c </i>and a respective dispatcher <b>22</b><i>a</i>, <b>22</b><i>b </i>and <b>22</b><i>c</i>. The virtual machine calls the write function when it encounters a write command in the application it is executing. The write function is standing by, so no queue is required for the write function tasks. The write function writes data from a virtual machine to the shared memory. A write operation does not invoke CP. The virtual machine calls the read function when it encounters a read command in the application it is executing. The read function is standing by, so no queue is required for the read function tasks. The read function reads data from the shared memory. Thus, the data is not copied from the writer's virtual address space to the reader's virtual address space. Also, CP is not invoked to read from shared memory, and this reduces overhead. Each virtual machine calls/invokes its dispatcher when it completes a work item and therefore, needs another work item, if any. In response to the call, the dispatcher checks for work items on its respective queue <b>26</b><i>a</i>, <b>26</b><i>b </i>or <b>26</b><i>c </i>within shared memory <b>21</b>.</p>
<p id="p-0023" num="0027">A table <b>24</b> is also stored in shared memory <b>21</b>. The table indicates the status of each virtual machine <b>12</b>, <b>14</b>, <b>16</b>. Each virtual machine <b>12</b>, <b>14</b> and <b>16</b> also includes a respective Work Queue Management Function (“WQMF”) <b>81</b><i>a</i>, <b>81</b><i>b </i>or <b>81</b><i>c </i>which adds work items to work queues when they arise and updates the status of each virtual machine as “idle” or “not idle” as described below. Table <b>24</b> includes an identity of each virtual machine and an indication whether or not the respective virtual machine is idle. Table <b>24</b> also includes for each virtual machine, a pointer to the respective work queue <b>26</b><i>a</i>, <b>26</b><i>b </i>or <b>26</b><i>c</i>. Table <b>24</b> changes as the status changes. In the example illustrated in <figref idref="DRAWINGS">FIG. 1</figref>, currently virtual machine <b>12</b> is not idle, i.e. it is currently executing another work item/task. However, virtual machine <b>12</b> currently has nothing in its work queue <b>26</b><i>a </i>to do after completing its current work item. Virtual machine <b>14</b> is currently idle, but has a work item in its queue <b>26</b><i>b</i>. The work item in queue <b>26</b><i>b </i>is to read the contents of the shared memory beginning at location <b>24</b>D<b>00</b> and extending for the specified length. (The word “null” following the work item indicates that there are no further work items in the queue.) Virtual machine <b>16</b> currently is not idle, and has a work item in its queue <b>26</b><i>c</i>. The work item in queue <b>26</b><i>c </i>is to read the contents of the shared memory beginning at location <b>24</b>D<b>00</b> and extending for the specified length.</p>
<p id="p-0024" num="0028"><figref idref="DRAWINGS">FIG. 2</figref> is a flow chart illustrating operation of each of the dispatchers, i.e. each of the dispatchers implements the steps of <figref idref="DRAWINGS">FIG. 2</figref> separately from the other dispatchers. After a virtual machine completes each work item/task it invokes its dispatcher to look for a new work item to perform (decision <b>48</b>). In response, the dispatcher within the virtual machine checks the respective work queue (work queue <b>26</b><i>a </i>for dispatcher <b>22</b><i>a</i>, work queue <b>26</b><i>b </i>for dispatcher <b>22</b><i>b </i>and work queue <b>26</b><i>c </i>for dispatcher <b>26</b><i>c</i>) for a work item (step <b>50</b>). If there is a work item in the queue (decision <b>52</b>), then the dispatcher parses the work item to determine its nature and what function to call to perform the work item. In the case of a read request, the dispatcher calls the read function to read the message/data at the location indicated by the work item. Thus, this read can be accomplished without the generation of an interrupt and without invoking interrupt handling. Then, the dispatcher loops back to decision <b>52</b> to check the work queue again. If during any iteration of decision <b>52</b>, there is no work item in the work queue, then the dispatcher sets the status field in the table <b>24</b> as “idle” for the respective virtual machine (step <b>60</b>). Then, the dispatcher notifies the virtual machine to enter into a wait state (step <b>62</b>). In this wait state, the virtual machine is in a “sleeping” or “idle” mode where it is not executing any work items for an application or itself. The virtual machine will remain in this wait state until receiving an interrupt indicative of a new work item in its work queue (decision <b>66</b>). When such an interrupt is received, the WQMF for the virtual machine sets the status field in the table <b>14</b> as “non idle” for the respective virtual machine (step <b>68</b>). Next, the dispatcher loops back to decision <b>52</b> to check the work queue for a work item. At this time, there should be a work item in the work queue.</p>
<p id="p-0025" num="0029"><figref idref="DRAWINGS">FIG. 3</figref> illustrates operation of one of the virtual machines, for example virtual machine <b>12</b> when it desires to send a message/data to another of the virtual machines, for example virtual machine <b>14</b>. In step <b>80</b>, virtual machine <b>12</b> calls its write function <b>33</b><i>a </i>to write data to the shared memory <b>21</b>. As explained above, each of the virtual machines has direct access to the shared memory by providing the appropriate address. So, the write function <b>33</b><i>a </i>of virtual machine <b>12</b> writes the data to the shared memory by specifying the address to be written and furnishing the data to be written. Next, Work Queue Management function (“WQMF”) <b>81</b><i>a </i>within virtual machine <b>12</b> adds a work item to the work queue <b>26</b><i>b </i>of virtual machine <b>14</b>, by writing the work item onto the work queue (step <b>82</b>). Because the work queue is in shared memory, this does not require invocation of CP. Next, WQMF <b>81</b><i>a </i>determines if virtual machine <b>14</b> is currently idle by checking the table <b>24</b> (decision <b>84</b>). If not, then virtual machine <b>12</b> does nothing further to complete this communication and CP is not invoked at any point in the communication process (termination step <b>86</b>). In accordance with the present invention, virtual machine <b>12</b> does not interrupt virtual machine <b>14</b> because of the overhead involved in interrupting the virtual machine. As explained above with reference to <figref idref="DRAWINGS">FIG. 2</figref>, when virtual machine <b>14</b> completes its current work item, it will automatically invoke/call its dispatcher to check its work queue for another work item (decision <b>48</b> and step <b>50</b>). At that time it will see the work item from virtual machine <b>12</b>. Referring again to decision <b>84</b>, if virtual machine <b>14</b> is idle, then in accordance with the present invention, virtual machine <b>12</b> issues a “wakening” type of interrupt to virtual machine <b>14</b> (step <b>88</b>). This requires invocation of CP. The wakening type of interrupt alerts/invokes virtual machine <b>14</b> that there is a work item in its queue <b>26</b><i>b</i>. With the issuance of this interrupt, virtual machine <b>12</b> has completed its part of the data communication. The “wakening” interrupt automatically causes virtual machine <b>14</b> to activate its dispatcher <b>22</b><i>b </i>(decision <b>48</b> of <figref idref="DRAWINGS">FIG. 2</figref>) to check its work queue for a work item. Dispatcher <b>22</b><i>b </i>then implements the steps illustrated in <figref idref="DRAWINGS">FIG. 2</figref> to check its work queue <b>26</b><i>b </i>(step <b>50</b> and decision <b>52</b>) and then read the data with read function <b>42</b>(<i>b</i>) (step <b>54</b>).</p>
<p id="p-0026" num="0030"><figref idref="DRAWINGS">FIG. 3</figref> also illustrates operation of one of the virtual machines, for example virtual machine <b>12</b> when it desires to communicate with two or more other virtual machines, for example virtual machines <b>14</b> and <b>16</b>. In step <b>80</b>, virtual machine <b>12</b> calls its write function <b>32</b><i>a </i>to write data to the shared memory <b>21</b>. So, virtual machine <b>12</b> writes the data to the shared memory by specifying the address to be written and furnishing the data to be written. In the example illustrated in <figref idref="DRAWINGS">FIG. 1</figref>, the data was written to shared memory locations beginning at address <b>24</b>D<b>00</b>. Next, WQMF <b>81</b><i>a </i>within virtual machine <b>12</b> adds a work item to the work queues <b>26</b><i>b </i>and <b>26</b><i>c </i>of virtual machines <b>14</b> and <b>16</b>, by writing the work item, data address and data length onto the work queues (step <b>82</b>). Next, WQMF <b>81</b><i>a </i>within virtual machine <b>12</b> determines if virtual machines <b>14</b> and <b>16</b> are currently idle by checking the table <b>24</b> (decision <b>84</b>). In the example illustrated in <figref idref="DRAWINGS">FIG. 1</figref>, virtual machine <b>14</b> is idle but virtual machine <b>16</b> is busy. So, for virtual machine <b>16</b> which is busy, virtual machine <b>12</b> does nothing further to complete the communication (termination step <b>86</b>). In accordance with the present invention, virtual machine <b>12</b> does not interrupt the busy virtual machine <b>16</b> because of the overhead involved in interrupting a virtual machine. As explained above with reference to <figref idref="DRAWINGS">FIG. 2</figref>, when the busy virtual machine <b>16</b> completes its current work item, it will automatically check its work queue for another work item (decision <b>48</b> and step <b>50</b>). At that time it will see the work item from virtual machine <b>12</b> and the communication will be completed without invocation of CP. Referring again to decision <b>84</b>, because virtual machine <b>14</b> is idle, then in accordance with the present invention, virtual machine <b>12</b> issues a “wakening” type of interrupt to the idle virtual machine <b>14</b> (step <b>88</b>). The wakening type of interrupt alerts/invokes the idle virtual machine <b>14</b> that there is a work item in its queue. With the issuance of this interrupt, virtual machine <b>12</b> has completed its part of the data communication. The “wakening” interrupt automatically causes the idle virtual machine <b>14</b> to invoke/call its dispatcher <b>22</b><i>b </i>to check its work queue for a work item. Dispatcher <b>22</b><i>b </i>then implements the steps illustrated in <figref idref="DRAWINGS">FIG. 2</figref> to check its work queue <b>26</b><i>b </i>(decision <b>52</b>) and then read the data (step <b>54</b>).</p>
<p id="p-0027" num="0031"><figref idref="DRAWINGS">FIG. 4</figref> illustrates a logically partitioned computer system generally designated <b>110</b> according to the present invention. System <b>10</b> is a logical partition of a physical computer <b>11</b> such as an IBM zSeries mainframe although the present invention can be implemented in other server computers or personal computers as well. System <b>110</b> comprises logical partitions <b>112</b>, <b>114</b>, <b>116</b>. Each logical partition <b>112</b>, <b>114</b> and <b>116</b> provides standard operating system functions such as I/O, communication, etc. to its applications. Each logical partition <b>112</b>, <b>114</b> and <b>116</b> is capable of concurrently executing a number of different applications such as applications <b>132</b>, <b>134</b> and <b>136</b> as shown. By way of examples, applications <b>132</b>, <b>134</b> and <b>136</b> can be Telnet, FTP and Ping (and use the present invention instead of the prior art communication mechanisms). Base portion <b>120</b> participates in the actual logical partitioning of the computer <b>111</b> and its resources, i.e. partitions the CPU(s), partitions memory, partitions I/O, etc. The functions of one example of base portion <b>120</b> and logical partitions <b>112</b>, <b>114</b> and <b>116</b>, aside from the present invention, are described in a document entitled “Enterprise System/9000 9221 Processors: Operating Your System—Volume 2 (Logically Partitioned Mode)”, Publication # SA24-4351-02, which document is available International Business Machines at PO Box 29570, IBM Publications, Raleigh, N.C. 27626-0570 or on the WWW at www.IBM.com/shop/publications/order.</p>
<p id="p-0028" num="0032">Computer <b>111</b> also includes memory area <b>121</b> which is shared by all of the logical partitions <b>112</b>, <b>114</b>, <b>116</b> etc. Being “shared” each logical partition can directly address and access the shared memory area <b>121</b> to read data therefrom or write data thereto. For data requested by an application or generated by an application, the application makes the read or write request to the respective logical partition on which it is running. This respective logical partition accesses the shared memory on behalf of the application as explained below with reference to <figref idref="DRAWINGS">FIGS. 5 and 6</figref>.</p>
<p id="p-0029" num="0033">Each logical partition <b>112</b>, <b>114</b>, and <b>116</b> includes a respective read function <b>142</b><i>a</i>, <b>142</b><i>b</i>, and <b>142</b><i>c</i>, a respective write function <b>133</b><i>a</i>, <b>133</b><i>b </i>and <b>133</b><i>c </i>and a respective dispatcher <b>122</b><i>a</i>, <b>122</b><i>b </i>and <b>122</b><i>c</i>. The logical partition calls the write function when it encounters a write command in the application it is executing. The write function is standing by, so no queue is required for the write function tasks. The write function writes data from a logical partition to the shared memory, and therefore does not invoke base portion <b>120</b>. The logical partition calls the read function when it encounters a read command in the application it is executing. The read function is standing by, so no queue is required for the read function tasks. The read function reads data from the shared memory, and therefore does not invoke base portion <b>120</b>. Also, the data is not copied from the writer's virtual address space to the reader's virtual address space. Each logical partition calls/invokes its dispatcher when it completes a work item and therefore, needs another work item, if any. In response to the call, the dispatcher checks for work items on its respective queue <b>126</b><i>a</i>, <b>126</b><i>b </i>or <b>126</b><i>c </i>within shared memory <b>121</b>.</p>
<p id="p-0030" num="0034">A table <b>124</b> is also stored in shared memory <b>121</b>. The table indicates the status of each logical partition <b>112</b>, <b>114</b>, <b>116</b>. Each logical partition <b>112</b>, <b>114</b> and <b>116</b> also includes a respective WQMF <b>181</b><i>a</i>, <b>181</b><i>b </i>or <b>181</b><i>c </i>which adds work items to work queues when they arise and updates the status of each logical partition as “idle” or “not idle” as described below. Table <b>124</b> includes an identity of each logical partition and an indication whether or not the respective logical partition is idle. Table <b>124</b> also includes for each logical partition, a pointer to the respective work queue <b>126</b><i>a</i>, <b>126</b><i>b </i>or <b>126</b><i>c</i>. Table <b>124</b> changes as the status changes. In the example illustrated in <figref idref="DRAWINGS">FIG. 4</figref>, currently logical partition <b>112</b> is not idle, i.e. it is currently executing another work item/task. However, logical partition <b>112</b> currently has nothing in its work queue <b>126</b><i>a </i>to do after completing its current work item. Logical partition <b>114</b> is currently idle, but has a work item in its queue <b>126</b><i>b</i>. The work item in queue <b>126</b><i>b </i>is to read the contents of the shared memory beginning at location <b>24</b>D<b>00</b> and extending for the specified length. (The word “null” following the work item indicates that there are no further work items in the queue.) Logical partition <b>116</b> currently is not idle, and has a work item in its queue <b>126</b><i>c</i>. The work item in queue <b>126</b><i>c </i>is to read the contents of the shared memory beginning at location <b>24</b>D<b>00</b> and extending for the specified length.</p>
<p id="p-0031" num="0035"><figref idref="DRAWINGS">FIG. 5</figref> is a flow chart illustrating operation of each of the dispatchers, i.e. each of the dispatchers implements the steps of <figref idref="DRAWINGS">FIG. 5</figref> separately from the other dispatchers. After a logical partition completes each work item/task it invokes its dispatcher to look for a new work item to perform (decision <b>148</b>). In response, the dispatcher within the logical partition checks the respective work queue (work queue <b>126</b><i>a </i>for dispatcher <b>122</b><i>a</i>, work queue <b>126</b><i>b </i>for dispatcher <b>122</b><i>b </i>and work queue <b>126</b><i>c </i>for dispatcher <b>126</b><i>c</i>) for a work item (step <b>150</b>). If there is a work item in the queue (decision <b>152</b>), then the dispatcher parses the work item to determine its nature and what function to call to perform the work item. In the case of a read request, the dispatcher calls the read function to read the message/data at the location indicated by the work item. Thus, this read can be accomplished without the generation of an interrupt and without invoking interrupt handling. Then, the dispatcher loops back to decision <b>152</b> to check the work queue again. If during any iteration of decision <b>152</b>, there is no work item in the work queue, then the dispatcher sets the status field in the table <b>124</b> as “idle” for the respective logical partition (step <b>160</b>). Then, the dispatcher notifies the logical partition to enter into a wait state (step <b>162</b>). In this wait state, the logical partition is in a “sleeping” or “idle” mode where it is not executing any work items for an application or itself. The logical partition will remain in this wait state until receiving an interrupt indicative of a new work item in its work queue (decision <b>166</b>). When such an interrupt is received, the WQMF for the logical partition sets the status field in the table <b>114</b> as “non idle” for the respective logical partition (step <b>168</b>). Next, the dispatcher loops back to decision <b>152</b> to check the work queue for a work item. At this time, there should be a work item in the work queue.</p>
<p id="p-0032" num="0036"><figref idref="DRAWINGS">FIG. 6</figref> illustrates operation of one of the logical partitions, for example logical partition <b>112</b> when it desires to send a message/data to another of the logical partition, for example logical partition <b>114</b>. In step <b>180</b>, logical partition <b>112</b> calls its write function <b>133</b><i>a </i>to write data to the shared memory <b>121</b>. As explained above, each of the logical partitions has direct access to the shared memory by providing the appropriate address. So, the write function <b>133</b><i>a </i>of logical partition <b>112</b> writes the data to the shared memory by specifying the address to be written and furnishing the data to be written. Next, WQMF <b>181</b><i>a </i>within logical partition <b>112</b> adds a work item to the work queue <b>126</b><i>b </i>of logical partition <b>114</b>, by writing the work item onto the work queue (step <b>182</b>). Next, WQMF <b>181</b><i>a </i>determines if logical partition <b>114</b> is currently idle by checking the table <b>124</b> (decision <b>184</b>). If not, then the logical partition does nothing further to complete this communication and the base portion <b>120</b> is not invoked at any point in the communication process (termination step <b>186</b>). In accordance with the present invention, logical partition <b>112</b> does not interrupt logical partition <b>114</b> because of the overhead involved in interrupting the logical partition. As explained above with reference to <figref idref="DRAWINGS">FIG. 5</figref>, when logical partition <b>114</b> completes its current work item, it will automatically invoke/call its dispatcher to check its work queue for another work item (decision <b>148</b> and step <b>150</b>). At that time it will see the work item from logical partition <b>112</b>. Referring again to decision <b>184</b>, if logical partition <b>114</b> is idle, then in accordance with the present invention, logical partition <b>112</b> issues a “wakening” type of interrupt to logical partition <b>114</b> (step <b>188</b>). The wakening type of interrupt alerts/invokes logical partition <b>114</b> that there is a work item in its queue <b>126</b><i>b</i>. With the issuance of this interrupt, logical partition <b>112</b> has completed its part of the data communication. The “wakening” interrupt automatically causes logical partition <b>114</b> to activate its dispatcher <b>122</b><i>b </i>(decision <b>148</b> of <figref idref="DRAWINGS">FIG. 5</figref>) to check its work queue for a work item. Dispatcher <b>122</b><i>b </i>then implements the steps illustrated in <figref idref="DRAWINGS">FIG. 5</figref> to check its work queue <b>126</b><i>b </i>(step <b>150</b> and decision <b>152</b>) and then read the data with read function <b>142</b>(<i>b</i>) (step <b>154</b>).</p>
<p id="p-0033" num="0037"><figref idref="DRAWINGS">FIG. 6</figref> also illustrates operation of one of the logical partitions, for example logical partition <b>112</b> when it desires to communicate with two or more other logical partitions, for example logical partitions <b>114</b> and <b>116</b>. In step <b>80</b>, logical partition <b>112</b> calls its write function <b>132</b><i>a </i>to write data to the shared memory <b>121</b>. So, logical partition <b>112</b> writes the data to the shared memory by specifying the address to be written and furnishing the data to be written. In the example illustrated in <figref idref="DRAWINGS">FIG. 4</figref>, the data was written to shared memory locations beginning at address <b>24</b>D<b>00</b>. Next, WQMF <b>81</b><i>a </i>within logical partition <b>112</b> adds a work item to the work queues <b>126</b><i>b </i>and <b>126</b><i>c </i>of logical partitions <b>114</b> and <b>116</b>, by writing the work item, data address and data length onto the work queues (step <b>182</b>). Next, WQMF <b>181</b><i>a </i>within logical partition <b>112</b> determines if logical partitions <b>114</b> and <b>116</b> are currently idle by checking the table <b>124</b> (decision <b>184</b>). In the example illustrated in <figref idref="DRAWINGS">FIG. 4</figref>, logical partition <b>114</b> is idle but logical partition <b>116</b> is busy. So, for logical partition <b>116</b> which is busy, logical partition <b>112</b> does nothing further to complete the communication (termination step <b>186</b>). In accordance with the present invention, logical partition <b>112</b> does not interrupt the busy logical partition <b>116</b> because of the overhead involved in interrupting a logical partition. As explained above with reference to <figref idref="DRAWINGS">FIG. 5</figref>, when the busy logical partition <b>116</b> completes its current work item, it will automatically check its work queue for another work item (decision <b>148</b> and step <b>150</b>). At that time it will see the work item from logical partition <b>112</b> and the communication will be completed without invocation of base portion <b>120</b>. Referring again to decision <b>184</b>, because logical partition <b>114</b> is idle, then in accordance with the present invention, logical partition <b>112</b> issues a “wakening” type of interrupt to the idle logical partition <b>114</b> (step <b>188</b>). The wakening type of interrupt alerts/invokes the idle logical partition <b>114</b> that there is a work item in its queue. With the issuance of this interrupt, logical partition <b>112</b> has completed its part of the data communication. The “wakening” interrupt automatically causes the idle logical partition <b>114</b> to invoke/call its dispatcher <b>122</b><i>b </i>to check its work queue for a work item. Dispatcher <b>122</b><i>b </i>then implements the steps illustrated in <figref idref="DRAWINGS">FIG. 5</figref> to check its work queue <b>126</b><i>b </i>(decision <b>152</b>) and then read the data (step <b>154</b>).</p>
<?DETDESC description="Detailed Description" end="tail"?>
</description>
<us-claim-statement>The invention claimed is:</us-claim-statement>
<claims id="claims">
<claim id="CLM-00001" num="00001">
<claim-text>1. A system for communication between a first computer program in a first logical partition (“LPAR”) and a second computer program in a second LPAR, said system comprising:
<claim-text>a real computer having first and second LPARs and a memory shared by said first and second LPARs, said first LPAR including a first work dispatcher for a first work queue in the shared memory, said second LPAR including a second work dispatcher for a second work queue in said shared memory;</claim-text>
<claim-text>said first computer program including means for writing a message or data from said first LPAR to said shared memory for said second computer program, said first LPAR including means, responsive to said message or data being written to said shared memory, for updating said second work queue with a first work item indicating said message or data for said second computer program, determining if said second LPAR is currently busy with another work item, and</claim-text>
<claim-text>if so, not interrupting said second LPAR regarding said message or data, and in response to said second LPAR subsequently becoming available to process said message or data, said second work dispatcher including means for detecting said first work item on said second work queue and notifying said second computer program to read said message or data from said shared memory,</claim-text>
<claim-text>if not, interrupting said second LPAR to invoke said second work dispatcher to detect said first work item on said second work queue and notify said second computer program to read said message or data from said shared memory.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00002" num="00002">
<claim-text>2. A system as set forth in <claim-ref idref="CLM-00001">claim 1</claim-ref> wherein:
<claim-text>said first computer program is a first application program in said first LPAR; and</claim-text>
<claim-text>said second computer program is a second application program in said second LPAR.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00003" num="00003">
<claim-text>3. A system as set forth in <claim-ref idref="CLM-00001">claim 1</claim-ref> wherein:
<claim-text>said real computer also has a third LPAR, and said memory is shared by said first, second and third LPARs;</claim-text>
<claim-text>said first computer program includes means for writing said message or data to said shared memory for both said second computer program in said second LPAR and a third computer program in said third LPAR;</claim-text>
<claim-text>said first LPAR includes means, responsive to said message or data being written to said shared memory for said third computer program, for updating a third work queue in said shared memory for said third LPAR with a third work item indicating said message or data for said third computer program, determining if said third LPAR is currently busy with another work item, and</claim-text>
<claim-text>if so, not interrupting said third LPAR regarding said message or data, and in response to said third LPAR subsequently becoming available to process said message or data, said third work dispatcher including means for detecting said third work item on said third work queue and notifying said third computer program to read said message or data from said shared memory,</claim-text>
<claim-text>if not, interrupting said third LPAR to invoke said third work dispatcher to detect said third work item on said third work queue and notify said third computer program to read said message or data from said shared memory.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00004" num="00004">
<claim-text>4. A computer program product for communication between a first computer program in a first logical partition (“LPAR”) in a real computer and a second computer program in a second LPAR in said real computer, said real computer having a memory shared by said first and second LPARs, said first computer program programmed to write a message or data from said first LPAR to said shared memory for said second computer program, said first LPAR including a first work dispatcher for a first work queue in the shared memory, said second LPAR including a second work dispatcher for a second work queue in said shared memory; said computer program product comprising:
<claim-text>a computer readable media;</claim-text>
<claim-text>first program instructions for execution in said first LPAR, responsive to said message or data being written to said shared memory, to update said second work queue with a first work item indicating said message or data for said second computer program, determine if said second LPAR is currently busy with another work item, and</claim-text>
<claim-text>if so, not interrupt said second LPAR regarding said message or data such that in response to said second LPAR subsequently becoming available to process said message or data said second work dispatcher detecting said first work item on said second work queue and said second work dispatcher notifying said second computer program to read said message or data from said shared memory,</claim-text>
<claim-text>if not, interrupt said second LPAR to invoke said second work dispatcher to detect said first work item on said second work queue and notify said second computer program to read said message or data from said shared memory; and wherein</claim-text>
<claim-text>said first program instructions are stored on said media in functional form.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00005" num="00005">
<claim-text>5. A computer program product as set forth in <claim-ref idref="CLM-00004">claim 4</claim-ref> wherein:
<claim-text>said first computer program is a first application program in said first LPAR; and</claim-text>
<claim-text>said second computer program is a second application program in said second LPAR.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00006" num="00006">
<claim-text>6. A computer program product as set forth in <claim-ref idref="CLM-00004">claim 4</claim-ref> wherein:
<claim-text>said real computer also has a third LPAR, and said memory is shared by said first, second and third LPARs;</claim-text>
<claim-text>said first computer program writes said message or data to said shared memory for both said second computer program in said second LPAR and a third computer program in said third LPAR;</claim-text>
<claim-text>said first LPAR also includes second program instructions, responsive to said message or data being written to said shared memory for said third computer program, to update a third work queue in said shared memory for said third LPAR with a third work item indicating said message or data for said third computer program, determine if said third LPAR is currently busy with another work item, and</claim-text>
<claim-text>if so, not interrupt said third LPAR regarding said message or data, such that in response to said third LPAR subsequently becoming available to process said message or data said third work dispatcher detecting said third work item on said third work queue and notifying said third computer program to read said message or data from said shared memory,</claim-text>
<claim-text>if not, interrupt said third LPAR to invoke said third work dispatcher to detect said third work item on said third work queue and notify said third computer program to read said message or data from said shared memory; and wherein</claim-text>
</claim-text>
<claim-text>said second program instructions are stored on said media in functional form.</claim-text>
</claim>
<claim id="CLM-00007" num="00007">
<claim-text>7. A system for communication between a first computer program in a first virtual machine in a real computer and a second computer program in a second virtual machine in said real computer, said system comprising:
<claim-text>said real computer having a memory shared by said first and second virtual machines, said first virtual machine including a first work dispatcher for a first work queue in the shared memory, said second virtual machine including a second work dispatcher for a second work queue in said shared memory;</claim-text>
<claim-text>said first computer program including means for writing a message or data from said first virtual machine to said shared memory for said second computer program, said first virtual machine including means, responsive to said message or data being written to said shared memory, for updating said second work queue with a first work item indicating said message or data for said second computer program, determining if said second virtual machine is currently busy with another work item, and</claim-text>
<claim-text>if so, not interrupting said second virtual machine regarding said message or data, and in response to said second virtual machine subsequently becoming available to process said message or data, said second work dispatcher including means for detecting said first work item on said second work queue and notifying said second computer program to read said message or data from said shared memory,</claim-text>
<claim-text>if not, interrupting said second virtual machine to invoke said second work dispatcher to detect said first work item on said second work queue and notify said second computer program to read said message or data from said shared memory.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00008" num="00008">
<claim-text>8. A system as set forth in <claim-ref idref="CLM-00007">claim 7</claim-ref> wherein:
<claim-text>said first computer program is a first application program in said first virtual machine; and</claim-text>
<claim-text>said second computer program is a second application program in said second virtual machine.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00009" num="00009">
<claim-text>9. A system as set forth in <claim-ref idref="CLM-00007">claim 7</claim-ref> wherein:
<claim-text>said real computer also has a third virtual machine, and said memory is shared by said first, second and third virtual machines;</claim-text>
<claim-text>said first computer program includes means for writing said message or data to said shared memory for both said second computer program in said second virtual machine and a third computer program in said third virtual machine;</claim-text>
<claim-text>said first virtual machine includes means, responsive to said message or data being written to said shared memory for said third computer program, for updating a third work queue in said shared memory for said third virtual machine with a third work item indicating said message or data for said third computer program, determining if said third virtual machine is currently busy with another work item, and</claim-text>
<claim-text>if so, not interrupting said third virtual machine regarding said message or data, and in response to said third virtual machine subsequently becoming available to process said message or data, said third work dispatcher including means for detecting said third work item on said third work queue and notifying said third computer program to read said message or data from said shared memory,</claim-text>
</claim-text>
<claim-text>if not, interrupting said third virtual machine to invoke said third work dispatcher to detect said third work item on said third work queue and notify said third computer program to read said message or data from said shared memory.</claim-text>
</claim>
</claims>
</us-patent-grant>
