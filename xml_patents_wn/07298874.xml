<us-patent-grant lang="EN" dtd-version="v4.2 2006-08-23" file="US07298874-20071120.XML" status="PRODUCTION" id="us-patent-grant" country="US" date-produced="20071106" date-publ="20071120">
<us-bibliographic-data-grant>
<publication-reference>
<document-id>
<country>US</country>
<doc-number>07298874</doc-number>
<kind>B2</kind>
<date>20071120</date>
</document-id>
</publication-reference>
<application-reference appl-type="utility">
<document-id>
<country>US</country>
<doc-number>10656921</doc-number>
<date>20030905</date>
</document-id>
</application-reference>
<us-application-series-code>10</us-application-series-code>
<priority-claims>
<priority-claim sequence="01" kind="national">
<country>KR</country>
<doc-number>2001-11441</doc-number>
<date>20010306</date>
</priority-claim>
</priority-claims>
<us-term-of-grant>
<us-term-extension>450</us-term-extension>
</us-term-of-grant>
<classifications-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>G</section>
<class>06</class>
<subclass>K</subclass>
<main-group>9</main-group>
<subgroup>00</subgroup>
<symbol-position>F</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20071120</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
</classifications-ipcr>
<classification-national>
<country>US</country>
<main-classification>382118</main-classification>
<further-classification>382117</further-classification>
</classification-national>
<invention-title id="d0e71">Iris image data processing for use with iris recognition system</invention-title>
<references-cited>
<citation>
<patcit num="00001">
<document-id>
<country>US</country>
<doc-number>4641349</doc-number>
<kind>A</kind>
<name>Flom et al.</name>
<date>19870200</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<patcit num="00002">
<document-id>
<country>US</country>
<doc-number>5291560</doc-number>
<kind>A</kind>
<name>Daugman</name>
<date>19940300</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>382117</main-classification></classification-national>
</citation>
<citation>
<patcit num="00003">
<document-id>
<country>US</country>
<doc-number>5572596</doc-number>
<kind>A</kind>
<name>Wildes et al.</name>
<date>19961100</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<patcit num="00004">
<document-id>
<country>WO</country>
<doc-number>WO 94/09446</doc-number>
<date>19940400</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
</references-cited>
<number-of-claims>38</number-of-claims>
<us-exemplary-claim>1</us-exemplary-claim>
<us-field-of-classification-search>
<classification-national>
<country>US</country>
<main-classification>382118</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>382117</main-classification>
</classification-national>
</us-field-of-classification-search>
<figures>
<number-of-drawing-sheets>7</number-of-drawing-sheets>
<number-of-figures>7</number-of-figures>
</figures>
<us-related-documents>
<continuation>
<relation>
<parent-doc>
<document-id>
<country>US</country>
<doc-number>PCT/KR01/01302</doc-number>
<kind>00</kind>
<date>20010731</date>
</document-id>
<parent-status>PENDING</parent-status>
</parent-doc>
<child-doc>
<document-id>
<country>US</country>
<doc-number>10656921</doc-number>
</document-id>
</child-doc>
</relation>
</continuation>
<related-publication>
<document-id>
<country>US</country>
<doc-number>20040114782</doc-number>
<kind>A1</kind>
<date>20040617</date>
</document-id>
</related-publication>
</us-related-documents>
<parties>
<applicants>
<applicant sequence="001" app-type="applicant-inventor" designation="us-only">
<addressbook>
<last-name>Cho</last-name>
<first-name>Seong-Won</first-name>
<address>
<city>Seoul</city>
<country>KR</country>
</address>
</addressbook>
<nationality>
<country>KR</country>
</nationality>
<residence>
<country>KR</country>
</residence>
</applicant>
</applicants>
<agents>
<agent sequence="01" rep-type="attorney">
<addressbook>
<orgname>Knobbe Martens Olson &amp; Bear LLP</orgname>
<address>
<country>unknown</country>
</address>
</addressbook>
</agent>
</agents>
</parties>
<assignees>
<assignee>
<addressbook>
<orgname>Senga Advisors, LLC</orgname>
<role>02</role>
<address>
<city>Boston</city>
<state>MA</state>
<country>US</country>
</address>
</addressbook>
</assignee>
</assignees>
<examiners>
<primary-examiner>
<last-name>Mehta</last-name>
<first-name>Bhavesh M</first-name>
<department>2624</department>
</primary-examiner>
<assistant-examiner>
<last-name>Schaffer</last-name>
<first-name>Jonathan</first-name>
</assistant-examiner>
</examiners>
</us-bibliographic-data-grant>
<abstract id="abstract">
<p id="p-0001" num="0000">The present invention relates to an iris recognition method which is one of biometric technologies. According to a non-contact-type human iris recognition method by correction of a rotated iris image of the present invention, the iris image is acquired by image acquisition equipment using an infrared illuminator. Inner and outer boundaries of the iris are detected by analyzing differences in pixels of a Canny edge detector and the image for the inputted iris image, so as to allow the boundaries of the iris to be more accurately detected from the eye image of a user. Thus, the iris image with a variety of deformation can be processed into a correct iris image, so that there is an advantage in that a false acceptance rate and a false rejection rate can be markedly reduce.</p>
</abstract>
<drawings id="DRAWINGS">
<figure id="Fig-EMI-D00000" num="00000">
<img id="EMI-D00000" he="78.74mm" wi="104.73mm" file="US07298874-20071120-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00001" num="00001">
<img id="EMI-D00001" he="203.79mm" wi="123.78mm" file="US07298874-20071120-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00002" num="00002">
<img id="EMI-D00002" he="61.72mm" wi="143.00mm" file="US07298874-20071120-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00003" num="00003">
<img id="EMI-D00003" he="98.13mm" wi="121.84mm" file="US07298874-20071120-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00004" num="00004">
<img id="EMI-D00004" he="89.58mm" wi="117.52mm" file="US07298874-20071120-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00005" num="00005">
<img id="EMI-D00005" he="138.77mm" wi="120.82mm" file="US07298874-20071120-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00006" num="00006">
<img id="EMI-D00006" he="100.25mm" wi="142.75mm" file="US07298874-20071120-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00007" num="00007">
<img id="EMI-D00007" he="201.25mm" wi="135.55mm" file="US07298874-20071120-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
</drawings>
<description id="description">
<?RELAPP description="Other Patent Relations" end="lead"?>
<heading id="h-0001" level="1">RELATED APPLICATIONS</heading>
<p id="p-0002" num="0001">This application is a continuing application under 35 U.S.C. ยง 365 (c) claiming the benefit of the filing date of PCT Application No. PCT/KR01/01302 designating the United States, filed Jul. 31, 2001. The PCT Application was published in English as WO 02/071316 A1 on Sep. 12, 2002, and claims the benefit of the earlier filing date of Korean Patent Application No. 2001/11441, filed Mar. 6, 2001. The contents of the Korean Patent Application No. 2001/11441 and the international application No. PCT/KR01/01302 including the publication WO 02/071316 A1 are incorporated herein by reference in their entirety.</p>
<?RELAPP description="Other Patent Relations" end="tail"?>
<?BRFSUM description="Brief Summary" end="lead"?>
<heading id="h-0002" level="1">FIELD OF INVENTION</heading>
<p id="p-0003" num="0002">The present invention relates to a non-contact-type human iris recognition method by correction of a rotated iris image. More particularly, the present invention relates to a non-contact-type human iris recognition method by correction of a rotated iris image, wherein the iris image is acquired by image acquisition equipment using an infrared illuminator, wherein inner and outer boundaries of the iris are detected by analyzing differences in pixels of a Canny edge detector and the image for the inputted iris image, so as to allow the boundaries of the iris to be more accurately detected from the eye image of a user, wherein if the iris in the eye image acquired by the image acquisition equipment has been rotated at an arbitrary angle with respect to a centerline of the iris, the rotated iris image is then corrected into a normal iris image, and wherein if a lower portion of a converted iris image in polar coordinates is curved with an irregular shape due to the acquisition of the slanted iris image, the iris image is normalized in predetermined dimensions, so that the iris image with a variety of deformation can be processed into a correct iris image.</p>
<heading id="h-0003" level="1">BACKGROUND OF INVENTION</heading>
<p id="p-0004" num="0003">An iris recognition system is an apparatus for identifying personal identity by distinguishing one's own peculiar iris pattern. The iris recognition system is superior in its accuracy in terms of the personal identification in comparison to the other biometric methods such as voice or fingerprint, and it has a high degree of security. The iris is a region existing between the pupil and the white sclera of an eye. The iris recognition method is a technique for identifying personal identities based on information obtained by analyzing respective one's own iris patterns different from each other.</p>
<p id="p-0005" num="0004">Generally, the kernel technique of the iris recognition system is to acquire a more accurate eye image by using image acquisition equipment and to efficiently acquire unique characteristic information on the iris from the inputted eye image.</p>
<p id="p-0006" num="0005">However, in a non-contact type human iris recognition system which acquires an iris image to be taken at a certain distance therefrom, the iris image with a variety of deformation may be acquired in practical. That is, it is unlikely that a complete eye image can be acquired since the eye is not necessarily directed toward a front face of a camera but positioned at a slight angle with respect to the camera. Thus, there may be a case where the information on an eye image rotated at an arbitrary angle with respect to a centerline of the iris is acquired.</p>
<p id="p-0007" num="0006">Therefore, in order to solve the above problem produced in the process of image acquisition, it is necessary to accurately detect inner/outer boundaries of the iris from the eye image of a user and to normalize the iris image extracted from the eye image. However, conventional iris recognition methods have a problem in that they cannot accurately detect the inner/outer boundaries of the iris since upon detection of the boundaries, critical values are manually assigned to respective images after defining an arbitrary center of the pupil, or a mean value of the entire image is used as the critical value.</p>
<p id="p-0008" num="0007">Furthermore, according to the conventional iris recognition system, since the normalization process of the iris image is not made or the correction to the rotated image is not considered, an incomplete eye image can be acquired if the eye is not directed toward the front face of the camera but positioned at a slight angle with respect to the camera, or a rotated iris image can be acquired due to movement of the user such as tilting of his/her head. Consequently, there were many cases where in spite of an iris image of the same user, the iris image may be falsely recognized as that of another user.</p>
<heading id="h-0004" level="1">SUMMARY OF THE INVENTION</heading>
<p id="p-0009" num="0008">One aspect of the present invention is to provides a method of detecting an outer boundary of an iris from an image of an eye. The method comprises: providing data representing an image of an eye comprising an image of the iris of the eye, the iris image being substantially annular and defined between inner and outer boundaries, the eye image comprising a plurality of pixels, the eye image data comprising location information and image information for each pixel of the eye image; providing location information of the inner boundary of the iris image; comparing the image information of a pixel on the inner boundary with the image information of pixels of the eye image; and determining a pixel is on the outer boundary of the iris image when a difference between the image information of that pixel and the image information of the pixel on the inner boundary becomes a maximum among differences of the image information. In the above-described method, the location information of the inner boundary is obtained with use of a Canny edge detection method.</p>
<p id="p-0010" num="0009">Another aspect of the present invention provides a method of obtaining an iris pattern. The method comprises: providing an image of an iris of an eye, the iris image being substantially annular and defined between inner and outer boundaries; obtaining data of a substantial portion, but not all, of the iris image; and processing the data of the substantial portion to obtain an iris pattern. In the above-described method, the data comprises positional information and image information of a point within the portion. The substantial portion of the iris image is from about 25% to about 95% of an area of the iris image. The substantial portion of the iris image is from about 40% to about 85% of an area of the iris image. The substantial portion of the iris image is from about 50% to about 75% of an area of the iris image. The substantial portion of the iris image is from about 55% to about 65% of an area of the iris image. The substantial portion of the iris image is substantially annular. The substantial portion is annular and defined from the inner boundary to an imaginary closed line between the inner and outer boundaries. The imaginary closed line is substantially parallel to the inner boundary.</p>
<p id="p-0011" num="0010">Still in the above-described method, a tangent at a point on the inner boundary is substantially parallel to a tangent at a point on the imaginary line that is on a line perpendicular to the tangent at the point on the inner boundary. The substantial portion is annular and defined from an imaginary closed line between the inner and outer boundaries to the outer boundary. The imaginary closed line is substantially parallel to the outer boundary. The substantial portion is annular and defined between a first imaginary closed line and a second imaginary closed line, wherein the first imaginary line is drawn between the inner and outer boundaries, and wherein the second imaginary line is drawn between the first imaginary line and the outer boundary. The first and second lines are substantially parallel to each other. The substantial portion of the iris image is not annular. The data of the substantial portion is transformed into a polar coordinate form.</p>
<p id="p-0012" num="0011">A further aspect of the present invention provides a device for use with a iris pattern recognition system. The method comprises: means for providing an image of an iris of an eye, the iris image being substantially annular and defined between inner and outer boundaries; means for obtaining data of a substantial portion, but not all, of the iris image; and means for processing the data of the substantial portion to obtain an iris pattern. In the above-described device, an iris image processing device comprises: an input device configured to receive an image of an eye comprising an image of an iris of an eye, the iris image being substantially annular and defined between inner and outer boundaries; a first circuit configured to identify data of the iris image from the image of the eye; and a second circuit configured to process the iris image data so as to obtain data of a substantial portion, but not all, of the iris image for further processing. The first and second circuits are integrated in a circuit board or a chip.</p>
<p id="p-0013" num="0012">The other aspect of the present invention is a security system using iris pattern recognition. The system comprises: an input device configured to receive an image of an eye comprising an image of an iris of an eye, the iris image being substantially annular and defined between inner and outer boundaries; a first circuit configured to identify data of the iris image from the image of the eye; a second circuit configured to process the iris image data so as to obtain data of a substantial portion, but not all, of the iris image for further processing; and a third circuit configured to process the data of the substantial portion of the iris image so as to determine whether the data of the iris image matches a pre-registered data.</p>
<p id="p-0014" num="0013">A further aspect of the present invention is a method of processing a iris image. The method comprises: providing data of an original image of an iris; and producing at least one modified iris image data with use of the data of the original iris image, the modified iris image data representing an iris image that is rotated by an angle about a point on the original image. In the above-described method the point of rotation is located at a substantially central position of the original image of the iris. The original iris image data to determine whether the original iris image data matches a pre-registered iris image data. The modified iris image data to determine whether the modified iris image data matches a pre-registered iris image data. The modified iris image data represents an iris image that is rotated in a clockwise direction. The modified iris image data represents an iris image that is rotated in a counter-clockwise direction. A plurality of modified iris image data are produced. The modified iris image data is processed in accordance with a wavelet transform method. The original iris image data is processed in accordance with a wavelet transform method.</p>
<p id="p-0015" num="0014">Still another aspect of the present invention provides an iris image processing device, which comprises: means for providing data of an original image of an iris; and means for producing at least one modified iris image data based on the data of the original iris image, the modified iris image data representing an iris image that is rotated by an angle about a point on the original image. The above-described device further comprises: means for determining whether the modified iris image data matches a pre-registered data.</p>
<p id="p-0016" num="0015">Still another aspect of the present invention provides an iris image processing device, which comprises: an input device configured to receive an image of an eye comprising an image of an iris of an eye; a first circuit configured to identify data of the iris image from the image of the eye; and a second circuit configured to process the iris image data so as to produce at least one modified iris image data based on the data of the original iris image, the modified iris image data representing an iris image that is rotated by an angle about a point on the original image.</p>
<p id="p-0017" num="0016">Still further aspect of the present invention provides a security system using iris pattern recognition, which comprises: the above-described iris image processing device; and a third circuit configured to process the modified iris image data to determine whether the modified iris image data matches a pre-registered data.</p>
<p id="p-0018" num="0017">The present invention is conceived to solve the above problems. An object of the present invention is to provide a non-contact type human iris recognition method for performing a pre-processing by detecting an iris image from an eye image of a user acquired by image acquisition equipment and converting the iris image into an iris image in polar coordinates, wherein inner and outer boundaries of an iris of the user are detected by analyzing differences in pixels of a Canny edge detector and the image.</p>
<p id="p-0019" num="0018">Another object of the present invention is to provide a human iris recognition method, wherein if an iris in an acquired eye image has been rotated at an arbitrary angle with respect to a centerline of the iris, i.e. in case of a rotated iris image, the rotated iris image is corrected into a normal iris image, and wherein if a lower portion of a converted iris image in the polar coordinates is curved with an irregular shape, i.e. in case of a slanted iris image, the iris image is normalized in predetermined dimensions, so that the iris image with a variety of deformation is processed into data on a correct iris image so as to markedly reduce a false acceptance rate and a false rejection rate.</p>
<p id="p-0020" num="0019">In order to achieve the objects of the present invention, the present invention provides a non-contact type human iris recognition method by correction of a rotated iris image for performing a pre-processing by acquiring an eye image of a user by means of image acquisition equipment using an infrared illuminator, by extracting an iris image from the acquired user's eye image, and by converting the extracted iris image into an iris image in polar coordinates. The pre-processing comprises the steps of detecting an inner boundary of an iris from the acquired user's eye image by means of a Canny edge detector; comparing a pixel value of image information at a beginning coordinates (x, y) of the detected inner boundary of the iris with the other pixel values of image information while proceeding upward and downward and leftward and rightward from the inner boundary, finding out the maximum value among values of difference in the compared pixels, and detecting an outer boundary of the iris; and extracting an iris region existing between the inner and outer boundaries, and converting the extracted iris region into the iris image in the polar coordinates.</p>
<p id="p-0021" num="0020">If the iris in the acquired eye image has been slanted, the method may further comprise a step of normalizing the converted iris image in the polar coordinates so as to have predetermined dimensions.</p>
<p id="p-0022" num="0021">If the iris in the acquired eye image has been rotated at an arbitrary angle with respect to a centerline of the iris, the method may further comprise the steps of temporarily generating a plurality of arrays of the iris image by means of shifts by an arbitrary angle with respect to an array of the converted iris image in the polar coordinates; performing wavelet transform in order to generate characteristic vectors of the iris corresponding to the plurality of arrays of the iris image that have been temporarily generated; comparing the respective characteristic vectors generated by the wavelet transform with previously registered characteristic vectors to obtain similarities; and accepting a characteristic vector corresponding to the maximum similarity among the obtained similarities as the characteristic vector of the user.</p>
<?BRFSUM description="Brief Summary" end="tail"?>
<?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?>
<description-of-drawings>
<heading id="h-0005" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading>
<p id="p-0023" num="0022"><figref idref="DRAWINGS">FIG. 1</figref> is a flowchart explaining the procedures of a normalization process of an iris image according to the present invention.</p>
<p id="p-0024" num="0023"><figref idref="DRAWINGS">FIG. 2</figref><i>a </i>is a view showing a result of detection of a pupillary boundary using a Canny edge detector.</p>
<p id="p-0025" num="0024"><figref idref="DRAWINGS">FIG. 2</figref><i>b </i>is a view showing center coordinates and diameter of a pupil.</p>
<p id="p-0026" num="0025"><figref idref="DRAWINGS">FIG. 2</figref><i>c </i>shows an iris image upon obtainment of a radius and center of an outer boundary of an iris according to the present invention.</p>
<p id="p-0027" num="0026"><figref idref="DRAWINGS">FIGS. 3(</figref><i>a</i>) to (<i>c</i>) show the procedures of the normalization process of a slanted iris image.</p>
<p id="p-0028" num="0027"><figref idref="DRAWINGS">FIGS. 4(</figref><i>a</i>) and (<i>b</i>) show a rotated iris image resulting from the tilting of the user's head.</p>
<p id="p-0029" num="0028"><figref idref="DRAWINGS">FIGS. 5(</figref><i>a</i>) and (<i>b</i>) show procedures of a correction process of the rotated iris image shown in <figref idref="DRAWINGS">FIGS. 4(</figref><i>a</i>) and (<i>b</i>).</p>
</description-of-drawings>
<?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?>
<?DETDESC description="Detailed Description" end="lead"?>
<heading id="h-0006" level="1">DETAILED DESCRIPTION FOR PREFERRED EMBODIMENT</heading>
<p id="p-0030" num="0029">Hereinafter, a non-contact type human iris recognition method by correction of a rotated iris image according to the present invention will be described in detail with reference to the accompanying drawings and, in particular, to <figref idref="DRAWINGS">FIG. 1</figref>.</p>
<p id="p-0031" num="0030"><figref idref="DRAWINGS">FIG. 1</figref> is a flowchart explaining procedures of a normalization process of an iris image according to the present invention. Referring to <figref idref="DRAWINGS">FIG. 1</figref>, at step <b>110</b>, an eye image is acquired by image acquisition equipment using an infrared illuminator and a visible light rejection filter. At this time, a reflective light is caused to be gathered in the pupil of an eye so that information on the iris image is not lost. At step <b>120</b>, inner and outer boundaries of the iris are detected in order to extract only an iris region from the acquired eye image, and the center of the detected inner and outer boundaries is set. Step <b>120</b> is performed by a method for detecting the inner and outer boundaries of the iris using differences in pixels of a Canny edge detector and the image according to the present invention, which will be specifically explained below.</p>
<p id="p-0032" num="0031"><figref idref="DRAWINGS">FIG. 2</figref><i>a </i>is a view showing a result of detection of a pupillary boundary, i.e. the inner boundary of the iris, using the Canny edge detector. Referring to <figref idref="DRAWINGS">FIG. 2</figref><i>a</i>, it is noted that only the pupillary boundary is detected by employing the Canny edge detector. That is, as shown in <figref idref="DRAWINGS">FIG. 2</figref><i>a</i>, the inner boundary of the iris is detected by using the Canny edge detector that is a kind of boundary detecting filter. The Canny edge detector smoothes an acquired image by using Gaussian filtering and then detects a boundary by using a Sobel operator. The Gaussian filtering process can be expressed as the following Equation 1, and the used Sobel operator can be expressed as the following Equation 2.</p>
<p id="p-0033" num="0032">[Equation 1]
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>I</i><sub>G</sub>(<i>x, y</i>)=<i>G</i>(<i>x, y</i>)ร<i>I</i>(<i>x, y</i>)โโ[Equation 1]<?in-line-formulae description="In-line Formulae" end="tail"?>
</p>
<p id="p-0034" num="0033">
<maths id="MATH-US-00001" num="00001">
<math overflow="scroll">
<mtable>
  <mtr>
    <mtd>
      <mrow>
        <mrow>
          <msub>
            <mi>S</mi>
            <mi>x</mi>
          </msub>
          <mo>=</mo>
          <mrow>
            <mrow>
              <mrow>
                <mi>I</mi>
                <mo>โก</mo>
                <mrow>
                  <mo>[</mo>
                  <mrow>
                    <mi>i</mi>
                    <mo>-</mo>
                    <mn>1</mn>
                  </mrow>
                  <mo>]</mo>
                </mrow>
              </mrow>
              <mo>โก</mo>
              <mrow>
                <mo>[</mo>
                <mrow>
                  <mi>j</mi>
                  <mo>+</mo>
                  <mn>1</mn>
                </mrow>
                <mo>]</mo>
              </mrow>
            </mrow>
            <mo>+</mo>
            <mrow>
              <mn>2</mn>
              <mo>โข</mo>
              <mrow>
                <mrow>
                  <mi>I</mi>
                  <mo>โก</mo>
                  <mrow>
                    <mo>[</mo>
                    <mi>i</mi>
                    <mo>]</mo>
                  </mrow>
                </mrow>
                <mo>โก</mo>
                <mrow>
                  <mo>[</mo>
                  <mrow>
                    <mi>j</mi>
                    <mo>+</mo>
                    <mn>1</mn>
                  </mrow>
                  <mo>]</mo>
                </mrow>
              </mrow>
            </mrow>
            <mo>+</mo>
            <mrow>
              <mrow>
                <mi>I</mi>
                <mo>โก</mo>
                <mrow>
                  <mo>[</mo>
                  <mrow>
                    <mi>i</mi>
                    <mo>+</mo>
                    <mn>1</mn>
                  </mrow>
                  <mo>]</mo>
                </mrow>
              </mrow>
              <mo>โก</mo>
              <mrow>
                <mo>[</mo>
                <mrow>
                  <mi>j</mi>
                  <mo>+</mo>
                  <mn>1</mn>
                </mrow>
                <mo>]</mo>
              </mrow>
            </mrow>
            <mo>-</mo>
            <mrow>
              <mrow>
                <mi>I</mi>
                <mo>โก</mo>
                <mrow>
                  <mo>[</mo>
                  <mrow>
                    <mi>i</mi>
                    <mo>-</mo>
                    <mn>1</mn>
                  </mrow>
                  <mo>]</mo>
                </mrow>
              </mrow>
              <mo>โก</mo>
              <mrow>
                <mo>[</mo>
                <mrow>
                  <mi>j</mi>
                  <mo>-</mo>
                  <mn>1</mn>
                </mrow>
                <mo>]</mo>
              </mrow>
            </mrow>
            <mo>-</mo>
            <mrow>
              <mn>2</mn>
              <mo>โข</mo>
              <mrow>
                <mrow>
                  <mi>I</mi>
                  <mo>โก</mo>
                  <mrow>
                    <mo>[</mo>
                    <mi>i</mi>
                    <mo>]</mo>
                  </mrow>
                </mrow>
                <mo>โก</mo>
                <mrow>
                  <mo>[</mo>
                  <mrow>
                    <mi>j</mi>
                    <mo>-</mo>
                    <mn>1</mn>
                  </mrow>
                  <mo>]</mo>
                </mrow>
              </mrow>
            </mrow>
            <mo>-</mo>
            <mrow>
              <mrow>
                <mi>I</mi>
                <mo>โก</mo>
                <mrow>
                  <mo>[</mo>
                  <mrow>
                    <mi>i</mi>
                    <mo>+</mo>
                    <mn>1</mn>
                  </mrow>
                  <mo>]</mo>
                </mrow>
              </mrow>
              <mo>โก</mo>
              <mrow>
                <mo>[</mo>
                <mrow>
                  <mi>j</mi>
                  <mo>-</mo>
                  <mn>1</mn>
                </mrow>
                <mo>]</mo>
              </mrow>
            </mrow>
          </mrow>
        </mrow>
        <mo>โข</mo>
        <mstyle>
          <mtext>
</mtext>
        </mstyle>
        <mo>โข</mo>
        <mrow>
          <msub>
            <mi>S</mi>
            <mi>y</mi>
          </msub>
          <mo>=</mo>
          <mrow>
            <mrow>
              <mrow>
                <mi>I</mi>
                <mo>โก</mo>
                <mrow>
                  <mo>[</mo>
                  <mrow>
                    <mi>i</mi>
                    <mo>+</mo>
                    <mn>1</mn>
                  </mrow>
                  <mo>]</mo>
                </mrow>
              </mrow>
              <mo>โก</mo>
              <mrow>
                <mo>[</mo>
                <mrow>
                  <mi>j</mi>
                  <mo>+</mo>
                  <mn>1</mn>
                </mrow>
                <mo>]</mo>
              </mrow>
            </mrow>
            <mo>+</mo>
            <mrow>
              <mn>2</mn>
              <mo>โข</mo>
              <mrow>
                <mrow>
                  <mi>I</mi>
                  <mo>โก</mo>
                  <mrow>
                    <mo>[</mo>
                    <mrow>
                      <mi>i</mi>
                      <mo>+</mo>
                      <mn>1</mn>
                    </mrow>
                    <mo>]</mo>
                  </mrow>
                </mrow>
                <mo>โก</mo>
                <mrow>
                  <mo>[</mo>
                  <mi>j</mi>
                  <mo>]</mo>
                </mrow>
              </mrow>
            </mrow>
            <mo>+</mo>
            <mrow>
              <mrow>
                <mi>I</mi>
                <mo>โก</mo>
                <mrow>
                  <mo>[</mo>
                  <mrow>
                    <mi>i</mi>
                    <mo>+</mo>
                    <mn>1</mn>
                  </mrow>
                  <mo>]</mo>
                </mrow>
              </mrow>
              <mo>โก</mo>
              <mrow>
                <mo>[</mo>
                <mrow>
                  <mi>j</mi>
                  <mo>-</mo>
                  <mn>1</mn>
                </mrow>
                <mo>]</mo>
              </mrow>
            </mrow>
            <mo>-</mo>
            <mrow>
              <mrow>
                <mi>I</mi>
                <mo>โก</mo>
                <mrow>
                  <mo>[</mo>
                  <mrow>
                    <mi>i</mi>
                    <mo>-</mo>
                    <mn>1</mn>
                  </mrow>
                  <mo>]</mo>
                </mrow>
              </mrow>
              <mo>โก</mo>
              <mrow>
                <mo>[</mo>
                <mrow>
                  <mi>j</mi>
                  <mo>+</mo>
                  <mn>1</mn>
                </mrow>
                <mo>]</mo>
              </mrow>
            </mrow>
            <mo>-</mo>
            <mrow>
              <mn>2</mn>
              <mo>โข</mo>
              <mrow>
                <mrow>
                  <mi>I</mi>
                  <mo>โก</mo>
                  <mrow>
                    <mo>[</mo>
                    <mrow>
                      <mi>i</mi>
                      <mo>-</mo>
                      <mn>1</mn>
                    </mrow>
                    <mo>]</mo>
                  </mrow>
                </mrow>
                <mo>โก</mo>
                <mrow>
                  <mo>[</mo>
                  <mi>j</mi>
                  <mo>]</mo>
                </mrow>
              </mrow>
            </mrow>
            <mo>-</mo>
            <mrow>
              <mrow>
                <mi>I</mi>
                <mo>โก</mo>
                <mrow>
                  <mo>[</mo>
                  <mrow>
                    <mi>i</mi>
                    <mo>-</mo>
                    <mn>1</mn>
                  </mrow>
                  <mo>]</mo>
                </mrow>
              </mrow>
              <mo>โก</mo>
              <mrow>
                <mo>[</mo>
                <mrow>
                  <mi>j</mi>
                  <mo>-</mo>
                  <mn>1</mn>
                </mrow>
                <mo>]</mo>
              </mrow>
            </mrow>
          </mrow>
        </mrow>
      </mrow>
    </mtd>
    <mtd>
      <mrow>
        <mo>[</mo>
        <mrow>
          <mi>Equation</mi>
          <mo>โข</mo>
          <mstyle>
            <mspace width="0.8em" height="0.8ex"/>
          </mstyle>
          <mo>โข</mo>
          <mn>2</mn>
        </mrow>
        <mo>]</mo>
      </mrow>
    </mtd>
  </mtr>
</mtable>
</math>
</maths>
</p>
<p id="p-0035" num="0034">In a case where the boundary detecting method employing the Canny edge detector is used, even though a normal eye image is not acquired since the eye of a user is not directed toward a front face of a camera but positioned at a slight angle with respect to the camera, the inner boundary of the iris, i.e. papillary boundary, can be correctly detected and center coordinates and radius of the pupil can also be easily obtained. <figref idref="DRAWINGS">FIG. 2</figref><i>b </i>shows the center coordinates and diameter of the pupil. Referring to <figref idref="DRAWINGS">FIG. 2</figref><i>b</i>, the pupil's radius is d/2, and the pupil's center coordinates are (x+d/2, y+d/2).</p>
<p id="p-0036" num="0035">On the other hand, the outer boundary of the iris in the image can be detected by comparing pixel values while proceeding upward and downward and leftward and rightward from the pupillary boundary, i.e. the inner boundary of the iris, and by finding out maximum values of differences in the pixel values. The detected maximum values are Max{I(x, y)โI(xโ1, y)}, Max{I(x, y)โI(x+1, y)}, Max{I(x, y)โI(x, yโ1)}, and Max{I(x, y)โI(x, y+1)}, where I(x, y) is a pixel value of the image at a point of (x, y). The reason why the differences in the pixel values are obtained while proceeding upward and downward and leftward and rightward from the inner boundary of the iris upon detection of the outer boundary of the iris in the image is to make the inner and outer centers different from each other. That is, in a case where a slanted iris image is acquired, since the pupil is located a little upward, downward, leftward or rightward of the image, the inner and outer centers should be set differently from each other.</p>
<p id="p-0037" num="0036"><figref idref="DRAWINGS">FIG. 2</figref><i>c </i>shows an iris image upon obtainment of the radius and center of the outer boundary of the iris according to the present invention. In a case where an incomplete eye image is acquired since the eye is not directed toward the front face of the camera but positioned at a slight angle with respect to the camera, a process of setting the centers of the inner/outer boundaries of the iris is required. First, values of distances R<sub>L</sub>, R<sub>R</sub>, R<sub>U </sub>and R<sub>D </sub>from the inner boundary to the left, right, upper and lower portions of the outer boundary, respectively, and a value of the radius RI of the inner boundary, i.e. pupillary boundary, are calculated. Then, the center of the outer boundary is obtained by finding out bisection points upward and downward and leftward and rightward of the image using the above calculated values.</p>
<p id="p-0038" num="0037">At step <b>130</b>, iris patterns are detected only at predetermined portions of the distances from the inner boundary to the outer boundary. At step <b>140</b>, the detected iris pattern is converted into an iris image in the polar coordinates. At step <b>150</b>, the converted iris image in the polar coordinates is normalized to obtain an image having predetermined dimensions in its width and height.</p>
<p id="p-0039" num="0038">The conversion of the extracted iris patterns into the iris image in the polar coordinates can be expressed as the following Equation 3.</p>
<p id="p-0040" num="0039">[Equation 3]
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>I</i>(<i>x</i>(<i>r</i>, ฮธ), <i>y</i>(<i>r</i>, ฮธ))<img id="CUSTOM-CHARACTER-00001" he="2.12mm" wi="2.46mm" file="US07298874-20071120-P00001.TIF" alt="custom character" img-content="character" img-format="tif"/><i>I</i>(<i>r</i>, ฮธ)โโ[Equation 3]<?in-line-formulae description="In-line Formulae" end="tail"?>
<br/>
where ฮธ is increased by 0.8 degrees, and r is calculated by using the second Cosine Rule from a distance between the outer center C<sub>O </sub>and the inner center C<sub>I </sub>of the iris, the radius R<sub>O </sub>of the outer boundary, and the value of ฮธ. The iris patterns between the inner and outer boundaries of the iris are extracted using the r and ฮธ. In order to avoid changes in features of the iris according to variations in the size of the pupil, when the iris image between the inner and outer boundaries of the iris is divided into 60 segments and the ฮธ is varied by 0.8 degrees to represent 450 data, the iris image is finally normalized into a 27000 segmented iris image (ฮธรr=450ร60).
</p>
<p id="p-0041" num="0040"><figref idref="DRAWINGS">FIG. 3(</figref><i>a</i>) shows the slanted iris image, and <figref idref="DRAWINGS">FIG. 3(</figref><i>b</i>) is the iris image in polar coordinates converted from the slanted iris image. It can be seen from <figref idref="DRAWINGS">FIG. 3</figref> (<i>b</i>) that a lower portion of the converted iris image in the polar coordinates is curved with an irregular shape. In addition, <figref idref="DRAWINGS">FIG. 3(</figref><i>c</i>) shows an iris image having the dimensions of M pixels in width and N pixels in height, which is normalized from the irregular image of the iris patterns. Hereinafter, the normalization process of the slanted iris image will be described with reference to <figref idref="DRAWINGS">FIGS. 3(</figref><i>a</i>) to (<i>c</i>). In the portion corresponding to the distance between the inner and outer boundaries of the iris in <figref idref="DRAWINGS">FIG. 3(</figref><i>a</i>), the iris patterns existing at only a portion corresponding to X % of the distance between the inner and outer boundaries of the iris are taken in order to eliminate interference from the illuminator and acquire a large amount of iris patterns. That is, when the inner and outer boundaries of the iris are detected, the iris patterns are taken and then converted into those in the polar coordinates. However, in a case where reflective light from the illuminator is gathered on the iris, iris patterns existing at only a portion corresponding to 60% of the distance from the inner boundary among the region from the inner boundary (pupillary boundary) of the iris to the outer boundary can be picked up and converted into those in the polar coordinates. The value of 60% selected in this embodiment of the present invention was experimentally determined as a range in which a greatest deal of iris patterns can be picked up while excluding the reflective light gathered on the iris.</p>
<p id="p-0042" num="0041">In <figref idref="DRAWINGS">FIG. 3(</figref><i>b</i>), the slanted iris image is converted into the iris image in the polar coordinates. As shown in <figref idref="DRAWINGS">FIG. 3(</figref><i>b</i>), when the iris patterns are converted into those in the polar coordinates, the lower portion of the converted iris pattern image in the polar coordinates is curved with an irregular shape. Thus, it is necessary to normalize the irregular iris pattern image. In <figref idref="DRAWINGS">FIG. 3(</figref><i>c</i>), the irregular image of the iris patterns is normalized to obtain the iris image with the dimensions of M pixels in width and N pixels in height.</p>
<p id="p-0043" num="0042">For reference, the performance of the iris recognition system is evaluated by two factors: a false acceptance rate (FAR) and a false rejection rate (FRR). The FAR means the probability that the iris recognition system incorrectly identifies an impostor as an enrollee and thus allows entrance of the impostor, and the FRR means the probability that the iris recognition system incorrectly identifies the enrollee as an impostor and thus rejects entrance to the enrollee. According to the present invention, when a pre-processing is made by employing the method for detecting the boundaries of the iris and the normalization of the slanted iris image, the FAR was reduced from 5.5% to 2.83% and the FRR is reduced from 5.0% to 2.0% as compared with the iris recognition system employing a conventional method for detecting the boundaries of the iris.</p>
<p id="p-0044" num="0043">Finally, at step <b>160</b>, if the iris in the acquired eye image has been rotated at an arbitrary angle with respect to a centerline of the iris, the arrays of pixels of the iris image information are moved and compared in order to correct the rotated iris image.</p>
<p id="p-0045" num="0044"><figref idref="DRAWINGS">FIGS. 4</figref> (<i>a</i>) to (<i>b</i>) show a rotated iris image resulting from the tilting of the user's head. Upon acquisition of an iris image, the user's head may be tilted a little toward the left or right, under which if the iris image is acquired, the rotated iris image is obtained as shown in <figref idref="DRAWINGS">FIG. 4(</figref><i>a</i>). That is, if the eye image acquired at step <b>110</b> has been rotated at an arbitrary angle with respect to a centerline of the eye, a process of correcting the rotated image is required. <figref idref="DRAWINGS">FIG. 4(</figref><i>a</i>) shows the iris image rotated by about 15 degrees in a clockwise or counterclockwise direction with respect to the centerline of the eye. When the rotated iris image is converted into an image in the polar coordinates, the iris patterns in the converted image are shifted leftward or rightward as shown in <figref idref="DRAWINGS">FIG. 4(</figref><i>b</i>), as compared with the normal iris pattern.</p>
<p id="p-0046" num="0045"><figref idref="DRAWINGS">FIGS. 5(</figref><i>a</i>) and (<i>b</i>) show procedures of the process of correcting the rotated iris image shown in <figref idref="DRAWINGS">FIGS. 4(</figref><i>a</i>) and (<i>b</i>). The process of correcting the rotated iris image, which has resulted from the tilting of the user's head, by comparing and moving the arrays of the iris image information will be described below with reference to <figref idref="DRAWINGS">FIGS. 5(</figref><i>a</i>) and (<i>b</i>).</p>
<p id="p-0047" num="0046">Referring to <figref idref="DRAWINGS">FIG. 5(</figref><i>a</i>), from the rotated iris image resulting from the tiling of the user's head, a plurality of arrays Array(n) of the iris image are temporarily generated by means of shifts by an arbitrary angle with respect to an Array(0) of the converted iris image in the polar coordinates. That is, by shifting columns leftward or rightward of the Array(0) based on the Array(0) of the converted iris image in the polar coordinates, 20 arrays of image information from Array(0) to Array(โ10) and from Array(0) to Array( 10) are temporarily generated.</p>
<p id="p-0048" num="0047">In order to generate characteristic vectors of the iris corresponding to the plurality of arrays of iris image that have been temporarily generated, wavelet transform is performed. The respective characteristic vectors generated by the wavelet transform are compared with previously registered characteristic vectors to obtain similarities. A characteristic vector corresponding to the maximum similarity among the obtained similarities is accepted as the characteristic vector of the user.</p>
<p id="p-0049" num="0048">In other words, by generating the arrays Array(n) of image information on the rotated image as mentioned above and performing the wavelet transform for the respective arrays of the image information as shown <figref idref="DRAWINGS">FIG. 5(</figref><i>b</i>), the characteristic vectors f<sub>T</sub>(n) of the iris corresponding to the temporarily generated plurality of arrays Array(n) of the iris image are then generated. The characteristic vectors f<sub>T</sub>(n) are generated from f<sub>T</sub>(0) to f<sub>T</sub>( 10) and from f<sub>T</sub>(0) to f<sub>T</sub>(โ10). The respective generated characteristic vectors f<sub>T</sub>(n) are compared with each of the characteristic vectors f<sub>R </sub>of the enrollees and thus similarities S<sub>n </sub>are obtained. A characteristic vector f<sub>T</sub>(n) corresponding to the maximum similarity among the obtained similarities S<sub>n </sub>is considered as a resulted value in which the rotation effect is corrected, and is accepted as the characteristic vector of the user's iris.</p>
<p id="p-0050" num="0049">As described above, according to the non-contact type human iris recognition method by the correction of the rotated iris image of the present invention, there is an advantage in that by detecting the inner and outer boundaries of the iris using the differences in pixels of the Canny edge detector and the image, the boundaries of the iris can be more correctly detected from the eye image of the user.</p>
<p id="p-0051" num="0050">Furthermore, according to the non-contact type human iris recognition method of the present invention, if the iris in the eye image acquired by the image acquisition equipment has been rotated at an arbitrary angle with respect to the centerline of the iris, the rotated iris image is corrected into the normal iris image. Otherwise, if a lower portion of the converted iris image in the polar coordinates is curved and thus has an irregular shape due to the acquisition of the slanted iris image, the iris image is normalized in predetermined dimensions. Thus, there is another advantage in that the iris image with a variety of deformation is processed into data on a correct iris image so as to markedly reduce a false acceptance rate and a false rejection rate.</p>
<p id="p-0052" num="0051">It should be noted that the above description merely exemplifies embodiments of the non-contact type human iris recognition method by the correction of the rotated iris image according to the present invention, and thus, the present invention is not limited to the above embodiments. A person skilled in the art can make various modifications and changes to the present invention without departing from the technical spirit and scope of the present invention defined by the appended claims.</p>
<?DETDESC description="Detailed Description" end="tail"?>
</description>
<us-math idrefs="MATH-US-00001" nb-file="US07298874-20071120-M00001.NB">
<img id="EMI-M00001" he="15.16mm" wi="76.20mm" file="US07298874-20071120-M00001.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-claim-statement>What is claimed is:</us-claim-statement>
<claims id="claims">
<claim id="CLM-00001" num="00001">
<claim-text>1. A method of processing an image of an eye, the method comprising:
<claim-text>providing data representing an image of an eye comprising an image of an iris of the eye, the iris image being defined between inner and outer boundaries, the eye image comprising a plurality of pixels, the eye image data comprising location information and image information for each pixel of the eye image;</claim-text>
<claim-text>providing location information of the inner boundary of the iris image, wherein a first inner boundary pixel and a second inner boundary pixel are located at different points on the inner boundary;</claim-text>
<claim-text>finding a first outer boundary pixel located on a first imaginary line extending from the first inner boundary pixel, wherein a pixel located on the first imaginary line is determined to be the first outer boundary pixel when the difference of the image information between the pixel and its neighboring pixel which are located on the first imaginary line becomes the maximum among differences of the image information between two neighboring pixels located on the first imaginary line;</claim-text>
<claim-text>finding a second outer boundary pixel located on a second imaginary line extending from the second inner boundary pixel, wherein a pixel located on the second imaginary line is determined to be the second outer boundary pixel when the difference of the image information between the pixel and its neighboring pixel which are located on the second imaginary line becomes the maximum among differences of the image information between two neighboring pixels located on the second imaginary line; and</claim-text>
<claim-text>using at least one of the first outer boundary pixel and the second outer boundary pixel for further processing.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00002" num="00002">
<claim-text>2. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the location information of the inner boundary is obtained with use of a Canny edge detection method.</claim-text>
</claim>
<claim id="CLM-00003" num="00003">
<claim-text>3. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising:
<claim-text>obtaining data of a substantial portion, but not all, of the iris image; and</claim-text>
<claim-text>processing the data of the substantial portion to obtain an iris pattern.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00004" num="00004">
<claim-text>4. The method of <claim-ref idref="CLM-00003">claim 3</claim-ref>, wherein the data comprises the location information and the image information of a pixel within the portion.</claim-text>
</claim>
<claim id="CLM-00005" num="00005">
<claim-text>5. The method of <claim-ref idref="CLM-00003">claim 3</claim-ref>, wherein the substantial portion of the iris image is from about 25% to about 95% of an area of the iris image.</claim-text>
</claim>
<claim id="CLM-00006" num="00006">
<claim-text>6. The method of <claim-ref idref="CLM-00003">claim 3</claim-ref>, wherein the substantial portion of the iris image is from about 40% to about 85% of an area of the iris image.</claim-text>
</claim>
<claim id="CLM-00007" num="00007">
<claim-text>7. The method of <claim-ref idref="CLM-00003">claim 3</claim-ref>, wherein the substantial portion of the iris image is from about 50% to about 75% of an area of the iris image.</claim-text>
</claim>
<claim id="CLM-00008" num="00008">
<claim-text>8. The method of <claim-ref idref="CLM-00003">claim 3</claim-ref>, wherein the substantial portion of the iris image is from about 55% to about 65% of an area of the iris image.</claim-text>
</claim>
<claim id="CLM-00009" num="00009">
<claim-text>9. The method of <claim-ref idref="CLM-00003">claim 3</claim-ref>, wherein the substantial portion of the iris image is substantially annular.</claim-text>
</claim>
<claim id="CLM-00010" num="00010">
<claim-text>10. The method of <claim-ref idref="CLM-00003">claim 3</claim-ref>, wherein the substantial portion is substantially annular and defined from the inner boundary to an imaginary closed line between the inner and outer boundaries.</claim-text>
</claim>
<claim id="CLM-00011" num="00011">
<claim-text>11. The method of <claim-ref idref="CLM-00010">claim 10</claim-ref>, wherein the imaginary closed line is substantially parallel to the inner boundary.</claim-text>
</claim>
<claim id="CLM-00012" num="00012">
<claim-text>12. The method of <claim-ref idref="CLM-00011">claim 11</claim-ref>, wherein a tangent at a point on the inner boundary is substantially parallel to a tangent at a point on the imaginary closed line that is on a line perpendicular to the tangent at the point on the inner boundary.</claim-text>
</claim>
<claim id="CLM-00013" num="00013">
<claim-text>13. The method of <claim-ref idref="CLM-00003">claim 3</claim-ref>, wherein the substantial portion is substantially annular and defined from an imaginary closed line between the inner and outer boundaries to the outer boundary.</claim-text>
</claim>
<claim id="CLM-00014" num="00014">
<claim-text>14. The method of <claim-ref idref="CLM-00013">claim 13</claim-ref>, wherein the imaginary closed line is substantially parallel to the outer boundary.</claim-text>
</claim>
<claim id="CLM-00015" num="00015">
<claim-text>15. The method of <claim-ref idref="CLM-00003">claim 3</claim-ref>, wherein the substantial portion is substantially annular and defined between a first imaginary closed line and a second imaginary closed line, wherein the first imaginary closed line is drawn between the inner and outer boundaries, and wherein the second imaginary closed line is drawn between the first imaginary closed line and the outer boundary.</claim-text>
</claim>
<claim id="CLM-00016" num="00016">
<claim-text>16. The method of <claim-ref idref="CLM-00015">claim 15</claim-ref>, wherein the first and second imaginary closed lines are substantially parallel to each other.</claim-text>
</claim>
<claim id="CLM-00017" num="00017">
<claim-text>17. The method of <claim-ref idref="CLM-00003">claim 3</claim-ref>, wherein the substantial portion of the iris image is not annular.</claim-text>
</claim>
<claim id="CLM-00018" num="00018">
<claim-text>18. The method of <claim-ref idref="CLM-00003">claim 3</claim-ref>, wherein the data of the substantial portion is transformed into a polar coordinate form.</claim-text>
</claim>
<claim id="CLM-00019" num="00019">
<claim-text>19. A device for processing an image of an eye, comprising:
<claim-text>means for providing data representing an image of an eye comprising an image of an iris of the eye, the iris image being defined between inner and outer boundaries, the eye image comprising a plurality of pixels, the eye image data comprising location information and image information for each pixel of the eye image;</claim-text>
<claim-text>means for providing location information of the inner boundary of the iris image, wherein a first inner boundary pixel and a second inner boundary pixel are located at different points on the inner boundary;</claim-text>
<claim-text>means for finding a first outer boundary pixel located on a first imaginary line extending from the first inner boundary pixel, wherein a pixel located on the first imaginary line is determined to be the first outer boundary pixel when the difference of the image information between the pixel and its neighboring pixel which are located on the first imaginary line becomes the maximum among differences of the image information between two neighboring pixels located on the first imaginary line;</claim-text>
<claim-text>means for finding a second outer boundary pixel located on a second imaginary line extending from the second inner boundary pixel, wherein a pixel located on the second imaginary line is determined to be the second outer boundary pixel when the difference of the image information between the pixel and its neighboring pixel which are located on the second imaginary line becomes the maximum among differences of the image information between two neighboring pixels located on the second imaginary line;</claim-text>
<claim-text>means for obtaining data of a substantial portion, but not all, of the iris image; and</claim-text>
<claim-text>means for processing the data of the substantial portion to obtain an iris pattern.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00020" num="00020">
<claim-text>20. An eye image processing system, comprising:
<claim-text>a first circuit configured to process the method of <claim-ref idref="CLM-00001">claim 1</claim-ref> and configured to identify data of the iris image from the image of the eye; and</claim-text>
<claim-text>a second circuit configured to process the iris image data so as to obtain data of a substantial portion, but not all, of the iris image for further processing.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00021" num="00021">
<claim-text>21. The system of <claim-ref idref="CLM-00020">claim 20</claim-ref>, wherein the first and second circuits are integrated in a circuit board or a chip.</claim-text>
</claim>
<claim id="CLM-00022" num="00022">
<claim-text>22. The system of <claim-ref idref="CLM-00020">claim 20</claim-ref>, further comprising
<claim-text>a third circuit configured to process the data of the substantial portion of the iris image so as to determine whether the data of the iris image matches a pre-registered data.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00023" num="00023">
<claim-text>23. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising:
<claim-text>obtaining data of the iris image; and</claim-text>
<claim-text>producing at least one modified iris image data with use of the data of the iris image, the modified iris image data representing an iris image that is rotated by an angle about a point on the iris image.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00024" num="00024">
<claim-text>24. The method of <claim-ref idref="CLM-00023">claim 23</claim-ref>, wherein the point of rotation is located at a substantially central position of the image of the iris.</claim-text>
</claim>
<claim id="CLM-00025" num="00025">
<claim-text>25. The method of <claim-ref idref="CLM-00023">claim 23</claim-ref>, further comprising processing the iris image data to determine whether the iris image data matches a pre-registered iris image data.</claim-text>
</claim>
<claim id="CLM-00026" num="00026">
<claim-text>26. The method of <claim-ref idref="CLM-00023">claim 23</claim-ref>, further comprising processing the modified iris image data to determine whether the modified iris image data matches a pre-registered iris image data.</claim-text>
</claim>
<claim id="CLM-00027" num="00027">
<claim-text>27. The method of <claim-ref idref="CLM-00023">claim 23</claim-ref>, wherein the modified iris image data represents an iris image that is rotated in a clockwise direction.</claim-text>
</claim>
<claim id="CLM-00028" num="00028">
<claim-text>28. The method of <claim-ref idref="CLM-00023">claim 23</claim-ref>, wherein the modified iris image data represents an iris image that is rotated in a counter-clockwise direction.</claim-text>
</claim>
<claim id="CLM-00029" num="00029">
<claim-text>29. The method of <claim-ref idref="CLM-00023">claim 23</claim-ref>, wherein a plurality of modified iris image data are produced.</claim-text>
</claim>
<claim id="CLM-00030" num="00030">
<claim-text>30. The method of <claim-ref idref="CLM-00023">claim 23</claim-ref>, wherein the modified iris image data is processed in accordance with a wavelet transform method.</claim-text>
</claim>
<claim id="CLM-00031" num="00031">
<claim-text>31. The method of <claim-ref idref="CLM-00023">claim 23</claim-ref>, wherein the original iris image data is processed in accordance with a wavelet transform method.</claim-text>
</claim>
<claim id="CLM-00032" num="00032">
<claim-text>32. An eye image processing system, comprising:
<claim-text>means for providing data representing an image of an eye comprising an image of an iris of the eye, the iris image being defined between inner and outer boundaries, the eye image comprising a plurality of pixels, the eye image data comprising location information and image information for each pixel of the eye image;</claim-text>
<claim-text>means for providing location information of the inner boundary of the iris image, wherein a first inner boundary pixel and a second inner boundary pixel are located at different points on the inner boundary;</claim-text>
<claim-text>means for finding a first outer boundary pixel located on a first imaginary line extending from the first inner boundary pixel, wherein a pixel located on the first imaginary line is determined to be the first outer boundary pixel when the difference of the image information between the pixel and its neighboring pixel which are located on the first imaginary line becomes the maximum among differences of the image information between two neighboring pixels located on the first imaginary line;</claim-text>
<claim-text>means for finding a second outer boundary pixel located on a second imaginary line extending from the second inner boundary pixel, wherein a pixel located on the second imaginary line is determined to be the second outer boundary pixel when the difference of the image information between the pixel and its neighboring pixel which are located on the second imaginary line becomes the maximum among differences of the image information between two neighboring pixels located on the second imaginary line;</claim-text>
<claim-text>means for identifying data of the iris image; and</claim-text>
<claim-text>means for producing at least one modified iris image data based on the data of the iris image, the modified iris image data representing an iris image that is rotated by an angle about a point on the image.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00033" num="00033">
<claim-text>33. The device of <claim-ref idref="CLM-00032">claim 32</claim-ref>, further comprising:
<claim-text>means for determining whether the modified iris image data matches a pre-registered data.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00034" num="00034">
<claim-text>34. An eye image processing system, comprising:
<claim-text>a first circuit configured to process the method of <claim-ref idref="CLM-00001">claim 1</claim-ref> and configured to identify data of the iris image from the image of the eye; and</claim-text>
<claim-text>a second circuit configured to process the iris image data so as to produce at least one modified iris image data based on the data of the original iris image, the modified iris image data representing an iris image that is rotated by an angle about a point on the original image.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00035" num="00035">
<claim-text>35. The system of <claim-ref idref="CLM-00034">claim 34</claim-ref>, further comprising:
<claim-text>a third circuit configured to process the modified iris image data to determine whether the modified iris image data matches a pre-registered data.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00036" num="00036">
<claim-text>36. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein a third inner boundary pixel is located at a different point on the inner boundary from the points on the inner boundary where the first inner boundary pixel and the second inner boundary pixel are located, and wherein the method further comprises finding a third outer boundary pixel located on a third imaginary line extending from the third inner boundary pixel, wherein a pixel located on the third imaginary line is determined to be the third outer boundary pixel when the difference of the image information between the pixel and its neighboring pixel which are located on the third imaginary line becomes the maximum among differences of the image information between two neighboring pixels located on the third imaginary line.</claim-text>
</claim>
<claim id="CLM-00037" num="00037">
<claim-text>37. The method of <claim-ref idref="CLM-00036">claim 36</claim-ref>, wherein the outer boundary is determined to be a generally circular line on which the first, second and third outer boundary pixels located.</claim-text>
</claim>
<claim id="CLM-00038" num="00038">
<claim-text>38. The method of <claim-ref idref="CLM-00036">claim 36</claim-ref>, wherein a fourth inner boundary pixel is located at a different point on the inner boundary from the points on the inner boundary where the first, second and third inner boundary pixels are located, and wherein the method further comprises finding a fourth outer boundary pixel located on a fourth imaginary line extending from the fourth inner boundary pixel, wherein a pixel located on the fourth imaginary line is determined to be the fourth outer boundary pixel when the difference of the image information between the pixel and its neighboring pixel which are located on the fourth imaginary line becomes the maximum among differences of the image information between two neighboring pixels located on the fourth imaginary line, wherein the first imaginary line is substantially perpendicular to the third and fourth imaginary lines, and wherein the second imaginary line is substantially perpendicular to the third and fourth imaginary lines.</claim-text>
</claim>
</claims>
</us-patent-grant>
