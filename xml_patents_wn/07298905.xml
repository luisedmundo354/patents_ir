<us-patent-grant lang="EN" dtd-version="v4.2 2006-08-23" file="US07298905-20071120.XML" status="PRODUCTION" id="us-patent-grant" country="US" date-produced="20071106" date-publ="20071120">
<us-bibliographic-data-grant>
<publication-reference>
<document-id>
<country>US</country>
<doc-number>07298905</doc-number>
<kind>B2</kind>
<date>20071120</date>
</document-id>
</publication-reference>
<application-reference appl-type="utility">
<document-id>
<country>US</country>
<doc-number>11122167</doc-number>
<date>20050504</date>
</document-id>
</application-reference>
<us-application-series-code>11</us-application-series-code>
<priority-claims>
<priority-claim sequence="01" kind="national">
<country>JP</country>
<doc-number>2000-159261</doc-number>
<date>20000529</date>
</priority-claim>
</priority-claims>
<us-term-of-grant>
<us-term-extension>345</us-term-extension>
</us-term-of-grant>
<classifications-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>G</section>
<class>06</class>
<subclass>K</subclass>
<main-group>9</main-group>
<subgroup>46</subgroup>
<symbol-position>F</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20071120</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>G</section>
<class>06</class>
<subclass>K</subclass>
<main-group>9</main-group>
<subgroup>34</subgroup>
<symbol-position>L</symbol-position>
<classification-value>N</classification-value>
<action-date><date>20071120</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
</classifications-ipcr>
<classification-national>
<country>US</country>
<main-classification>382190</main-classification>
</classification-national>
<invention-title id="d0e71">Image processing apparatus and method, communication apparatus, communication system and method, and recording medium</invention-title>
<references-cited>
<citation>
<patcit num="00001">
<document-id>
<country>US</country>
<doc-number>5680226</doc-number>
<kind>A</kind>
<name>Takayanagi</name>
<date>19971000</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>358462</main-classification></classification-national>
</citation>
<citation>
<patcit num="00002">
<document-id>
<country>US</country>
<doc-number>5966462</doc-number>
<kind>A</kind>
<name>Linder et al.</name>
<date>19991000</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>382173</main-classification></classification-national>
</citation>
<citation>
<patcit num="00003">
<document-id>
<country>US</country>
<doc-number>6021221</doc-number>
<kind>A</kind>
<name>Takaha</name>
<date>20000200</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>382199</main-classification></classification-national>
</citation>
<citation>
<patcit num="00004">
<document-id>
<country>US</country>
<doc-number>6467039</doc-number>
<kind>B1</kind>
<name>Fredriksson</name>
<date>20021000</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>713151</main-classification></classification-national>
</citation>
<citation>
<patcit num="00005">
<document-id>
<country>US</country>
<doc-number>6510243</doc-number>
<kind>B1</kind>
<name>Ikeda</name>
<date>20030100</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>382173</main-classification></classification-national>
</citation>
<citation>
<patcit num="00006">
<document-id>
<country>US</country>
<doc-number>6532461</doc-number>
<kind>B2</kind>
<name>Evans</name>
<date>20030300</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>707  3</main-classification></classification-national>
</citation>
<citation>
<patcit num="00007">
<document-id>
<country>US</country>
<doc-number>6862367</doc-number>
<kind>B1</kind>
<name>Granlund</name>
<date>20050300</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>382195</main-classification></classification-national>
</citation>
<citation>
<patcit num="00008">
<document-id>
<country>US</country>
<doc-number>2002/0172421</doc-number>
<kind>A1</kind>
<name>Kondo et al.</name>
<date>20021100</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>382173</main-classification></classification-national>
</citation>
<citation>
<patcit num="00009">
<document-id>
<country>JP</country>
<doc-number>7-222145</doc-number>
<date>19950800</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<patcit num="00010">
<document-id>
<country>JP</country>
<doc-number>8-79757</doc-number>
<date>19960300</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<patcit num="00011">
<document-id>
<country>JP</country>
<doc-number>11-164305</doc-number>
<date>19990600</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<nplcit num="00012">
<othercit>Salembier, et al., “Segmentation-Based Video Coding System Allowing the Manipulation of Objects,” IEEE Transactions on Circuits and Systems for Video Technology, vol. 7, No. 1, Feb. 1997, pp. 60-74.</othercit>
</nplcit>
<category>cited by other</category>
</citation>
<citation>
<nplcit num="00013">
<othercit>Moon, et al., “Boundary Block-Merging (BBM) Technique for Efficient Texture Coding of Arbitrarily Shaped Object,” IEEE Transactions on Circuits and Systems for Video Technology, vol. 9, No. 1, Feb. 1999, pp. 35-43.</othercit>
</nplcit>
<category>cited by other</category>
</citation>
<citation>
<nplcit num="00014">
<othercit>Guo, et al., “Fast Video Object Segmentation Using Affine Motion and Gradient-Based Color Clustering,” IEEE Second Workshop on Multimedia Signal Processing, Dec. 7-9, 1998, pp. 486-491.</othercit>
</nplcit>
<category>cited by other</category>
</citation>
<citation>
<nplcit num="00015">
<othercit>Sakaida, et al., “An Image Segmentation Method by the Region Integration Using the Initial Dependence of the Clustering Algorithm,” NHK Giken R&amp;D, No. 53, Nov. 1998, pp. 42-56.</othercit>
</nplcit>
<category>cited by other</category>
</citation>
</references-cited>
<number-of-claims>8</number-of-claims>
<us-exemplary-claim>1</us-exemplary-claim>
<us-field-of-classification-search>
<classification-national>
<country>US</country>
<main-classification>382164</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>382173</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>382190</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>382195</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>382199</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>382284</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>382305</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>358444</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>358449</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>358462</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>707  3</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>713151</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>455 95</main-classification>
</classification-national>
</us-field-of-classification-search>
<figures>
<number-of-drawing-sheets>15</number-of-drawing-sheets>
<number-of-figures>26</number-of-figures>
</figures>
<us-related-documents>
<division>
<relation>
<parent-doc>
<document-id>
<country>US</country>
<doc-number>10048103</doc-number>
<kind>00</kind>
</document-id>
<parent-grant-document>
<document-id>
<country>US</country>
<doc-number>7076097</doc-number>
<kind>A </kind>
</document-id>
</parent-grant-document>
<parent-pct-document>
<document-id>
<country>WO</country>
<doc-number>PCT/JP01/04331</doc-number>
<date>20010523</date>
</document-id>
</parent-pct-document>
</parent-doc>
<child-doc>
<document-id>
<country>US</country>
<doc-number>11122167</doc-number>
</document-id>
</child-doc>
</relation>
</division>
<related-publication>
<document-id>
<country>US</country>
<doc-number>20050201625</doc-number>
<kind>A1</kind>
<date>20050915</date>
</document-id>
</related-publication>
</us-related-documents>
<parties>
<applicants>
<applicant sequence="001" app-type="applicant-inventor" designation="us-only">
<addressbook>
<last-name>Kondo</last-name>
<first-name>Tetsujiro</first-name>
<address>
<city>Tokyo</city>
<country>JP</country>
</address>
</addressbook>
<nationality>
<country>JP</country>
</nationality>
<residence>
<country>JP</country>
</residence>
</applicant>
<applicant sequence="002" app-type="applicant-inventor" designation="us-only">
<addressbook>
<last-name>Watanabe</last-name>
<first-name>Yoshinori</first-name>
<address>
<city>Kanagawa</city>
<country>JP</country>
</address>
</addressbook>
<nationality>
<country>JP</country>
</nationality>
<residence>
<country>JP</country>
</residence>
</applicant>
</applicants>
<agents>
<agent sequence="01" rep-type="attorney">
<addressbook>
<orgname>Frommer Lawrence &amp; Haug LLP</orgname>
<address>
<country>unknown</country>
</address>
</addressbook>
</agent>
<agent sequence="02" rep-type="attorney">
<addressbook>
<last-name>Frommer</last-name>
<first-name>William S.</first-name>
<address>
<country>unknown</country>
</address>
</addressbook>
</agent>
</agents>
</parties>
<assignees>
<assignee>
<addressbook>
<orgname>Sony Corporation</orgname>
<role>03</role>
<address>
<city>Tokyo</city>
<country>JP</country>
</address>
</addressbook>
</assignee>
</assignees>
<examiners>
<primary-examiner>
<last-name>Alavi</last-name>
<first-name>Amir</first-name>
<department>2624</department>
</primary-examiner>
</examiners>
</us-bibliographic-data-grant>
<abstract id="abstract">
<p id="p-0001" num="0000">The present invention provides an image processing apparatus and method destined for processing image data sent via a communication system. In the apparatus and method, image data corresponding to designation data supplied from the user are coupled to each other for grouping, features of the image data corresponding to the designation data, and the image data once coupled together as in the above are uncoupled according to the features of the grouped image data, to thereby accurately localize an area the user is interested in. Also, according to the present invention, a time interval between the designation data is computed, and the image data are coupled to each other when the computed time interval is less than a predetermined threshold, while they are not coupled to each other when the computed time interval exceeds the predetermined threshold, thereby detecting when the user's interest shifts to another image area. With this detection of the shift of the user's interest, it is possible to accurately localize an area the user is interested in.</p>
</abstract>
<drawings id="DRAWINGS">
<figure id="Fig-EMI-D00000" num="00000">
<img id="EMI-D00000" he="125.39mm" wi="153.33mm" file="US07298905-20071120-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00001" num="00001">
<img id="EMI-D00001" he="154.35mm" wi="134.03mm" file="US07298905-20071120-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00002" num="00002">
<img id="EMI-D00002" he="226.65mm" wi="165.86mm" file="US07298905-20071120-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00003" num="00003">
<img id="EMI-D00003" he="213.28mm" wi="164.42mm" file="US07298905-20071120-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00004" num="00004">
<img id="EMI-D00004" he="136.99mm" wi="162.98mm" file="US07298905-20071120-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00005" num="00005">
<img id="EMI-D00005" he="146.73mm" wi="159.77mm" file="US07298905-20071120-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00006" num="00006">
<img id="EMI-D00006" he="182.20mm" wi="122.09mm" orientation="landscape" file="US07298905-20071120-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00007" num="00007">
<img id="EMI-D00007" he="198.20mm" wi="164.00mm" file="US07298905-20071120-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00008" num="00008">
<img id="EMI-D00008" he="215.90mm" wi="134.37mm" orientation="landscape" file="US07298905-20071120-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00009" num="00009">
<img id="EMI-D00009" he="238.08mm" wi="121.24mm" orientation="landscape" file="US07298905-20071120-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00010" num="00010">
<img id="EMI-D00010" he="140.21mm" wi="116.67mm" file="US07298905-20071120-D00010.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00011" num="00011">
<img id="EMI-D00011" he="193.38mm" wi="153.59mm" orientation="landscape" file="US07298905-20071120-D00011.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00012" num="00012">
<img id="EMI-D00012" he="214.38mm" wi="141.65mm" orientation="landscape" file="US07298905-20071120-D00012.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00013" num="00013">
<img id="EMI-D00013" he="238.25mm" wi="154.69mm" file="US07298905-20071120-D00013.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00014" num="00014">
<img id="EMI-D00014" he="229.53mm" wi="130.05mm" file="US07298905-20071120-D00014.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00015" num="00015">
<img id="EMI-D00015" he="233.51mm" wi="154.35mm" file="US07298905-20071120-D00015.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
</drawings>
<description id="description">
<?RELAPP description="Other Patent Relations" end="lead"?>
<p id="p-0002" num="0001">This is a division of application Ser. No. 10/048,103, filed May 31, 2002, now U.S. Pat. No. 7,076,097 which is a 371 of PCT/JP01/04331, the entirety of which is incorporated herein by reference.</p>
<?RELAPP description="Other Patent Relations" end="tail"?>
<?BRFSUM description="Brief Summary" end="lead"?>
<heading id="h-0001" level="1">TECHNICAL FIELD</heading>
<p id="p-0003" num="0002">The present invention generally relates to the field of image processing in which an area is localized in image data as designated by the user. More particularly, the present invention relates to an image processing apparatus and method, communication apparatus, communication system and method, and a recording medium, destined for extracting an object image from image data as designated by the user.</p>
<heading id="h-0002" level="1">BACKGROUND ART</heading>
<p id="p-0004" num="0003">In the Japanese Published Unexamined Application No. 112856 of 1998, there is disclosed an image transmitter adapted to send image data in an area in an image in an amount of information and image data in another area in another amount of information from a sending side to a receiving side as designated by the receiving side. With the image transmitter disclosed in the Japanese Published Unexamined Application No. 112856 of 1998, it is possible to display an image of a specific area including designated points at a high spatial resolution and images other areas at low spatial resolutions.</p>
<p id="p-0005" num="0004">More particularly, in case image data are sent from the sending side to receiving side via a transmission line, it is not possible to send image data whose data rate exceeds the transmission rate of the transmission line. Therefore, for real-time display of an image at the receiving side, image data have to be sent from the sending side to receiving side at a data rate lower than the transmission rate of the transmission line. As a result, if the transmission rate is not sufficient, all images will be displayed at the receiving side at lower spatial resolutions.</p>
<p id="p-0006" num="0005">On the other hand, by permitting to send image data in an area in an image in an amount of information and image data in other areas in the image in another amount of information as in the image transmitter disclosed in the Japanese Published Unexamined Application No. 112856 of 1998 and sending image data in a specific area including points designated by the receiving side in an increased amount of information and image data in other areas in a decreased amount of information, it is possible to display the image of the specific area including the points designated by the receiving side at a high spatial resolution and the images of other areas at a low spatial resolution. Thus, it is possible to display an image area the user wants to view in detail at a high spatial resolution while displaying other image areas at a low spatial resolution. That is, with the image transmitter disclosed in the Japanese Published Unexamined Application No. 112856 of 1998, it is possible to display an image area the user desires to view in detail at an improved spatial resolution at the sacrifice of the spatial resolution of the other areas.</p>
<p id="p-0007" num="0006">Also, in the PCT Published Unexamined Application No. WO01/1189A1 of the Applicant of the present invention, there is disclosed an image processor adapted to effect the spatial resolution control as disclosed in the Japanese Published Unexamined Application No. 112856 of 1998 as well as the time resolution control in order to control the amount of information in an area designated at the receiving side. Further, the above PCT Published Unexamined Application No. WO01/1189A1 discloses an image processor adapted to extract object images from image data sent from the sending side to receiving side by judging, based on a position clicked with the mouse by the user at the receiving side and time interval between clicks, whether an image in the clicked position is moving or stationary and also whether the clicks are temporally successive.</p>
<p id="p-0008" num="0007">It should be reminded here that to improve the spatial resolution of for example an image area (will be referred to as “interesting area” wherever appropriate hereunder) the user takes interest in seeing in detail and reduce the spatial resolution of other image areas such as background for example, it is necessary at the sending side to localize an image area the user at the receiving side, that is, an interesting area.</p>
<p id="p-0009" num="0008">As in the above, if an area the user at the receiving side is interested in can be localized, it is possible when sending image data from the sending side to the receiving side to send an increased amount of image data in the interesting area.</p>
<p id="p-0010" num="0009">The PCT Published Unexamined Application No. WO01-1189A1 discloses also a method for localizing an area the user is interested in as designated in a received image data by clicking the mouse or the like at the receiving side, for example, a method for extracting object images for example. In this method, however, since any object image is not reviewed, so there is a problem that objects which are extracted as object-images at a time and which is uncoupled as another object image will always be handled as belonging to the same object.</p>
<p id="p-0011" num="0010">Further, in case the user's interest shifts from one area to another image area, it is necessary to detect the shift of the user's interest and localize an image area the user is currently interested in. That is, processing of image data based on existing information after the user's interest has shifted to another image area, will possibly lead to a misjudgment. Therefore, in case the user's interest has shifted to another image area, the shift has to be detected for changing the manner of image data processing.</p>
<heading id="h-0003" level="1">DISCLOSURE OF THE INVENTION</heading>
<p id="p-0012" num="0011">Accordingly, the present invention has an object to overcome the above-mentioned drawbacks of the prior art by providing an image processing apparatus and method, communication system and method, and a recording medium, capable of localizing an optimum interesting area by reviewing an area the user is interested in as well as of detecting an image area to which the user's interest has shifted.</p>
<p id="p-0013" num="0012">The above object can be attained by providing an image processor comprising:</p>
<p id="p-0014" num="0013">means for acquiring designation data from the user;</p>
<p id="p-0015" num="0014">means for coupling image data corresponding to the designation data to each other to group them;</p>
<p id="p-0016" num="0015">means for detecting a feature of the image data corresponding to the designation data; and</p>
<p id="p-0017" num="0016">means for uncoupling the image data, coupled by the coupling means and corresponding to the designation data, according to each feature of image data corresponding to the designation data in the same group.</p>
<p id="p-0018" num="0017">Also, the above object can be attained by providing a communication system comprising:</p>
<p id="p-0019" num="0018">a transmitter to send image data; and</p>
<p id="p-0020" num="0019">a receiver to receive the image data sent from the transmitter;</p>
<p id="p-0021" num="0020">the receiver including:
<ul id="ul0001" list-style="none">
    <li id="ul0001-0001" num="0000">
    <ul id="ul0002" list-style="none">
        <li id="ul0002-0001" num="0021">a first means for receiving the image data sent from the transmitter;</li>
        <li id="ul0002-0002" num="0022">means for outputting the image data received by the first receiving means;</li>
        <li id="ul0002-0003" num="0023">means for outputting the image data outputted from the outputting means;</li>
        <li id="ul0002-0004" num="0024">means for designating a time-spatial position of the received image data outputted from the outputting means; and</li>
        <li id="ul0002-0005" num="0025">a first means for sending designation data indicative of the time-spatial position of the image data, designated by the designating means; and</li>
    </ul>
    </li>
</ul>
</p>
<p id="p-0022" num="0026">the transmitter including:
<ul id="ul0003" list-style="none">
    <li id="ul0003-0001" num="0000">
    <ul id="ul0004" list-style="none">
        <li id="ul0004-0001" num="0027">an input means to which image data are supplied continuously;</li>
        <li id="ul0004-0002" num="0028">a second means for receiving the designation data sent from the first sending means;</li>
        <li id="ul0004-0003" num="0029">means for coupling the image data corresponding to the designation data received by the second receiving means to each other to group them;</li>
        <li id="ul0004-0004" num="0030">means for detecting features of the image data corresponding to the designation data; and</li>
        <li id="ul0004-0005" num="0031">means for uncoupling the image data, coupled by the coupling means and corresponding to the designation data, according to each feature of the image data corresponding to the designation data in the same group.</li>
    </ul>
    </li>
</ul>
</p>
<p id="p-0023" num="0032">In the transmitter provided in the above communication system, the means for coupling image data to group them couples small areas corresponding to earlier designation data and small areas corresponding to current designation data to each other by storing the same identifier information as identifier information corresponding to the earlier designation data stored in a storage means so as to correspond to the small areas corresponding to the current designation data. Also, the feature detecting means detects, as the feature, the improvement of an object in image data in interesting small areas in interesting image data of moving image data consisting of a plurality of image data. When one of a plurality of small areas to which the same identifier information is appended by the coupling means is different in feature from other small areas, the uncoupling means uncouples the one and other small areas from each other by changing the identifier information of the one small area to different one of the other small areas.</p>
<p id="p-0024" num="0033">Also, the above object can be attained by providing an image processing method including the steps of:</p>
<p id="p-0025" num="0034">acquiring designation data from the user;</p>
<p id="p-0026" num="0035">coupling image data corresponding to the designation data to each other to group them;</p>
<p id="p-0027" num="0036">detecting a feature of the image data corresponding to the designation data; and</p>
<p id="p-0028" num="0037">uncoupling the image data, coupled by the coupling means and corresponding to the designation data, according to each feature of image data corresponding to the designation data in the same group.</p>
<p id="p-0029" num="0038">Also, the above object can be attained by providing a communication method for communications of image data between a transmitter and receiver, wherein:</p>
<p id="p-0030" num="0039">the receiver functions to:
<ul id="ul0005" list-style="none">
    <li id="ul0005-0001" num="0000">
    <ul id="ul0006" list-style="none">
        <li id="ul0006-0001" num="0040">receive the image data sent from the transmitter;</li>
        <li id="ul0006-0002" num="0041">output the image data received by the first receiving means;</li>
        <li id="ul0006-0003" num="0042">output the image data outputted from the outputting means;</li>
        <li id="ul0006-0004" num="0043">designate a time-spatial position of the received image data outputted from the outputting means; and</li>
        <li id="ul0006-0005" num="0044">send designation data indicative of the time-spatial position of the image data, designated by the designating means; and</li>
    </ul>
    </li>
</ul>
</p>
<p id="p-0031" num="0045">the transmitter functions to:
<ul id="ul0007" list-style="none">
    <li id="ul0007-0001" num="0000">
    <ul id="ul0008" list-style="none">
        <li id="ul0008-0001" num="0046">receive image data continuously;</li>
        <li id="ul0008-0002" num="0047">receive the designation data sent from the first sending means;</li>
        <li id="ul0008-0003" num="0048">couple the image data corresponding to the designation data received by the second receiving means to each other to group them;</li>
        <li id="ul0008-0004" num="0049">detect features of the image data corresponding to the designation data; and</li>
        <li id="ul0008-0005" num="0050">uncouple the image data, coupled by the coupling means and corresponding to the designation data, according to each feature of the image data corresponding to the designation data in the same group.</li>
    </ul>
    </li>
</ul>
</p>
<p id="p-0032" num="0051">Also, the above object can be attained by providing a recording medium having provided there a program which can be read by an information processing means, the program comprising the steps of:</p>
<p id="p-0033" num="0052">acquiring designation data from the user;</p>
<p id="p-0034" num="0053">coupling image data corresponding to the designation data to each other to group them;</p>
<p id="p-0035" num="0054">detecting a feature of the image data corresponding to the designation data; and</p>
<p id="p-0036" num="0055">uncoupling the image data, coupled by the coupling means and corresponding to the designation data, according to each feature of image data corresponding to the designation data in the same group.</p>
<p id="p-0037" num="0056">Also, the above object can be attained by providing a recording medium having recorded therein a program which can be read by an information processing means, the program controlling:</p>
<p id="p-0038" num="0057">the receiver to:
<ul id="ul0009" list-style="none">
    <li id="ul0009-0001" num="0000">
    <ul id="ul0010" list-style="none">
        <li id="ul0010-0001" num="0058">receive the image data sent from the transmitter;</li>
        <li id="ul0010-0002" num="0059">output the image data received by the first receiving means;</li>
        <li id="ul0010-0003" num="0060">designate a time-spatial position of the received image data outputted from the outputting means; and</li>
        <li id="ul0010-0004" num="0061">send designation data indicative of the time-spatial position of the image data, designated by the designating means; and</li>
    </ul>
    </li>
</ul>
</p>
<p id="p-0039" num="0062">the transmitter to:
<ul id="ul0011" list-style="none">
    <li id="ul0011-0001" num="0000">
    <ul id="ul0012" list-style="none">
        <li id="ul0012-0001" num="0063">receive image data continuously;</li>
        <li id="ul0012-0002" num="0064">receive the designation data sent from the first sending means;</li>
        <li id="ul0012-0003" num="0065">couple the image data corresponding to the designation data received by the second receiving means to each other to group them;</li>
        <li id="ul0012-0004" num="0066">detect features of the image data corresponding to the designation data; and</li>
        <li id="ul0012-0005" num="0067">uncouple the image data, coupled by the coupling means and corresponding to the designation data, according to each feature of the image data corresponding to the designation data in the same group.</li>
    </ul>
    </li>
</ul>
</p>
<p id="p-0040" num="0068">These objects and other objects, features and advantages of the present invention will become more apparent from the following detailed description of the best mode for carrying out the present invention when taken in conjunction with the accompanying drawings.</p>
<?BRFSUM description="Brief Summary" end="tail"?>
<?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?>
<description-of-drawings>
<heading id="h-0004" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading>
<p id="p-0041" num="0069"><figref idref="DRAWINGS">FIG. 1</figref> is a block diagram of the communication system according to the present invention.</p>
<p id="p-0042" num="0070"><figref idref="DRAWINGS">FIG. 2</figref> is a block diagram of the transmitter included in the communication system in <figref idref="DRAWINGS">FIG. 1</figref>.</p>
<p id="p-0043" num="0071"><figref idref="DRAWINGS">FIG. 3</figref> shows a flow of operations made in the transmitter in <figref idref="DRAWINGS">FIG. 2</figref>.</p>
<p id="p-0044" num="0072"><figref idref="DRAWINGS">FIG. 4</figref> is a block diagram of the receiver included in the communication system in <figref idref="DRAWINGS">FIG. 1</figref>.</p>
<p id="p-0045" num="0073"><figref idref="DRAWINGS">FIG. 5</figref> shows a flow of operations made in the receiver in <figref idref="DRAWINGS">FIG. 4</figref>.</p>
<p id="p-0046" num="0074"><figref idref="DRAWINGS">FIG. 6</figref> is a block diagram of the transmission processor included in the transmitter in <figref idref="DRAWINGS">FIG. 2</figref>.</p>
<p id="p-0047" num="0075"><figref idref="DRAWINGS">FIG. 7</figref> is a block diagram of the encoder of the transmission processor in <figref idref="DRAWINGS">FIG. 6</figref>.</p>
<p id="p-0048" num="0076"><figref idref="DRAWINGS">FIGS. 8A</figref>, <b>8</b>B and <b>8</b>C explain the hierarchical encoding/decoding.</p>
<p id="p-0049" num="0077"><figref idref="DRAWINGS">FIG. 9</figref> shows a flow of operations made in the transmission processor in <figref idref="DRAWINGS">FIG. 6</figref>.</p>
<p id="p-0050" num="0078"><figref idref="DRAWINGS">FIG. 10</figref> is a block diagram of the reception processor included in the receiver in <figref idref="DRAWINGS">FIG. 4</figref>.</p>
<p id="p-0051" num="0079"><figref idref="DRAWINGS">FIG. 11</figref> is a block diagram of the decoder included in the reception processor in <figref idref="DRAWINGS">FIG. 10</figref>.</p>
<p id="p-0052" num="0080"><figref idref="DRAWINGS">FIG. 12</figref> is a block diagram of the synthesizer of the receiver in <figref idref="DRAWINGS">FIG. 4</figref>.</p>
<p id="p-0053" num="0081"><figref idref="DRAWINGS">FIG. 13</figref> shows a flow of operations made in the synthesizer in <figref idref="DRAWINGS">FIG. 12</figref>.</p>
<p id="p-0054" num="0082"><figref idref="DRAWINGS">FIGS. 14A</figref>, <b>14</b>B and <b>14</b>C show examples of displays of an image on the image output unit of the receiver in <figref idref="DRAWINGS">FIG. 4</figref>.</p>
<p id="p-0055" num="0083"><figref idref="DRAWINGS">FIGS. 15A</figref> ans <b>15</b>B explain the relation between the spatial resolution and time resolution of an image sent from the transmitter and receiver, included in the communication system in <figref idref="DRAWINGS">FIG. 1</figref>.</p>
<p id="p-0056" num="0084"><figref idref="DRAWINGS">FIG. 16</figref> is a block diagram of the object extraction unit included in the transmitter shown in <figref idref="DRAWINGS">FIG. 2</figref> to extract an object based on click data.</p>
<p id="p-0057" num="0085"><figref idref="DRAWINGS">FIG. 17</figref> shows a flow of operations made in the object extraction unit shown in <figref idref="DRAWINGS">FIG. 16</figref>.</p>
<p id="p-0058" num="0086"><figref idref="DRAWINGS">FIG. 18</figref> shows, in detail, the flow of operations made in step S<b>42</b> in the object extracting procedure in <figref idref="DRAWINGS">FIG. 17</figref>.</p>
<p id="p-0059" num="0087"><figref idref="DRAWINGS">FIG. 19</figref> shows, in detail, the flow of operations made in step S<b>47</b> in the object extracting procedure in <figref idref="DRAWINGS">FIG. 17</figref>.</p>
<p id="p-0060" num="0088"><figref idref="DRAWINGS">FIG. 20</figref> shows another example of the flow of operations made in the object extraction unit shown in <figref idref="DRAWINGS">FIG. 16</figref>.</p>
<p id="p-0061" num="0089"><figref idref="DRAWINGS">FIG. 21</figref> is a block diagram of the computer according to the present invention.</p>
</description-of-drawings>
<?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?>
<?DETDESC description="Detailed Description" end="lead"?>
<heading id="h-0005" level="1">BEST MODE FOR CARRYING OUT THE INVENTION</heading>
<p id="p-0062" num="0090">The preferred modes of carrying out the present invention will be described herebelow with reference to the accompanying drawings.</p>
<p id="p-0063" num="0091">The data communication system according to the present invention is configured as shown in <figref idref="DRAWINGS">FIG. 1</figref>. The term “system” used herein refers to a logical assembly of a plurality of apparatuses whether the apparatuses of different constructions are in the same enclosure or not.</p>
<p id="p-0064" num="0092">As shown in <figref idref="DRAWINGS">FIG. 1</figref>, the data communication system includes at least two terminals <b>1</b> and <b>2</b> each being for example a mobile telephone, PHS (personal handy-phone system: registered trademark) or the like, a radio base station <b>3</b> or <b>5</b> which makes radio communications with the terminal <b>1</b> or <b>2</b>, and an exchange <b>4</b> such as a telephone station which provides a connection between the base stations <b>3</b> and <b>5</b>. Note that the radio base stations <b>3</b> and <b>5</b> are identical to each other or different from each other. With the above system configuration, the terminals <b>1</b> and <b>2</b> can send signals from one to the other of them and receive signals from their counterpart, via a transmission line formed from the radio base stations <b>3</b> and <b>5</b>, exchange <b>4</b>, etc.</p>
<p id="p-0065" num="0093">In the data communication system shown in <figref idref="DRAWINGS">FIG. 1</figref>, each of the terminals <b>1</b> and <b>2</b> being a mobile telephone, PHS or the like includes at least a key pad <b>8</b> for input of a phone number, characters, symbols, etc., a microphone <b>10</b> for input of a sound, a speaker <b>9</b> for output of a sound, a video camera <b>6</b> having an imaging device and optical system, capable of picking up a still image and moving image, respectively, and a display unit <b>7</b> capable of displaying characters and symbols as well as an image.</p>
<p id="p-0066" num="0094">Between the terminals <b>1</b> and <b>2</b>, there can be transferred sound signals as well as image data captured by the video camera <b>6</b>. Therefore, each of the terminals <b>1</b> and <b>2</b> can display an image picked up by its counterpart on the display unit <b>7</b>.</p>
<p id="p-0067" num="0095">There will be described here by way of example a data communication in which for example the terminal <b>1</b> sends image data while the terminal <b>2</b> receives the image data from the terminal <b>1</b>. In the following description, the terminal <b>1</b> or <b>2</b> will be referred to as “transmitter <b>1</b>” or “receiver <b>2</b>”, respectively, wherever appropriate.</p>
<p id="p-0068" num="0096">In this case, image data are sent along with information on its frame rate from the transmitter <b>1</b> to the receiver <b>2</b> via a transmission line including the base stations <b>3</b> and <b>5</b>, exchange <b>4</b>, etc. Receiving the image data sent from the transmitter <b>1</b>, the receiver <b>2</b> will display, on the display unit <b>7</b> such as a liquid crystal display (LCD) or the like, a moving image reproduced from the image data according the received frame rate information. On the other hand, the receiver <b>2</b> will send, to the transmitter <b>1</b> via the transmission line, control information used to control the spatial resolution and time resolution of the image displayed on the display unit <b>7</b>. That is, the receiver <b>2</b> will send, to the transmitter <b>1</b>, control information (“click data” as designation data which will further be described later) used at the transmitter <b>1</b> to localize an area the user of the receiver <b>2</b> is interested in.</p>
<p id="p-0069" num="0097">Receiving the control information (click data) from the receiver <b>2</b>, the transmitter <b>1</b> will localize, based on the click data, the image area (interesting area) the user of the receiver <b>2</b> is interested in from an image to be displayed at the receiver <b>2</b>, such as an image picked up by the video camera <b>6</b> of the transmitter <b>1</b>. Further, the transmitter <b>1</b> will control the amount of information in image data to be sent to the receiver <b>2</b> in such a manner that the spatial resolution and time resolution of the localized image area will be changed while satisfying given conditions. Note that in case each of the transmitter <b>1</b> and receiver <b>2</b> is a PHS terminal for example, the frequency range of the transmission line will be 1895.1500 to 1905.9500 MHz and the transmission rate will be 128 kbps (bit per second).</p>
<p id="p-0070" num="0098">Referring now to <figref idref="DRAWINGS">FIG. 2</figref>, there is illustrated in the form of a block diagram the transmitter <b>1</b> included in the data communication system shown in <figref idref="DRAWINGS">FIG. 1</figref>. As shown, the transmitter <b>1</b> further includes an image input unit <b>11</b> consisting of the video camera <b>6</b> having for example the imaging device (CCD: charge-coupled device) and optical system, an image signal processing circuit to generate image data from the image signal picked up by the video camera <b>6</b>, etc. That is, the user of the transmitter <b>1</b> images a desired object by the video camera <b>6</b>, the image signal processing circuit generates image data from the image supplied from the video camera <b>6</b>, and the image input unit <b>11</b> sends the image data to a pre-processor <b>12</b>.</p>
<p id="p-0071" num="0099">The pre-processor <b>12</b> includes mainly a background extraction unit <b>13</b>, object extraction unit <b>14</b> and an appended information computation circuit <b>15</b>. Further detailed description of the pre-processor <b>12</b> will be given later.</p>
<p id="p-0072" num="0100">Based on the click data sent from the receiver <b>2</b>, the object extraction unit <b>14</b> of the pre-processor <b>12</b> extracts, from the image captured by the video camera <b>6</b> of the image input unit <b>11</b>, an area the user of the receiver <b>2</b> is interested in, that is, an interesting area, and supplies a transmission processor <b>16</b> with image data corresponding to the extracted interesting area. Note that in case the image picked up by the video camera <b>6</b> of the image input unit <b>11</b> includes a plurality of areas the user of the receiver <b>2</b> is interested in, the object extraction unit <b>14</b> will supply image data corresponding to the plurality of interesting areas to the transmission processor <b>16</b>. Also, the image data corresponding to the interesting area extracted by the object extraction unit <b>14</b> is also supplied to the appended information computation unit <b>15</b>.</p>
<p id="p-0073" num="0101">The area the user is interested in is for example an object such as a substance found in an image. Note that the “object” referred to herein is one of pieces of an image divided by a unit and which can be processed per unit and for processing each of special substances in an image, the substance is defined as “object”. According to the present invention, object data are extracted from an image on the basis of click data and processed per object. Note that object is prepared in different manners depending upon the content of a required image.</p>
<p id="p-0074" num="0102">In the object extraction unit <b>14</b>, an object as an example of interesting area (will be referred to as “appropriate object” hereunder) is extracted as will be described below. Note that the interesting area has not to always be an object but may be an image area other than an object, image area in an object or background image part which will be described later or the like. The present invention will be described concerning an object as an interesting area.</p>
<p id="p-0075" num="0103">In the embodiment of the present invention, small-object image data corresponding to click data derived from clicking of an image by the user are extracted and the small-object image data are connected or not to each other to extract object image data and then extract an object image from the object image data.</p>
<p id="p-0076" num="0104">Also, when the interest of the user of the receiver <b>2</b> is directed to another area, the object extraction unit <b>14</b> will detect the change of interest to extract an object image being a new interesting area based on the result of detection of the change in interest of the user. The object extraction effected in the object extraction unit <b>14</b>, that is, the localization of an interesting area and detection of an area to which the user's interest has shifted will further be described later.</p>
<p id="p-0077" num="0105">Next, the background extraction unit <b>13</b> of the pre-processor <b>12</b> extracts, based on the object extraction result supplied from the object extraction unit <b>14</b>, signals (will be referred to as “background image data” hereunder) equivalent to a background portion (image area other than the interesting area; will be referred to as “background image” hereunder) of an image from the image data supplied from the image input unit <b>11</b>, and supplies the extracted background image data to the transmission processor <b>16</b> and appended information computation unit <b>15</b>. The background image is a flat image area whose activity is low and having no special meaning as an image. Of course, the background image includes an image having no special meaning as well as an object the user is interested in, but for the simplicity of the explanation, the above flat image area will be taken as a background image in the following description of the present invention.</p>
<p id="p-0078" num="0106">The appended information computation unit <b>15</b> detects, based on the background image data supplied from the background extraction unit <b>13</b>, a movement of the background caused by the change in imaging direction of the image input unit <b>11</b> during imaging, for example a background movement vector indicative of a panning and tilting. The appended information computation unit <b>15</b> detects, based on the image data of an object (will be referred to as “object image data” hereunder) supplied from the object extraction unit <b>14</b>, an object movement vector indicative of a movement of the object. The appended information computation unit <b>15</b> supplies the transmission processor <b>16</b> with the movement vectors as appended information. Also, based on the object image data supplied from the object extraction unit <b>14</b>, the appended information computation unit <b>15</b> supplies the transmission processor <b>16</b> with an image picked up by the video camera <b>6</b> of the image input unit <b>11</b>, that is, information on the object such as a position, profile indicating a shape, etc. of the object in a frame image, as appended information. Namely, the object extraction unit <b>14</b> extracts also information on the object such as position, shape, etc. of the object during extraction of an object image, and supplies them to the appended information computation unit <b>15</b>. The appended information computation unit <b>15</b> will output the information on the object as appended information.</p>
<p id="p-0079" num="0107">Based on the click data supplied from the receiver <b>2</b>, the transmission processor <b>16</b> encodes the object image data from the object extraction unit <b>14</b>, background image data from the background extraction unit <b>13</b> and appended information from the appended information computation unit <b>15</b> so as to meet the requirement for a data rate at which data can be transmitted on the transmission line while raising the spatial and time resolutions of the object image in an image to be displayed on the display unit <b>2</b>. Further, the transmission processor <b>16</b> multiplexes the encoded object image data (will be referred to as “object encoded data” hereunder), background image data (will be referred to as “background encoded data” hereunder) and appended information (will be referred to as “appended information encoded data” hereunder), and sends the multiplexed data along with frame rate information to the receiver <b>2</b> via the transmission line.</p>
<p id="p-0080" num="0108">Next, the operations of the transmitter <b>1</b> shown in <figref idref="DRAWINGS">FIG. 2</figref> will be described with reference to the flow chart shown in <figref idref="DRAWINGS">FIG. 3</figref>.</p>
<p id="p-0081" num="0109">First in step S<b>1</b> in <figref idref="DRAWINGS">FIG. 3</figref>, the video camera <b>6</b> of the image input unit <b>11</b> at the transmitter <b>1</b> captures an image, and sends the image data to the pre-processor <b>12</b>.</p>
<p id="p-0082" num="0110">Next in step S<b>2</b>, the transmitter <b>1</b> receives the click data sent from the transmitter <b>2</b>, and supplies the click data to the pre-processor <b>12</b>.</p>
<p id="p-0083" num="0111">In step S<b>3</b>, the pre-processor <b>12</b> having received the image data and click data pre-processes the data for background extraction, object extraction and appended information computation, and sends background image data, object image data and appended information thus obtained to the transmission processor <b>16</b>. The object extraction includes detection of interesting-object change as well.</p>
<p id="p-0084" num="0112">In step S<b>4</b>, the transmission processor <b>16</b> computes amounts of the object image data, background image data and appended information so as to meet the requirement for a data rate at which data can be transmitted via the transmission line, and multiplexes the object image data, background image data and appended information according to their data amount by encoding them as will further be described later. Thereafter, the transmission processor <b>16</b> will send the multiplexed data along with frame rate information to the receiver <b>2</b> via the transmission line.</p>
<p id="p-0085" num="0113">Subsequently, the procedure returns to step S<b>1</b> and similar operations are repeated.</p>
<p id="p-0086" num="0114"><figref idref="DRAWINGS">FIG. 4</figref> shows the construction of the receiver <b>2</b> included in the data communication system shown in <figref idref="DRAWINGS">FIG. 1</figref>.</p>
<p id="p-0087" num="0115">The multiplexed data sent from the transmitter <b>1</b> via the transmission line are received by a reception processor <b>21</b> in the receiver <b>2</b> shown in <figref idref="DRAWINGS">FIG. 4</figref>. The reception processor <b>21</b> will demultiplex, from the received multiplexed data, the background encoded data, object encoded data and appended information encoded data and decode them, and send the decoded background image data, object image data and appended information to a synthesizer <b>22</b>.</p>
<p id="p-0088" num="0116">The synthesizer <b>22</b> will synthesize an image from the decoded background image date, object image data and appended information supplied from the reception processor <b>21</b>, and supply the synthesized image signals to an image output unit <b>23</b>. Also, the synthesizer <b>22</b> controls the spatial and time resolutions of the synthesized image based on the click data supplied from a click data input unit <b>24</b>.</p>
<p id="p-0089" num="0117">Based on the supplied image data, the image output unit <b>23</b> will generate a drive signal for driving the liquid crystal display or the like in the display unit <b>7</b> and send the drive signal to the liquid crystal display or the like at a frame rate which is based on the frame rate information received along with the aforementioned multiplexed data. Thus, the image synthesized by the synthesizer <b>22</b> is displayed on the display unit <b>7</b>.</p>
<p id="p-0090" num="0118">When the user operates the key pad <b>8</b> having the function as a pointing device to designate a coordinate position of an image on the display unit <b>7</b>, the click data input unit <b>24</b> generates click data indicating a clicked position corresponding to the operation of the key pad <b>8</b> by the user, that is, a coordinate position, and a clicked time. Namely, when the user clicks the key pad <b>8</b> to designate an interesting area being a desired image portion in an image displayed on the display unit <b>7</b>, the click data input unit <b>24</b> generates click data indicating coordinate information on the clicked position and a clicked time. The click data generated by the click data input unit <b>24</b> are sent to the synthesizer <b>22</b> and click data transmission unit <b>25</b>.</p>
<p id="p-0091" num="0119">Receiving the click data from the click data input unit <b>24</b>, the click data transmission unit <b>25</b> sends it to the transmitter <b>1</b> via the transmission line.</p>
<p id="p-0092" num="0120">Next, the operations of the receiver <b>2</b> shown in <figref idref="DRAWINGS">FIG. 4</figref> will be outlined with reference to the flow chart shown in <figref idref="DRAWINGS">FIG. 5</figref>.</p>
<p id="p-0093" num="0121">First in step S<b>11</b> in <figref idref="DRAWINGS">FIG. 5</figref>, the reception processor <b>21</b> in the receiver <b>2</b> receives the nultiplexed data from the transmitter <b>1</b> via the transmission line.</p>
<p id="p-0094" num="0122">Next in step S<b>12</b>, the reception processor <b>21</b> demultiplexes, from the multiplexed data, the background encoded data, object encoded data and appended information encoded data, and then decodes the thus demultiplexed encoded data. The decoded background image data, object image data and appended information are sent to the synthesizer <b>22</b>.</p>
<p id="p-0095" num="0123">In step S<b>13</b>, the click data input unit <b>24</b> of the receiver <b>2</b> acquires the click data by clicking the key pad <b>8</b> by the user, sends it to the synthesizer <b>22</b> and also to the click data transmission unit <b>25</b>, and the click data are sent from the click data transmission unit <b>25</b> to the transmitter <b>1</b>.</p>
<p id="p-0096" num="0124">Next in step S<b>14</b>, the synthesizer <b>22</b> synthesizes an image from the background image data, object image data and appended information supplied from the reception processor <b>21</b> and the click data supplied from the click data input unit <b>24</b>, and controls the spatial and time resolutions of the synthesized image.</p>
<p id="p-0097" num="0125">Thereafter in step S<b>15</b>, the image output unit <b>23</b> has the liquid crystal display or the like in the display unit <b>7</b> display the image synthesized by the synthesizer <b>22</b> thereon on the basis of the frame rate information received along with the multiplexed data.</p>
<p id="p-0098" num="0126">Thereafter, the procedure returns to step S<b>11</b>, and similar operations are repeated.</p>
<p id="p-0099" num="0127">Next, the construction of the transmission processor <b>16</b> included in the transmitter <b>1</b> shown in <figref idref="DRAWINGS">FIG. 2</figref> will be described in detail with reference to <figref idref="DRAWINGS">FIG. 6</figref>.</p>
<p id="p-0100" num="0128">As shown in <figref idref="DRAWINGS">FIG. 6</figref>, the transmission processor <b>16</b> is supplied with the background image data, object image data and appended information from the pre-processor <b>12</b> in <figref idref="DRAWINGS">FIG. 2</figref>. The background image data, object image data and appended information are supplied to an encoder <b>31</b> and controller <b>35</b>.</p>
<p id="p-0101" num="0129">The encoder <b>31</b> hierarchically encodes the supplied background image data, object image data and appended information as will further be described later, and supplies each data thus encoded to a multiplexer (MUX) <b>32</b>.</p>
<p id="p-0102" num="0130">Under the control of the controller <b>35</b>, the MUX <b>32</b> selects the background encoded data, object encoded data and appended information encoded data supplied from the encoder <b>31</b> and supplies them as multiplexed data to a transmission unit <b>33</b>.</p>
<p id="p-0103" num="0131">The transmission unit <b>33</b> modulates the multiplexed data supplied from the MUX <b>32</b> according to the aforementioned frame rate information and the transmission standard for a downstream transmission line, and sends the modulated multiplexed data to the receiver <b>2</b> via the downstream transmission line.</p>
<p id="p-0104" num="0132">Also, the data amount computation unit <b>34</b> monitors the multiplexed data outputted from the MUX <b>32</b> to the transmission unit <b>33</b>, compute a data rate for the multiplexed data and supplies the computed data rate to the controller <b>35</b>.</p>
<p id="p-0105" num="0133">The controller <b>35</b> controls the output of multiplexed data from the MUX <b>32</b> so that the data rate computed by the data amount computation unit <b>34</b> will not exceed the transmission rate of the transmission line while receiving the click data received from the receiver <b>2</b> via the transmission line to control the multiplexing of the encoded data in the MUX <b>32</b>.</p>
<p id="p-0106" num="0134">The encoder <b>31</b> shown in <figref idref="DRAWINGS">FIG. 6</figref> is constructed as shown in detail in <figref idref="DRAWINGS">FIG. 7</figref>.</p>
<p id="p-0107" num="0135">In the encoder <b>31</b> in <figref idref="DRAWINGS">FIG. 7</figref>, the background image data are supplied to a difference computation unit <b>41</b>B. The difference computation unit <b>41</b>B subtracts one frame-precedent, already processed background image data from background image data contained in an image frame (will be referred to as “current frame” hereunder wherever appropriate) supplied from a local decoder <b>44</b>B and going to be processed at present, and supplies difference data of the background image (will be referred to as “background image difference data” hereunder) as the subtraction result to a hierarchical encoder <b>42</b>B.</p>
<p id="p-0108" num="0136">The hierarchical encoder <b>42</b>B hierarchically encodes the background image difference data from the difference computation unit <b>41</b>B, and supplies data obtained by the encoding, that is, background encoded data, to a storage unit <b>43</b>B.</p>
<p id="p-0109" num="0137">The storage unit <b>43</b>B provisionally stores the background encoded data supplied from the hierarchical encoder <b>42</b>B. The background encoded data stored in the storage unit <b>43</b>B are sent to the MUX <b>32</b> in <figref idref="DRAWINGS">FIG. 6</figref>.</p>
<p id="p-0110" num="0138">Further, the background encoded data stored in the storage unit <b>43</b>B are supplied to the local decoder <b>44</b>B. The local decoder <b>44</b>B locally decodes the background encoded data to decode their initial background image data, and supplies the decoded background image data to the difference computation unit <b>41</b>B. The background image data thus decoded by the local decoder <b>44</b>B are used in the difference computation unit <b>41</b>B to acquire data on a difference from background image data in a next frame.</p>
<p id="p-0111" num="0139">In the encoder <b>31</b> in <figref idref="DRAWINGS">FIG. 7</figref>, the object image data are supplied to a difference computation unit <b>41</b>F. The difference computation unit <b>41</b>F subtracts one frame-preceding, already processed background image data from object image data contained in an image frame (current frame) supplied from a local decoder <b>44</b>F and going to be processed at present, and supplies difference data of the object (will be referred to as “object image difference data” hereunder) as the subtraction result to a hierarchical encoder <b>42</b>F.</p>
<p id="p-0112" num="0140">The hierarchical encoder <b>42</b>F hierarchically encodes the background image difference data from the difference computation unit <b>41</b>F, and supplies data obtained by the encoding (object encoded data) to a storage unit <b>43</b>F.</p>
<p id="p-0113" num="0141">The storage unit <b>43</b>F provisionally stores the object encoded data supplied from the hierarchical encoder <b>42</b>F. The object encoded data stored in the storage unit <b>43</b>F are sent to the MUX <b>32</b> in <figref idref="DRAWINGS">FIG. 6</figref>.</p>
<p id="p-0114" num="0142">Further, the object encoded data stored in the storage unit <b>43</b>F are supplied to the local decoder <b>44</b>F. The local decoder <b>44</b>F locally decodes the object encoded data to decode their initial object image data, and supplies the decoded object image data to the difference computation unit <b>41</b>F. The object image data thus decoded by the local decoder <b>44</b>F are used in the difference computation unit <b>41</b>F to acquire data on a difference from object image data in a next frame.</p>
<p id="p-0115" num="0143">Note that in case there exists a plurality of objects (#1, #2, #3, . . . ), image data corresponding to the plurality of objects are subject to difference computation, hierarchical encoding, storage and local decoding by the difference computation unit <b>41</b>F, hierarchical encoder <b>42</b>F, storage unit <b>43</b>F and local decoder <b>44</b>F, respectively.</p>
<p id="p-0116" num="0144">Also, in the encoder <b>31</b> shown in <figref idref="DRAWINGS">FIG. 7</figref>, the appended information is supplied to a VLC (variable-length encoder) <b>45</b>. The VLC <b>45</b> makes variable-length encoding of the appended information. The variable-length encoding method may be a one which can compress the data by reducing the redundance. Namely, the variable-length encoding may be the run-length encoding, Huffman encoding or the like. The variable-length encoded appended information is sent as the aforementioned appended information encoded data to the MUX <b>32</b> in <figref idref="DRAWINGS">FIG. 6</figref>.</p>
<p id="p-0117" num="0145">Next, the hierarchical encoding effected in the encoder <b>31</b> in <figref idref="DRAWINGS">FIG. 7</figref> and the decoding effected at the receiving side correspondingly to the hierarchical encoding will be described with reference to <figref idref="DRAWINGS">FIG. 8</figref>.</p>
<p id="p-0118" num="0146">The encoder <b>31</b> in <figref idref="DRAWINGS">FIG. 7</figref> makes, for each of 3 layers, for example, such a hierarchical encoding as to take a mean value (mean pixel value) of 4 pixels consisting of 2 horizontal pixels and 2 vertical pixels in a low layer for example as a value of one pixel in a layer one level higher than the low layer. Note that the term “pixel value” used herein refers to a difference obtained by the difference computation effected as a preliminary operation for the hierarchical encoding, namely, a difference of each pixel. Of course, in case no difference computation is done before the hierarchical encoding, the pixel value is not such a specified one.</p>
<p id="p-0119" num="0147">The above will further be described below on the assumption that an image in the lowest layer (first layer) consists of 4 horizontal pixels and 4 vertical pixels (will be referred to as “4×4 pixels” hereunder) for example as shown in <figref idref="DRAWINGS">FIG. 8A</figref>. In this case, the hierarchical encoding will be such that there is calculated a mean value of four pixels h<b>00</b>, h<b>01</b>, h<b>02</b> and h<b>03</b> including the two horizontal pixels and two vertical pixels (will be referred to as “2×2 pixels” hereunder) at the upper left of the 4×4 pixels and the mean value is taken as the value of a pixel m<b>0</b> at the upper left in the second layer. Similarly, a mean value of 2×2 pixels h<b>10</b>, h<b>11</b>, h<b>12</b> and h<b>13</b> at the upper right of the 4×4 pixels in the first layer is taken as the value of a pixel m<b>1</b> at the upper right of the second layer; mean value of 2×2 pixels h<b>20</b>, h<b>21</b>, h<b>22</b> and h<b>23</b> at the upper left of the 4×4 pixels in the first layer is taken as the value of a pixel m<b>2</b> at the lower left of the second layer; and mean value of 2×2 pixels h<b>30</b>, h<b>31</b>, h<b>32</b> and h<b>33</b> at the lower right of the 4×4 pixels in the first layer is taken as the value of a pixel m<b>3</b> at the lower right of the second layer. Further in the hierarchical encoding, a mean value of four pixels m<b>0</b>, m<b>1</b>, m<b>2</b> and m<b>3</b> included in the 2×2 pixels in the second layer is determined and taken as the value of a pixel q in the third layer (highest layer).</p>
<p id="p-0120" num="0148">In the encoder <b>31</b> in <figref idref="DRAWINGS">FIG. 7</figref>, the hierarchical encoding is effected as having been described just above. Note that with such a hierarchical encoding, the spatial resolution of an image in the highest layer (third layer) is lowest, that of images in the lower layers will be higher and that of an image in the lowest layer (first layer) will be highest.</p>
<p id="p-0121" num="0149">In case all the above pixels h<b>00</b> to h<b>03</b>, h<b>10</b> to h<b>13</b>, h<b>20</b> to h<b>23</b>, h<b>30</b> to h<b>33</b>, m<b>0</b> to m<b>3</b> and q are to be sent, the data amount will be larger by the pixels m<b>0</b> to m<b>3</b> in the second layer and pixel q in the third layer than when only the pixels in the lowest layer are sent.</p>
<p id="p-0122" num="0150">To decrease the data amount for sending, the pixel q in the third layer is embedded in place of for example the pixel m<b>3</b>, at the upper right, of the pixels m<b>0</b> to m<b>3</b> in the second layer as shown in <figref idref="DRAWINGS">FIG. 8B</figref> and the data of the second layer thus consisting of the pixels m<b>0</b>, m<b>1</b>, m<b>2</b> and q and data of the first layer are sent. Thus, the data amount can be smaller by the data amount of the third layer.</p>
<p id="p-0123" num="0151">For sending the data in an amount decreased more than in the measure shown in <figref idref="DRAWINGS">FIG. 8B</figref>, the pixel m<b>0</b> in the second layer is replaced by for example the pixel h<b>03</b>, at the lower right, of the 2×2 pixels h<b>00</b> to h<b>03</b> in the first layer, used to determine the pixel m<b>0</b>, similarly the pixel m<b>1</b> in the second layer is replaced by for example the pixel h<b>13</b>, at the lower right, of the 2×2 pixels h<b>10</b> to h<b>13</b> in the first layer, used to determine the pixel m<b>1</b>, also the m<b>2</b> in the second layer is replaced by for example the pixel h<b>23</b>, at the lower right, of the 2×2 pixels h<b>20</b> to h<b>23</b> in the first layer, used to determine the pixel m<b>2</b>, as shown in <figref idref="DRAWINGS">FIG. 8C</figref>, and further the q in the third layer, buried in an pixel at the lower right of the pixels m<b>0</b> to m<b>3</b> in the second layer in <figref idref="DRAWINGS">FIG. 8B</figref> is replaced by for example the pixel h<b>33</b>, at the lower right, of the 2×2 pixels h<b>30</b> to h<b>33</b> in the first layer. The data amount in the third and second layers can thus be reduced. That is, in the example shown in <figref idref="DRAWINGS">FIG. 8C</figref>, 16 pixels (4×4 pixels) are thus transmitted, which number of pixels is the same as that of the pixels in the lowest (first) layer as shown in <figref idref="DRAWINGS">FIG. 8A</figref>. Therefore, it is possible in this case to send data equivalent to pixels in each of the first to third layers in an amount not increased.</p>
<p id="p-0124" num="0152">Note that the pixel m<b>3</b> in the second layer, replaced with the pixel q as in <figref idref="DRAWINGS">FIG. 8B</figref>, and the pixels h<b>03</b>, h<b>13</b>, h<b>23</b> and h<b>33</b> in the first layer, replaced with the pixels m<b>0</b>, m<b>1</b>, m<b>2</b> and q, as in <figref idref="DRAWINGS">FIG. 8C</figref>, can be decoded as will be described below.</p>
<p id="p-0125" num="0153">Namely, since the value of the pixel q is a mean value of the pixels m<b>0</b> to m<b>3</b>, it can be given by an equation of q=(m<b>0</b>+m<b>1</b>+m<b>2</b>+m<b>3</b>)/4. Thus, an equation of m<b>3</b>=4×q−(m<b>0</b>+m<b>1</b>+m<b>2</b>) can be used to determine (decode) the value of the pixel m<b>3</b> in the second layer from the pixel q in the third layer and pixels m<b>0</b> to m<b>2</b> in the second layer.</p>
<p id="p-0126" num="0154">Also, since the value of the pixel m<b>0</b> is a mean value of the pixels h<b>00</b> to h<b>03</b>, it can be given by an equation of m<b>0</b>=(h<b>00</b>+h<b>01</b>+h<b>02</b>+h<b>03</b>)/4. Thus, an equation of h<b>03</b>=4×m<b>0</b>−(h<b>00</b>+h<b>01</b>+h<b>02</b>) can be used to determine the value of the pixel h<b>03</b> in the first layer from the pixel m<b>0</b> in the second layer and pixels h<b>00</b> to h<b>02</b> in the first layer. Similarly, the value of each of the pixels h<b>13</b>, h<b>23</b> and h<b>33</b> can be determined.</p>
<p id="p-0127" num="0155">As in the above, pixels included in a layer and not sent can be decoded from sent pixels included in the layer and sent pixels included in a one-level higher layer.</p>
<p id="p-0128" num="0156">Next, the transmission processing effected in the transmission processor <b>16</b> in <figref idref="DRAWINGS">FIG. 6</figref> will be described with reference to the flow chart in <figref idref="DRAWINGS">FIG. 9</figref>.</p>
<p id="p-0129" num="0157">First in step S<b>21</b>, the controller <b>35</b> in the transmission processor <b>16</b> judges whether click data has been sent from the receiver <b>2</b>. If it judges that no click data has been transmitted from the receiver <b>2</b>, namely, if the controller <b>35</b> has not received any click data, the controller <b>35</b> will control, in step S<b>22</b>, the MUX <b>32</b> to select and multiplex background encoded data, object encoded data and appended information encoded data so that the receiver <b>2</b> can display an image with an ordinary time resolution such as a default time resolution.</p>
<p id="p-0130" num="0158">That is, when for example 30 frames/sec is set as the ordinary time resolution, the receiver <b>2</b> will display an image at a rate of 30 frames/sec. In this case, when multiplexed data has been sent at the transmission rate of the transmission line while maintaining the time resolution of 30 frames/sec, the MUX <b>32</b> will select and multiplex the encoded data of background, object and appended information so that the spatial resolution of an image displayed at the receiver <b>2</b> will be highest.</p>
<p id="p-0131" num="0159">More particularly, in case the hierarchical encoding has been effected with the three layers for example as in the above, if only the data in the third layer can be sent at the transmission rate of the transmission line for display of an image at the rate of 30 frames/sec, the MUX <b>32</b> will select the encoded data of background, object and appended data for display of the image data in the third layer. In this case, the receiver <b>2</b> ill display an image at the time resolution is 30 frames/sec and at horizontal and vertical spatial resolutions being a quarter of those of the first layer image data as their initial data.</p>
<p id="p-0132" num="0160">Next in step S<b>23</b>, the transmission processor <b>16</b> will send, from the transmission unit <b>33</b>, the multiplexed data from the MUX <b>32</b> along with the aforementioned set frame rate information via the transmission line, and then the procedure returns to step S<b>21</b>.</p>
<p id="p-0133" num="0161">If the controller <b>35</b> judges in step S<b>21</b> that click data has been sent from the receiver <b>2</b>, namely, if it has received the click data, it will recognize, in step S<b>24</b>, based on the click data, a clicked position being a coordinate of a focus of interest designated by the user by operating the clock data input unit <b>24</b> of the receiver <b>2</b>, and a clicked time.</p>
<p id="p-0134" num="0162">Then in step S<b>25</b>, the controller <b>35</b> will localize, based on the coordinate of the focus of interest, and clicked time, an area the user of the receiver <b>2</b> is interested in and set the thus localized interesting area as a preferred range in which the spatial resolution of an image displayed at the receiver <b>2</b> is preferentially improved, to thereby detect an image in the preferred range and corresponding appended information. Note that in the present invention, the image inside the preferred range corresponds to an object image while an image outside the preferred range corresponds to an image outside the interesting area such as a background image.</p>
<p id="p-0135" num="0163">In step S<b>26</b>, the controller <b>35</b> controls the MUX <b>32</b> to select and multiplex encoded data of an image inside the preferred range (object image), image outside the preferred range (background image) and appended information so that the image inside the preferred range will be displayed with a higher spatial resolution at the receiver <b>2</b>. Namely, when having received clock data from the receiver <b>2</b>, the controller <b>35</b> will control the MUX <b>32</b> to improve the spatial resolution of an image inside the preferred range by the sacrifice of the time resolution.</p>
<p id="p-0136" num="0164">In the above, the controller <b>35</b> controls the MUX <b>32</b> to improve the spatial resolution of an image inside the preferred range. However, the controller <b>35</b> may control the MUX <b>32</b> to improve the time resolution of an image inside the preferred range, that is, the frame rate of an image inside the preferred range. In this case, the controller <b>35</b> may attain the improvement by the sacrifice of the spatial resolution. Also, the controller <b>35</b> may control the total information amount by handling an image corresponding to a background image outside the preferred range as a still image.</p>
<p id="p-0137" num="0165">Thus, for an image inside the preferred range, the MUX <b>32</b> preferentially selects and multiplexes encoded data for display of image data in the third layer and also in the second layer, and outputs the multiplexed data.</p>
<p id="p-0138" num="0166">Further, in step S<b>26</b>, the controller <b>35</b> controls the MUX <b>32</b> to insert information on the position, size, etc. of the preferred range into appended information selected as the multiplexed data, and then goes to step S<b>23</b>.</p>
<p id="p-0139" num="0167">In step S<b>23</b>, the transmission unit <b>33</b> sends the multiplexed data output from the MUX <b>32</b> along with frame rate information via the transmission line, and then the procedure returns to step S<b>21</b>.</p>
<p id="p-0140" num="0168">For the simplicity of the explanation, it is assumed here that in step S<b>26</b>, encoded data for display of image data in the third layer are continuously selected for an image outside the preferred range, for example, a background image, as in step S<b>22</b>. In this case, in the controller <b>35</b>, the amount of the multiplexed data in step S<b>26</b> will be larger by an image with a higher spatial resolution inside the preferred range, that is, image data in the second layer for the object image, than in step S<b>22</b>.</p>
<p id="p-0141" num="0169">At this time, even if it is intended to display an image at a rate of for example 30 frames/sec, since the transmission rate of the transmission line permits to display only the image data in the third layer, the multiplexed data including the data in the second layer, acquired in step S<b>26</b>, will not be such data as permits to display an image at the rate of 30 frames/sec.</p>
<p id="p-0142" num="0170">In this case, the transmission unit <b>33</b> will send multiplexed data whose rate is lower than 30 frames/sec or is 0 frame/sec in an extreme case, namely, a still image. Thus, at the receiver <b>2</b>, there will be displayed an image included in the preferred range and whose horizontal and vertical spatial resolutions are a half of those of their initial image (image in the first layer), that is, an image (image in the second layer) whose horizontal and vertical spatial resolutions are double those of a third layer image having so far been displayed. At this time, however, the time resolution of an image displayed at the receiver <b>2</b> will be less than 30 frames/sec.</p>
<p id="p-0143" num="0171">After data in the second layer for an image inside the preferred range are sent as in the above, if it is judged in step S<b>21</b> that clock data has been sent from the receiver <b>2</b> as in the above, namely, when the user continuously operates the clock data input unit <b>24</b> to designate a focus of interest identical to or near the preceding one, the focus of interest identical to or near the preceding one is recognized in step S<b>24</b>, and the same preferred range as the preceding one is set in step S<b>25</b>, and the procedure goes to step S<b>26</b>. Thereby, in step S<b>26</b>, the controller <b>35</b> will control the MUX <b>32</b> to select and multiplex encoded data so that an image inside the preferred range can be displayed with a higher spatial resolution at the receiver <b>2</b>.</p>
<p id="p-0144" num="0172">Since the encoded data of images in the third and second layers and information appended to the images are preferentially selected for the image inside the preferred range as in the above, the encoded data of images in the first layer and information appended to the images are also preferentially selected and multiplexed. The high-resolution information is inserted into the appended information as having been described with respect to step S<b>26</b>, and the multiplexed data from the MUX <b>32</b> are sent along with the frame rate information from the transmission unit <b>33</b> via the transmission line in step S<b>23</b>, and then the procedure returns to step S<b>21</b>.</p>
<p id="p-0145" num="0173">In this case, the receiver <b>2</b> will display an image included in the preferred range and whose spatial resolution is the same as that of their initial image (image in the first layer), that is, an image (image in the first layer) whose horizontal and vertical spatial resolutions are 4 times higher than those of a fourth layer image displayed first. However, an image whose time resolution is lower than the 30 frames/sec or is 0 frame/sec in an extreme case is handled as a still image.</p>
<p id="p-0146" num="0174">With the above operations, since data intended for improvement of the spatial resolution of an image inside the preferred range including a focus of interest, namely, an interesting area such as an object image, are preferentially sent if the user of the receiver <b>2</b> continuously operates the click data input unit <b>24</b> to designate for example the same focus of interest, that is, an interesting area, the spatial resolution of the image inside the preferred range including the focus of interest is gradually improved with the result that the image in the preferred range will be displayed more definitely. That is, an interesting area being an image of a portion the user of the receiver <b>2</b> is interested in, for example, an object image, will be displayed more definitely.</p>
<p id="p-0147" num="0175">As in the above, since image data sending is controlled so that the spatial resolution or time resolution of an interesting area being an image within a preferred area localized with a focus of interest which is based on click data, such as an object image, is changed within a resolution range corresponding to the transmission rate of a transmission line, an image corresponding to a focus of interest, sent at a limited transmission rate, can be displayed at the receiver <b>2</b> with a higher spatial resolution. That is, by improving the spatial resolution of an object image inside a preferred range at the sacrifice of the time resolution of the image, the object image, even when sent at the limited transmission rate, can be displayed at the receiver <b>2</b> more definitely, namely, with a higher spatial resolution.</p>
<p id="p-0148" num="0176">Next, the reception processor <b>21</b> included in the receiver <b>2</b> shown in <figref idref="DRAWINGS">FIG. 4</figref> will be described in further detail below with reference to <figref idref="DRAWINGS">FIG. 10</figref>.</p>
<p id="p-0149" num="0177">As shown in <figref idref="DRAWINGS">FIG. 10</figref>, the multiplexed data supplied via the transmission line are received and modulated by a reception unit <b>51</b>, and then supplied to a demultiplexer (DMUX) <b>52</b>.</p>
<p id="p-0150" num="0178">The DMUX <b>52</b> demultiplexed the multiplexed data supplied from the reception unit <b>51</b> into background encoded data, object encoded data and appended information encoded data, and supplies these encoded data to a decoder <b>53</b>.</p>
<p id="p-0151" num="0179">Reversely following the encoding-compression procedure, the decoder <b>53</b> decodes the encoded data of background, object or appended information (data derived from encoding of the difference in this embodiment) to their respective initial data, and outputs it to the synthesizer <b>22</b> shown in <figref idref="DRAWINGS">FIG. 4</figref>.</p>
<p id="p-0152" num="0180"><figref idref="DRAWINGS">FIG. 11</figref> shows in detail the decoder <b>53</b> in <figref idref="DRAWINGS">FIG. 10</figref>.</p>
<p id="p-0153" num="0181">As shown in <figref idref="DRAWINGS">FIG. 11</figref>, the hierarchically encoded background image difference data being background encoded data are supplied to an adder <b>61</b>B which is also supplied with background image data preceding by one frame the background encoded data, stored in a storage unit <b>62</b>B and already decoded. The adder <b>61</b>B adds, to the supplied background image difference data, the background image data one frame before the difference data, supplied from the storage unit <b>62</b>B, to thereby decode background image data in a layer required for the current frame. The decoded background image data are supplied to and stored in the storage unit <b>62</b>B, and then read and supplied to the adder <b>61</b>B while being sent to the synthesizer <b>22</b> in <figref idref="DRAWINGS">FIG. 4</figref>.</p>
<p id="p-0154" num="0182">The hierarchically encoded object image difference data being object encoded data are supplied to an adder <b>61</b>F which is also supplied with object image data one frame before the difference data, stored in a storage unit <b>62</b>F and already decoded. By adding, to the supplied object image difference data, object image data one frame before the difference data, supplied from the storage unit <b>62</b>F, the adder <b>61</b>F decodes object image data in a layer required for the current frame. The decoded object image data are supplied to and stored in the storage unit <b>62</b>F, and then read and supplied to the adder <b>61</b>F while being sent to the synthesizer <b>22</b> in <figref idref="DRAWINGS">FIG. 4</figref>. Note that when there is a plurality of objects, the adder <b>61</b>F and storage unit <b>62</b>F will repeat the above-mentioned hierarchical decoding for the difference data of the plurality of objects.</p>
<p id="p-0155" num="0183">The above-mentioned variable length-encoded appended information being appended information encoded data are supplied to a reverse VLC (variable-length coder) unit <b>63</b> where they are variable length-decoded to their initial appended information. The initial appended information is supplied to the synthesizer <b>22</b>.</p>
<p id="p-0156" num="0184">Note that the local decoder <b>44</b>B in <figref idref="DRAWINGS">FIG. 7</figref> is constructed similarly to the adder <b>61</b>B and storage unit <b>62</b>B and the local decoder <b>44</b>F is also constructed similarly to the adder <b>61</b>F and storage unit <b>62</b>F.</p>
<p id="p-0157" num="0185">Next, the synthesizer <b>22</b> included in the receiver <b>2</b> shown in <figref idref="DRAWINGS">FIG. 4</figref> will be described in detail below with reference to <figref idref="DRAWINGS">FIG. 12</figref>.</p>
<p id="p-0158" num="0186">As shown in <figref idref="DRAWINGS">FIG. 12</figref>, from the decoder <b>53</b> in <figref idref="DRAWINGS">FIG. 10</figref>, background image data are supplied to a background write unit <b>71</b>, object image data are supplied to a object write unit <b>72</b>, and appended information is supplied to a background write unit <b>71</b>, object write unit <b>72</b> and a synthesizer <b>77</b>.</p>
<p id="p-0159" num="0187">The background write unit <b>71</b> writes the supplied background image data one after another to the background memory <b>73</b>. In case there is for example a movement of the background, due to a panning or tilting during imaging by the video camera <b>6</b> in the transmitter <b>1</b>, the background write unit <b>71</b> will write the background image data to the background memory <b>73</b> with the background being positioned based on a background movement vector included in the appended information. Therefore, the background memory <b>73</b> can store data on an image spatially wider than one frame of image.</p>
<p id="p-0160" num="0188">The object write nit <b>72</b> will write the supplied object image data one after another to the object memory <b>75</b>. Note that in case there is for example a plurality of objects, the object write unit <b>72</b> will write image data of the plurality of objects to the object memory <b>75</b> for each object. Also, for write of image data of objects having the same object number which will further be described later, namely, same object data, the object write unit <b>72</b> will write, to the object memory <b>75</b>, new object image data, namely, object image data newly supplied to the object write memory <b>72</b> in place of object image data already stored in the object memory <b>75</b>.</p>
<p id="p-0161" num="0189">Further, when an object whose spatial resolution is high have been written to the object memory <b>75</b>, the object write unit <b>72</b> will change, from “0” to “1”, the object flag stored at an address in an object flag memory <b>76</b> correspondingly to each of pixels forming the object in consideration. More specifically, when writing object image data to the object memory <b>75</b>, the object write unit <b>72</b> will make reference to the object flag memory <b>76</b>. No object image data whose spatial resolution is low will be written to the object memory <b>75</b> having already stored therein an object whose flag is “1”, namely, image data of an object whose spatial resolution is high. Therefore, basically, each time object image data are supplied to the object write unit <b>72</b>, they are written to the object memory <b>75</b>; however, no object image data whose spatial resolution is low will be written to the object memory <b>75</b> having already stored therein object image data whose spatial resolution is high. As a result, in the object memory <b>75</b>, each time object image data whose spatial resolution is high are supplied to the object write unit <b>72</b>, the number of object images whose spatial resolution is high will be larger.</p>
<p id="p-0162" num="0190">The synthesizer <b>77</b> reads a background image of a current frame to be displayed at the present from the background image data stored in the background memory <b>73</b> based on the background movement vector included in the appended information while pasting, to the background image, the object image stored in the object memory <b>75</b> based on the object movement vector included in the appended information, whereby an image in the current frame is reproduced and supplied to a display memory <b>78</b>.</p>
<p id="p-0163" num="0191">Further, upon reception of click data from the click data input unit <b>24</b> in <figref idref="DRAWINGS">FIG. 4</figref>, the synthesizer <b>77</b> reads, from the object memory <b>75</b>, object image data containing a coordinate position of a focus of interest included in the click data, and supplies the data to the sub window memory <b>79</b>.</p>
<p id="p-0164" num="0192">There is further provided a display memory <b>78</b> as a so-called VRAM (video read-only memory) to buffer or provisionally store an image in the current frame supplied from the synthesizer <b>77</b> and then read it out for supply to the image output unit <b>23</b> in <figref idref="DRAWINGS">FIG. 4</figref>. Also, the sub window memory <b>79</b> provisionally stores object image data supplied from the synthesizer <b>77</b> and then reads it for supply to the image output unit <b>23</b> in <figref idref="DRAWINGS">FIG. 4</figref>. At this time, the display unit <b>7</b> driven by the image output unit <b>23</b> will display, along with the image in the current frame, a sub window which will further be described later, and display an object image in the sub window.</p>
<p id="p-0165" num="0193">Next, the operations effected in the synthesizer <b>22</b> in <figref idref="DRAWINGS">FIG. 12</figref> will be described below with reference to the flow chart in <figref idref="DRAWINGS">FIG. 13</figref>.</p>
<p id="p-0166" num="0194">First in step S<b>31</b>, the object write unit <b>72</b> writes the object image data supplied from the decoder <b>53</b> in <figref idref="DRAWINGS">FIG. 10</figref> as in the above on the basis of an object flag stored in the object flag memory <b>75</b>.</p>
<p id="p-0167" num="0195">More particularly, referring to the object flag stored in the object flag memory <b>76</b>, the object write unit <b>72</b> writes object image data supplied thereto at an address, in the object memory <b>75</b>, corresponding to a pixel for which the object flag is “0”, and only object image data supplied thereto and whose spatial resolution is high at an address, in the object memory <b>75</b>, corresponding to a pixel for which the object flag is “1”.</p>
<p id="p-0168" num="0196">Note that when object image data are written at an address, in the object memory <b>75</b>, where object image data are already stored, they will be written over the existing object image data in the object memory <b>75</b>.</p>
<p id="p-0169" num="0197">Thereafter in step S<b>32</b>, the object write unit <b>72</b> judges whether the appended information includes high-resolution information. If it is judged in step S<b>32</b> that the appended information includes high-resolution, namely, when click data are sent to the transmitter <b>1</b> by operating the clock data input unit <b>24</b> in <figref idref="DRAWINGS">FIG. 4</figref> by the user of the receiver <b>2</b> and thus object image data whose spatial resolution is high are sent for an image inside the preferred range from the transmitter <b>1</b>, the object write unit <b>72</b> goes to step S<b>33</b> where it will set a predetermined object flag in the object flag memory <b>76</b> to “1”.</p>
<p id="p-0170" num="0198">That is, when object image data whose spatial resolution is high are sent for the image inside the preferred range from the transmitter <b>1</b>, they are written to the object memory <b>75</b> in step S<b>31</b>. Thus in step S<b>33</b>, the object flag for pixels included in an object image whose spatial resolution is high is set to “1”.</p>
<p id="p-0171" num="0199">Thereafter the procedure goes to step S<b>34</b> where the synthesizer <b>77</b> will read object image data inside the preferred range from the object memory <b>75</b>, and write them to the sub window memory <b>79</b>.</p>
<p id="p-0172" num="0200">Namely, when it is judged in step S<b>32</b> that the appended information includes high-resolution, click data have been sent to the transmitter <b>1</b> by operating the clock data input unit <b>24</b> in <figref idref="DRAWINGS">FIG. 4</figref> by the user of the receiver <b>2</b> and thus object image data whose spatial resolution is high have been sent for an image inside the preferred range from the transmitter <b>1</b>. The click data supplied to the transmitter <b>1</b> are also supplied to the synthesizer <b>77</b>. Upon reception of the click data, the synthesizer <b>77</b> will recognize, in step S<b>34</b>, the preferred range from the coordinate of the focus of interest an clicked time included in the click data, read an object, sent from the transmitter <b>1</b>, included in the preferred range and having a high spatial resolution, from the object memory <b>75</b>, and write the data to the sub window memory <b>79</b>.</p>
<p id="p-0173" num="0201">Then in step S<b>35</b>, the synthesizer <b>77</b> reads, based on the background movement vector included in the appended information, background image data in the current frame from the background image data stored in the background memory <b>73</b>, reads object image data to be displayed in the current frame from the object memory <b>75</b>, and further combines the background image data in the current frame and object image data read from the object memory <b>75</b> according to the object movement vector included in the appended information. Thus, the synthesizer <b>77</b> reproduces the image in the current frame and writes it to the display memory <b>78</b>. That is, the synthesizer <b>77</b> writes the background image data to the display memory <b>78</b> for example, and then writes the object image data over the background image data, thereby writing, to the display memory <b>78</b>, the image data in the current frame obtained by combining the background image and object image.</p>
<p id="p-0174" num="0202">As in the above, the image data in the current frame written to the display memory <b>78</b>, and object image data written to the sub window memory <b>79</b> will be supplied to the image output unit <b>23</b> in <figref idref="DRAWINGS">FIG. 4</figref> and displayed on the display unit <b>7</b>.</p>
<p id="p-0175" num="0203">On the other hand, if it is judged in step S<b>32</b> that the appended information includes no high-resolution information, namely, when the click data input unit <b>24</b> has not been operated by the user of the receiver <b>2</b>, the procedure skips over steps S<b>33</b> and S<b>34</b> to step S<b>35</b> where the synthesizer <b>77</b> will read the background image data in the current frame from the background memory <b>73</b> and necessary object image data from the object memory <b>75</b>, to combine the background image in the current frame and object image read from the object memory <b>75</b> according to the appended information. Thus, the synthesizer <b>77</b> reproduces image data in the current frame and writes them to the display memory <b>78</b>. Then the procedure returns to step S<b>31</b> and similar operations will be repeated.</p>
<p id="p-0176" num="0204">With the above operations for the synthesis, when the user of the receiver <b>2</b> has not operated the click data input unit <b>24</b>, that is, when no clicking has been made at the click data input unit <b>24</b>, an image whose spatial resolution is low will be displayed on the display screen of the display unit <b>7</b> with a default time resolution as shown in <figref idref="DRAWINGS">FIG. 14A</figref>. Note that <figref idref="DRAWINGS">FIG. 14A</figref> shows an example in which an object image whose spatial resolution is low is being moved rightward over a background image whose spatial resolution is low.</p>
<p id="p-0177" num="0205">When the user of the receiver <b>2</b> moves the cursor over the object image by operating the click data input unit <b>24</b> and clicks with the cursor on the object image, click data is sent to the transmitter <b>1</b> and the transmitter <b>1</b> receives data intended for display, as a high spatial-resolution image, of an image inside a preferred range localized based on the click data by the sacrifice of the time resolution. As the result, there will be displayed on the display screen of the display unit <b>7</b>, as shown in <figref idref="DRAWINGS">FIG. 14B</figref>, an image corresponding to an object image included in the preferred range around the clicked position and whose time resolution is low but whose spatial resolution is gradually improved. That is, an image is displayed which corresponds to an object image included in the preferred range and whose spatial resolution is gradually improved correspondingly to a time when clicking has been made on the image.</p>
<p id="p-0178" num="0206">Further, on the display unit <b>7</b>, the sub window is opened and an image corresponding to an object in an extracted preferred range including a clicked position is displayed in the sub window with the spatial resolution of the object being gradually improved, as shown in <figref idref="DRAWINGS">FIG. 14B</figref>.</p>
<p id="p-0179" num="0207">Thereafter, when the user of the receiver <b>2</b> stops clicking with the click data input unit <b>24</b>, the synthesizer <b>77</b> reads background image data in the current frame from the background memory <b>73</b> and object image data from the object memory <b>75</b>, combines the background image data and object image data according to the appended data, and writes the data to the display memory <b>78</b>, in step S<b>35</b> as having been described above. As in the above, since the object image data whose spatial resolution has been elevated by clicking is continuously stored as it is in the object memory <b>75</b>, the object image whose spatial resolution has thus been improved by clicking is moved according to the appended information movement vector is displayed in a due position in the current frame on the display unit <b>7</b> as shown in <figref idref="DRAWINGS">FIG. 14C</figref>.</p>
<p id="p-0180" num="0208">Therefore, by clicking in a position where an object image whose detail is to be observed, the user of the receiver <b>2</b> will be able to view an object image having an improved spatial resolution. Thus, the user will be able to view a detailed image of an object.</p>
<p id="p-0181" num="0209">Note that since the background image data is stored in the background memory <b>73</b> as in the above, the transmitter <b>1</b> has not to send any background sent once and whose spatial resolution is low. Therefore, the transmission rate for the background can be allocated preferentially to sending of object image data whose spatial resolution is higher.</p>
<p id="p-0182" num="0210">In the above case, the object image data having the spatial resolution raised by clicking is stored in the object memory <b>75</b> and the object image having the high spatial resolution is pasted on the background image after the clicking is stopped. Thus, the object image displayed at the receiver <b>2</b> will have a high spatial resolution but will not reflect any change in state of an object image picked up at the transmitter <b>1</b>.</p>
<p id="p-0183" num="0211">So, with the object flag being disregarded after the clicking is stopped, the object image data stored in the storage unit <b>62</b>F in the decoder <b>53</b> shown in <figref idref="DRAWINGS">FIG. 11</figref> can be written over the object image data stored in the object memory <b>75</b> and whose spatial resolution is high. That is, since object image data sent from the transmitter <b>1</b> are stored one after another into the storage unit <b>62</b>F of the decoder <b>53</b>, the object image in an image displayed on the display unit <b>7</b> will be made to reflect a change in state of the object captured at the transmitter <b>1</b> as in the above by writing the object image data to the object memory <b>75</b>. However, the displayed object image will have a low spatial resolution.</p>
<p id="p-0184" num="0212">Next, the relation between the spatial and time resolutions of an image sent from the transmitter <b>1</b> to the receiver <b>2</b> via the transmission line will be described below with reference to <figref idref="DRAWINGS">FIG. 15</figref>.</p>
<p id="p-0185" num="0213">Assume here that the transmission rate of the transmission line is R [bps] and a background image and data including three objects #1 to #3 are sent from the transmitter <b>1</b>. For the simplicity of the explanation, no consideration will be given to the appended information, and it is assumed that for displaying the background image and object images #1 to #3 with a certain spatial resolution, the same data amount is required for each of the images.</p>
<p id="p-0186" num="0214">In this case, when no clicking has been made at the receiver <b>2</b>, the transmitter <b>1</b> will send the background image and object images #1 to #3 each at a rate R/4 [bps] being a quarter of the transmission rate of the transmission line, as shown in <figref idref="DRAWINGS">FIG. 15A</figref>. Note that when the ordinary time resolution is 1/T frame/sec, the transmitter <b>1</b> will send one frame of each of the background image and object images #1 to #3 in a maximum of T sec. Therefore in this case, there will be displayed on the receiver <b>2</b> background image and object images #1 to #3 each having a spatial resolution of T×R/4 bits/frame.</p>
<p id="p-0187" num="0215">When the user clicks at a time t<sub>1 </sub>and in a position of the object image #1 for example, the transmitter <b>1</b> will stop sending the background image and object images #2 and #3 for example while sending only the object image #1 at the full transmission rate R of the transmission line, as shown in <figref idref="DRAWINGS">FIG. 15A</figref>. Thereafter, when the user stops clicking at a time t<sub>2 </sub>which is later by a time 4T than the time t<sub>1</sub>, the transmitter <b>1</b> will send the background image and object images #1 to #3 again at a transmission rate of R/4.</p>
<p id="p-0188" num="0216">Therefore, while the user is clicking, 4T×R bits of the object #1 are sent. So, when the time resolution during clicking is 0 frame/sec, the receiver <b>2</b> will display the object image #1 with a spatial resolution of 4T×R bits/frame. That is, when the horizontal and vertical spatial resolutions have been improved to the same extent, the time resolution at the receiver <b>2</b> is 0 frame/sec but the object image #1 clicked by the user will be displayed with horizontal and vertical spatial resolutions 4 times higher than those before clicking (=√{square root over ( )}4T×R/(T×R/4 bits))).</p>
<p id="p-0189" num="0217">Thus, the spatial resolution can be improved at the sacrifice of the time resolution, and the spatial resolution of object image the user is interested in can be improved more rapidly than when the time resolution is sacrificed.</p>
<p id="p-0190" num="0218">In an example shown in <figref idref="DRAWINGS">FIG. 15A</figref>, while the object image #1 is being clicked, a transmission rate of 0 frame/sec is set for sending the background image and other object images #2 and #3 so that these data will not be sent. In an example shown in <figref idref="DRAWINGS">FIG. 15B</figref>, however, a high transmission rate may be allocated for sending the object image #1 while a low transmission rate may be allocated for sending the background image and object images #2 and #3.</p>
<p id="p-0191" num="0219">Even if the clicking is made, the transmission rate allocated for sending the background image and object images #1 to #3 can be kept as R/4. That is, since the spatial resolution is improved at the sacrifice of the time resolution, sending of the data takes a time even without allocating any other transmission rate.</p>
<p id="p-0192" num="0220">In the above, object image having the spatial resolution elevated by clicking is stored in the object memory <b>75</b>, and after the clicking is stopped, the object image having the high spatial resolution is pasted on the background image. However, where the high spatial-resolution object image is to be pasted on the background image depends upon an object movement vector included in the appended information on the object, sent from the transmitter <b>1</b> later.</p>
<p id="p-0193" num="0221">Therefore, since the receiver <b>2</b> has to recognize an object image in a frame, corresponding to an image in a frame adjacent to the former frame, the object extraction unit <b>14</b> of the transmitter <b>1</b> appends information intended for use by the receiver <b>2</b> to make such a recognition for extraction of an object.</p>
<p id="p-0194" num="0222">Next, in the pre-processor <b>12</b> of the transmitter <b>1</b>, it is possible to extract an interesting object image and detect when the user's interest has shifted to another area, based on click data supplied from the receiver <b>2</b>, that is, designation data from the user.</p>
<p id="p-0195" num="0223">The first embodiment of the image processor according to the present invention and operations thereof will be described herebelow. Note that in this embodiment, there will be described, by way of example, the extraction of an interesting area and detection of shift of the user's interest to another area, based on the designation data from the user, effected at the transmitter <b>1</b> of the system shown in <figref idref="DRAWINGS">FIG. 1</figref>.</p>
<p id="p-0196" num="0224"><figref idref="DRAWINGS">FIG. 16</figref> shows the first embodiment of the image processor according to the present invention, in which the object extraction unit <b>14</b> of the pre-processor <b>12</b> shown in <figref idref="DRAWINGS">FIG. 2</figref> extracts an object image (interesting area) the user of the receiver <b>2</b> is interested in from a captured image on the basis of the click data sent from the receiver <b>2</b>.</p>
<p id="p-0197" num="0225">The image processor shown in <figref idref="DRAWINGS">FIG. 16</figref> receives a designation data (clock data) given by the user of the receiver <b>2</b>, for example, clicking an image while viewing the image, and extracts an object from the image based on the click data.</p>
<p id="p-0198" num="0226">By storing not only click data supplied from the user of the receiver <b>2</b> and based on which an object is extracted (will be referred to as “current click data” hereunder wherever appropriate) but click data already supplied at various earlier times and using the stored click data and current click data, it is possible to predict a demand from the user. The “click data” referred to herein include clicked-position information and clicking-time information.</p>
<p id="p-0199" num="0227">However, since click data having simply been stored will be averaged, their significance will gradually be lower. Therefore, to accurately predict a user's demand, it will be necessary to extract only click data required currently, that is, significant click data, from the stored click data.</p>
<p id="p-0200" num="0228">To extract only significant ones of the stored click data, features of a plurality of objects are compared with each other and significant click data are extracted based on the result of comparison. Namely, the dominance of each of the stored click data is judged and an object is extracted with the use of dominant click data. In other words, object images produced based on click data from the user are coupled or uncoupled to be more appropriate objects according to the features of the object image data to reconstruct the original images, namely, the dominance of the click data is judged according to predetermined features of the object image data and object images are reconstructed through the coupling or uncoupling to correspond to the click data judged to be more dominant. However, the method of comparing the features varies depending upon how the click data from the user are used.</p>
<p id="p-0201" num="0229">For reconstruction of an object image, the small objects corresponding to the click data are possibly coupled to each other incorrectly or an object image is possibly divided into incorrect small objects. According to the image processing algorithm used in this embodiment, however, the appropriateness of the coupling or division of small object image data is verified according to a predetermined one of the plurality of features of the object image data, whereby the coupling or division is effected appropriately to produce object image data, thereby providing an object image.</p>
<p id="p-0202" num="0230">This embodiment of the image processor processes an image in units of object image data reconstructed with the use of the predetermined feature of each small object image data, thereby permitting to extract more appropriate object images.</p>
<p id="p-0203" num="0231">The predetermined feature of each small object image data is for example a movement of the small object in relation to the object image data. Namely, the problem in coupling or division of the above-mentioned small object images is a so-called tracking. When an object image is pasted while being tracked, the plurality of small object images which can be considered to be integral with each other should show the same movement as a feature. Therefore, when for example two small object images show the same movement, they may be taken as belonging to the same object and may be coupled to each other. On the other hand, if the two small object images show different movements, processing them as belonging to the same object image will cause an inconsistency between them and so the two small object image have to be taken as independent of each other.</p>
<p id="p-0204" num="0232">Thus, by coupling small object images showing the same movement to each other, so to speak, making a rigid-body movement, because they belong to the same object while uncoupling small object images showing different movements, it is possible to make consistent coupling or division and thus use object image data optimally.</p>
<p id="p-0205" num="0233">As in the above, more appropriate object images can be extracted by extracting object images using the click data from the user of the receiver <b>2</b> and reconstructing object image through coupling or division of objects according to a predetermined feature, namely, movement, of the object images.</p>
<p id="p-0206" num="0234">Further, a shift of the user's interest to another image area can be detected based on an inter-click time interval which is a time interval between designation data from the user of the receiver <b>2</b>.</p>
<p id="p-0207" num="0235">That is, the analytical experiments have already provided that when the user sees an image, he or she sees each somewhat meaningful image area of the image. In this embodiment of the system according to the present invention, when the user wants an improved quality of image data he is interested in, he will click the interesting image area. When an object image can be extracted correctly, an image of a high quality can be presented very effectively. However, an incorrect extraction of an object image will lead to a failure of image quality improvement, rather, to an image quality degradation. Particularly, when the user's interest shifts to another image area, an object image to be improved in quality will be changed from an object image in which the user has so far been interested in to a new object image the user is currently interested in. Therefore, when the user's interest shifts to another image area, it will be a problem how the shift of the user's interest should be detected.</p>
<p id="p-0208" num="0236">To solve the above problem, this embodiment measures a time interval between click data which are designations from the user of the receiver <b>2</b> based on the fact that the time required for continuously designating an area the user is interested in is shorter than that for designating another area to which the user's interest shifts, to thereby permitting to detect the shift of the user's interest to the other area.</p>
<p id="p-0209" num="0237"><figref idref="DRAWINGS">FIG. 16</figref> shows the object extraction unit to attain the above.</p>
<p id="p-0210" num="0238">As shown in <figref idref="DRAWINGS">FIG. 16</figref>, the object extraction unit includes an image input unit <b>11</b> corresponding to the image input unit <b>11</b> shown in <figref idref="DRAWINGS">FIG. 2</figref>. The image input unit <b>11</b> includes for example a video camera, video input terminal, etc. It acquires image data at every predetermined times and send them to a feature extraction unit <b>91</b>, extraction unit <b>83</b> and a processor <b>92</b>.</p>
<p id="p-0211" num="0239">The object extraction unit further includes a designation acquisition unit <b>85</b> which acquires click data from the user of the receiver <b>2</b> and sends, to the processor <b>92</b>, click data from the user and a signal indicating that a click being a signal indicative of an input event made by the user has been has been entered. The click data from the designation acquisition unit <b>85</b> is sent to a time elapse computation unit <b>86</b> via a data storage unit <b>93</b>, and also stored into the data storage unit <b>93</b>.</p>
<p id="p-0212" num="0240">The time elapse computation unit <b>86</b> makes comparison between the current click data supplied from the designation acquisition unit <b>85</b> and the earlier click data stored in the data storage unit <b>93</b> to compute a time interval between the earlier and current click data. More particularly, the time elapse computation unit <b>86</b> computes a time interval between the click data supplied from the user of the receiver <b>2</b> from the clicking-time information included in each click data. The time-interval data computed by the time elapse computation unit <b>86</b> are sent to the processor <b>92</b>.</p>
<p id="p-0213" num="0241">The processor <b>92</b> controls operations of all the component blocks of the object extraction unit.</p>
<p id="p-0214" num="0242">Supplied with a click input event from the designation acquisition unit <b>85</b>, the processor <b>92</b> will determine small object images corresponding to the click data supplied along with the input event, and judges, based on the time interval data from the time elapse computation unit <b>86</b> and earlier click data read from the data storage unit <b>93</b>, whether user of the receiver <b>2</b> has shifted his interest to another object image.</p>
<p id="p-0215" num="0243">If the processor <b>92</b> judges, since for example the time elapse data does not exceed a predetermined threshold, that the user has not shifted his interest to any other object image, it will use earlier click data store din the data storage unit <b>93</b> appropriately to determine an object image. Namely, the processor <b>92</b> will take, as belonging to the same object image, and couple, to each other, a small object image selected based on the current click data supplied from the designation acquisition unit <b>85</b> and a small object image being an object image having been selected based on the earlier click data or an object image being a collection of small object images. That is, the processor <b>92</b> will regard the small object image corresponding to the current click data as belonging to an object image corresponding to the earlier click data, and store the same identifier information as that appended correspondingly to the earlier click data along with position and time information on the current click data into the data storage unit <b>93</b> correspondingly to the earlier click data.</p>
<p id="p-0216" num="0244">On the other hand, if the processor <b>92</b> judges, since the time interval data exceeds the predetermined threshold, that the interest of the user of the receiver <b>2</b> has shifted to another object image, it will select an object image correspondingly to a newly entered click data, not to any earlier click data stored in the data storage unit <b>93</b>. Namely, the small object image selected correspondingly to the current click data supplied from the designation acquisition unit <b>85</b>, and an object image having been selected correspondingly to the earlier clock data, that is, a small object image or an object image being a collection of small object images, will be taken as being different from each other, and will not be coupled with each other. That is to say, the processor <b>92</b> will regard the small object image corresponding to the current click data as belonging to an object image not corresponding to the earlier click data, and will store the identifier information different from that appended correspondingly to the earlier click data along with the position and time information on the current click data into the data storage unit <b>93</b> correspondingly to the current click data.</p>
<p id="p-0217" num="0245">The processor <b>92</b> will store coupling information indicative of whether object images are to be coupled with each other or not into the data storage unit <b>93</b>. These information corresponds to the above identifier information.</p>
<p id="p-0218" num="0246">Also, supplied with a click input event from the designation acquisition unit <b>85</b>, the processor <b>92</b> will select an object image and send a feature extraction requesting signal for extraction of a predetermined feature of the object image data, predetermined parameters concurrently corresponding to the click data, for example, position data indicative of an object for example, etc. to the feature extraction unit <b>91</b>.</p>
<p id="p-0219" num="0247">In response to the feature extraction requesting signal supplied from the processor <b>92</b>, the feature extraction unit <b>91</b> will extract a movement being a predetermined feature of each small object image from the image data correspondingly to position data etc. being parameters sent along with the request signal and used for computation of a movement of an object corresponding to an object image, and send the extracted object feature data to the processor <b>92</b>. Then processor <b>92</b> will store the feature data on each small object image data into the data storage unit <b>93</b> correspondingly to the small object image data. That is, data storage unit <b>93</b> will store the feature data correspondingly to click data corresponding to the small object image data.</p>
<p id="p-0220" num="0248">On the other hand, when no click input event is supplied from the designation acquisition unit <b>85</b>, the processor <b>92</b> will send a feature extraction requesting signal for requesting to extract a feature of each small object image data and parameters for extraction of a feature of each small object image data to the feature extraction unit <b>91</b> at a predetermined time point. Thus, the feature extraction unit <b>91</b> will extract each object image data corresponding to the predetermined time point from the image data, and supply feature data of each small object image data, extracted at the predetermined time points, to the processor <b>92</b>. The feature data of each small object image data will be stored in the data storage unit <b>93</b> correspondingly to the small object image data. That is, the data storage unit <b>93</b> stores feature data correspondingly to click data corresponding to the small object image data.</p>
<p id="p-0221" num="0249">Note that the feature extraction unit <b>91</b> will compute different features depending on whether a click input event is supplied or not as in the above but the unit <b>91</b> may be adapted to compute the same features. Also, the data storage unit <b>93</b> can send data stored therein upon request from an external processor <b>94</b>.</p>
<p id="p-0222" num="0250">Further, when no click input event is supplied from the designation acquisition unit <b>85</b>, the processor <b>92</b> judges the appropriateness of the current feature data according to the feature data on each of small object image data acquired at the above time points and feature data on each small object image data already stored to check if the characteristic of the current feature data has been changed from the earlier one, and also judges, based on the result of checking, whether the small object image data are to be coupled with, or uncoupled from, each other. If the data characteristic is found changed, the processor <b>92</b> will update or erase information on the coupling and uncoupling of small object images, stored in the data storage unit <b>93</b>, that is, the coupling and uncoupling information.</p>
<p id="p-0223" num="0251">That is, when supplied with no click input event from the designation acquisition unit <b>85</b>, the processor <b>92</b> will make comparison between feature data of the small object image data, extracted at different time points, and those of the small object image data already stored, and judge, based on the result of comparison, whether the plurality of small object images are to be coupled to or uncoupled from each other.</p>
<p id="p-0224" num="0252">More particularly, in case for example two small object images show the same movement (feature), the processor <b>92</b> will judge that the two small object images are to be coupled to each other. On the other hand, when the two small object images are different in movement from each other, the processor <b>92</b> will judge that the two small object images should be left not coupled to each other and taken as being independent of each other.</p>
<p id="p-0225" num="0253">When the processor <b>92</b> judges that the two small object images should be coupled to each other, it will store coupling information indicating that the small object images should be coupled to each other as one of to-be-stored data into the data storage unit <b>93</b>. If the two objects have already been coupled to each other, the coupling information is maintained as it is. The coupling information is the aforementioned identifier information.</p>
<p id="p-0226" num="0254">Also, concerning small object images judged to be independent of each other, the processor <b>92</b> will store uncoupling information indicating that they are independent of each other as one of to-be-stored data into the data storage unit <b>93</b>. Note that when the two small object images are already known as independent of each other, the uncoupling information is maintained as it is.</p>
<p id="p-0227" num="0255">On the other hand, if two small object images having been judged to belong to the same object image and coupled to each other thereafter show different movements, the processor <b>92</b> will judge that they should be uncoupled from each other. Thus, when the processor <b>92</b> judges that two object images having initially belonged to the same object should be uncoupled from each other, it will erase, from the data storage unit <b>93</b>, the coupling information that the two small object images are to be coupled to each other, and update the uncoupling information that the small object images should be taken as independent of each other.</p>
<p id="p-0228" num="0256">As in the above, the processor <b>92</b> selects objects and decides whether they should be coupled to or uncoupled from each other, based on click data supplied from the designation acquisition unit <b>85</b>, earlier click data stored in the data storage unit <b>93</b> and inter-click time interval data supplied from the time elapse computation unit <b>86</b>, and judges whether the objects should be coupled to or uncoupled from each other according to the feature (movement) of each object image.</p>
<p id="p-0229" num="0257">The processor <b>92</b> sends, to an extraction unit <b>83</b>, a control signal for extraction of the objects selected as in the above from image data. That is, the processor <b>92</b> has the image data supplied thereto from the image input unit <b>11</b>, determines positions and shapes of the objects from the image data, and sends signals indicative of the object positions and shapes to the extraction unit <b>83</b>.</p>
<p id="p-0230" num="0258">The extraction unit <b>83</b> has image data supplied thereto from the image input unit <b>11</b>, and extracts object images from the image data based on signals indicatives of the positions and shapes of the objects from the processor <b>92</b>. The extracted object image data are delivered at the object extraction unit <b>14</b> of the pre-processor <b>12</b> in <figref idref="DRAWINGS">FIG. 2</figref>.</p>
<p id="p-0231" num="0259">Next, the flow of operations made in the object extraction unit in <figref idref="DRAWINGS">FIG. 16</figref> will further be described with reference to the flow chart in <figref idref="DRAWINGS">FIG. 17</figref>.</p>
<p id="p-0232" num="0260">In step S<b>41</b> in <figref idref="DRAWINGS">FIG. 17</figref>, the designation acquisition unit <b>85</b> detects whether a designation event, that is, a click input event, has been sent from the user of the receiver <b>2</b>. If the result of judgment is that the click input event has been sent, the designation acquisition unit <b>85</b> sends a signal indicative of the fact to the processor <b>92</b>. Receiving the signal, the processor <b>92</b> judges whether the click input event being a designation event from the user of the receiver <b>2</b> has been made. When it is judged that the event has been supplied, the processor <b>92</b> goes to step S<b>42</b>. If the judgement is that no event has been supplied, the processor <b>92</b> goes to step S<b>47</b>.</p>
<p id="p-0233" num="0261">In step S<b>42</b>, the processor <b>92</b> judges, based on the time interval data from the time elapse computation unit <b>86</b> and earlier click data from the data storage unit <b>93</b>, whether the user of the receiver <b>2</b> designates the same object. That is, when the time interval data exceeds a predetermined threshold, the processor <b>92</b> will judge that the user of the receiver <b>2</b> has shifted his interest to another object image and the user has clicked the other object image, and go to step S<b>44</b>. On the other hand, when the time interval data does not exceed the predetermined threshold, that is, when the time interval data is less than the threshold, the processor <b>92</b> judges that the user of the receiver <b>2</b> is still interested in the same object image and the user has clicked the same object image, and goes to step S<b>43</b>.</p>
<p id="p-0234" num="0262">In step S<b>43</b>, the processor <b>92</b> will make the full use of the earlier clock data stored in the data storage unit <b>93</b> to select object images, namely, take the small object images corresponding to the current click data supplied from the designation acquisition unit <b>85</b> as belonging to the same object image selected based on the earlier click data to couple them to each other.</p>
<p id="p-0235" num="0263">On the other hand, in step S<b>44</b>, the processor <b>92</b> will use newly entered clock data, not the earlier click data stored in the data storage unit <b>93</b>, to select an object image. That is, the processor <b>92</b> will take the small object image corresponding to the current click data supplied from the designation input unit <b>85</b> as being different from the object image having been selected based on the earlier clock data, and do not couple them.</p>
<p id="p-0236" num="0264">After completion of the operations in steps S<b>43</b> and S<b>44</b>, the processor <b>92</b> controls the data storage unit <b>93</b> to store the click data and coupling and uncoupling information of each of the small object images.</p>
<p id="p-0237" num="0265">In step S<b>47</b> to which the processor <b>92</b> has gone because of the judgment in step S<b>41</b> that no designation input event has been supplied from the user of the receiver <b>2</b>, the processor <b>92</b> will have the feature extraction unit <b>91</b> extract features of small object images at various time points, and use the features of the small object images stored in the data storage unit <b>93</b> to check if the characteristic of the feature data has been changed. Based on the result of checking, the processor <b>92</b> will judge whether the small object images should be coupled to or uncoupled from each other.</p>
<p id="p-0238" num="0266">Next, in step S<b>48</b>, the processor <b>92</b> optimizes (update and erase) the coupling and uncoupling information on the small object images stored in the data storage unit <b>93</b> and which have been judged in step S<b>47</b> to have the characteristic thereof changed.</p>
<p id="p-0239" num="0267">More specifically, if the two small object images have the same features (movement), the processor <b>92</b> will store the coupling information for coupling the two small object images to each other, as one of to-be-stored data, into the data storage unit <b>93</b>. Note that, when the two small object images have already been coupled to each other, the processor <b>92</b> will maintain the coupling information as it is. On the other hand, if two small object images initially taken as belonging to the same object are found to show different movements, the processor <b>92</b> will uncouple the two coupled small object images from each other, and erase the coupling information of each of the small object images from the data storage unit <b>93</b> and updates the information to uncoupling information.</p>
<p id="p-0240" num="0268">After completion of the operations in steps S<b>48</b> and S<b>45</b>, the processor <b>92</b> will send signals indicative of positions and shapes of the objects to the extraction unit <b>83</b>, and judges in step S<b>46</b> whether the extraction is to be ended or not. When the result of judgment is that the extraction is not to be ended, the processor <b>92</b> returns to step S<b>41</b>.</p>
<p id="p-0241" num="0269">Next, the operation in step S<b>42</b> in <figref idref="DRAWINGS">FIG. 17</figref> will further be described with reference to the flow chart shown in <figref idref="DRAWINGS">FIG. 18</figref>.</p>
<p id="p-0242" num="0270">After arrival at step S<b>42</b> in <figref idref="DRAWINGS">FIG. 17</figref>, the processor <b>92</b> first goes to step S<b>51</b> in <figref idref="DRAWINGS">FIG. 18</figref> where it will control the designation acquisition unit <b>85</b> to acquire current click data and send the click data to the time elapse computation unit <b>86</b> via the data storage unit <b>93</b>. Then in step S<b>52</b>, the time elapse computation unit <b>86</b> will acquire earlier click data from the data storage unit <b>93</b>. Assume here that the click data includes position information indicative of the position of a clicked image and time information indicative of a time when the image is clicked. Further in step S<b>53</b>, the time elapse computation unit <b>86</b> computes an inter-click time interval rom the time information in the current click data from the designation acquisition unit <b>85</b> and time information in the earlier clock data from the data storage unit <b>93</b>, and sends the time interval data to the processor <b>92</b>.</p>
<p id="p-0243" num="0271">In step S<b>54</b>, the processor <b>92</b> will judge, based on the time interval data from the time elapse computation unit <b>86</b>, whether the time interval is less than the threshold. When the judgment is that the time interval is less than the threshold, the processor <b>92</b> judges, in step S<b>55</b> corresponding to step S<b>43</b>, that the interest of the user of the receiver <b>2</b> has not shifted to any other object image but still stays in the same object image and generates identifier information for small object images corresponding to the current click data. That is, the identifier information (coupling information) appended to each of the plurality of small object images indicates whether the plurality of small object images belongs to the same object image. For the small object images in which, it is judged, the user of the receivers <b>2</b> is still interested, there are stored into the data storage unit <b>93</b> the position and time information on the small object images (click data), to which the same identifier information as that for the small object images clicked in the past is appended. On the other hand, when the time interval exceeds the threshold, the processor <b>92</b> will judge in step S<b>56</b> corresponding to step S<b>44</b> that the user of the receiver <b>2</b> has shifted his interest to another object image, append identifier information different from that for the small object images clicked in the past to the other object image and store the position and time information on the small object images (click data) into the data storage unit <b>93</b>.</p>
<p id="p-0244" num="0272">Next, the flows of the operations in steps S<b>47</b> and S<b>48</b> in <figref idref="DRAWINGS">FIG. 17</figref> will further be described with reference to <figref idref="DRAWINGS">FIG. 19</figref>.</p>
<p id="p-0245" num="0273">After arrival at step S<b>47</b> in <figref idref="DRAWINGS">FIG. 17</figref>, the processor <b>92</b> first goes to step S<b>61</b> in <figref idref="DRAWINGS">FIG. 19</figref> where it will acquire information on coupling of objects stored in the data storage unit <b>93</b>. That is, in the data storage unit <b>93</b>, the same identifier information or the like is appended to a plurality of small object images to indicate that the plurality of small object images belongs to the same object image.</p>
<p id="p-0246" num="0274">Next, in step S<b>62</b>, the feature extraction unit <b>91</b> is controlled by the processor <b>92</b> to detect features of the small object images (elements) and send the feature data to the processor <b>92</b>.</p>
<p id="p-0247" num="0275">Receiving the feature data, the processor <b>92</b> judges in step S<b>63</b> whether the small object images show the same feature, namely, a movement within a certain range. If the result of judgment is that the small object images show the same feature, the processor <b>92</b> will store the coupling information on the two small object images into the data storage unit <b>93</b>. In case the two small object images are already coupled to each other, the processor <b>92</b> will keep the coupling information unchanged. That is, the processor <b>92</b> will not change the identifier information appended to each of the small object images (click data). This operation corresponds to that in step S<b>48</b>.</p>
<p id="p-0248" num="0276">On the other hand, if the judgment made in step S<b>63</b> is that the two small object images show different features, the processor <b>92</b> will go to step S<b>64</b> where it will take the two small object images as being different from each other and uncouple them from each other. For example, in case two objects initially belonging to the same object image have shown different movements, the processor <b>92</b> will uncouple, in step S<b>64</b>, the two small object images from each other, then erase, in step S<b>48</b>, the coupling information on the small object images from the data storage unit <b>93</b> and update the information to uncoupling information. That is, a new identifier information is appended to small object images (click data) having been judged to have different features. This operation corresponds to that in step S<b>48</b>.</p>
<p id="p-0249" num="0277">In the object extraction unit shown in <figref idref="DRAWINGS">FIG. 16</figref>, the flow of operations as will be shown in <figref idref="DRAWINGS">FIG. 20</figref> may be done in place of that shown in <figref idref="DRAWINGS">FIG. 17</figref>. Note that in <figref idref="DRAWINGS">FIG. 20</figref>, the same steps of operation as those in <figref idref="DRAWINGS">FIG. 17</figref> are indicated with the same references as in <figref idref="DRAWINGS">FIG. 17</figref>, and will not be described any longer. Only operations different from those in <figref idref="DRAWINGS">FIG. 17</figref> will be described below.</p>
<p id="p-0250" num="0278">As shown in <figref idref="DRAWINGS">FIG. 20</figref>, when it is judged in step S<b>41</b> that a designation event has been entered by the user, the processor <b>92</b> will go to step S<b>71</b>.</p>
<p id="p-0251" num="0279">In step S<b>71</b>, the processor <b>92</b> judges whether small object images corresponding to the click data sent from the receiver <b>2</b> are moving or stationary ones and whether the clicks are successive or discrete. More particularly, when a difference between frames of the small object images is smaller than a predetermined threshold, the processor <b>92</b> will judge that the objects in the small object images are stationary. In case the difference is greater than the threshold, the processor <b>92</b> will judge that the objects are moving ones. Further, if the inter-click time interval is shorter than a predetermined time, the processor <b>92</b> will judge that the clicks are successive. If it is judged in step S<b>71</b> that the small object images corresponding to earlier click data are stationary object images, current click data are successive clicks and the small object images corresponding to the current click data are stationary object images, the processor <b>92</b> goes to step S<b>73</b> where it will take the stationary objects are the same and couple the small object images to each other by assimilating identifier information corresponding to the current small object images to identifier information corresponding to the earlier object images. Further, if it is judged in step S<b>71</b> that the small object images corresponding to the earlier click data are moving objects, current click data are successive clicks and the small object images corresponding to the current click data are moving objects, the processor <b>91</b> goes to step S<b>72</b> where it will take the moving objects are the same and couple them by assimilating identifier information corresponding to the earlier small object images to identifier information corresponding to the current object images. On the other hand, if it is judged in step S<b>71</b> that the current click data are discrete ones or if it is judged in the same step that the small object images corresponding to the earlier click data are moving and stationary, respectively, the processor <b>92</b> goes to step S<b>44</b> where it will take the objects as being different from each other and uncouple them by dissimilating the identifier information corresponding to the current small object images from the identification information corresponding to the earlier small object images. After completion of the operations in these steps S<b>72</b>, S<b>73</b> and S<b>44</b>, the processor <b>92</b> goes to step S<b>45</b>.</p>
<p id="p-0252" num="0280">As in the above, it is possible to accurately localize an area (object) the user is interested in, that is, to couple objects which should be coupled to each other and uncouple objects which should be uncoupled from each other, as well as to detect even an area to which the user's interest has shifted.</p>
<p id="p-0253" num="0281">In the foregoing, the application of the object extraction unit shown in <figref idref="DRAWINGS">FIG. 16</figref> to the communication system shown in <figref idref="DRAWINGS">FIG. 1</figref> has been described by way of example. Note however that the aforementioned object extraction and detection of an area the user is newly interested in can of course be applied to various other applications of extraction of an interesting area from an image and detection of another area to which the user's interest has shifted as well as to the communication system in <figref idref="DRAWINGS">FIG. 1</figref>. For example, the object extraction unit shown in <figref idref="DRAWINGS">FIG. 16</figref> can be constructed independently and can be applied to all systems for extraction of an area the user is interested in, coupling small areas to each other or uncoupling small areas once coupled to each other, and also for detection of another area to which the user's interest has shifted. Also, the features detected from the object images may include, in addition to the movement, brightness histogram, standard deviation, deformation or color of images.</p>
<p id="p-0254" num="0282">The above series of operations can be done by a hardware or software. In case the operations are to be done by a software, programs forming together the software is installed in a computer incorporated in the transmitter <b>1</b> and receiver <b>2</b> as dedicated hardware, a genera-purpose computer, or the like.</p>
<p id="p-0255" num="0283">Here will be described a recording medium having the program for doing the series of operations recorded therein and used to install the program into a computer and enable them to be executed by the computer.</p>
<p id="p-0256" num="0284">The program for effecting the above series of operations can be pre-recorded in a hard disc or semiconductor memory as a recording medium incorporated in a computer. The program may be stored (recorded) provisionally or permanently in recording medium such as a floppy disc, CD-ROM (compact disc read-only memory), MO (magneto-optical) disc, DVD (digital versatile disc), magnetic disc or a semiconductor memory.</p>
<p id="p-0257" num="0285">Note that the program can be installed from the above-mentioned recording medium to a computer or it can be wirelessly transferred from a download site to a computer via a digital-broadcasting artificial satellite or via a network such as LAN (local area network) or Internet and installed into a hard disc incorporated in the computer.</p>
<p id="p-0258" num="0286">Also note that the steps for description of the program for various operations may not always be done on the time series in a description sequence in a flow chart but may include operations which are effected in parallel or individually (for example, parallel operations or object-based operations).</p>
<p id="p-0259" num="0287">An example construction of a computer for executing the program under the aforementioned series of operations are effected will be described with reference to <figref idref="DRAWINGS">FIG. 21</figref>.</p>
<p id="p-0260" num="0288">The computer shown in <figref idref="DRAWINGS">FIG. 21</figref> incorporates a CPU (central processing unit) <b>142</b>. An input/output interface <b>145</b> is connected via a bus <b>141</b> to the CPU <b>142</b>. When supplied with an instruction from the user operating an input unit <b>147</b> including a keyboard, mouse and the like via the input/output interface <b>145</b>, the CPU <b>142</b> will execute a program stored in a ROM (read-only memory) <b>143</b> corresponding to the above-mentioned semiconductor memory. Also, the CPU <b>142</b> will load, into a RAM (random-access memory) <b>144</b>, and execute, a program stored in the hard disc <b>102</b>, a program transferred from the satellite or network, received by a communication unit <b>148</b> and installed in the hard disc <b>102</b> or a program read out from a floppy disc, CD-ROM, MO disc, DVD or magnetic disc provided in a drive <b>149</b> and installed in the hard disc <b>102</b>. Then the CPU <b>142</b> outputs the result of execution of the program to a display unit <b>146</b> composed of an LCD (liquid crystal display) or the like via the input/output interface <b>145</b> for example.</p>
<p id="p-0261" num="0289">According to the present invention, the time and spatial resolutions of an image displayed at the receiver <b>2</b> are changed by hierarchically encoding the image data at the transmitter <b>1</b> and selecting and sending data in a hierarchical layer. Alternatively, the time resolution and spatial resolution of an image displayed at the receiver <b>2</b> can be changed by making discrete cosine conversion of the image to select a degree of coefficient or quantizing the image to change the quantizing step in the transmitter <b>1</b>, for example.</p>
<p id="p-0262" num="0290">Also, the time and spatial resolutions can also be changed by using another image encoding method in the transmitter <b>1</b>. That is, for displaying for example an object image (interesting area) at an ordinary time resolution, the profile of the image may be chain-encoded, a mean value of pixels (color) of the object image be determined as a representative value and the data be subject to an entropy encoding such as Huffman encoding, in the transmitter <b>1</b>, while the inside of the object image area may be painted in a color as the representative value in the receiver <b>2</b>. For displaying an image at an improved spatial resolution, it is possible to use the hierarchical encoding as having previously been described.</p>
<p id="p-0263" num="0291">In the above embodiment, the spatial resolution of an image is improved. On the contrary, the time resolution of the image can be improved.</p>
<p id="p-0264" num="0292">Also in the above embodiment, the spatial resolution of a preferred range as an area of an image is improved, but the spatial resolution of the entire image can also be improved.</p>
<p id="p-0265" num="0293">Further, in the above example, an image is divided into a background and object before being processed. However, the image may be processed without being so divided.</p>
<p id="p-0266" num="0294">Besides, the present invention is applicable to image data as well as to sound data. For example, the present invention can be applied to extraction of sound features such as sound pitch, desired part of human voice, characteristic sounds of musical instruments, etc. based on a certain basic frequency included in sound signal.</p>
<heading id="h-0006" level="1">INDUSTRIAL APPLICABILITY</heading>
<p id="p-0267" num="0295">According to the present invention, image data corresponding to designation data from the user are coupled to each other to group them, features of image data corresponding to the designation data, and the image data coupled once to each other are uncoupled according to the features of the image data in a group, to thereby accurately localize an area in which the user is interested in. Also, according to the present invention, a time interval between designation data is computed, and image data are coupled when the computed time interval is less than a predetermined threshold but not coupled when the time interval exceeds the predetermined threshold, whereby it is possible to detect another area to which the user's interest has shifted.</p>
<?DETDESC description="Detailed Description" end="tail"?>
</description>
<us-claim-statement>The invention claimed is:</us-claim-statement>
<claims id="claims">
<claim id="CLM-00001" num="00001">
<claim-text>1. A communication system comprising:
<claim-text>a transmitter to send image data; and</claim-text>
<claim-text>a receiver to receive the image data sent from the transmitter;</claim-text>
<claim-text>the receiver comprising:
<claim-text>a first means for receiving the image data sent from the transmitter;</claim-text>
<claim-text>means for outputting the image data received by the first receiving means;</claim-text>
<claim-text>means for outputting the image data outputted from the outputting means;</claim-text>
<claim-text>means for designating a time-spatial position of the received image data outputted from the outputting means; and</claim-text>
<claim-text>a first means for sending designation data indicative of the time-spatial position of the image data, designated by the designating means; and</claim-text>
</claim-text>
<claim-text>the transmitter comprising:
<claim-text>an input means to which image data are supplied continuously;</claim-text>
<claim-text>a second means for receiving the designation data sent from the first sending means;</claim-text>
<claim-text>means for coupling the image data corresponding to the designation data received by the second receiving means to each other to group them;</claim-text>
<claim-text>means for detecting features of the image data corresponding to the designation data; and</claim-text>
<claim-text>means for uncoupling the image data, coupled by the coupling means and corresponding to the designation data, according to each feature of the image data corresponding to the designation data in the same group.</claim-text>
</claim-text>
</claim-text>
</claim>
<claim id="CLM-00002" num="00002">
<claim-text>2. The system according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein:
<claim-text>there is further provided means for dividing the image data into small areas;</claim-text>
<claim-text>the coupling means couples the small areas of the image data to each other to group them according to the designation data;</claim-text>
<claim-text>the feature detecting means detects a feature of each of the small areas; and</claim-text>
<claim-text>the uncoupling means uncouples the small areas, coupled together by the coupling means,</claim-text>
<claim-text>according to the features of the small areas in the same group.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00003" num="00003">
<claim-text>3. The system according to <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein:
<claim-text>there is further provided means for storing, for each of the small areas, information on positions of designation data corresponding to the small areas in the image data, designation-time information and identifier information indicative of whether each of the small areas belongs to any object image;</claim-text>
<claim-text>the coupling means couples small areas corresponding to earlier designation data and small areas corresponding to current designation data to each other by storing the same identifier information as identifier information corresponding to the earlier designation data stored in a storage means so as to correspond to the small areas corresponding to the current designation data; and</claim-text>
<claim-text>the uncoupling means uncouples, when one of a plurality of small areas to which the same identifier information is appended by the coupling means is different in feature from other small areas, the one and other small areas from each other by changing the identifier information of the one small area to different one of the other small areas.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00004" num="00004">
<claim-text>4. The system according to <claim-ref idref="CLM-00003">claim 3</claim-ref>, wherein the feature detecting means detects, as the feature, the movement of an object in image data in interesting small areas in interesting image data of moving image data consisting of a plurality of image data.</claim-text>
</claim>
<claim id="CLM-00005" num="00005">
<claim-text>5. The system according to <claim-ref idref="CLM-00004">claim 4</claim-ref>, wherein:
<claim-text>there is further provided means for computing a time interval between designation data earlier than the designation-time information on the designation data and current designation data; and</claim-text>
<claim-text>the coupling means couples the small areas to each other when the time interval computed by the time interval computing means is less than a predetermined threshold while storing, when the time interval exceeds the predetermined threshold into the data storing means, different identifier information from identifier information corresponding to the earlier designation data information correspondingly to the current designation data not to couple the small areas corresponding to the earlier designation data and those corresponding to the current designation to each other.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00006" num="00006">
<claim-text>6. The system according to <claim-ref idref="CLM-00005">claim 5</claim-ref>, wherein:
<claim-text>the feature detecting means judges whether objects in interesting small area image data corresponding to the interesting image data are stationary or moving, based on a difference between the interesting small area image data and peripheral image data adjacent in the direction of time to the interesting small area image data; and</claim-text>
<claim-text>the coupling means works to:
<claim-text>store, when the small area image data corresponding to the earlier designation data are stationary objects, the interesting small area image data are judged to be stationary and the time interval computed by the time interval computing means is less than the predetermined threshold, the same identifier information as the identifier information corresponding to the earlier designation data stored in the storing means into the storing means correspondingly to the small areas corresponding to the current designation data to couple the small areas corresponding to the earlier designation data and those corresponding to the current designation data to each other;</claim-text>
<claim-text>store, when the small area image data corresponding to the earlier designation data are moving objects, the interesting small area image data are judged to be moving and the time interval computed by the time interval computing means is less than the predetermined threshold, the same identifier information as the identifier information corresponding to the earlier designation data stored in the storing means into the storing means correspondingly to the small areas corresponding to the current designation data to couple the small areas corresponding to the earlier designation data and those corresponding to the current designation data to each other; and</claim-text>
<claim-text>store, when the time interval exceeds the predetermined threshold or when the result of the stationary/moving judgment of the small area image data corresponding to the earlier designation data is different from that of the small area image data corresponding to the current designation data, identification information different from the identifier information corresponding to the earlier designation data into the data storing means correspondingly to the current designation data not to couple the small areas corresponding to the earlier designation data and those corresponding to the current designation data to each other.</claim-text>
</claim-text>
</claim-text>
</claim>
<claim id="CLM-00007" num="00007">
<claim-text>7. A communication method for communications of image data between a transmitter and receiver, wherein:
<claim-text>the receiver functions to:
<claim-text>receive the image data sent from the transmitter;</claim-text>
<claim-text>output the image data received by the first receiving means;</claim-text>
<claim-text>output the image data outputted from the outputting means;</claim-text>
<claim-text>designate a time-spatial position of the received image data outputted from the outputting means; and</claim-text>
<claim-text>send designation data indicative of the time-spatial position of the image data, designated by the designating means; and</claim-text>
</claim-text>
<claim-text>the transmitter functions to:
<claim-text>receive image data continuously;</claim-text>
<claim-text>receive the designation data sent from the first sending means;</claim-text>
<claim-text>couple the image data corresponding to the designation data received by the second receiving means to each other to group them;</claim-text>
<claim-text>detect features of the image data corresponding to the designation data; and</claim-text>
<claim-text>uncouple the image data, coupled by the coupling means and corresponding to the designation data, according to each feature of the image data corresponding to the designation data in the same group.</claim-text>
</claim-text>
</claim-text>
</claim>
<claim id="CLM-00008" num="00008">
<claim-text>8. A computer readable recording medium having recorded therein a program which can be read by an information processing means, the program controlling:
<claim-text>the receiver to:
<claim-text>receive the image data sent from the transmitter;</claim-text>
<claim-text>output the image data received by the first receiving means;</claim-text>
<claim-text>designate a time-spatial position of the received image data outputted from the outputting means; and</claim-text>
<claim-text>send designation data indicative of the time-spatial position of the image data, designated by the designating means; and</claim-text>
</claim-text>
<claim-text>the transmitter to:
<claim-text>receive image data continuously;</claim-text>
<claim-text>receive the designation data sent from the first sending means;</claim-text>
<claim-text>couple the image data corresponding to the designation data received by the second receiving means to each other to group them;</claim-text>
<claim-text>detect features of the image data corresponding to the designation data; and</claim-text>
<claim-text>uncouple the image data, coupled by the coupling means and corresponding to the designation data, according to each feature of the image data corresponding to the designation data in the same group.</claim-text>
</claim-text>
</claim-text>
</claim>
</claims>
</us-patent-grant>
