<us-patent-grant lang="EN" dtd-version="v4.2 2006-08-23" file="US07298877-20071120.XML" status="PRODUCTION" id="us-patent-grant" country="US" date-produced="20071106" date-publ="20071120">
<us-bibliographic-data-grant>
<publication-reference>
<document-id>
<country>US</country>
<doc-number>07298877</doc-number>
<kind>B1</kind>
<date>20071120</date>
</document-id>
</publication-reference>
<application-reference appl-type="utility">
<document-id>
<country>US</country>
<doc-number>10288766</doc-number>
<date>20021106</date>
</document-id>
</application-reference>
<us-application-series-code>10</us-application-series-code>
<us-term-of-grant>
<us-term-extension>919</us-term-extension>
</us-term-of-grant>
<classifications-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>G</section>
<class>06</class>
<subclass>K</subclass>
<main-group>9</main-group>
<subgroup>00</subgroup>
<symbol-position>F</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20071120</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>G</section>
<class>06</class>
<subclass>K</subclass>
<main-group>9</main-group>
<subgroup>62</subgroup>
<symbol-position>L</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20071120</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>G</section>
<class>06</class>
<subclass>K</subclass>
<main-group>9</main-group>
<subgroup>34</subgroup>
<symbol-position>L</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20071120</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>G</section>
<class>01</class>
<subclass>N</subclass>
<main-group>23</main-group>
<subgroup>00</subgroup>
<symbol-position>L</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20071120</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>A</section>
<class>61</class>
<subclass>B</subclass>
<main-group>6</main-group>
<subgroup>00</subgroup>
<symbol-position>L</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20071120</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
</classifications-ipcr>
<classification-national>
<country>US</country>
<main-classification>382128</main-classification>
<further-classification>382155</further-classification>
<further-classification>382173</further-classification>
<further-classification>382181</further-classification>
<further-classification>378  1</further-classification>
<further-classification>378  4</further-classification>
</classification-national>
<invention-title id="d0e53">Information fusion with Bayes networks in computer-aided detection systems</invention-title>
<references-cited>
<citation>
<patcit num="00001">
<document-id>
<country>US</country>
<doc-number>5260871</doc-number>
<kind>A</kind>
<name>Goldberg</name>
<date>19931100</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>382128</main-classification></classification-national>
</citation>
<citation>
<patcit num="00002">
<document-id>
<country>US</country>
<doc-number>5627907</doc-number>
<kind>A</kind>
<name>Gur et al.</name>
<date>19970500</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>382132</main-classification></classification-national>
</citation>
<citation>
<patcit num="00003">
<document-id>
<country>US</country>
<doc-number>5661820</doc-number>
<kind>A</kind>
<name>Kegelmeyer, Jr.</name>
<date>19970800</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<patcit num="00004">
<document-id>
<country>US</country>
<doc-number>5696884</doc-number>
<kind>A</kind>
<name>Heckerman et al.</name>
<date>19971200</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>706 61</main-classification></classification-national>
</citation>
<citation>
<patcit num="00005">
<document-id>
<country>US</country>
<doc-number>5704018</doc-number>
<kind>A</kind>
<name>Heckerman et al.</name>
<date>19971200</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>706 12</main-classification></classification-national>
</citation>
<citation>
<patcit num="00006">
<document-id>
<country>US</country>
<doc-number>5799100</doc-number>
<kind>A</kind>
<name>Clarke et al.</name>
<date>19980800</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>382132</main-classification></classification-national>
</citation>
<citation>
<patcit num="00007">
<document-id>
<country>US</country>
<doc-number>5802256</doc-number>
<kind>A</kind>
<name>Heckerman et al.</name>
<date>19980900</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>706 59</main-classification></classification-national>
</citation>
<citation>
<patcit num="00008">
<document-id>
<country>US</country>
<doc-number>5815591</doc-number>
<kind>A</kind>
<name>Roehrig et al.</name>
<date>19980900</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>382130</main-classification></classification-national>
</citation>
<citation>
<patcit num="00009">
<document-id>
<country>US</country>
<doc-number>5999639</doc-number>
<kind>A</kind>
<name>Rogers et al.</name>
<date>19991200</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>382132</main-classification></classification-national>
</citation>
<citation>
<patcit num="00010">
<document-id>
<country>US</country>
<doc-number>6056690</doc-number>
<kind>A</kind>
<name>Roberts</name>
<date>20000500</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<patcit num="00011">
<document-id>
<country>US</country>
<doc-number>6075879</doc-number>
<kind>A</kind>
<name>Roehrig et al.</name>
<date>20000600</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>382132</main-classification></classification-national>
</citation>
<citation>
<patcit num="00012">
<document-id>
<country>US</country>
<doc-number>6115488</doc-number>
<kind>A</kind>
<name>Rogers et al.</name>
<date>20000900</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>382132</main-classification></classification-national>
</citation>
<citation>
<patcit num="00013">
<document-id>
<country>US</country>
<doc-number>6138045</doc-number>
<kind>A</kind>
<name>Kupinski et al.</name>
<date>20001000</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>600425</main-classification></classification-national>
</citation>
<citation>
<patcit num="00014">
<document-id>
<country>US</country>
<doc-number>6198838</doc-number>
<kind>B1</kind>
<name>Roehrig et al.</name>
<date>20010300</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>382132</main-classification></classification-national>
</citation>
<citation>
<patcit num="00015">
<document-id>
<country>US</country>
<doc-number>6205236</doc-number>
<kind>B1</kind>
<name>Rogers et al.</name>
<date>20010300</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>382132</main-classification></classification-national>
</citation>
<citation>
<patcit num="00016">
<document-id>
<country>US</country>
<doc-number>6404908</doc-number>
<kind>B1</kind>
<name>Schneider et al.</name>
<date>20020600</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<patcit num="00017">
<document-id>
<country>US</country>
<doc-number>6529888</doc-number>
<kind>B1</kind>
<name>Heckerman et al.</name>
<date>20030300</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>706 45</main-classification></classification-national>
</citation>
<citation>
<patcit num="00018">
<document-id>
<country>US</country>
<doc-number>6601055</doc-number>
<kind>B1</kind>
<name>Roberts</name>
<date>20030700</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>706 45</main-classification></classification-national>
</citation>
<citation>
<patcit num="00019">
<document-id>
<country>US</country>
<doc-number>6738499</doc-number>
<kind>B1</kind>
<name>Doi et al.</name>
<date>20040500</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>382128</main-classification></classification-national>
</citation>
<citation>
<patcit num="00020">
<document-id>
<country>US</country>
<doc-number>6801645</doc-number>
<kind>B1</kind>
<name>Collins et al.</name>
<date>20041000</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>382130</main-classification></classification-national>
</citation>
<citation>
<patcit num="00021">
<document-id>
<country>US</country>
<doc-number>6810391</doc-number>
<kind>B1</kind>
<name>Birkhoelzer et al.</name>
<date>20041000</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>706  8</main-classification></classification-national>
</citation>
<citation>
<patcit num="00022">
<document-id>
<country>US</country>
<doc-number>7024399</doc-number>
<kind>B2</kind>
<name>Sumner et al.</name>
<date>20060400</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>706 45</main-classification></classification-national>
</citation>
<citation>
<patcit num="00023">
<document-id>
<country>US</country>
<doc-number>2003/0104499</doc-number>
<kind>A1</kind>
<name>Pressman et al.</name>
<date>20030600</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>435  723</main-classification></classification-national>
</citation>
<citation>
<patcit num="00024">
<document-id>
<country>US</country>
<doc-number>2003/0190602</doc-number>
<kind>A1</kind>
<name>Pressman et al.</name>
<date>20031000</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>435  5</main-classification></classification-national>
</citation>
<citation>
<patcit num="00025">
<document-id>
<country>US</country>
<doc-number>2003/0199685</doc-number>
<kind>A1</kind>
<name>Pressman et al.</name>
<date>20031000</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>536 243</main-classification></classification-national>
</citation>
<citation>
<patcit num="00026">
<document-id>
<country>US</country>
<doc-number>2006/0171573</doc-number>
<kind>A1</kind>
<name>Rogers</name>
<date>20060800</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>382128</main-classification></classification-national>
</citation>
<citation>
<nplcit num="00027">
<othercit>Heang-Ping Chan, et al., Computer-aided classification of mammographic masses and normal tissue: linear discriminant analysis in texture feature space; Phys. Med. Biol., 1995, pp. 857-876; vol. 40; IOP Publishing Ltd.; UK.</othercit>
</nplcit>
<category>cited by other</category>
</citation>
<citation>
<nplcit num="00028">
<othercit>Yuan-Hsiang Chang et al., Robustness of Computerized Identification of Masses in Digitized Mammograms; Investigative Radiology; Sep. 1996; pp. 563-568; vol. 31. No. 9: Lippincott-Raven Publishers: Philadelphia.</othercit>
</nplcit>
<category>cited by other</category>
</citation>
<citation>
<nplcit num="00029">
<othercit>Yuan-Hsiang Chang et al., Computerized Identification of Suspicious Regions for Masses in Digitized Mammograms; Investigative Radiology;Mar. 1996; pp. 146-153; vol. 31, No. 3; Lippincott-Raven Publishers; Philadelphia.</othercit>
</nplcit>
<category>cited by other</category>
</citation>
<citation>
<nplcit num="00030">
<othercit>Ioanna Christoyianni et al., Fast Detection of Masses in Computer-Aided Mammography; IEEE Signal Processing Magazine; Jan. 2000; pp. 54-64; vol. 17, No. 1; IEEE Signal Processing Society; Piscataway NJ.</othercit>
</nplcit>
<category>cited by other</category>
</citation>
<citation>
<nplcit num="00031">
<othercit>David B. Fogel et al., Linear and Neural Models for Classifying Breast Masses; IEEE Transactions on Medical Imaging; Jun. 1998; pp. 485-488; vol. 17, No. 3; Engineering in Medicine and Biology Society; Piscataway NJ.</othercit>
</nplcit>
<category>cited by other</category>
</citation>
<citation>
<nplcit num="00032">
<othercit>Maryellen L. Giger et al., Computerized characterization of mammographic masses: analysis of spiculation; Cancer Letters; 1994; pp. 201-211; vol. 77; Elsevier Scientific Publishers Ireland Ltd.; Ireland.</othercit>
</nplcit>
<category>cited by other</category>
</citation>
<citation>
<nplcit num="00033">
<othercit>R. Gupta et al.; The use of tecture analysis to delineate suspicious masses in mammography; Phys. Med. Biol. 1995; pp. 835-855; vol. 40; IOP Publishing Ltd.; UK.</othercit>
</nplcit>
<category>cited by other</category>
</citation>
<citation>
<nplcit num="00034">
<othercit>Lubomir Hadjiiski et al.; Classification of Malignant and Benign Masses Based on Hybrid ART2LDA Approach; IEEE Transactions on Medical Imaging; Dec. 1999; pp. 1178-1187; vol. 18, No. 12; Engineering in Medicine and Biology Society; Piscataway NJ.</othercit>
</nplcit>
<category>cited by other</category>
</citation>
<citation>
<nplcit num="00035">
<othercit>Lubomir Hadjiiski et al.; Hybrid unsupervised approach for computerized classification of malignant and benign masses on mammograms; Part of the SPIE Conference on Image Processing, San Diego; Feb. 1999; pp. 464-473; vol. 3661; International Society for Optical Engineering; Bellingham WA.</othercit>
</nplcit>
<category>cited by other</category>
</citation>
<citation>
<nplcit num="00036">
<othercit>Trevor Hastie et al.; Statistical Measures for the Computer-Aided Diagnosis of Mammographic Masses; Journal of Computational and Graphical Statistics; 1999; pp. 531-543; vol. 8, No. 3; American Statistical Association et al., Alexandria VA.</othercit>
</nplcit>
<category>cited by other</category>
</citation>
<citation>
<nplcit num="00037">
<othercit>Zhimin Huo et al; Analysis of spiculation in the computerized classification of mammographic masses; Medical Physics; Oct. 1995; pp. 1569-1579; vol. 22, No. 10; American Institute of Physics; Melville NY.</othercit>
</nplcit>
<category>cited by other</category>
</citation>
<citation>
<nplcit num="00038">
<othercit>Zhimin Huo et al.; Robustness of Computerized Scheme for the Classification of Malignant and Benign Masses on Digitized Mammograms; Computer-Aided Diagnosis in Medical Imaging; 1999; pp. 277-280; Elsevier Science B. V. New York.</othercit>
</nplcit>
<category>cited by other</category>
</citation>
<citation>
<nplcit num="00039">
<othercit>Zhimin Huo et al; Automated Computerized Classification of Malignant and Benign Masses on Digitized Mammograms; Academic Radiology; Mar. 1998; pp. 155-168; vol. 5, No. 3; Association of University Radiologists; Oak Brook IL.</othercit>
</nplcit>
<category>cited by other</category>
</citation>
<citation>
<nplcit num="00040">
<othercit>S. K. Kinoshita et al.; Characterization of breast masses using texture and shape features; Computer-Aided Diagnosis in Medical Imaging; 1999; pp. 265-270; Elsevier Science B. V. New York.</othercit>
</nplcit>
<category>cited by other</category>
</citation>
<citation>
<nplcit num="00041">
<othercit>Shuk-Mei Lai et al.; On Techniques for Detecting Circumscribed Masses in Mammograms; IEEE Transactions on Medical Imaging; Dec. 1989; pp. 377-386; vol. 8, No. 4; Engineering in Medicine and Biology Society; Piscataway NJ.</othercit>
</nplcit>
<category>cited by other</category>
</citation>
<citation>
<nplcit num="00042">
<othercit>Huai Li et al; Mammographic Mass Detection by Stochastic Modeling and a Multi-Modular Neural Network; SPIE Conference on Image Processing, Newport Beach; Feb. 25-28, 1997; pp. 480-490; vol. 3034; International Society for Optical Engineering; Bellingham WA.</othercit>
</nplcit>
<category>cited by other</category>
</citation>
<citation>
<nplcit num="00043">
<othercit>Shih-Chung B. Lo et al; A Multiple Circular Path Convolution Neural Network System for Detection of Mammographic Masses; IEEE Transactions on Medical Imaging; Feb. 2002; pp. 150-158; vol. 21, No. 2; Engineering in Medicine and Biology Society; Piscatawy NJ.</othercit>
</nplcit>
<category>cited by other</category>
</citation>
<citation>
<nplcit num="00044">
<othercit>Naga R. Mudigonda et al; Detection of Breast Masses in Mammograms by Density Slicing and Texture Flow-Field Analysis; IEEE Transactions on Medical Imaging; Dec. 2001; pp. 1215-1227; vol. 20, No. 12; Engineering in Medicine and Biology Society; Piscataway NJ.</othercit>
</nplcit>
<category>cited by other</category>
</citation>
<citation>
<nplcit num="00045">
<othercit>Naga R. Mudigonda et al; Gradient and Texture Analysis for the Classification of Mammographic Masses; IEEE Transactions on Medical Imaging; Oct. 2000; pp. 1032-1043; vol. 19, No. 10; Engineering in Medicine and Biology Society; Piscataway NJ.</othercit>
</nplcit>
<category>cited by other</category>
</citation>
<citation>
<nplcit num="00046">
<othercit>Robert M. Nishikawa et al; Computer-Aided Detection and Diagnosis of Masses and Clustered Microcalcifications from Digital Mammograms; State of the Art in Digital Mammographic Image Analysis; 1994; pp. 82-102; World Scientific Publishing Co., New Jersey.</othercit>
</nplcit>
<category>cited by other</category>
</citation>
<citation>
<nplcit num="00047">
<othercit>Nicholas Petrick et al; An Adaptive Density-Weighted Contrast Enhancement Filter for Mammographic Breast Mass Detection; IEEE Transactions on Medical Imaging; Feb. 1996; pp. 59-67; vol. 15, No. 1; Engineering in Medicine and Biology Society; Piscataway NJ.</othercit>
</nplcit>
<category>cited by other</category>
</citation>
<citation>
<nplcit num="00048">
<othercit>Nicholas Petrick et al; Unitary Ranking in the Automated Detection of Mammographic Masses; Proceedings of SPIE Conference on Image Processing, Newport Beach; Feb. 25-28, 1997; pp. 522-530; vol. 3034; International Society for Optical Engineering; Bellingham WA.</othercit>
</nplcit>
<category>cited by other</category>
</citation>
<citation>
<nplcit num="00049">
<othercit>Nicholas Petrick et al; Automated detection of breast masses on mammograms using adaptive contrast enhancement and texture classification; Medical Physics; Oct. 1996; pp. 1685-1696; vol. 23, No. 10; American Institute of Physics; Melville NY.</othercit>
</nplcit>
<category>cited by other</category>
</citation>
<citation>
<nplcit num="00050">
<othercit>Arthur Petrosian et al; Computer-aided diagnosis in mamography: classification of mass and normal tissue by texture analysis; Physics in Medicine and Biology; 1994; pp. 2273-2288; vol. 39; IOP Publishing Ltd.; UK.</othercit>
</nplcit>
<category>cited by other</category>
</citation>
<citation>
<nplcit num="00051">
<othercit>Wei Qian et al; Hybrid Adaptive Wavelet-based CAD Method for Mass Detection; proceedings of SPIE Conference on Image Processing, Newport Beach; Feb. 25-28, 1997; pp. 790-801; vol. 3034; International Society for Optical Engineering; Bellingham WA.</othercit>
</nplcit>
<category>cited by other</category>
</citation>
<citation>
<nplcit num="00052">
<othercit>Wei Qian et al. Adaptive Directional Wavelet-based CAD Method for Mass Detection; Computer-Aided Diagnosis in Medical Imaging; 1999; pp. 253-259; Elsevier Science B.V. ; New York.</othercit>
</nplcit>
<category>cited by other</category>
</citation>
<citation>
<nplcit num="00053">
<othercit>Berkman Sahiner et al; Image feature selection by a genetic algorithm: Application to classification of mass and normal breast tissue; Medical Physics; Oct. 1996; pp. 1671-1684; vol. 23, No. 10; American Institute of Physics; Melville NY.</othercit>
</nplcit>
<category>cited by other</category>
</citation>
<citation>
<nplcit num="00054">
<othercit>Berkman Sahiner et al; Classification of Mass and Normal Breast tissue: Feature Selection Using a Genetic Algorithm; Colloquium on Digital Mammography; Feb. 1996: pp. 379-384: Elsevier Science B. V.. New York.</othercit>
</nplcit>
<category>cited by other</category>
</citation>
<citation>
<nplcit num="00055">
<othercit>Berkman Sahiner et al; Classification of Mass and Normal Breast Tissue: A Convolution Neural Network Classifier with Spatial Domain and Texture Images; IEEE Transactions on Medical Imaging; Oct. 1996; pp. 598-610; vol. 15, No. 5; Engineering in Medicine and Biology Society; Piscataway NJ.</othercit>
</nplcit>
<category>cited by other</category>
</citation>
<citation>
<nplcit num="00056">
<othercit>Guido M. Te Brake et al; Single and Multiscale Detection of Masses in Digital Mammograms; IEEE Transactions on Medical Imaging; Jul. 1999; pp. 628-639; vol. 18, No. 7: Engineering in Medicine and Biology Society; Piscataway NJ.</othercit>
</nplcit>
<category>cited by other</category>
</citation>
<citation>
<nplcit num="00057">
<othercit>Celia Varela et al; A set of texture features to differentiate between mass and normal breast tissue on digital mammograms; Computer-Aided Diagnosis in Medical Imaging; 1999; pp. 271-276; Elsevier Science B. V. New York.</othercit>
</nplcit>
<category>cited by other</category>
</citation>
<citation>
<nplcit num="00058">
<othercit>Datong Wei et al;Classification of mass and normal breast tissue on digital mammograms: Multiresolution texture analysis; Medical Physics; Sep. 1995; pp. 1501-1513: vol. 22. No. 9: American Institute of Physics: Melville NY.</othercit>
</nplcit>
<category>cited by other</category>
</citation>
<citation>
<nplcit num="00059">
<othercit>Datong Wei et al; False-positive reduction technique for detection of masses on digital mammograms: Global and local multiresolution texture analysis; Medical Physics; Jun. 1997; pp. 903-914; vol. 24, No. 6; American Institute of Physics; Melville NY.</othercit>
</nplcit>
<category>cited by other</category>
</citation>
<citation>
<nplcit num="00060">
<othercit>Fang-Fang Yin et al; Computerized detection of masses in digital mammograms: Analysis of bilateral subtraction images; Medical Physics; Sep./Oct. 1991; pp. 955-963; vol. 18, No. 5; American Institute of Physics; Melville NY.</othercit>
</nplcit>
<category>cited by other</category>
</citation>
<citation>
<nplcit num="00061">
<othercit>Fang-Fang Yin et al ; Computerized Detection of Masses in Digital Mammograms: Investigation of Feature-Analysis Techniques; Journal of Digital Imaging; Feb. 1994; pp. 18-26; vol. 7, No. 1; Springer-Verlag New York.</othercit>
</nplcit>
<category>cited by other</category>
</citation>
<citation>
<nplcit num="00062">
<othercit>Bin Zheng et al; Computerized Detection of Masses from Digitized Mammograms: Comparison of Single-Image Segmentation and Bilateral-Image Subtraction; Acad Radiol; Dec. 1995; pp. 1056-1061; vol. 2, No. 12; Association of University Radiologists; Oak Brook IL.</othercit>
</nplcit>
<category>cited by other</category>
</citation>
<citation>
<nplcit num="00063">
<othercit>Reyer Zwiggelaar et al; Detection of the Central Mass of Spiculated Lesions—Signature Normalisation and Model Data Aspects; IPMI'99, LNCS; 1999; pp. 406-411; vol. 1613; Springer-Verlag Berlin.</othercit>
</nplcit>
<category>cited by other</category>
</citation>
<citation>
<nplcit num="00064">
<othercit>R. Zwiggelaar et al; Model-based detection of spiculated lesions in mammograms; Medical Image Analysis; 1999; pp. 1-25; vol. 3, No. 1; Oxford University Press; UK.</othercit>
</nplcit>
<category>cited by other</category>
</citation>
<citation>
<nplcit num="00065">
<othercit>Reyer Zwiggelaar et al; Statistical modelling of lines and structures in mammograms; proceedings of SPIE Conference on Image Processing, Newport Beach; Feb. 25-28, 1997; p. 510-521; vol. 3034; International Society for Optical Engineering; Bellingham WA.</othercit>
</nplcit>
<category>cited by other</category>
</citation>
<citation>
<nplcit num="00066">
<othercit>Xiao-Hui Wang et al; Computer-assisted diagnosis of breast cancer using a data-driven Bayesian belief network; International Journal of Medical Informatics; 1999; pp. 115-126; vol. 54; Elsevier Science Ireland Ltd.; Irelend.</othercit>
</nplcit>
<category>cited by other</category>
</citation>
<citation>
<nplcit num="00067">
<othercit>Bin Zheng et al; Comparison of Artificial Neural Network and Bayesian Belief Network in a Computer-Assisted Diagnosis Scheme for Mammography; International Joint Conference on Neural Networks 1999; Jul. 10-16, 1999; pp. 4181-4185; IEEE; Piscataway NJ.</othercit>
</nplcit>
<category>cited by other</category>
</citation>
</references-cited>
<number-of-claims>20</number-of-claims>
<us-exemplary-claim>1</us-exemplary-claim>
<us-field-of-classification-search>
<classification-national>
<country>US</country>
<main-classification>382 32</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>382155-160</main-classification>
<additional-info>unstructured</additional-info>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>382173</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>382180</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>382181</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>382190</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>382192</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>382195</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>382203</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>382206</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>382224</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>382225</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>378  1</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>378  4</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>378 21</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>378 62</main-classification>
</classification-national>
</us-field-of-classification-search>
<figures>
<number-of-drawing-sheets>15</number-of-drawing-sheets>
<number-of-figures>23</number-of-figures>
</figures>
<us-related-documents>
<us-provisional-application>
<document-id>
<country>US</country>
<doc-number>60333825</doc-number>
<kind>00</kind>
<date>20011120</date>
</document-id>
</us-provisional-application>
</us-related-documents>
<parties>
<applicants>
<applicant sequence="001" app-type="applicant-inventor" designation="us-only">
<addressbook>
<last-name>Collins</last-name>
<first-name>Michael J.</first-name>
<address>
<city>Beavercreek</city>
<state>OH</state>
<country>US</country>
</address>
</addressbook>
<nationality>
<country>omitted</country>
</nationality>
<residence>
<country>US</country>
</residence>
</applicant>
<applicant sequence="002" app-type="applicant-inventor" designation="us-only">
<addressbook>
<last-name>Mitchell</last-name>
<first-name>Richard A.</first-name>
<address>
<city>Springboro</city>
<state>OH</state>
<country>US</country>
</address>
</addressbook>
<nationality>
<country>omitted</country>
</nationality>
<residence>
<country>US</country>
</residence>
</applicant>
<applicant sequence="003" app-type="applicant-inventor" designation="us-only">
<addressbook>
<last-name>Worrell</last-name>
<first-name>Steven W.</first-name>
<address>
<city>Beavercreek</city>
<state>OH</state>
<country>US</country>
</address>
</addressbook>
<nationality>
<country>omitted</country>
</nationality>
<residence>
<country>US</country>
</residence>
</applicant>
</applicants>
<agents>
<agent sequence="01" rep-type="attorney">
<addressbook>
<orgname>Dinsmore &amp; Shohl LLP</orgname>
<address>
<country>unknown</country>
</address>
</addressbook>
</agent>
</agents>
</parties>
<assignees>
<assignee>
<addressbook>
<orgname>ICad, Inc.</orgname>
<role>02</role>
<address>
<city>Nashua</city>
<state>NH</state>
<country>US</country>
</address>
</addressbook>
</assignee>
</assignees>
<examiners>
<primary-examiner>
<last-name>Mehta</last-name>
<first-name>Bhavesh M</first-name>
<department>2624</department>
</primary-examiner>
<assistant-examiner>
<last-name>Seth</last-name>
<first-name>Manav</first-name>
</assistant-examiner>
</examiners>
</us-bibliographic-data-grant>
<abstract id="abstract">
<p id="p-0001" num="0000">This invention provides an information fusion method for multiple indicators of cancers detected with multiple channels. Each channel consists of specifically tuned detectors, features, classifiers and Bayes networks. The outputs of the Bayes networks are probabilities of malignancy for the detections passing the corresponding classifier.</p>
</abstract>
<drawings id="DRAWINGS">
<figure id="Fig-EMI-D00000" num="00000">
<img id="EMI-D00000" he="253.75mm" wi="163.58mm" file="US07298877-20071120-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00001" num="00001">
<img id="EMI-D00001" he="233.51mm" wi="109.98mm" file="US07298877-20071120-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00002" num="00002">
<img id="EMI-D00002" he="232.49mm" wi="170.35mm" file="US07298877-20071120-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00003" num="00003">
<img id="EMI-D00003" he="233.17mm" wi="120.40mm" file="US07298877-20071120-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00004" num="00004">
<img id="EMI-D00004" he="224.79mm" wi="162.39mm" file="US07298877-20071120-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00005" num="00005">
<img id="EMI-D00005" he="183.22mm" wi="163.07mm" file="US07298877-20071120-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00006" num="00006">
<img id="EMI-D00006" he="236.30mm" wi="147.49mm" file="US07298877-20071120-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00007" num="00007">
<img id="EMI-D00007" he="235.29mm" wi="163.07mm" file="US07298877-20071120-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00008" num="00008">
<img id="EMI-D00008" he="237.32mm" wi="163.83mm" file="US07298877-20071120-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00009" num="00009">
<img id="EMI-D00009" he="207.86mm" wi="171.79mm" file="US07298877-20071120-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00010" num="00010">
<img id="EMI-D00010" he="234.61mm" wi="158.58mm" file="US07298877-20071120-D00010.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00011" num="00011">
<img id="EMI-D00011" he="260.52mm" wi="165.86mm" file="US07298877-20071120-D00011.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00012" num="00012">
<img id="EMI-D00012" he="238.42mm" wi="161.04mm" file="US07298877-20071120-D00012.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00013" num="00013">
<img id="EMI-D00013" he="188.81mm" wi="92.96mm" file="US07298877-20071120-D00013.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00014" num="00014">
<img id="EMI-D00014" he="228.01mm" wi="158.58mm" file="US07298877-20071120-D00014.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00015" num="00015">
<img id="EMI-D00015" he="238.68mm" wi="168.99mm" file="US07298877-20071120-D00015.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
</drawings>
<description id="description">
<?RELAPP description="Other Patent Relations" end="lead"?>
<heading id="h-0001" level="1">CROSS REFERENCE TO RELATED APPLICATION</heading>
<p id="p-0002" num="0001">This invention claims benefit of U.S. Provisional filing, Ser. No. 60/333,825, filed Nov. 20, 2001.</p>
<?RELAPP description="Other Patent Relations" end="tail"?>
<?BRFSUM description="Brief Summary" end="lead"?>
<heading id="h-0002" level="1">BACKGROUND OF THE INVENTION</heading>
<p id="p-0003" num="0002">1. Field of the Invention</p>
<p id="p-0004" num="0003">This invention pertains to methods of combining multiple types of information in computer-aided detection (CAD) systems.</p>
<p id="p-0005" num="0004">2. Description of the Prior Art</p>
<p id="p-0006" num="0005">Mammographic CAD systems have existed in research environments since the 1960's. Currently available systems find indicators of cancer in mammograms. Typical indicators are clusters of microcalcifications, masses, and spiculated masses. Each indicator may be detected with a specific processing channel. Each channel provides a set of detected regions considered as potentially cancerous. Various methods have been described for combining sets of detections from multiple channels.</p>
<p id="p-0007" num="0006">The issue of combining multiple detectors in a CAD system has been addressed in prior US patents. Rogers et al, in U.S. application Ser. No. 09/602,762 describe a method for combining outputs of microcalcification and density channels. Case Based Ranking (CBR) limits the total number of marks in a case by retaining only the “best” subset. Additional limits are applied to the total number of marks per image and the total number of each type of mark allowed on any one image of the case. In application Ser. No. 09/602,762, the ranking is based on a difference of discriminants score, which is proportional to the probability of cancer.</p>
<p id="p-0008" num="0007">Roehrig et al, in U.S. Pat. No. 6,198,838 describe a method for combining outputs of two independent channels: mass detector and spiculation detector. Each detector processes the image, detects regions of interest (ROIs), and computes sets of mass and spiculation feature vectors characterizing the ROI. The combining method taught by Roehrig consists of concatenating mass and spiculation information into a feature vector, then applying it to a classifier. ROIs passing the classifier are then displayed as final detections.</p>
<p id="p-0009" num="0008">In Roehrig et al, the simple concatenation of features from distinct channels has one very undesirable effect. The probability distributions of the cancer/not cancer concatenated feature are more confusable than either of the original feature vectors. This is because both spiculated and non-spiculated lesions are represented by a single feature vector consisting of mass and spiculation elements. The features specific for spiculatedness will be “noiselike” for regions of interest containing masses. Similarly, the massness features will be noiselike for regions containing spiculations. Assuming an equal number of massness and spiculatedness features, half of the features for any lesion will be noise. The end result is a reduction in classifier accuracy in a fielded system.</p>
<p id="p-0010" num="0009">Viewed from a different perspective, the classifier is forced to consider masses and spiculated masses as a single “cancer” category. Since feature vectors derived from independent and possibly orthogonal sources, the values of feature vectors in the “cancer” category are dispersed over a larger volume of feature space than would be required if the separate classifiers were applied for the mass and spiculated mass channels.</p>
<p id="p-0011" num="0010">To achieve higher classification accuracy in CAD systems, there is clearly a need for an improved method to combine lesion information.</p>
<heading id="h-0003" level="1">SUMMARY OF THE INVENTION</heading>
<p id="p-0012" num="0011">The present invention provides a means for computing a probability of malignancy for a lesion by combining multiple types of lesion information and, in particular, provides a system and method for combining information obtained from detections relating to different lesion-types to provide an output comprising a final display of suspicious lesions. Plural channels are provided for detecting and processing information specific to each lesion-type and, in the preferred embodiment, the channels correspond to calcifications, masses and spiculations. Each channel includes a tuned detector, classifier, and Bayes network wherein the outputs of the Bayes networks are probabilities of malignancy. The malignancies are rank ordered in a rule based post-processing operation to provide the final display of suspicious lesions.</p>
<p id="p-0013" num="0012">According to one aspect of the invention, a method of detecting malignancy of a lesion is provided, comprising the steps of: computing evidence from each lesion, providing the evidence to a Bayes network, and computing a probability of malignancy for each lesion.</p>
<p id="p-0014" num="0013">According to another aspect of the invention, a method for use in a CAD system for providing detections of suspicious lesions is provided, wherein the method provides for selection of lesions for final display and comprises the steps of: calculating a probability of malignancy for each lesion, and using the probability of malignancy as a selection criteria affecting a final display of suspicious lesions.</p>
<p id="p-0015" num="0014">According to yet another aspect of the invention, a method of detecting malignancy of a lesion is provided, comprising the steps of: using independent lesion-type specific detectors to detect lesions, computing lesion-type specific evidence corresponding to each detected lesion, providing the lesion-type specific evidence to lesion-type specific Bayes networks, and computing a lesion-type specific probability value relating to malignancy for at least some of the detected lesions.</p>
<p id="p-0016" num="0015">According to a further aspect of the invention, A method of enhancing image regions corresponding to spiculations comprising: providing a set of line kernels and a set of spiculation kernels, filtering a medical image with the set of line kernels to create a binary line response image and an angle orientation image, removing spurious lines from the line response image producing a pruned line image, separating lines in the pruned line image according to directions specified by the angle orientation image to produce a set of orientation specific line images, multiplying the orientation specific line images with the medical image to produce a set of intensity-weighted line images, filtering the intensity-weighted line images with the set of spiculation kernels to produce a set of spiculation response images, and storing the maximum spiculation response value at each pixel location, producing an image with enhanced spiculations.</p>
<p id="p-0017" num="0016">According to another aspect of the invention, a method for detecting spiculations in a medical image is provided, comprising: providing a set of line kernels and a set of spiculation kernels, filtering a medical image with the set of line kernels to create a binary line response image and an angle orientation image, removing spurious lines from the line response image producing a pruned line image, separating lines in the pruned line image according to directions specified by the angle orientation image to produce a set of orientation specific line images, multiplying the orientation specific line images with the medical image to produce a set of intensity-weighted line images, filtering the intensity-weighted line images with the set of spiculation kernels to produce a set of spiculation response images, storing the maximum spiculation response value at each pixel location, producing a spiculation image, and thresholding the spiculation image to produce image regions containing spiculations.</p>
<p id="p-0018" num="0017">Other objects and advantages of the invention will be apparent from the following description, the accompanying drawings and the appended claims.</p>
<?BRFSUM description="Brief Summary" end="tail"?>
<?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?>
<description-of-drawings>
<heading id="h-0004" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading>
<p id="p-0019" num="0018"><figref idref="DRAWINGS">FIG. 1</figref> is an overview of the invention.</p>
<p id="p-0020" num="0019"><figref idref="DRAWINGS">FIG. 2</figref> shows a CAD system with independent channels, and information fusion by Bayes Networks and Case Based Ranking.</p>
<p id="p-0021" num="0020"><figref idref="DRAWINGS">FIG. 3</figref> shows a system to detect spiculations.</p>
<p id="p-0022" num="0021"><figref idref="DRAWINGS">FIG. 4</figref> is a block diagram of a line detection method.</p>
<p id="p-0023" num="0022"><figref idref="DRAWINGS">FIG. 5</figref> illustrates line kernels.</p>
<p id="p-0024" num="0023"><figref idref="DRAWINGS">FIG. 6</figref> is a block diagram for the method of generating orientation specific line images.</p>
<p id="p-0025" num="0024"><figref idref="DRAWINGS">FIG. 7</figref> shows the method for removing spurious lines.</p>
<p id="p-0026" num="0025"><figref idref="DRAWINGS">FIG. 8</figref> is a block diagram for the method of creating a spiculation image.</p>
<p id="p-0027" num="0026"><figref idref="DRAWINGS">FIG. 9</figref> illustrates spiculation kernels.</p>
<p id="p-0028" num="0027"><figref idref="DRAWINGS">FIG. 10</figref> is a block diagram for the spiculated region detector.</p>
<p id="p-0029" num="0028"><figref idref="DRAWINGS">FIG. 11</figref> illustrates the architecture of the CAD system with Bayes Networks incorporated into each channel.</p>
<p id="p-0030" num="0029"><figref idref="DRAWINGS">FIG. 12</figref> shows Bayes network topologies for the Calc, Mass, and Spiculation channels.</p>
<p id="p-0031" num="0030"><figref idref="DRAWINGS">FIG. 13</figref> shows means and variances for Calc, Mass, and spiculation discriminant scores.</p>
<p id="p-0032" num="0031"><figref idref="DRAWINGS">FIG. 14</figref> shows conditional probability tables of collocation for the Calc, Mass, and Spiculation channels.</p>
<p id="p-0033" num="0032"><figref idref="DRAWINGS">FIG. 15</figref> is a flow chart for the Case Based Ranking method.</p>
</description-of-drawings>
<?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?>
<?DETDESC description="Detailed Description" end="lead"?>
<heading id="h-0005" level="1">DETAILED DESCRIPTION OF THE PREFERRED EMBODIMENT</heading>
<p id="p-0034" num="0033">The primary advantage of this invention is obtained by providing an information fusion method for multiple indicators of cancers detected with multiple channels. Each channel is lesion type specific and comprises specifically tuned detectors, features, classifiers and Bayes networks. The outputs of the Bayes networks are probabilities of malignancy for the detections passing the corresponding classifier. These probabilities are rank ordered in a rule based post-processing step to provide final CAD system outputs.</p>
<p id="p-0035" num="0034"><figref idref="DRAWINGS">FIG. 1</figref> shows an overview of the system. Mammograms, <b>1000</b>, are obtained during a radiologic exam. The output of the exam may be either a set of films or a set of digital images depending on the type of equipment used during the exam. When the mammograms are films, the CAD system, <b>2000</b>, uses a film digitizer to generate digital images of each film for subsequent analysis. When the mammograms are created by a full field digital mammography (FFDM) system, the digitization step is not required and the CAD system operates directly on the digital images. The mammograms are processed by the CAD system, producing a set of output detections and associated attributes, <b>3000</b>. In the prior art, as in Roehrig, such an output provides final detections. In this invention, additional attributes are associated with each detection which are then used to generate fusion information, <b>4000</b>. The fusion information is input to a Bayesian Information Fusion step, <b>5000</b>. In this step, probabilities of cancer, <b>6000</b>, are computed for each detection. We will show that the union of the Generate Fusion Input Information and Bayesian Information Fusion steps, <b>4000</b> and <b>5000</b>, with the preceding and subsequent steps is an important aspect of this invention. The detections are ranked by probabilities and cancer categories in the Case Based Ranking step, <b>7000</b>. This step limits the number of detections to a maximum number per category per case, producing the final set of detections, <b>8000</b>. Each element is now described in more detail.</p>
<p id="p-0036" num="0035">The invention applied to a CAD system is shown in <figref idref="DRAWINGS">FIG. 2</figref>. CAD systems typically provide separate channels for each category of cancer to be detected, as shown in the dashed box, <b>2000</b>. Here we show a three channel system. The left channel detects and classifies microcalcification regions of interest (ROIs). The center channel detects and classifies mass ROIs. The right channel detects and classifies spiculation ROIs. The first two steps in each channel are to detect potential lesions and compute features for each ROI. The third step provides ROI classification within each channel. The classifiers output difference of discriminant scores for each ROI. Prior to computing context information, <b>4000</b>, the number of detections per image is limited to a predetermined maximum value per category, not shown. This limiting improves the quality of subsequent context information. In a preferred embodiment, the microcalcification channel is limited to two detections per image, while the mass and spiculation channels are limited to three detections per image. The discriminant scores and other information comprise evidence input to corresponding Bayes networks, <b>5000</b>, as the final step within each channel. The Bayes networks compute the probability of cancer for each ROI. The probability scores are passed to the Case Based Ranking step, <b>7000</b>. Here, ROIs are rank ordered and rules applied for restricting the number of detections per case for different indicators of cancer. The output of post processing is a final set of ROIs, <b>8000</b>. The final detections are displayed to assist a radiologist in the detection of cancer.</p>
<p id="p-0037" num="0036">The microcalcification and mass channels, the left and center channels in <figref idref="DRAWINGS">FIG. 2</figref>, have been described in U.S. application Ser. No. 09/602,762, filed Jun. 23, 2000, which application is assigned to the assignee of the present application and is incorporated herein by reference. Although context fusion is general and may be applied to a system with only two channels, such as mass and microcalcification, a preferred embodiment of the present application additionally includes a spiculation channel as the third indicator of cancer which is now described in further detail.</p>
<heading id="h-0006" level="1">Spiculation Channel</heading>
<p id="p-0038" num="0037">An overview of the spiculation detector is shown in <figref idref="DRAWINGS">FIG. 3</figref>. Preprocessing, not shown here, is applied to the set of input images as described in U.S. application Ser. No. 09/602,762. The spiculation channel operates on images with approximately 175 micron pixel spacing. Line kernels of eight different orientations, <b>2310</b>, are convolved with the input image, <b>1001</b>, in step <b>2300</b>. Lines are detected in the step <b>2400</b>. Detected line images are formed at each of the eight orientations. The detected line images are then weighted by the original image intensity in step <b>2500</b>.</p>
<p id="p-0039" num="0038">Weighting by original image intensity reduces the contribution of lines with low intensity. Using the raw intensity at any stage beyond the initial line detection is considered undesirable when striving for a robust system design. However, if the spiculation system had to rely solely on binary line images to detect spiculation it would result in many false positives detections in dim, low contrast regions. To eliminate this problem the line images are weighted by the original image intensity. In so doing, a variable level of importance is established to the line structure. The importance is directly related to the intensity of the detected lines.</p>
<p id="p-0040" num="0039">After weighting by original image intensity, the detected line images are then subsampled by a factor of four in each dimension, step <b>2600</b>. A set of spiculation kernels, <b>2710</b>, which detect regions of diametrically opposed radiating lines, are convolved with the subsampled line images in step <b>2700</b>. Regions with large responses to the spiculation kernels are detected in step <b>2800</b>, producing a set of spiculation ROIs, <b>2900</b>. Each step in the spiculation detector is now described in more detail.</p>
<heading id="h-0007" level="1">Line Detection</heading>
<p id="p-0041" num="0040">An overview of the line detection algorithm is shown in <figref idref="DRAWINGS">FIG. 4</figref>. First, eight line kernels are convolved with the input image. The kernel orientations span 180 degrees in 22.5 degree steps. Each kernel consists of a single pixel width line at eight different orientations in a 9 by 9 pixel window, normalized to zero mean. The line kernels are shown in <figref idref="DRAWINGS">FIG. 5</figref>.</p>
<p id="p-0042" num="0041">Referring again to <figref idref="DRAWINGS">FIG. 4</figref>, convolving an input image, <b>1001</b>, with the set of line kernels, <b>2401</b> through <b>2408</b>, produces a set of eight filtered images, <b>2411</b> through <b>2418</b>. Let the n<sup>th </sup>filtered image be denoted by I<sub>n</sub>(x,y). Then, the line response image, L(x,y), and angle response image, A(x,y) are formed as:
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>L</i>(<i>x,y</i>)=max<sub>n</sub><i>{I</i><sub>n</sub>(<i>x,y</i>)}; <i>n=</i>1, . . . , 8  (1)<?in-line-formulae description="In-line Formulae" end="tail"?>
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>A</i>(<i>x,y</i>)=<i>arg</i>max<sub>n</sub><i>{I</i><sub>n</sub>(<i>x,y</i>)}; <i>n=</i>1, . . . , 8  (2)<?in-line-formulae description="In-line Formulae" end="tail"?>
</p>
<p id="p-0043" num="0042">Equation 1 shows the value of the line response image at location (x,y) is selected as the maximum response across the eight filtered images at that location. Equation 2 shows the value of the angle response image at location (x,y) is the index of the filtered image that provided the maximum response.</p>
<p id="p-0044" num="0043">In step <b>2420</b> the maximum values from the set of filtered images at each (x,y) location are stored to form the line response image, <b>2430</b>. In step <b>2440</b>, the index of the filtered image providing the maximal response is stored, producing the angle orientation image, <b>2450</b>.</p>
<p id="p-0045" num="0044">A local constant false alarm rate (CFAR) thresholding method is applied to detect lines from the line response image, as shown in <figref idref="DRAWINGS">FIG. 6</figref>. The local mean and standard deviation of the line response image, <b>2430</b>, are computed in a 21 by 21 pixel neighborhood centered at pixel location (x,y) in step <b>2452</b>. In step <b>2454</b>, the pixel under test, L(x,y), is compared to a local threshold, in a preferred embodiment computed as the local mean plus 0.5 times the local standard deviation. If L(x,y) is greater than the threshold, location (x,y) of the local thresholded line image, <b>2456</b>, is set to ‘1’, otherwise, location (x,y) is set to ‘0’.</p>
<p id="p-0046" num="0045">The local threshold line image may have unwanted lines detected at areas in the original image with high intensity or contrast, such as along the chest wall or the skin-air line. These types of spurious lines are removed in step, <b>2460</b>, shown in <figref idref="DRAWINGS">FIG. 7</figref>.</p>
<p id="p-0047" num="0046">The line removal algorithm requires as inputs the breast mask, computed in a pre-processing step, the line response image, <b>2430</b>, and the local threshold line image, <b>2456</b>. The breast mask, a binary image with values of ‘1’ corresponding to regions of breast tissue and ‘0’ elsewhere, is eroded by N=31 pixels, in step <b>2642</b>. Erosion produces a smaller, eroded breast mask, <b>2464</b>, free from the artifacts that we are trying to remove. The global mean and standard deviation in the line response image is calculated from the area of the eroded breast mask in step <b>2465</b>. The spurious lines are then detected and removed by determining the lines that are statistical outliers. A global threshold is computed, in a preferred embodiment, as
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>T</i>=global mean+3.5*global standard deviation<?in-line-formulae description="In-line Formulae" end="tail"?>
<br/>
and applied in step <b>2466</b>. For pixels (x,y) in the line response image greater than the threshold T, the remove line mask image, <b>2467</b>, at location (x,y) equals ‘1’. Otherwise location (x,y) of the remove line mask image equals ‘0’. In step <b>2468</b>, the remove line image mask is applied to the local threshold line image, <b>2456</b>. Pixels in the local threshold line image are cleared, that is set to ‘0’, at pixel locations for which the remove line mask image equals ‘1’. This step effectively removes spurious lines detected due to the chest wall and the skin-air line, producing a pruned line image, <b>2470</b>.
</p>
<p id="p-0048" num="0047">Referring again, to <figref idref="DRAWINGS">FIG. 6</figref>, the final step of the line detection method is to separate the lines by orientation, <b>2480</b>. This is accomplished by associating each ‘on’ pixel in the pruned line image <b>2470</b> to an orientation using the angle orientation image, <b>2450</b>. This step produces eight orientation specific line images, <b>2490</b>, for subsequent use in the spiculation detector.</p>
<heading id="h-0008" level="1">Spiculation Detection</heading>
<p id="p-0049" num="0048">A block diagram of the spiculation detection method is shown in <figref idref="DRAWINGS">FIG. 8</figref>.</p>
<p id="p-0050" num="0049">The role of the spiculation detector is to identify regions exhibiting diametrically opposed radiating line structure. The inputs to the spiculation detection method are the set of spiculation kernels, <b>2710</b> and the set of orientation specific line images, <b>2490</b>. In a preferred embodiment, there are four sizes of spiculation kernels oriented at eight angles.</p>
<p id="p-0051" num="0050">Each orientation specific line image is convolved with an associated orientation specific spiculation kernel of specified edge length in step <b>2700</b>. The response is accumulated across the range of orientations, step <b>2810</b>. In this embodiment, four spiculation response images are created, one for each speculation kernel size. The set of spiculation response images are shown in box <b>2815</b>. In step <b>2820</b>, a spiculation image, <b>2825</b>, is created by keeping the maximum response from the set of spiculation response images at each (x,y) pixel location. In step <b>2830</b>, the index of the spiculation kernel providing the maximum response of step <b>2820</b> is stored, producing the winning kernel image, <b>2835</b>.</p>
<p id="p-0052" num="0051">Referring to <figref idref="DRAWINGS">FIG. 9</figref>, each spiculation kernel consists of a disc with wedges at a specific orientation, corresponding to a line kernel orientation. In the hashed regions outside the disc, the kernel has zero contribution, the white wedges denote a constant positive contribution, and the wavy wedges denote a constant negative contribution. Each of the kernels are zero mean. The positive wedges span 45 degrees, and are oriented from 0 to 180 degrees in steps of 22.5 degrees. To detect a range of sizes, four sizes of spiculation kernels are used. The spiculation kernels are square windows with edge lengths, L, of 15, 25, 35, and 45 pixels.</p>
<p id="p-0053" num="0052">Given the spiculation image <b>2825</b>, detections must be created. A block diagram of the method is given in <figref idref="DRAWINGS">FIG. 10</figref>. The region of interest (ROI) detector for the spiculation system is a global CFAR. The global mean, Gm, and standard deviation Gsd, are calculated over the breast tissue area of the spiculation response image in step <b>2840</b>. If the spiculation response for a given pixel is greater than Gm+n1*Gsd, then the pixel is retained as part of an ROI. In a preferred embodiment, n1=4.5. Connected component analysis, step <b>2850</b>, groups pixels satisfying the threshold into a set of distinct objects where an object is a set of adjacent pixels (4 or 8 connected), producing a set of binary detection masks, <b>2855</b>. The binary detection masks indicate the location and extent of detections in the spiculation image. The detection masks are rectangular and bound each object from the connected component analysis. Pixels in the mask corresponding to image locations exceeding the threshold of step <b>2845</b> are set to ‘1’.</p>
<p id="p-0054" num="0053">The spiculation image is re-thresholded in step <b>2860</b>. A second threshold is computed as Gm+n2*Gsd with n2=3.0. Connected component analysis of the re-thresholded image provides a second set of detections. Only those re-thresholded detections containing an object from the first thresholding are retained.</p>
<p id="p-0055" num="0054">The outputs are bounding boxes, <b>2870</b>, and detection maps, <b>2880</b>, for the re-thresholded objects. A detection map is a binary mask with ‘1’s indicating pixel locations corresponding to the re-thresholded object. The bounding box is specified as the row and column coordinates of the upper left and lower right corners of a rectangle sized to circumscribe the object.</p>
<heading id="h-0009" level="1">Bayes Network</heading>
<p id="p-0056" num="0055"><figref idref="DRAWINGS">FIG. 11</figref> shows how Bayes networks are used in this invention. The individual channels, <b>2010</b>, <b>2020</b>, and <b>2030</b>, detect specific types of lesions. Features are computed and applied to corresponding classifiers, producing discriminant scores. The discriminant scores and other information are used as evidence, <b>4010</b>, <b>4020</b>, and <b>4030</b>, for the Bayes networks, <b>5100</b>, <b>5200</b>, and <b>5300</b>, within each channel. The Bayes networks compute probabilities of cancer given the evidence, <b>6100</b>, <b>6200</b>, and <b>6300</b>. These probabilities are provided to a Case Based Ranking step, <b>7000</b>, to produce final detections, <b>8000</b>.</p>
<heading id="h-0010" level="1">Bayes Network Evidence</heading>
<p id="p-0057" num="0056">Many different possible types of information are potentially useful as evidence to Bayes networks. In a preferred embodiment, shown in <figref idref="DRAWINGS">FIG. 11</figref>, steps <b>4010</b>, <b>4020</b>, and <b>4030</b> show two basic types of evidence: normalized difference of discriminants, and collocation indicators. Each of these are now described. Normalized differences of discriminants are computed as follows. First, classifiers are trained in each channel as described in application Ser. No. 09/602,762, producing two discriminant functions and a threshold value for each indicator of cancer. Within each channel, the true positive (TP) discriminant corresponds to the classifier representing that channel's category and the false positive (FP) discriminant to the non-TP categories. Discriminant scores are equivalent to distances from prototypical examples. For example, if in the calc channel a detection produces a small TP discriminant and a large FP discriminant, the detection is more likely to be a calc than not. Within each channel, the normalized difference is computed by first subtracting the TP from the FP discriminant. Then, the threshold value is subtracted from the difference of discriminants score and the result raised to a power transform value. In the calc and mass channel, the power transform value equals 0.35; in the spiculation channel, the power transform value is 0.25.</p>
<p id="p-0058" num="0057">Collocation indicators are binary units of evidence. A collocation indicator is set to ‘1’ if a detection in one channel overlaps a detection in another channel.</p>
<p id="p-0059" num="0058">The specific evidence used in the Bayes networks of each channel is now described.</p>
<heading id="h-0011" level="1">Calc Bayes Network</heading>
<p id="p-0060" num="0059">In the calc channel, the normalized difference of discriminants and a collocation indicator are input to the Calc Bayes network. The collocation indicator is ‘1’ for calc detections overlapping either a mass or spiculation detection. The topology of the Mass Bayes Network is shown in <figref idref="DRAWINGS">FIG. 12(</figref><i>a</i>).</p>
<heading id="h-0012" level="1">Mass Bayes Network</heading>
<p id="p-0061" num="0060">In the mass channel, the normalized difference of discriminants and two collocation indicators are input to the Mass Bayes network. The first collocation indicator is ‘1’ for mass detections overlapping a microcalcification detection. The second collocation indicator is ‘1’ for mass detections overlapping a spiculation detection. The topology of the Mass Bayes Network is shown in <figref idref="DRAWINGS">FIG. 12(</figref><i>b</i>).</p>
<heading id="h-0013" level="1">Spiculation Bayes Network</heading>
<p id="p-0062" num="0061">In the spiculation channel, the normalized difference of discriminants and two collocation indicators are input to the Spiculation Bayes network. The first collocation indicator is ‘1’ for spiculation detections overlapping a microcalcification detection. The second collocation indicator is ‘1’ for spiculation detections overlapping a mass detection. The topology of the Spiculation Bayes Network is shown in <figref idref="DRAWINGS">FIG. 12(</figref><i>c</i>).</p>
<heading id="h-0014" level="1">Calculating Probabilities</heading>
<p id="p-0063" num="0062">The evidence for each ROI is supplied to the Bayes Network, and the probability of cancer is calculated for each ROI. The probability calculation requires conditional probability distributions or functions to be estimated from a set of training data. That is, the evidence is tabulated for detections of known “cancer” or “not cancer” categories. In a preferred embodiment, let the conditional distributions resulting from training data be as shown in <figref idref="DRAWINGS">FIGS. 13 and 14</figref>. The conditional distributions of the normalized difference of discriminants is assumed to be Gaussian, and therefore specified by mean and variance. <figref idref="DRAWINGS">FIG. 13</figref> shows the mean and standard variance values for the three channels. The remaining evidence is discrete, and represented in conditional probability tables. <figref idref="DRAWINGS">FIG. 14</figref> shows the conditional probability tables for the collocation evidence. Collocation is defined as the centroid of the first detection type being located inside the second detection type.</p>
<p id="p-0064" num="0063">When computing the probability of cancer for a lesion in an input image, the lesion's particular values of evidence are used to obtain conditional probability values from <figref idref="DRAWINGS">FIGS. 13 and 14</figref>. The probabilities of not cancer and cancer (i=0, 1) after evidence is considered, P(C<sub>i</sub>|e<sub>1</sub>, e<sub>2</sub>, . . . , e<sub>n</sub>), is given by</p>
<p id="p-0065" num="0064">
<maths id="MATH-US-00001" num="00001">
<math overflow="scroll">
<mtable>
  <mtr>
    <mtd>
      <mrow>
        <mrow>
          <mi>Pr</mi>
          <mo>⁡</mo>
          <mrow>
            <mo>(</mo>
            <mrow>
              <mrow>
                <msub>
                  <mi>C</mi>
                  <mi>i</mi>
                </msub>
                <mo>|</mo>
                <msub>
                  <mi>e</mi>
                  <mn>1</mn>
                </msub>
              </mrow>
              <mo>,</mo>
              <msub>
                <mi>e</mi>
                <mn>2</mn>
              </msub>
              <mo>,</mo>
              <mi>…</mi>
              <mo>⁢</mo>
              <mstyle>
                <mspace width="0.8em" height="0.8ex"/>
              </mstyle>
              <mo>,</mo>
              <msub>
                <mi>e</mi>
                <mi>n</mi>
              </msub>
            </mrow>
            <mo>)</mo>
          </mrow>
        </mrow>
        <mo>=</mo>
        <mfrac>
          <mrow>
            <mrow>
              <mi>p</mi>
              <mo>⁡</mo>
              <mrow>
                <mo>(</mo>
                <mrow>
                  <msub>
                    <mi>e</mi>
                    <mn>1</mn>
                  </msub>
                  <mo>|</mo>
                  <msub>
                    <mi>C</mi>
                    <mi>i</mi>
                  </msub>
                </mrow>
                <mo>)</mo>
              </mrow>
            </mrow>
            <mo>⁢</mo>
            <mrow>
              <mi>p</mi>
              <mo>⁡</mo>
              <mrow>
                <mo>(</mo>
                <mrow>
                  <msub>
                    <mi>e</mi>
                    <mn>2</mn>
                  </msub>
                  <mo>|</mo>
                  <msub>
                    <mi>C</mi>
                    <mi>i</mi>
                  </msub>
                </mrow>
                <mo>)</mo>
              </mrow>
            </mrow>
            <mo>⁢</mo>
            <mstyle>
              <mspace width="0.8em" height="0.8ex"/>
            </mstyle>
            <mo>⁢</mo>
            <mi>…</mi>
            <mo>⁢</mo>
            <mstyle>
              <mspace width="0.8em" height="0.8ex"/>
            </mstyle>
            <mo>⁢</mo>
            <mrow>
              <mi>p</mi>
              <mo>⁡</mo>
              <mrow>
                <mo>(</mo>
                <mrow>
                  <msub>
                    <mi>e</mi>
                    <mi>n</mi>
                  </msub>
                  <mo>|</mo>
                  <msub>
                    <mi>C</mi>
                    <mi>i</mi>
                  </msub>
                </mrow>
                <mo>)</mo>
              </mrow>
            </mrow>
            <mo>⁢</mo>
            <mrow>
              <mi>Pr</mi>
              <mo>⁡</mo>
              <mrow>
                <mo>(</mo>
                <msub>
                  <mi>C</mi>
                  <mi>i</mi>
                </msub>
                <mo>)</mo>
              </mrow>
            </mrow>
          </mrow>
          <mrow>
            <munderover>
              <mo>∑</mo>
              <mrow>
                <mi>j</mi>
                <mo>=</mo>
                <mn>0</mn>
              </mrow>
              <mn>1</mn>
            </munderover>
            <mo>⁢</mo>
            <mrow>
              <mrow>
                <mi>p</mi>
                <mo>⁡</mo>
                <mrow>
                  <mo>(</mo>
                  <mrow>
                    <msub>
                      <mi>e</mi>
                      <mn>1</mn>
                    </msub>
                    <mo>|</mo>
                    <msub>
                      <mi>C</mi>
                      <mi>j</mi>
                    </msub>
                  </mrow>
                  <mo>)</mo>
                </mrow>
              </mrow>
              <mo>⁢</mo>
              <mrow>
                <mi>p</mi>
                <mo>⁡</mo>
                <mrow>
                  <mo>(</mo>
                  <mrow>
                    <msub>
                      <mi>e</mi>
                      <mn>2</mn>
                    </msub>
                    <mo>|</mo>
                    <msub>
                      <mi>C</mi>
                      <mi>j</mi>
                    </msub>
                  </mrow>
                  <mo>)</mo>
                </mrow>
              </mrow>
              <mo>⁢</mo>
              <mstyle>
                <mspace width="0.8em" height="0.8ex"/>
              </mstyle>
              <mo>⁢</mo>
              <mi>…</mi>
              <mo>⁢</mo>
              <mstyle>
                <mspace width="0.8em" height="0.8ex"/>
              </mstyle>
              <mo>⁢</mo>
              <mrow>
                <mi>p</mi>
                <mo>⁡</mo>
                <mrow>
                  <mo>(</mo>
                  <mrow>
                    <msub>
                      <mi>e</mi>
                      <mi>n</mi>
                    </msub>
                    <mo>|</mo>
                    <msub>
                      <mi>C</mi>
                      <mi>j</mi>
                    </msub>
                  </mrow>
                  <mo>)</mo>
                </mrow>
              </mrow>
              <mo>⁢</mo>
              <mrow>
                <mi>Pr</mi>
                <mo>⁡</mo>
                <mrow>
                  <mo>(</mo>
                  <msub>
                    <mi>C</mi>
                    <mi>j</mi>
                  </msub>
                  <mo>)</mo>
                </mrow>
              </mrow>
            </mrow>
          </mrow>
        </mfrac>
      </mrow>
    </mtd>
    <mtd>
      <mrow>
        <mo>(</mo>
        <mn>3</mn>
        <mo>)</mo>
      </mrow>
    </mtd>
  </mtr>
</mtable>
</math>
</maths>
<br/>
where p(e<sub>k</sub>|C<sub>i</sub>) is the conditional probability value of the k<sup>th </sup>element of evidence given the i<sup>th </sup>cancer category and is obtained from conditional distribution tables. The calculations will be demonstrated for the calc Bayes network. The mass and spiculation calculations are accomplished in the same fashion.
</p>
<heading id="h-0015" level="1">Example Computations for the Calc Bayes Network</heading>
<p id="p-0066" num="0065">The Calc Bayes Network, <b>5100</b>, computes the probabilities of “not cancer” and “cancer” given Calc Evidence, <b>6100</b>. These probabilities are computed as</p>
<p id="p-0067" num="0066">
<maths id="MATH-US-00002" num="00002">
<math overflow="scroll">
<mtable>
  <mtr>
    <mtd>
      <mrow>
        <mrow>
          <mi>Pr</mi>
          <mo>⁡</mo>
          <mrow>
            <mo>(</mo>
            <mrow>
              <mrow>
                <msub>
                  <mi>C</mi>
                  <mi>i</mi>
                </msub>
                <mo>|</mo>
                <msub>
                  <mi>e</mi>
                  <mn>1</mn>
                </msub>
              </mrow>
              <mo>,</mo>
              <msub>
                <mi>e</mi>
                <mn>2</mn>
              </msub>
            </mrow>
            <mo>)</mo>
          </mrow>
        </mrow>
        <mo>=</mo>
        <mfrac>
          <mrow>
            <mrow>
              <mi>p</mi>
              <mo>⁡</mo>
              <mrow>
                <mo>(</mo>
                <mrow>
                  <msub>
                    <mi>e</mi>
                    <mn>1</mn>
                  </msub>
                  <mo>|</mo>
                  <msub>
                    <mi>C</mi>
                    <mi>i</mi>
                  </msub>
                </mrow>
                <mo>)</mo>
              </mrow>
            </mrow>
            <mo>⁢</mo>
            <mrow>
              <mi>p</mi>
              <mo>⁡</mo>
              <mrow>
                <mo>(</mo>
                <mrow>
                  <msub>
                    <mi>e</mi>
                    <mn>2</mn>
                  </msub>
                  <mo>|</mo>
                  <msub>
                    <mi>C</mi>
                    <mi>i</mi>
                  </msub>
                </mrow>
                <mo>)</mo>
              </mrow>
            </mrow>
            <mo>⁢</mo>
            <mrow>
              <mi>Pr</mi>
              <mo>⁡</mo>
              <mrow>
                <mo>(</mo>
                <msub>
                  <mi>C</mi>
                  <mi>i</mi>
                </msub>
                <mo>)</mo>
              </mrow>
            </mrow>
          </mrow>
          <mrow>
            <munderover>
              <mo>∑</mo>
              <mrow>
                <mi>j</mi>
                <mo>=</mo>
                <mn>0</mn>
              </mrow>
              <mn>1</mn>
            </munderover>
            <mo>⁢</mo>
            <mrow>
              <mrow>
                <mi>p</mi>
                <mo>⁡</mo>
                <mrow>
                  <mo>(</mo>
                  <mrow>
                    <msub>
                      <mi>e</mi>
                      <mn>1</mn>
                    </msub>
                    <mo>|</mo>
                    <msub>
                      <mi>C</mi>
                      <mi>j</mi>
                    </msub>
                  </mrow>
                  <mo>)</mo>
                </mrow>
              </mrow>
              <mo>⁢</mo>
              <mrow>
                <mi>p</mi>
                <mo>⁡</mo>
                <mrow>
                  <mo>(</mo>
                  <mrow>
                    <msub>
                      <mi>e</mi>
                      <mn>2</mn>
                    </msub>
                    <mo>|</mo>
                    <msub>
                      <mi>C</mi>
                      <mi>j</mi>
                    </msub>
                  </mrow>
                  <mo>)</mo>
                </mrow>
              </mrow>
              <mo>⁢</mo>
              <mrow>
                <mi>Pr</mi>
                <mo>⁡</mo>
                <mrow>
                  <mo>(</mo>
                  <msub>
                    <mi>C</mi>
                    <mi>j</mi>
                  </msub>
                  <mo>)</mo>
                </mrow>
              </mrow>
            </mrow>
          </mrow>
        </mfrac>
      </mrow>
    </mtd>
    <mtd>
      <mrow>
        <mo>(</mo>
        <mn>4</mn>
        <mo>)</mo>
      </mrow>
    </mtd>
  </mtr>
</mtable>
</math>
</maths>
<br/>
where Pr(C<sub>0</sub>) and Pr(C<sub>1</sub>) are probabilities of “not cancer” and “cancer”, assumed here to both equal 0.5. Assume the evidence for a calc detection are as follows: the calc detection is not collocated with a mass or spiculation detection and the calc classifier produces a discriminant value of 1.75. Thus, the first evidence value is ‘0’, and the conditional probabilities of “not cancer” and “cancer” are taken from Table 14(c) as p(e<sub>1</sub>=0|C<sub>0</sub>)=0.0948 and p(e<sub>1</sub>=0|C<sub>1</sub>)=0.1662. To obtain p(e<sub>2</sub>=1.75|C<sub>0</sub>), use the means and variances for the calc discriminant in Table 13(a). Evaluating the Gaussian probability density function with mean of 1.4661 and variance of 0.2282 at 1.75 gives
</p>
<p id="p-0068" num="0067">
<maths id="MATH-US-00003" num="00003">
<math overflow="scroll">
<mrow>
  <mrow>
    <mi>p</mi>
    <mo>⁡</mo>
    <mrow>
      <mo>(</mo>
      <mrow>
        <msub>
          <mi>e</mi>
          <mn>2</mn>
        </msub>
        <mo>=</mo>
        <mrow>
          <mn>1.75</mn>
          <mo>|</mo>
          <msub>
            <mi>C</mi>
            <mn>0</mn>
          </msub>
        </mrow>
      </mrow>
      <mo>)</mo>
    </mrow>
  </mrow>
  <mo>=</mo>
  <mrow>
    <mrow>
      <mfrac>
        <mn>1</mn>
        <msqrt>
          <mrow>
            <mn>2</mn>
            <mo>⁢</mo>
            <mstyle>
              <mspace width="0.3em" height="0.3ex"/>
            </mstyle>
            <mo>⁢</mo>
            <mrow>
              <mi>π</mi>
              <mo>⁡</mo>
              <mrow>
                <mo>(</mo>
                <mn>0.2282</mn>
                <mo>)</mo>
              </mrow>
            </mrow>
          </mrow>
        </msqrt>
      </mfrac>
      <mo>⁢</mo>
      <mrow>
        <mi>exp</mi>
        <mo>(</mo>
        <mrow>
          <mfrac>
            <mrow>
              <mo>-</mo>
              <mn>1</mn>
            </mrow>
            <mn>2</mn>
          </mfrac>
          <mo>⁢</mo>
          <mfrac>
            <msup>
              <mrow>
                <mo>(</mo>
                <mrow>
                  <mn>1.75</mn>
                  <mo>-</mo>
                  <mn>1.4661</mn>
                </mrow>
                <mo>)</mo>
              </mrow>
              <mn>2</mn>
            </msup>
            <mn>0.2282</mn>
          </mfrac>
        </mrow>
        <mo>)</mo>
      </mrow>
    </mrow>
    <mo>=</mo>
    <mn>0.6999</mn>
  </mrow>
</mrow>
</math>
</maths>
<br/>
and
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>p</i>(<i>e</i><sub>2</sub>=1.75<i>|C</i><sub>1</sub>)=0.5219.<?in-line-formulae description="In-line Formulae" end="tail"?>
</p>
<p id="p-0069" num="0068">Using the values above, the conditional probabilities of “not cancer” and “cancer” given the particular evidence are computed according to Equation 4:</p>
<p id="p-0070" num="0069">
<maths id="MATH-US-00004" num="00004">
<math overflow="scroll">
  <mrow>
    <mrow>
      <mi>Pr</mi>
      <mo>⁡</mo>
      <mrow>
        <mo>(</mo>
        <mrow>
          <mrow>
            <mrow>
              <msub>
                <mi>C</mi>
                <mn>0</mn>
              </msub>
              <mo>|</mo>
              <msub>
                <mi>e</mi>
                <mn>1</mn>
              </msub>
            </mrow>
            <mo>=</mo>
            <mn>0</mn>
          </mrow>
          <mo>,</mo>
          <mrow>
            <msub>
              <mi>e</mi>
              <mn>2</mn>
            </msub>
            <mo>=</mo>
            <mn>1.75</mn>
          </mrow>
        </mrow>
        <mo>)</mo>
      </mrow>
    </mrow>
    <mo>=</mo>
    <mrow>
      <mfrac>
        <mrow>
          <mn>0.0948</mn>
          <mo>·</mo>
          <mn>0.6999</mn>
          <mo>·</mo>
          <mn>0.5</mn>
        </mrow>
        <mrow>
          <mrow>
            <mn>0.0948</mn>
            <mo>·</mo>
            <mn>0.6999</mn>
            <mo>·</mo>
            <mn>0.5</mn>
          </mrow>
          <mo>+</mo>
          <mrow>
            <mn>0.1662</mn>
            <mo>·</mo>
            <mn>0.5219</mn>
            <mo>·</mo>
            <mn>0.5</mn>
          </mrow>
        </mrow>
      </mfrac>
      <mo>=</mo>
      <mn>0.8844</mn>
    </mrow>
  </mrow>
</math>
</maths>
<maths id="MATH-US-00004-2" num="00004.2">
<math overflow="scroll">
  <mrow>
    <mrow>
      <mi>Pr</mi>
      <mo>⁡</mo>
      <mrow>
        <mo>(</mo>
        <mrow>
          <mrow>
            <mrow>
              <msub>
                <mi>C</mi>
                <mn>1</mn>
              </msub>
              <mo>|</mo>
              <msub>
                <mi>e</mi>
                <mn>1</mn>
              </msub>
            </mrow>
            <mo>=</mo>
            <mn>0</mn>
          </mrow>
          <mo>,</mo>
          <mrow>
            <msub>
              <mi>e</mi>
              <mn>2</mn>
            </msub>
            <mo>=</mo>
            <mn>1.75</mn>
          </mrow>
        </mrow>
        <mo>)</mo>
      </mrow>
    </mrow>
    <mo>=</mo>
    <mrow>
      <mfrac>
        <mrow>
          <mn>0.1662</mn>
          <mo>·</mo>
          <mn>0.5219</mn>
          <mo>·</mo>
          <mn>0.5</mn>
        </mrow>
        <mrow>
          <mrow>
            <mn>0.0948</mn>
            <mo>·</mo>
            <mn>0.6999</mn>
            <mo>·</mo>
            <mn>0.5</mn>
          </mrow>
          <mo>+</mo>
          <mrow>
            <mn>0.1662</mn>
            <mo>·</mo>
            <mn>0.5219</mn>
            <mo>·</mo>
            <mn>0.5</mn>
          </mrow>
        </mrow>
      </mfrac>
      <mo>=</mo>
      <mn>0.1156</mn>
    </mrow>
  </mrow>
</math>
</maths>
</p>
<p id="p-0071" num="0070">The calculations of both “not cancer” and “cancer” probabilities are shown here for completeness. However, only the probability of cancer is used in subsequent processing. Mass and spiculation probabilities are computed in the same fashion. The Bayes probabilities for each lesion from each channel are provided to the Case Based Ranking step, <b>7000</b>.</p>
<heading id="h-0016" level="1">Case Based Ranking</heading>
<p id="p-0072" num="0071">This section describes the context processing flow, ROI probability thresholding and case-based rank ordering. <figref idref="DRAWINGS">FIG. 15</figref> shows an overall block diagram of the post-processing system. The processing is performed on each image of the case and surviving ROIs accumulate across the images in the case. Steps above the Image/Case line are performed on each image. These steps provide a set of case values for processing below the Image/Case line. The operation of the context and case processing section is now described.</p>
<p id="p-0073" num="0072">Once the probability of cancer is calculated using the Bayesian network, the detections within an image are analyzed separately for each detection type. First, the probability of cancer for each ROI is compared to a predetermined channel-specific threshold. All ROIs with “cancer” probabilities less than the threshold are removed. ROIs with “cancer” probabilities greater than a threshold are retained for subsequent processing. In a preferred embodiment, the thresholds are as shown in Table 1.</p>
<p id="p-0074" num="0073">
<tables id="TABLE-US-00001" num="00001">
<table frame="none" colsep="0" rowsep="0">
<tgroup align="left" colsep="0" rowsep="0" cols="1">
<colspec colname="1" colwidth="217pt" align="center"/>
<thead>
<row>
<entry namest="1" nameend="1" rowsep="1">TABLE 1</entry>
</row>
</thead>
<tbody valign="top">
<row>
<entry namest="1" nameend="1" align="center" rowsep="1"/>
</row>
<row>
<entry>Bayes Thresholds.</entry>
</row>
</tbody>
</tgroup>
<tgroup align="left" colsep="0" rowsep="0" cols="3">
<colspec colname="offset" colwidth="49pt" align="left"/>
<colspec colname="1" colwidth="28pt" align="left"/>
<colspec colname="2" colwidth="140pt" align="center"/>
<tbody valign="top">
<row>
<entry/>
<entry>Channel</entry>
<entry>Threshold</entry>
</row>
<row>
<entry/>
<entry namest="offset" nameend="2" align="center" rowsep="1"/>
</row>
</tbody>
</tgroup>
<tgroup align="left" colsep="0" rowsep="0" cols="3">
<colspec colname="offset" colwidth="49pt" align="left"/>
<colspec colname="1" colwidth="28pt" align="left"/>
<colspec colname="2" colwidth="140pt" align="char" char="."/>
<tbody valign="top">
<row>
<entry/>
<entry>Calc</entry>
<entry>0.3</entry>
</row>
<row>
<entry/>
<entry>Mass</entry>
<entry>0.29</entry>
</row>
<row>
<entry/>
<entry>Spic</entry>
<entry>0.44</entry>
</row>
<row>
<entry/>
<entry namest="offset" nameend="2" align="center" rowsep="1"/>
</row>
</tbody>
</tgroup>
</table>
</tables>
</p>
<p id="p-0075" num="0074">In the Mass and Spiculation channels, detections with probabilities greater than corresponding thresholds are retained. If the detections are collocated, the detection with the greater Bayes probability is retained. Otherwise, both are retained. Collocation is determined by computing the centroid of both ROIs. If the centroid of a first ROI is within the second ROI, collocation is declared. After this point, mass and spiculation ROIs are considered to be one type of detection, referred to as a “density” detection.</p>
<p id="p-0076" num="0075">All remaining ROIs are then jointly ranked by Bayes probabilities, largest to smallest, across the case in step <b>7100</b>. If more than a predetermined maximum number, C<sub>R</sub>, of ROIs remain, only the first C<sub>R </sub>ranked ROIs are retained, step <b>7200</b>. Additional restrictions are then applied to the number of calc and density ROIs in the case. Only the C<sub>C </sub>most probable calc detections are retained in step <b>7300</b>. Similarly, only the C<sub>D </sub>most probable density ROIs are retained in step <b>7400</b>. Recall, density ROIs are obtained from the combined list of mass and spiculation ROIs. The surviving ROIs are the final detections, and are displayed in step <b>8000</b>.</p>
<p id="p-0077" num="0076">In a preferred embodiment, the maximum rank values are as shown in Table 2. The values in Table 2 are based on four image cases. If the actual number of images in a case does not equal four, rank limits may be recomputed.</p>
<p id="p-0078" num="0077">
<tables id="TABLE-US-00002" num="00002">
<table frame="none" colsep="0" rowsep="0">
<tgroup align="left" colsep="0" rowsep="0" cols="1">
<colspec colname="1" colwidth="217pt" align="center"/>
<thead>
<row>
<entry namest="1" nameend="1" rowsep="1">TABLE 2</entry>
</row>
</thead>
<tbody valign="top">
<row>
<entry namest="1" nameend="1" align="center" rowsep="1"/>
</row>
<row>
<entry>Maximum rank values in four image cases.</entry>
</row>
</tbody>
</tgroup>
<tgroup align="left" colsep="0" rowsep="0" cols="3">
<colspec colname="offset" colwidth="49pt" align="left"/>
<colspec colname="1" colwidth="35pt" align="left"/>
<colspec colname="2" colwidth="133pt" align="center"/>
<tbody valign="top">
<row>
<entry/>
<entry>Case Rank</entry>
<entry>Case Rank</entry>
</row>
<row>
<entry/>
<entry>Parameter</entry>
<entry>Limit</entry>
</row>
<row>
<entry/>
<entry namest="offset" nameend="2" align="center" rowsep="1"/>
</row>
<row>
<entry/>
<entry>Case</entry>
<entry>10</entry>
</row>
<row>
<entry/>
<entry>Calc</entry>
<entry>10</entry>
</row>
<row>
<entry/>
<entry>Mass</entry>
<entry> 5</entry>
</row>
<row>
<entry/>
<entry namest="offset" nameend="2" align="center" rowsep="1"/>
</row>
</tbody>
</tgroup>
</table>
</tables>
</p>
<p id="p-0079" num="0078">In a preferred embodiment, when only one image is in the case, limit the total number of marks to two; the calc and mass case rank limits also equal two. When the case consists of 2, 3, or four images, the rank limits of Table 2 apply. When the case consists of more than four images, the rank limits are recomputed as:
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>C</i><sub>R</sub>=round(10<i>*Num</i>Images/4)<?in-line-formulae description="In-line Formulae" end="tail"?>
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>C</i><sub>C</sub>=round(10<i>*Num</i>Images/4)<?in-line-formulae description="In-line Formulae" end="tail"?>
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>C</i><sub>M</sub>=round(5<i>*Num</i>Images/4)<?in-line-formulae description="In-line Formulae" end="tail"?>
<br/>
where NumImages is the number of images in a case, and round( ) denotes rounding to the nearest integer.
</p>
<p id="p-0080" num="0079">The final marks that may appear on an output are those passing both the probability thresholding and the case rank ordering from the context images.</p>
<p id="p-0081" num="0080">Although the present invention has been described in terms of specific embodiments which are set forth in detail, it should be understood that this is by illustration only and that the present invention is not necessarily limited thereto, since alternative embodiments not described in detail herein will become apparent to those skilled in the art in view of the above description, the attached drawings and the appended claims. Accordingly, modifications are contemplated which can be made without departing from either the spirit or the scope of the present invention.</p>
<?DETDESC description="Detailed Description" end="tail"?>
</description>
<us-math idrefs="MATH-US-00001" nb-file="US07298877-20071120-M00001.NB">
<img id="EMI-M00001" he="10.92mm" wi="76.20mm" file="US07298877-20071120-M00001.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-math idrefs="MATH-US-00002" nb-file="US07298877-20071120-M00002.NB">
<img id="EMI-M00002" he="10.92mm" wi="76.20mm" file="US07298877-20071120-M00002.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-math idrefs="MATH-US-00003" nb-file="US07298877-20071120-M00003.NB">
<img id="EMI-M00003" he="7.79mm" wi="76.20mm" file="US07298877-20071120-M00003.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-math idrefs="MATH-US-00004 MATH-US-00004-2" nb-file="US07298877-20071120-M00004.NB">
<img id="EMI-M00004" he="20.49mm" wi="76.20mm" file="US07298877-20071120-M00004.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-claim-statement>What is claimed is:</us-claim-statement>
<claims id="claims">
<claim id="CLM-00001" num="00001">
<claim-text>1. A method of detecting malignancy of a lesion, comprising the steps of:
<claim-text>computing evidence from each lesion based on specific type of said lesion using multiple lesion-specific channels, wherein each channel comprises a lesion-type specific tuned detector, lesion-type specific feature and lesion-type specific classifier;</claim-text>
<claim-text>providing said evidence to a lesion-type specific Bayes network; and</claim-text>
<claim-text>computing a probability of malignancy for each lesion.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00002" num="00002">
<claim-text>2. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref> wherein said evidence comprises collocation to detected lesions, a ranking of each lesion relative to other lesions in an image, and a discriminant score associated with the lesion.</claim-text>
</claim>
<claim id="CLM-00003" num="00003">
<claim-text>3. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref> wherein each of said probabilities is compared to a threshold.</claim-text>
</claim>
<claim id="CLM-00004" num="00004">
<claim-text>4. The method of <claim-ref idref="CLM-00003">claim 3</claim-ref> including performing a case-based-ranking, based on the probability value for each lesion, on all lesions passing the threshold, wherein a predetermined number of lesions having the highest probability value are retained.</claim-text>
</claim>
<claim id="CLM-00005" num="00005">
<claim-text>5. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref> wherein said lesions comprise specific lesion types, said step of computing evidence comprises computing lesion-type specific evidence, and said lesion-type specific evidence is provided to lesion-type specific Bayes networks for computing lesion-type specific probabilities.</claim-text>
</claim>
<claim id="CLM-00006" num="00006">
<claim-text>6. The method of <claim-ref idref="CLM-00005">claim 5</claim-ref> wherein each of said probabilities is compared to a lesion-type specific threshold.</claim-text>
</claim>
<claim id="CLM-00007" num="00007">
<claim-text>7. The method of <claim-ref idref="CLM-00006">claim 6</claim-ref> including determining whether two lesions of different lesion-types are collocated and, if collocated, selecting the lesion of the collocated lesions having the greater probability value.</claim-text>
</claim>
<claim id="CLM-00008" num="00008">
<claim-text>8. The method of <claim-ref idref="CLM-00007">claim 7</claim-ref> wherein said two lesions of different lesion-types comprise a mass lesion and a spiculation lesion.</claim-text>
</claim>
<claim id="CLM-00009" num="00009">
<claim-text>9. The method of <claim-ref idref="CLM-00008">claim 8</claim-ref> including performing a case-based-ranking, based on the probability value for each lesion, on all of the lesions passing the lesion-type specific thresholds, wherein a predetermined number of lesions having the highest probability value are retained.</claim-text>
</claim>
<claim id="CLM-00010" num="00010">
<claim-text>10. The method of <claim-ref idref="CLM-00009">claim 9</claim-ref> including limiting the number of calcification lesions retained to a first predetermined number, limiting the combined number of mass lesions and spiculation lesions retained to a second predetermined number, and displaying the remaining retained lesions.</claim-text>
</claim>
<claim id="CLM-00011" num="00011">
<claim-text>11. In a CAD system for detecting suspicious lesions, a method of selecting lesions for final display comprising the steps of:
<claim-text>calculating a probability of malignancy for each lesion based on specific type of said lesion using multiple lesion-specific channels, wherein each channel comprises a lesion-type specific tuned detector, lesion-type specific feature, lesion-type specific classifier and lesion-type specific Bayes network; and</claim-text>
<claim-text>using the probability of malignancy as a selection criteria affecting a final display of suspicious lesions.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00012" num="00012">
<claim-text>12. The method of <claim-ref idref="CLM-00011">claim 11</claim-ref> wherein the probability of malignancy is computed based on evidence derived from each lesion and provided to a Bayes network.</claim-text>
</claim>
<claim id="CLM-00013" num="00013">
<claim-text>13. The method of <claim-ref idref="CLM-00012">claim 12</claim-ref> wherein said evidence comprises collocation of detected lesions, a ranking of each lesion relative to other lesions in an image, and a discriminant score associated with the lesion.</claim-text>
</claim>
<claim id="CLM-00014" num="00014">
<claim-text>14. The method of <claim-ref idref="CLM-00011">claim 11</claim-ref> wherein the lesions are rank-ordered by probability of malignancy, and a predetermined maximum number of lesions having the highest probability value, with reference to all lesions in a case, are retained.</claim-text>
</claim>
<claim id="CLM-00015" num="00015">
<claim-text>15. The method of <claim-ref idref="CLM-00014">claim 14</claim-ref> wherein a subset of the predetermined maximum number of lesions of a specific lesion type, is retained.</claim-text>
</claim>
<claim id="CLM-00016" num="00016">
<claim-text>16. The method of <claim-ref idref="CLM-00011">claim 11</claim-ref> wherein the lesions are rank-ordered by probability of malignancy, and a predetermined maximum number of lesions having the highest probability value, with reference to all lesions in an image, are retained.</claim-text>
</claim>
<claim id="CLM-00017" num="00017">
<claim-text>17. The method of <claim-ref idref="CLM-00016">claim 16</claim-ref> wherein a subset of the predetermined maximum number of lesions of a specific lesion type, is retained.</claim-text>
</claim>
<claim id="CLM-00018" num="00018">
<claim-text>18. A method of detecting malignancy of a lesion, comprising the steps of:
<claim-text>using multiple independent lesion-type specific detectors to detect lesions, lesion-type specific features and lesion-type specific classifiers to detect lesions;</claim-text>
<claim-text>computing lesion-type specific evidence corresponding to each detected lesion;</claim-text>
<claim-text>providing said lesion-type specific evidence to lesion-type specific Bayes networks; and</claim-text>
<claim-text>computing a lesion-type specific probability value relating to malignancy for at least some of said detected lesions.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00019" num="00019">
<claim-text>19. The method of <claim-ref idref="CLM-00018">claim 18</claim-ref> wherein the step of computing a lesion-type specific probability value for at least some of said detected lesions comprises computing said probability value for each lesion of a particular lesion-type independently of lesion-type specific evidence associated with lesions of a different lesion-type.</claim-text>
</claim>
<claim id="CLM-00020" num="00020">
<claim-text>20. The method of <claim-ref idref="CLM-00018">claim 18</claim-ref> wherein said lesion-type specific Bayes networks independently compute probability values for calcification, mass and spiculation lesions.</claim-text>
</claim>
</claims>
</us-patent-grant>
