<us-patent-grant lang="EN" dtd-version="v4.2 2006-08-23" file="US07298880-20071120.XML" status="PRODUCTION" id="us-patent-grant" country="US" date-produced="20071106" date-publ="20071120">
<us-bibliographic-data-grant>
<publication-reference>
<document-id>
<country>US</country>
<doc-number>07298880</doc-number>
<kind>B2</kind>
<date>20071120</date>
</document-id>
</publication-reference>
<application-reference appl-type="utility">
<document-id>
<country>US</country>
<doc-number>10792882</doc-number>
<date>20040305</date>
</document-id>
</application-reference>
<us-application-series-code>10</us-application-series-code>
<priority-claims>
<priority-claim sequence="01" kind="national">
<country>JP</country>
<doc-number>2003-060876</doc-number>
<date>20030307</date>
</priority-claim>
</priority-claims>
<us-term-of-grant>
<us-term-extension>753</us-term-extension>
</us-term-of-grant>
<classifications-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>G</section>
<class>06</class>
<subclass>K</subclass>
<main-group>9</main-group>
<subgroup>00</subgroup>
<symbol-position>F</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20071120</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
</classifications-ipcr>
<classification-national>
<country>US</country>
<main-classification>382128</main-classification>
<further-classification>382130</further-classification>
<further-classification>382131</further-classification>
<further-classification>128922</further-classification>
</classification-national>
<invention-title id="d0e71">Image processing apparatus and image processing method</invention-title>
<references-cited>
<citation>
<patcit num="00001">
<document-id>
<country>US</country>
<doc-number>5643179</doc-number>
<kind>A</kind>
<name>Fujimoto</name>
<date>19970700</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>601  2</main-classification></classification-national>
</citation>
<citation>
<patcit num="00002">
<document-id>
<country>US</country>
<doc-number>5669382</doc-number>
<kind>A</kind>
<name>Curwen et al.</name>
<date>19970900</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<patcit num="00003">
<document-id>
<country>US</country>
<doc-number>6086532</doc-number>
<kind>A</kind>
<name>Panescu et al.</name>
<date>20000700</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>600437</main-classification></classification-national>
</citation>
<citation>
<patcit num="00004">
<document-id>
<country>US</country>
<doc-number>6282918</doc-number>
<kind>B1</kind>
<name>Levin et al.</name>
<date>20010900</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification> 62476</main-classification></classification-national>
</citation>
<citation>
<patcit num="00005">
<document-id>
<country>US</country>
<doc-number>6443894</doc-number>
<kind>B1</kind>
<name>Sumanaweera et al.</name>
<date>20020900</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>600443</main-classification></classification-national>
</citation>
<citation>
<patcit num="00006">
<document-id>
<country>US</country>
<doc-number>6728394</doc-number>
<kind>B1</kind>
<name>Chen et al.</name>
<date>20040400</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>382107</main-classification></classification-national>
</citation>
<citation>
<patcit num="00007">
<document-id>
<country>JP</country>
<doc-number>8-106519</doc-number>
<date>19960400</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<patcit num="00008">
<document-id>
<country>JP</country>
<doc-number>10-99334</doc-number>
<date>19980400</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<patcit num="00009">
<document-id>
<country>JP</country>
<doc-number>10-165401</doc-number>
<date>19980600</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<patcit num="00010">
<document-id>
<country>JP</country>
<doc-number>11-339048</doc-number>
<date>19991200</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<patcit num="00011">
<document-id>
<country>JP</country>
<doc-number>2001-109885</doc-number>
<date>20010400</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<patcit num="00012">
<document-id>
<country>JP</country>
<doc-number>2001-331799</doc-number>
<date>20011100</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
</references-cited>
<number-of-claims>18</number-of-claims>
<us-exemplary-claim>1</us-exemplary-claim>
<us-exemplary-claim>7</us-exemplary-claim>
<us-exemplary-claim>14</us-exemplary-claim>
<us-field-of-classification-search>
<classification-national>
<country>US</country>
<main-classification>382130</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>382131</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>382209</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>382219</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>382278</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>128128</main-classification>
</classification-national>
</us-field-of-classification-search>
<figures>
<number-of-drawing-sheets>5</number-of-drawing-sheets>
<number-of-figures>9</number-of-figures>
</figures>
<us-related-documents>
<related-publication>
<document-id>
<country>US</country>
<doc-number>20040247165</doc-number>
<kind>A1</kind>
<date>20041209</date>
</document-id>
</related-publication>
</us-related-documents>
<parties>
<applicants>
<applicant sequence="001" app-type="applicant-inventor" designation="us-only">
<addressbook>
<last-name>Nishiura</last-name>
<first-name>Masahide</first-name>
<address>
<city>Kanagawa</city>
<country>JP</country>
</address>
</addressbook>
<nationality>
<country>JP</country>
</nationality>
<residence>
<country>JP</country>
</residence>
</applicant>
</applicants>
<agents>
<agent sequence="01" rep-type="attorney">
<addressbook>
<orgname>Oblon, Spivak, McClelland, Maier &amp; Neustadt, P.C.</orgname>
<address>
<country>unknown</country>
</address>
</addressbook>
</agent>
</agents>
</parties>
<assignees>
<assignee>
<addressbook>
<orgname>Kabushiki Kaisha Toshiba</orgname>
<role>03</role>
<address>
<city>Tokyo</city>
<country>JP</country>
</address>
</addressbook>
</assignee>
</assignees>
<examiners>
<primary-examiner>
<last-name>Kassa</last-name>
<first-name>Yosef</first-name>
<department>2624</department>
</primary-examiner>
</examiners>
</us-bibliographic-data-grant>
<abstract id="abstract">
<p id="p-0001" num="0000">A boundary <b>1</b> of a wall in an initial image of time-series images is decided automatically or manually. A boundary <b>2</b> is generated automatically or manually in a neighbor of the other boundary of the wall. A normalized image is generated on the basis of an image pattern of a region surrounded by the boundaries <b>1</b> and <b>2</b> and registered as a template image. In each of the time-series images, boundaries <b>1</b> and <b>2</b> that generate a normalized image most similar to the template image are calculated. In this manner, wall thickness change can be calculated automatically stably and accurately even in the case where luminance change in the boundaries of a wall is obscure. This method is useful for diagnosis of heart disease etc. in medical image diagnosis.</p>
</abstract>
<drawings id="DRAWINGS">
<figure id="Fig-EMI-D00000" num="00000">
<img id="EMI-D00000" he="110.41mm" wi="173.06mm" file="US07298880-20071120-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00001" num="00001">
<img id="EMI-D00001" he="239.44mm" wi="179.83mm" file="US07298880-20071120-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00002" num="00002">
<img id="EMI-D00002" he="257.05mm" wi="175.09mm" file="US07298880-20071120-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00003" num="00003">
<img id="EMI-D00003" he="269.16mm" wi="191.94mm" file="US07298880-20071120-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00004" num="00004">
<img id="EMI-D00004" he="252.65mm" wi="178.48mm" file="US07298880-20071120-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00005" num="00005">
<img id="EMI-D00005" he="216.83mm" wi="155.70mm" file="US07298880-20071120-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
</drawings>
<description id="description">
<?BRFSUM description="Brief Summary" end="lead"?>
<p id="p-0002" num="0001">This application is based upon and claims the benefit of priority from the prior Japanese Patent Application No. 2003-60876 filed on Mar. 7, 2003; the entire contents of which are incorporated herein by reference.</p>
<heading id="h-0001" level="1">BACKGROUND OF THE INVENTION</heading>
<p id="p-0003" num="0002">1. Field of the Invention</p>
<p id="p-0004" num="0003">The present invention relates to an image processing apparatus and a method for aiding diagnosis using medical images.</p>
<p id="p-0005" num="0004">2. Description of the Related Art</p>
<p id="p-0006" num="0005">Change in thickness of cardiac muscle of a heart is a significant indicator in diagnosis of heart disease such as myocardial infarction.</p>
<p id="p-0007" num="0006">As a method for measuring change in thickness of cardiac muscle, there is a method including acquiring time-series cross-sectional images of a heart and its vicinity; detecting an inner boundary and an outer boundary of the cardiac muscle by image processing; and the calculating thickness of the cardiac muscle in accordance with each image.</p>
<p id="p-0008" num="0007">Artifacts, noise, etc. superposed on the images, however, make it difficult to extract the boundaries of the cardiac muscle automatically. Therefore, various methods for extracting the boundaries have been proposed.</p>
<p id="p-0009" num="0008">U.S. Pat. No. 5,669,382 has disclosed a boundary extraction method using snake. Snake is a technique for obtaining a boundary by minimizing the value of an energy function defined on the basis of conditions such as change in luminance of an image and smoothness of a boundary surface.</p>
<p id="p-0010" num="0009">When change in luminance is used, it is however difficult to detect a boundary low in luminance change.</p>
<p id="p-0011" num="0010">Taking the case of a heart as an example, an endocardium, which is an inner boundary of cardiac muscle, is a boundary between cardiac muscle and blood. Accordingly, in the endocardium, luminance change is clear even on images obtained by various image diagnostic systems.</p>
<p id="p-0012" num="0011">On the other hand, an epicardium, which is an outer boundary of cardiac muscle, is a boundary between cardiac muscle and a tissue surrounding the outside of cardiac muscle. Accordingly, in the epicardium, luminance change is often obscure on images. For this reason, it is difficult to extract both the endocardium and the epicardium accurately, stably and automatically.</p>
<p id="p-0013" num="0012">JP-A-10-165401 has disclosed a technique in which a luminance distribution in a direction of wall thickness is calculated and a position exhibiting n % (e.g., 70 to 80%) of the maximum luminance is regarded as a boundary position.</p>
<p id="p-0014" num="0013">Even in this technique, sufficient accuracy cannot be obtained because there is the possibility that the foot of the luminance distribution may be extended while it exceeds n % of the maximum luminance when luminance change in a boundary and its vicinity is obscure.</p>
<heading id="h-0002" level="1">BRIEF SUMMARY OF THE INVENTION</heading>
<p id="p-0015" num="0014">Therefore, an object of the invention is to provide an apparatus and a method in which boundaries of a subject, for example, boundaries of cardiac muscle in time-series images of a heart can be obtained stably and accurately even in a case where luminance change in the boundaries of the subject is obscure.</p>
<p id="p-0016" num="0015">According to a first aspect of the invention, an image processing apparatus includes an image input section, a boundary setting section, a normalized-image generating section, a template storage section, and a boundary retrieval section. The image input section is configured to input time-series images obtained by acquiring an image including a first boundary of a wall and a second boundary of the wall. The boundary setting section is configured to set the first boundary and the second boundary in an initial image of the time-series images. The normalized-image generating section is configured to generate a normalized image on the basis of an image pattern between the set first boundary and the set second boundary. The template storage section is configured to store the normalized image generated from the initial image as a template image. The boundary retrieval section is configured to search each of time-series images for the second boundary which generates the most similar normalized image to the template image.</p>
<p id="p-0017" num="0016">According to a second aspect of the invention, an image processing apparatus includes an image input section, a boundary setting section, a sixth section, a normalized-image generating section, and a boundary retrieval section. The image input section is configured to input time-series images obtained by acquiring an image including a first boundary of a wall and a second boundary of the wall. The boundary setting section is configured to set the first boundary and the second boundary in an initial image of the time-series images, and set boundaries of wall parts into which the wall is divided in the initial image of time-series images. The sixth section is configured to obtain local wall thickness information of each wall part. The normalized-image generating section is configured to generate a normalized image on the basis of image patterns of the wall parts. The template storage section is configured to store the normalized image generated from the initial image as a template image. The boundary retrieval section is configured to search each of time-series images for the first boundary and the second boundary, which generate the most similar normalized image to the template image.</p>
<p id="p-0018" num="0017">According to a third aspect of the invention, an image processing method detects a wall of an internal organ from time-series images. The method includes inputting time-series images obtained by acquiring an image including a first boundary of a wall and a second boundary of the wall, setting the first boundary in an initial image of the time-series images, setting the second boundary in the initial image, generating a normalized image on the basis of an image pattern between the set first boundary and the set second boundary from the initial image to store the generated normalized image as a template image, and searching each of time-series images for the first boundary and the second boundary, which generates the most similar normalized image to the template image.</p>
<?BRFSUM description="Brief Summary" end="tail"?>
<?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?>
<description-of-drawings>
<heading id="h-0003" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading>
<p id="p-0019" num="0018"><figref idref="DRAWINGS">FIG. 1</figref> is a diagram showing the configuration of an embodiment of the invention.</p>
<p id="p-0020" num="0019"><figref idref="DRAWINGS">FIG. 2</figref> is a flow chart showing a flow of processing in an embodiment of the invention.</p>
<p id="p-0021" num="0020"><figref idref="DRAWINGS">FIG. 3</figref> shows an example of expression of contours.</p>
<p id="p-0022" num="0021"><figref idref="DRAWINGS">FIG. 4</figref> is an explanatory view concerning generation of a normalized image and comparison of the normalized image with a template image.</p>
<p id="p-0023" num="0022"><figref idref="DRAWINGS">FIG. 5</figref> shows an example of expression of boundaries by small number of parameters.</p>
<p id="p-0024" num="0023"><figref idref="DRAWINGS">FIGS. 6A and 6B</figref> are graphs for explaining a normalizing process.</p>
<p id="p-0025" num="0024"><figref idref="DRAWINGS">FIG. 7</figref> is a view for explaining the outline of a active contour model capable of extracting boundaries while tracking local portions of a wall.</p>
<p id="p-0026" num="0025"><figref idref="DRAWINGS">FIG. 8</figref> shows an example of display of a heart image in a gray scale manner according to the thickness of the heart wall.</p>
</description-of-drawings>
<?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?>
<?DETDESC description="Detailed Description" end="lead"?>
<heading id="h-0004" level="1">DETAILED DESCRIPTION OF THE INVENTION</heading>
<p id="p-0027" num="0026">An image processing apparatus according to an embodiment of the invention will be described below with reference to the drawings.</p>
<p id="p-0028" num="0027">This embodiment is an image processing apparatus for receiving time-series images output from an image acquiring device and performing a process of measuring wall thickness change of a subject contained in the image data. Particularly this embodiment is suitable for measurement of wall thickness change of a heart wall based on ultrasound images of the heart, the heart wall being constituted by cardiac muscle that separates the inside and outside of a heart from each other.</p>
<p id="p-0029" num="0028">(Configuration) <figref idref="DRAWINGS">FIG. 1</figref> is a diagram showing the configuration of this embodiment. The image processing apparatus according to this embodiment includes an image input section <b>101</b> for inputting time-series heart images obtained from an external device such as an ultrasound imaging device, and a memory <b>102</b> for storing image data and intermediate data in the middle of processing.</p>
<p id="p-0030" num="0029">The image processing apparatus according to this embodiment further includes a boundary setting section <b>103</b> for detecting boundaries of the heart wall semiautomatically on the basis of luminance information of the image data, and a normalized-image generating section <b>104</b> for generating a normalized image used for retrieving the boundaries.</p>
<p id="p-0031" num="0030">The image processing apparatus according to this embodiment further includes a boundary retrieval section <b>105</b> for retrieving the boundaries of the heart wall by comparing the normalized image obtained from a processed image with a template image obtained from an initial image, and a parameter calculation section <b>106</b> for calculating values useful for diagnosis, such as thickness change of the heart wall, on the basis of the decided boundaries.</p>
<p id="p-0032" num="0031">The image processing apparatus according to this embodiment further includes a pointing device <b>107</b> used for various instructing operations (e.g., for setting the boundaries manually), and an output section <b>108</b> for outputting information such as obtained boundaries, thickness change of the heart wall, images in the middle of measurement of heart wall thickness, GUI, etc. to a screen.</p>
<p id="p-0033" num="0032">The image input section <b>101</b> receives images acquired by the external image acquiring device (such as an ultrasound image acquiring device, an MRI image acquiring device or an X-ray image acquiring device). The images received by the image input sections <b>101</b> are frame by frame stored successively as images <b>102</b>-<b>1</b> in the memory <b>102</b>.</p>
<p id="p-0034" num="0033">The boundary setting section <b>103</b> sets the boundaries of the heart wall on the basis of the luminance information of the initial image. In this embodiment, a user uses the pointing device <b>107</b> for designating the boundaries of the heart wall.</p>
<p id="p-0035" num="0034">Each of the detected boundaries is expressed as a set of representative points. The coordinates of each point detected by the boundary setting section <b>103</b> are stored as boundary coordinates <b>102</b>-<b>2</b> in the memory <b>102</b>.</p>
<p id="p-0036" num="0035">The normalized-image generating section <b>104</b> generates a normalized image by normalizing an image of a region of the heart wall surrounded by the boundaries constituted by points expressed by the boundary coordinates <b>102</b>-<b>2</b>.</p>
<p id="p-0037" num="0036">The normalization is performed by deformation into a rectangular region according to affine transformation. Respective normalized images generated by the normalized-image generating section <b>104</b> are stored as normalized images <b>102</b>-<b>3</b> in the memory <b>102</b>. Incidentally, a normalized image of a region of the heart wall on the basis of the initial image is stored as template image <b>102</b>-<b>4</b> in the memory <b>102</b>.</p>
<p id="p-0038" num="0037">The boundary retrieval section <b>105</b> retrieves the boundaries in each frame image by using the boundaries obtained in an immediately preceding image and the template image <b>102</b>-<b>4</b>. The details of the retrieval method will be described later (see S<b>207</b> and steps following S<b>207</b> which will be described later). The coordinates of points constituting the boundaries obtained by the boundary retrieval section <b>105</b> are stored as boundary coordinates <b>102</b>-<b>2</b> in the memory <b>102</b>.</p>
<p id="p-0039" num="0038">The parameter calculation section <b>106</b> calculates the wall thickness in each image by using the boundary coordinates <b>102</b>-<b>2</b>. The parameter calculation section <b>106</b> further calculates parameters for expressing the rate of wall thickness change, the velocity of wall thickness change and deformation of the wall.</p>
<p id="p-0040" num="0039">The output section <b>108</b> includes a display device such as an LCD, a CRT or a PDP. The output section <b>108</b> outputs information such as obtained boundaries, thickness change of the heart wall, images in the middle of measurement of heart wall thickness, GUI, etc. to a screen of the display device. The user uses this apparatus while watching the screen.</p>
<p id="p-0041" num="0040">(Operation) <figref idref="DRAWINGS">FIG. 2</figref> is a flow chart showing a flow of processing according to the embodiment of the invention. The operation will be described below according to the flow of processing.</p>
<p id="p-0042" num="0041">A total flow of a process of measuring wall thickness change of the heart wall will be described first. An image pattern of a heart wall portion is normalized on the basis of the boundaries of the heart wall set manually in the initial image. This normalized image is registered as a template image. After that, boundaries are retrieved from each of the time-series images so that a normalized image resembling the template image most closely can be obtained. In this manner, in each image, the boundaries of the heart wall are decided and parameters such as wall thickness change of the heart wall are calculated.</p>
<p id="p-0043" num="0042">The processing will be described below in detail.</p>
<p id="p-0044" num="0043">(S<b>201</b>) An initial image of time-series images is input in order to start measurement of wall thickness change of the heart wall.</p>
<p id="p-0045" num="0044">Time-series image data from the external image acquiring device are input into this apparatus. The input image data are successively stored in the memory <b>102</b>.</p>
<p id="p-0046" num="0045">In this embodiment, a top frame of the input time-series image data is used as the initial image.</p>
<p id="p-0047" num="0046">(S<b>202</b>) The boundary setting section <b>103</b> sets boundaries on both sides of the heart wall in the initial image.</p>
<p id="p-0048" num="0047">In this embodiment, the inner boundary (endocardium) of the heart wall is referred to as boundary <b>1</b> and the outer boundary (epicardium) of the heart wall is referred to as boundary <b>2</b>.</p>
<p id="p-0049" num="0048">In this embodiment, the initial image is output to a screen, and a user designates representative points constituting each of the boundaries <b>1</b> and <b>2</b> by using the pointing device <b>107</b>. Each of the boundaries <b>1</b> and <b>2</b> is set as line segments connecting these representative points.</p>
<p id="p-0050" num="0049"><figref idref="DRAWINGS">FIG. 3</figref> shows an example of the boundaries <b>1</b> and <b>2</b>. In <figref idref="DRAWINGS">FIG. 3</figref>, characters with subscript a such as x<sub>a1 </sub>and y<sub>ak </sub>show points constituting the boundary <b>1</b>, and characters with subscript b such as x<sub>b1 </sub>and y<sub>bk </sub>show points constituting the boundary <b>2</b>.</p>
<p id="p-0051" num="0050">The coordinates of points constituting each of the set boundaries <b>1</b> and <b>2</b> are stored in the memory <b>102</b>.</p>
<p id="p-0052" num="0051">(S<b>203</b>) The normalized-image generating section <b>104</b> generates a normalized image from image data located between the boundaries <b>1</b> and <b>2</b>.</p>
<p id="p-0053" num="0052">The normalized image is generated as shown in <figref idref="DRAWINGS">FIG. 4</figref>. That is, even in the case where the heart wall is curved, the shape of a region is normalized to a rectangular shape having a predetermined size.</p>
<p id="p-0054" num="0053">In this embodiment, normalization is performed in accordance with tetragonal regions (unit regions) into which the heart wall is partitioned by points constituting the boundaries <b>1</b> and <b>2</b>. The normalized unit regions are connected to obtain a normalized image expressing the whole heart wall.</p>
<p id="p-0055" num="0054">The normalizing process in this embodiment will be described with reference to <figref idref="DRAWINGS">FIGS. 6A and 6B</figref>. <figref idref="DRAWINGS">FIG. 6A</figref> is a view showing a unit region. <figref idref="DRAWINGS">FIG. 6B</figref> is a view showing a normalized unit region obtained by normalizing the unit region depicted in <figref idref="DRAWINGS">FIG. 6A</figref>.</p>
<p id="p-0056" num="0055">In the following description, the case where the pixels in the normalized unit region are 7 pixels (vertically) by 6 pixels (horizontally) is taken as an example. Among sides of the unit region, sides corresponding to the boundaries <b>1</b> and <b>2</b> are taken as horizontal sides while sides corresponding to neither the boundary <b>1</b> nor the boundary <b>2</b> are taken as vertical sides.</p>
<p id="p-0057" num="0056">First, equal division points for dividing each of the vertical and horizontal sides of the unit region into N equal parts are obtained. In this case, since the pixels in the normalized unit region are 7 pixels (vertically) by 6 pixels (horizontally), points for dividing the vertical sides into 8 (=7+1) equal parts and points for dividing the horizontal sides into 7 (=6+1) equal parts are obtained. As shown in <figref idref="DRAWINGS">FIG. 6A</figref>, attention is paid to a grid constituted by equal division lines (V<b>601</b> to V<b>607</b> and H<b>611</b> to H<b>616</b>) connecting the equal division points on both sides of the vertical and horizontal sides.</p>
<p id="p-0058" num="0057">Then, a pixel value of each grid point in the unit region is obtained. In this case, an average of pixel values of surrounding pixels is used as the pixel value of the grid point. Alternatively, the pixel value of a pixel containing each grid point may be used as the pixel value of the grid point or the pixel value of each grid point may be calculated on the basis of the pixel values of surrounding pixels by means of Gaussian distribution or the like.</p>
<p id="p-0059" num="0058">Then, the coordinates of each grid point are expressed on the basis of the equal division lines. For example, the coordinate values of a point <b>641</b> in <figref idref="DRAWINGS">FIG. 6A</figref> are expressed as (2, 4) because the point <b>641</b> is a point of intersection between the fourth equal division line V<b>604</b> from the bottom and the second equal division line H<b>612</b> from the left.</p>
<p id="p-0060" num="0059">Pixels having the coordinate values and pixel values obtained in this manner are collected to generate a normalized unit region. For example, the point <b>641</b> is transformed into a point <b>642</b> located in (2, 4) in the normalized image (see <figref idref="DRAWINGS">FIG. 6B</figref>).</p>
<p id="p-0061" num="0060">Such processing is performed on all unit regions constituting the heart wall. The normalized unit regions obtained thus are connected to generate a normalized image. Incidentally, the connection of the normalized unit regions is performed in such a manner that vertical sides of the normalized unit regions are connected to one another while the boundary <b>1</b> of each normalized unit region is arranged so as to face upward.</p>
<p id="p-0062" num="0061">The normalized image generated from the initial image is stored as a template image in the memory <b>102</b>.</p>
<p id="p-0063" num="0062">(S<b>204</b>) A next image (hereinafter referred to as image N) of the time-series images is input.</p>
<p id="p-0064" num="0063">(S<b>205</b>) The boundary retrieval section <b>105</b> retrieves the boundaries <b>1</b> and <b>2</b> in the image N.</p>
<p id="p-0065" num="0064">The retrieval of the boundaries <b>1</b> and <b>2</b> in the image N is carried out as follows. The outline of the boundary retrieval process will be described first.</p>
<p id="p-0066" num="0065">Temporary boundaries <b>1</b> and <b>2</b> are set in the image N. The temporary boundaries <b>1</b> and <b>2</b> are set in the same places as the boundaries obtained in an immediately preceding image.</p>
<p id="p-0067" num="0066">Then, a normalized image is generated from the temporary boundaries <b>1</b> and <b>2</b>. Similarity S of the normalized image to the template image is calculated. SSD (Sum of Square Difference), which will be described later, is used as the similarity S.</p>
<p id="p-0068" num="0067">The temporary boundaries <b>1</b> and <b>2</b> that give the highest one of similarities obtained by changing the temporary boundaries <b>1</b> and <b>2</b> variously are regarded as the boundaries <b>1</b> and <b>2</b> in one processed image of the time-series images.</p>
<p id="p-0069" num="0068">The boundary retrieval process will be described below in detail.</p>
<p id="p-0070" num="0069">Because each of the boundaries <b>1</b> and <b>2</b> is expressed as a set of points, each of the temporary boundaries <b>1</b> and <b>2</b> is expressed as a set of points in the same manner. A set X of points constituting the temporary boundaries <b>1</b> and <b>2</b> can be given by the following expression.
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?>X={x<sub>a1</sub>,y<sub>a1</sub>, . . . ,x<sub>ak</sub>,y<sub>ak</sub>,x<sub>b1</sub>,y<sub>b1</sub>, . . . ,x<sub>bk</sub>,y<sub>bk</sub>}  (1)<?in-line-formulae description="In-line Formulae" end="tail"?>
</p>
<p id="p-0071" num="0070">Incidentally, characters with subscript a such as x<sub>a1</sub>, y<sub>a1 </sub>show points constituting the temporary boundary <b>1</b>, and characters with subscript b such as x<sub>bk</sub>, y<sub>bk </sub>show points constituting the temporary boundary <b>2</b>.</p>
<p id="p-0072" num="0071">When SSD (Sum of Square Difference) of pixel values is used as a distance scale D for measuring similarity S on the assumption that f(i,j) are pixel values of the normalized image generated on the basis of the temporary boundaries <b>1</b> and <b>2</b> and f<sub>t</sub>(i,j) are pixel values of the template image, the distance D between the normalized image and the template image can be given by the following expression.</p>
<p id="p-0073" num="0072">
<maths id="MATH-US-00001" num="00001">
<math overflow="scroll">
<mtable>
  <mtr>
    <mtd>
      <mrow>
        <mi>D</mi>
        <mo>=</mo>
        <mrow>
          <munder>
            <mo>∑</mo>
            <mrow>
              <mi>i</mi>
              <mo>,</mo>
              <mi>j</mi>
            </mrow>
          </munder>
          <mo>⁢</mo>
          <msup>
            <mrow>
              <mo>{</mo>
              <mrow>
                <mrow>
                  <mi>f</mi>
                  <mo>⁡</mo>
                  <mrow>
                    <mo>(</mo>
                    <mrow>
                      <mi>i</mi>
                      <mo>,</mo>
                      <mi>j</mi>
                    </mrow>
                    <mo>)</mo>
                  </mrow>
                </mrow>
                <mo>-</mo>
                <mrow>
                  <msub>
                    <mi>f</mi>
                    <mi>t</mi>
                  </msub>
                  <mo>⁡</mo>
                  <mrow>
                    <mo>(</mo>
                    <mrow>
                      <mi>i</mi>
                      <mo>,</mo>
                      <mi>j</mi>
                    </mrow>
                    <mo>)</mo>
                  </mrow>
                </mrow>
              </mrow>
              <mo>}</mo>
            </mrow>
            <mn>2</mn>
          </msup>
        </mrow>
      </mrow>
    </mtd>
    <mtd>
      <mrow>
        <mo>(</mo>
        <mn>2</mn>
        <mo>)</mo>
      </mrow>
    </mtd>
  </mtr>
</mtable>
</math>
</maths>
</p>
<p id="p-0074" num="0073">Because the distance D between the template image and the normalized image depends on the set X of points constituting the temporary boundaries, the distance D can be given by the following expression.</p>
<p id="p-0075" num="0074">
<maths id="MATH-US-00002" num="00002">
<math overflow="scroll">
<mtable>
  <mtr>
    <mtd>
      <mrow>
        <mrow>
          <mi>D</mi>
          <mo>⁡</mo>
          <mrow>
            <mo>(</mo>
            <mi>X</mi>
            <mo>)</mo>
          </mrow>
        </mrow>
        <mo>=</mo>
        <mrow>
          <munder>
            <mo>∑</mo>
            <mrow>
              <mi>i</mi>
              <mo>,</mo>
              <mi>j</mi>
            </mrow>
          </munder>
          <mo>⁢</mo>
          <msup>
            <mrow>
              <mo>{</mo>
              <mrow>
                <mrow>
                  <mi>f</mi>
                  <mo>⁡</mo>
                  <mrow>
                    <mo>(</mo>
                    <mrow>
                      <mi>i</mi>
                      <mo>,</mo>
                      <mrow>
                        <mi>j</mi>
                        <mo>|</mo>
                        <mi>X</mi>
                      </mrow>
                    </mrow>
                    <mo>)</mo>
                  </mrow>
                </mrow>
                <mo>-</mo>
                <mrow>
                  <msub>
                    <mi>f</mi>
                    <mi>t</mi>
                  </msub>
                  <mo>⁡</mo>
                  <mrow>
                    <mo>(</mo>
                    <mrow>
                      <mi>i</mi>
                      <mo>,</mo>
                      <mi>j</mi>
                    </mrow>
                    <mo>)</mo>
                  </mrow>
                </mrow>
              </mrow>
              <mo>}</mo>
            </mrow>
            <mn>2</mn>
          </msup>
        </mrow>
      </mrow>
    </mtd>
    <mtd>
      <mrow>
        <mo>(</mo>
        <mn>3</mn>
        <mo>)</mo>
      </mrow>
    </mtd>
  </mtr>
</mtable>
</math>
</maths>
</p>
<p id="p-0076" num="0075">The similarity S of the normalized image to the template image is maximized when the distance D is minimized. The set X of points that gives the minimum distance D is a set of points constituting the boundaries <b>1</b> and <b>2</b> in the image N.
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>X</i><sub>n+1</sub><i>=X</i><sub>n</sub><i>−α∇D</i>(<i>X</i><sub>n</sub>)  (4)<?in-line-formulae description="In-line Formulae" end="tail"?>
</p>
<p id="p-0077" num="0076">Because this expression cannot be solved analytically, there is used a method of obtaining a local minimum solution by numerical repetitive calculation according to a steepest descent method or the like. That is, a combination of points that minimizes the distance D is searched for while each of points constituting the boundaries is moved little by little.</p>
<p id="p-0078" num="0077">If a predetermined range is entirely used for calculation based on the temporary boundaries in order to search for the combination, the volume of calculation becomes huge. Therefore, calculation is performed in such a manner that each point is moved in a direction of decreasing the distance D. When the distance D is not decreased any more though each point is moved in any direction, a decision is made that the distance D is set at a local minimum value.</p>
<p id="p-0079" num="0078">Incidentally, another similarity index such as a cross correlation value or a sum of absolute difference (SAD) may be used as the similarity S.</p>
<p id="p-0080" num="0079">Incidentally, the temporary boundaries need not be the same as the boundaries in an immediately preceding image. For example, the boundaries in the initial image may be directly used as the temporary boundaries or the boundaries in an image, which is near the image N in terms of time (but is not necessarily immediately precede the image N), may be used as the temporary boundaries. Alternatively, the temporary boundaries may be decided while the motions of the boundaries are estimated by some means, that is, for example, the temporary boundaries may be set at places estimated on the basis of the motions of the boundaries in the past.</p>
<p id="p-0081" num="0080">(S<b>206</b>) The parameter calculation section <b>106</b> calculates parameters such as thickness of the heart wall, thickness change, etc. on the basis of the retrieved boundaries <b>1</b> and <b>2</b>. Then, the output section <b>108</b> outputs a superposed image in forms of graph display and color coding to the display.</p>
<p id="p-0082" num="0081">The thickness of the heart wall is calculated in accordance with each of line segments connecting points constituting the boundary <b>1</b>. For example, with respect to a line segment A<b>1</b>-A<b>2</b> connecting points A<b>1</b> and A<b>2</b>, a point Bx that minimizes the distance from the points A<b>1</b> and A<b>2</b> is extracted from the points constituting the boundary <b>2</b> and the distance between the point Bx and a line passing through the line segment A<b>1</b>-A<b>2</b> is calculated. The calculated distance is regarded as the thickness of the heart wall.</p>
<p id="p-0083" num="0082">The parameter calculation section <b>106</b> not only calculates parameters but also generates graphs or the like by processing the parameters.</p>
<p id="p-0084" num="0083">(S<b>207</b>) The boundary retrieval process of S<b>205</b> is performed on each of the time-series images. While there is any image that has not been processed yet, the process of S<b>204</b> and steps after S<b>204</b> is repeated.</p>
<p id="p-0085" num="0084">By the aforementioned processing, the boundaries of the heart wall in each of the time-series images are decided.</p>
<p id="p-0086" num="0085">(Effect of the Embodiment) In the related art, the boundaries of the heart wall were obtained by edge detection using luminance value change in each of time-series images. The obtained boundaries were used for examining time-series change in the thickness of the heart wall.</p>
<p id="p-0087" num="0086">It was however difficult to detect the outer boundary of the heart wall stably because luminance change in the outer boundary of the heart wall was obscure. That is, the positions of detected edges fluctuated because of another factor (obscurity) than the motion of the boundaries. As a result, detection of the thickness of the heart wall was apt to be unstable because of the influence of fluctuation caused by obscurity.</p>
<p id="p-0088" num="0087">In this respect, according to this embodiment, attention is paid to the fact that the pattern of the heart wall little changes (in spite of variation in size) even in the case where the thickness of the heart wall changes according to the motion of the heart. That is, attention is paid to the fact that the pattern of the heart wall little fluctuates when comparison is performed after the heart wall is scaled up/down by image processing to keep the size of the heart wall constant.</p>
<p id="p-0089" num="0088">Further, a normalized image generated from the initial image is stored as a template image. A normalized image generated from each image is compared with the template image (template matching) to thereby examine the heart wall.</p>
<p id="p-0090" num="0089">As a result, the boundaries of the heart wall can be detected while the influence of obscurity of luminance change is minimized. That is, the boundaries of the heart wall can be detected stably. Accordingly, the thickness of the heart wall can be also calculated stably while the influence of fluctuation caused by obscurity of luminance change is minimized.</p>
<p id="p-0091" num="0090">(Other Examples of the Retrieval Method Used in S<b>205</b>) Various methods for performing the boundary retrieval process in S<b>205</b> are conceived according to the form of presentation of boundaries and the means of optimum solution. The methods will be described below by way of example.</p>
<p id="p-0092" num="0091">(Example 1 of Retrieval Method) This method is particularly preferably applied to a heart image.</p>
<p id="p-0093" num="0092">In the heart image, the endocardium (boundary <b>1</b>) can be identified easily by means of edge detection or the like because luminance change in the image is clear in most cases. On the contrary, the epicardium (boundary <b>2</b>) can hardly be identified stably by means of edge detection or the like because luminance change in the image is obscure in most cases.</p>
<p id="p-0094" num="0093">Therefore, in this method, the endocardium is decided by ordinary means of edge detection using luminance change or by other means such as snake, etc.</p>
<p id="p-0095" num="0094">On the other hand, the epicardium is decided by the method that has been described in the explanation of S<b>204</b>. In this case, only the epicardium exerts influence on the similarity of the normalized image to the template image. Accordingly, the set X of points can be given by the following expression.
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?>X={x<sub>b1</sub>,y<sub>b1</sub>, . . . ,x<sub>bk</sub>,y<sub>bk</sub>}  (5)<?in-line-formulae description="In-line Formulae" end="tail"?>
</p>
<p id="p-0096" num="0095">The set X of points that minimizes the distance D can be calculated in the same manner as described above in the explanation of S<b>207</b>. A solution can be obtained more speedily because the number of variables used is reduced by half compared with the method using both the endocardium (boundary <b>1</b>) and the epicardium (boundary <b>2</b>) as described above in the explanation of S<b>204</b>.</p>
<p id="p-0097" num="0096">Incidentally, when the endocardium is decided by a method of extracting a contour on the basis of luminance change (edge) in each image, correspondence between points on the endocardium at a certain point of time t<b>1</b> and points on the endocardium at an another point of time t<b>2</b> is not clear. When wall thickness change is to be measured, it is however preferable to measure wall thickness change in identical wall portions.</p>
<p id="p-0098" num="0097">Therefore, it is preferable to use a method of corresponding points on contour, which is divided at a position of a feature point, with each other, as described in JP-A-10-99334, which is incorporated herein by reference in its entirety.</p>
<p id="p-0099" num="0098">In the case of the heart, the apex of the heart, heart valves and their vicinity can be detected as feature points easily because they are large in curvature or unique in shape.</p>
<p id="p-0100" num="0099">Accordingly, points I<sub>C </sub>and O<sub>C </sub>corresponding the apex of the heart are obtained in the endocardium and epicardium obtained by the retrieval. Further, division points for equally dividing lines between ends I<sub>A0 </sub>and O<sub>A0 </sub>of the endocardium and epicardium and the points I<sub>C </sub>and O<sub>C </sub>corresponding the apex of the heart are obtained. Similarly, division points for equally dividing lines between the other ends I<sub>B0 </sub>and O<sub>B0 </sub>of the endocardium and epicardium and the points I<sub>C </sub>and O<sub>C </sub>corresponding the apex of the heart are obtained.</p>
<p id="p-0101" num="0100">When, for example, each line is divided into three, points I<sub>A0</sub>, I<sub>A1</sub>, I<sub>A2</sub>, I<sub>C</sub>, I<sub>B2</sub>, I<sub>B1 </sub>and I<sub>B0 </sub>concerning the endocardium are obtained successively viewed from one end I<sub>A0 </sub>of the endocardium whereas points O<sub>A0</sub>, O<sub>A1</sub>, O<sub>A2</sub>, O<sub>C</sub>, O<sub>B2</sub>, O<sub>B1</sub>, and O<sub>B0 </sub>concerning the epicardium are obtained successively viewed from one end O<sub>A0 </sub>of the epicardium.</p>
<p id="p-0102" num="0101">Among these points, points with the same subscript are regarded as points corresponding to each other. Incidentally, number of the division points may be changed suitably according to the place of measurement of wall thickness, etc.</p>
<p id="p-0103" num="0102">More preferably, the endocardium may be extracted by use of a active contour model that will be described below.</p>
<p id="p-0104" num="0103">The active contour model capable of extracting a boundary while tracking a local portion of a wall will be described below. For example, similarity of image patterns <b>702</b> in a region near a contour <b>701</b> as shown in <figref idref="DRAWINGS">FIG. 7</figref> is used in the active contour model. Accordingly, image energy in the active contour model is defined as follows:
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>E</i><sub>image</sub>=−∫Edge(<i>V</i>(<i>s</i>))+<i>P</i><sub>t</sub>(<i>V</i>(<i>s</i>))<i>ds</i>  (6)<?in-line-formulae description="In-line Formulae" end="tail"?>
<br/>
where V(s) means the contour <b>701</b> expressed by parameter s, Edge(V(s)) means the magnitude of luminance gradient in an image on the contour, and Pt(V(s)) means the similarity of each image pattern <b>702</b> in the region near the contour to a template pattern set in advance. Cross correlation values or the like may be used as the similarity.
</p>
<p id="p-0105" num="0104">When image energy is defined in this manner, the boundary contour can be extracted while the image patterns in the region near the contour are being tracked. That is, the boundary of the wall can be obtained while correspondence in local portions of the wall is taken.</p>
<p id="p-0106" num="0105">(Example 2 of Retrieval Method) In order to obtain a higher speed, in the boundary setting section <b>103</b>, a subject of measurement in wall thickness change of the heart wall is divided into a plurality of segments on the basis of the endocardium and epicardium set in the initial image. The epicardium may be expressed on the basis of the thickness t and a displacement parameter s from the endocardium for the boundaries of each segment. A direction of the displacement parameter s is different from that of the thickness t, and preferably is perpendicular to that of the thickness t.</p>
<p id="p-0107" num="0106">Incidentally, in each image, the endocardium is decided by ordinary means of edge detection using luminance change or by other means such as snake.</p>
<p id="p-0108" num="0107">When the endocardium is decided by a method of extracting the contour on the basis of luminance change (edge) in each image, correspondence between points on the endocardium at a certain point of time t<b>1</b> and points on the endocardium at another point of time t<b>2</b> is not clear. In this method, it is however necessary to set the correspondence because the epicardium is expressed on the basis of the thickness t and a displacement parameter s from the endocardium.</p>
<p id="p-0109" num="0108">Therefore, the correspondence is set in the same manner as described above in Example 1 of Retrieval Method. That is, there is used a method of corresponding points on contour parts, which is divided at the position of the feature point with each other.</p>
<p id="p-0110" num="0109">First, division points for dividing the endocardium and epicardium on the basis of a feature point are obtained in the endocardium and epicardium set in the initial image and are associated with each other. Then, the heart wall is divided into segments by the positions of points corresponding to each other, and the thickness t and the displacement parameter s are obtained in accordance with each of the corresponding points.</p>
<p id="p-0111" num="0110">After that, in each image, division points for dividing the endocardium decided by means of edge detection or the like on the basis of the position of the feature point are obtained in the same manner as in the initial image. Because a division point of the epicardium is located in a position far by the optimum thickness t and the optimum displacement parameter s from each division point of the endocardium, the optimum values are calculated while the thickness t and the displacement parameter s are changed.</p>
<p id="p-0112" num="0111">The optimum values of s and t are values at which a normalized image of highest similarity to the template image obtained on the basis of segments of the initial image can be obtained from the image N.</p>
<p id="p-0113" num="0112"><figref idref="DRAWINGS">FIG. 5</figref> is a view for explaining a state in which a cardiac muscle image is divided into six segments <b>501</b> to <b>506</b>. In <figref idref="DRAWINGS">FIG. 5</figref>, points on the epicardium are expressed for the boundaries of each segment on the basis of the thickness t and the displacement parameter s from points on the endocardium.</p>
<p id="p-0114" num="0113">When the epicardium is expressed in this manner, a set of the coordinates of points constituting the epicardium can be given by the following expression.
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?>X={t<sub>1</sub>,s<sub>1</sub>, . . . ,t<sub>k</sub>,s<sub>k</sub>}  (7)<?in-line-formulae description="In-line Formulae" end="tail"?>
</p>
<p id="p-0115" num="0114">In the example shown in <figref idref="DRAWINGS">FIG. 5</figref>, only fourteen parameters are required because of k=7. Because the boundaries can be expressed by far smaller number of parameters than the number of parameters in the expression 5 or 1, a solution can be obtained more speedily.</p>
<p id="p-0116" num="0115">Incidentally, the endocardium may be extracted by use of the active contour model as explained in Example 1 of Retrieval Method.</p>
<p id="p-0117" num="0116">(Example 3 of Retrieval Method) Although Example 2 of Retrieval Method has shown the case where a subject of measurement of wall thickness change of the heart wall is divided into segments so that points expressing the boundary <b>2</b> are set for the boundaries of each segment, the boundaries may be expressed by smaller number of parameters according to an interpolating method using a splined curve or the like.</p>
<p id="p-0118" num="0117">Although the description has been made while the heart is taken as an example, thickness change of a wall or the like can be also detected in other internal organs (such as the stomach, the liver, the urinary bladder, the kidney and the uterus) and an unborn baby as well as the heart.</p>
<p id="p-0119" num="0118">(Modification) This embodiment may be provided as a mode for a medical work station containing not only a process of measuring wall thickness change of the heart wall but also other image measuring process and a data management function such as filing.</p>
<p id="p-0120" num="0119">Alternatively, a series of heart wall thickness change measuring steps for image data (e.g., functions of the boundary setting section <b>103</b>, the normalized-image generating section <b>104</b>, the boundary retrieval section <b>105</b> and the parameter calculation section <b>106</b>) may be implemented by a program executed by a general-purpose computer. Or the series of heart wall thickness change measuring steps for image data may be implemented in the form in which the functions are incorporated into an image diagnostic system.</p>
<p id="p-0121" num="0120">Although the description has been made on the case where the user sets the boundaries manually in the boundary setting section <b>103</b> of this apparatus, the boundaries may be detected automatically by applying a method called snake or an ordinary edge detection method to the initial image. In this case, configuration may be made so that the user can correct the boundaries by using the pointing device <b>107</b> if the boundaries automatically detected cannot be always reliable due to the obscure boundaries.</p>
<p id="p-0122" num="0121">In the output section <b>108</b> of this apparatus, gray scale or colors may be assigned according to the thickness of the heart wall to each image to display a heart wall image. <figref idref="DRAWINGS">FIG. 8</figref> shows an example of expression of a monotone image in a gray scale manner according to the thickness of the heart wall. For example, a thick portion is displayed with being shaded, a thin portion is displayed with being lightened, and an intermediate portion is displayed with intermediate tone. In this manner, by only one glance at intensive change in gray scale, a judgment can be made as to whether cardiac muscle operates actively (repeats expansion and contraction) or not. This is useful for diagnosis.</p>
<p id="p-0123" num="0122">Gray scale may be assigned to an image according to thickness change of the heart wall per predetermined unit time instead of the thickness of the heart wall itself. In this case, the thickness of each portion of the heart wall per predetermined unit time is stored in the memory <b>102</b> in advance. The degree of thickness change is calculated on the basis of the data stored in the memory <b>102</b>.</p>
<p id="p-0124" num="0123">As described above, in accordance with the embodiments of the invention, deformation of a wall is obtained on the basis of not only the edge of the image but also an image pattern. As a result, even in the case where the wall had such obscure boundaries that the detected edge fluctuated in the related art, the boundaries can be detected stably.</p>
<?DETDESC description="Detailed Description" end="tail"?>
</description>
<us-math idrefs="MATH-US-00001" nb-file="US07298880-20071120-M00001.NB">
<img id="EMI-M00001" he="7.45mm" wi="76.20mm" file="US07298880-20071120-M00001.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-math idrefs="MATH-US-00002" nb-file="US07298880-20071120-M00002.NB">
<img id="EMI-M00002" he="7.45mm" wi="76.20mm" file="US07298880-20071120-M00002.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-claim-statement>What is claimed is:</us-claim-statement>
<claims id="claims">
<claim id="CLM-00001" num="00001">
<claim-text>1. An image processing apparatus comprising:
<claim-text>an image input section configured to input time-series images obtained by acquiring an image of an internal organ including a first boundary of a wall and a second boundary of the wall;</claim-text>
<claim-text>a boundary setting section configured to set the first boundary and the second boundary in an initial image of the time-series images;</claim-text>
<claim-text>a normalized-image generating section configured to generate a normalized image on the basis of an image pattern between the set first boundary and the set second boundary;</claim-text>
<claim-text>a template storage section configured to store the normalized image generated from the initial image as a template image; and</claim-text>
<claim-text>a boundary retrieval section configured to search each of time-series images for the second boundary which generates the most similar normalized image to the template image.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00002" num="00002">
<claim-text>2. The image processing apparatus according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the boundary retrieval section searches each of time-series images for the first boundary and the second boundary, which generate the most similar normalized image to the template image, in each of time-series images.</claim-text>
</claim>
<claim id="CLM-00003" num="00003">
<claim-text>3. The image processing apparatus according to <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein the boundary retrieval section includes:
<claim-text>a first section configured to set a first temporary boundary and a second temporary boundary in each of time-series images on the basis of the first boundary and the second boundary in an image input preceding each of time-series image; and</claim-text>
<claim-text>a second section configured to search each of time-series images for a region in which the most similar normalized image to the template image is generated, while changing coordinates of points representing the first temporary boundary and the second temporary boundary.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00004" num="00004">
<claim-text>4. The image processing apparatus according to <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein the boundary retrieval section includes:
<claim-text>a third section configured to detect the first boundary in each of time-series images on the basis of luminance values thereof;</claim-text>
<claim-text>a fourth section configured to set a second temporary boundary in each of time-series images on the basis of the second boundary in an image input preceding each of time-series images; and</claim-text>
<claim-text>a fifth section configured to search each of time-series images for a region in which the most similar normalized image to the template image is generated, while changing coordinates of points representing the second temporary boundary.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00005" num="00005">
<claim-text>5. The image processing apparatus according to <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein the boundary retrieval section includes:
<claim-text>a third section configured to detect the first boundary in each of time-series images on the basis of luminance values thereof;</claim-text>
<claim-text>a fourth section configured to set a second temporary boundary in each of time-series images on the basis of the second boundary in the initial image; and</claim-text>
<claim-text>a fifth section configured to search each of time-series images for a region in which the most similar normalized image to the template image is generated, while changing coordinates of points representing the second temporary boundary.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00006" num="00006">
<claim-text>6. The image processing apparatus according to <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein the boundary retrieval section includes:
<claim-text>a third section configured to detect the first boundary in each of time-series images on the basis of luminance values thereof;</claim-text>
<claim-text>a fourth section configured to estimate a second temporary boundary in each of time-series images on the basis of the second boundaries in the preceding images of each time series image; and</claim-text>
<claim-text>a fifth section configured to search each of time-series images for a region in which the most similar normalized image to the template image is generated, while changing coordinates of points representing the second temporary boundary.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00007" num="00007">
<claim-text>7. An image processing apparatus comprising:
<claim-text>an image input section configured to input time-series images obtained by acquiring an image of an internal organ including a first boundary of a wall and a second boundary of the wall;</claim-text>
<claim-text>a boundary setting section configured to set the first boundary and the second boundary in an initial image of the time-series images, and set boundaries of wall parts into which the wall is divided in the initial image of time-series images;</claim-text>
<claim-text>a sixth section configured to obtain local wall thickness information of each wall part;</claim-text>
<claim-text>a normalized-image generating section configured to generate a normalized image on the basis of image patterns of the wall parts;</claim-text>
<claim-text>a template storage section configured to store the normalized image generated from the initial image as a template image; and</claim-text>
<claim-text>a boundary retrieval section configured to search each of time-series images for the first boundary and the second boundary, which generate the most similar normalized image to the template image.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00008" num="00008">
<claim-text>8. The image processing apparatus according to claim <b>2</b>, wherein:
<claim-text>the image of the wall is an image of section of a cardiac muscle;</claim-text>
<claim-text>the first boundary is an epicardial boundary; and</claim-text>
<claim-text>the second boundary is an endocardial boundary.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00009" num="00009">
<claim-text>9. The image processing apparatus according to <claim-ref idref="CLM-00008">claim 8</claim-ref>, wherein the boundary retrieval section includes:
<claim-text>a first section configured to set an epicardial temporary boundary and an endocardial temporary boundary in each of time-series images on the basis of the epicardial boundary and the endocardial boundary in an image input immediately preceding each of time-series image; and</claim-text>
<claim-text>a second section configured to search each of time-series images for a region in which the most similar normalized image to the template image is generated, while changing coordinates of points representing the epicardial temporary boundary and the endocardial temporary boundary.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00010" num="00010">
<claim-text>10. The image processing apparatus according to <claim-ref idref="CLM-00008">claim 8</claim-ref>, wherein the boundary retrieval section includes:
<claim-text>a third section configured to detect the endocardial boundary in each of time-series images on the basis of luminance values thereof;</claim-text>
<claim-text>a fourth section configured to set an epicardial temporary boundary in each of time-series images on the basis of the epicardial boundary in an image input preceding each of time-series images; and</claim-text>
<claim-text>a fifth section configured to search each of time-series images for a region in which the most similar normalized image to the template image is generated, while changing coordinates of points representing the epicardial temporary boundary.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00011" num="00011">
<claim-text>11. The image processing apparatus according to <claim-ref idref="CLM-00008">claim 8</claim-ref>, wherein the boundary retrieval section includes:
<claim-text>a third section configured to detect the endocardial boundary in each of time-series images on the basis of luminance values thereof;</claim-text>
<claim-text>a fourth section configured to set an epicardial temporary boundary in each of time-series images on the basis of the epicardial boundary in the initial image; and</claim-text>
<claim-text>a fifth section configured to search each of time-series images for a region in which the most similar normalized image to the template image is generated, while changing coordinates of points representing the epicardial temporary boundary.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00012" num="00012">
<claim-text>12. The image processing apparatus according to <claim-ref idref="CLM-00008">claim 8</claim-ref>, wherein the boundary retrieval section includes:
<claim-text>a third section configured to detect the endocardial boundary in each of time-series images on the basis of luminance values thereof;</claim-text>
<claim-text>a fourth section configured to estimate an epicardial temporary boundary in each of time-series images on the basis of the epicardial boundaries in the preceding images of each time series image; and</claim-text>
<claim-text>a fifth section configured to search each of time-series images for a region in which the most similar normalized image to the template image is generated, while changing coordinates of points representing the epicardial temporary boundary.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00013" num="00013">
<claim-text>13. The image processing apparatus according to <claim-ref idref="CLM-00007">claim 7</claim-ref>, wherein the boundary setting section sets boundaries of wall parts into which the cardiac muscle is divided in an initial image of time-series images,
<claim-text>the image processing apparatus further comprising:
<claim-text>a sixth section for obtaining local wall thickness information of each wall part, wherein the boundary retrieval section searches each of time-series images for the region in which the most similar normalized image to the template image is generated, while changing the local wall thickness information of each wall part.</claim-text>
</claim-text>
</claim-text>
</claim>
<claim id="CLM-00014" num="00014">
<claim-text>14. An image processing method for detecting a wall of an internal organ from time-series images, the method comprising:
<claim-text>inputting time-series images obtained by acquiring an image including a first boundary of a wall and a second boundary of the wall;</claim-text>
<claim-text>setting the first boundary in an initial image of the time-series images;</claim-text>
<claim-text>setting the second boundary in the initial image;</claim-text>
<claim-text>generating a normalized image on the basis of an image pattern between the set first boundary and the set second boundary from the initial image to store the generated normalized image as a template image; and</claim-text>
<claim-text>searching each of time-series images for the first boundary and the second boundary, which generate the most similar normalized image to the template image.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00015" num="00015">
<claim-text>15. The image processing method according to <claim-ref idref="CLM-00014">claim 14</claim-ref>, wherein the searching includes:
<claim-text>setting a first temporary boundary and a second temporary boundary in each of time-series images on the basis of the first boundary and the second boundary in an image input preceding each of time-series images; and</claim-text>
<claim-text>searching each of time-series images while changing coordinates of points representing the first temporary boundary and the second temporary boundary.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00016" num="00016">
<claim-text>16. The image processing method according to <claim-ref idref="CLM-00014">claim 14</claim-ref>, wherein the searching includes:
<claim-text>detecting the first boundary in each of time-series images on the basis of luminance values;</claim-text>
<claim-text>setting a first temporary boundary in each of time-series images on the basis of the first boundary and the second boundary in an image input preceding each of time-series images; and</claim-text>
<claim-text>searching each of time-series images while changing coordinates of points representing the second temporary boundary.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00017" num="00017">
<claim-text>17. The image processing method according to <claim-ref idref="CLM-00014">claim 14</claim-ref>, further comprising:
<claim-text>obtaining local wall thickness information of wall parts into which the wall is divided, before the searching, wherein:</claim-text>
<claim-text>the searching includes searching each of time-series images for a region in which the most similar normalized image to the template image is generated, while changing the local wall thickness information of each wall part.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00018" num="00018">
<claim-text>18. The image processing apparatus according to <claim-ref idref="CLM-00008">claim 8</claim-ref>, further comprising:
<claim-text>a parameter calculation section configured to calculate the thickness of each parts of the cardiac muscle on the basis of the determined endocardial boundary and the determined epicardial boundary in each time-series image; and</claim-text>
<claim-text>an output section configured to output images obtained by adding one of gray scale and colors to the cardiac muscle of the time-series images in accordance with the thickness of the each part of the cardiac muscle.</claim-text>
</claim-text>
</claim>
</claims>
</us-patent-grant>
