<us-patent-grant lang="EN" dtd-version="v4.2 2006-08-23" file="US07298412-20071120.XML" status="PRODUCTION" id="us-patent-grant" country="US" date-produced="20071106" date-publ="20071120">
<us-bibliographic-data-grant>
<publication-reference>
<document-id>
<country>US</country>
<doc-number>07298412</doc-number>
<kind>B2</kind>
<date>20071120</date>
</document-id>
</publication-reference>
<application-reference appl-type="utility">
<document-id>
<country>US</country>
<doc-number>10244670</doc-number>
<date>20020917</date>
</document-id>
</application-reference>
<us-application-series-code>10</us-application-series-code>
<priority-claims>
<priority-claim sequence="01" kind="national">
<country>JP</country>
<doc-number>2001-284162</doc-number>
<date>20010918</date>
</priority-claim>
<priority-claim sequence="02" kind="national">
<country>JP</country>
<doc-number>2001-284163</doc-number>
<date>20010918</date>
</priority-claim>
<priority-claim sequence="03" kind="national">
<country>JP</country>
<doc-number>2001-304342</doc-number>
<date>20010928</date>
</priority-claim>
<priority-claim sequence="04" kind="national">
<country>JP</country>
<doc-number>2001-304343</doc-number>
<date>20010928</date>
</priority-claim>
<priority-claim sequence="05" kind="national">
<country>JP</country>
<doc-number>2001-304638</doc-number>
<date>20010928</date>
</priority-claim>
</priority-claims>
<us-term-of-grant>
<us-term-extension>714</us-term-extension>
</us-term-of-grant>
<classifications-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>G</section>
<class>03</class>
<subclass>B</subclass>
<main-group>13</main-group>
<subgroup>00</subgroup>
<symbol-position>F</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20071120</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>H</section>
<class>04</class>
<subclass>N</subclass>
<main-group>5</main-group>
<subgroup>232</subgroup>
<symbol-position>L</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20071120</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>H</section>
<class>04</class>
<subclass>N</subclass>
<main-group>5</main-group>
<subgroup>222</subgroup>
<symbol-position>L</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20071120</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
</classifications-ipcr>
<classification-national>
<country>US</country>
<main-classification>348348</main-classification>
<further-classification>348350</further-classification>
<further-classification>34833303</further-classification>
</classification-national>
<invention-title id="d0e143">Image pickup device, automatic focusing method, automatic exposure method, electronic flash control method and computer program</invention-title>
<references-cited>
<citation>
<patcit num="00001">
<document-id>
<country>US</country>
<doc-number>5111231</doc-number>
<kind>A</kind>
<name>Tokunaga</name>
<date>19920500</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>396123</main-classification></classification-national>
</citation>
<citation>
<patcit num="00002">
<document-id>
<country>US</country>
<doc-number>5343303</doc-number>
<kind>A</kind>
<name>Delmas</name>
<date>19940800</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>348346</main-classification></classification-national>
</citation>
<citation>
<patcit num="00003">
<document-id>
<country>US</country>
<doc-number>5357962</doc-number>
<kind>A</kind>
<name>Green</name>
<date>19941000</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>600443</main-classification></classification-national>
</citation>
<citation>
<patcit num="00004">
<document-id>
<country>US</country>
<doc-number>5430809</doc-number>
<kind>A</kind>
<name>Tomitaka</name>
<date>19950700</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<patcit num="00005">
<document-id>
<country>US</country>
<doc-number>5488429</doc-number>
<kind>A</kind>
<name>Kojima et al.</name>
<date>19960100</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>348653</main-classification></classification-national>
</citation>
<citation>
<patcit num="00006">
<document-id>
<country>US</country>
<doc-number>5550580</doc-number>
<kind>A</kind>
<name>Zhou</name>
<date>19960800</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>348 141</main-classification></classification-national>
</citation>
<citation>
<patcit num="00007">
<document-id>
<country>US</country>
<doc-number>5835616</doc-number>
<kind>A</kind>
<name>Lobo et al.</name>
<date>19981100</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<patcit num="00008">
<document-id>
<country>US</country>
<doc-number>6292575</doc-number>
<kind>B1</kind>
<name>Bortolussi et al.</name>
<date>20010900</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>382118</main-classification></classification-national>
</citation>
<citation>
<patcit num="00009">
<document-id>
<country>US</country>
<doc-number>6396540</doc-number>
<kind>B1</kind>
<name>Ohkawara</name>
<date>20020500</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>348345</main-classification></classification-national>
</citation>
<citation>
<patcit num="00010">
<document-id>
<country>US</country>
<doc-number>6456328</doc-number>
<kind>B1</kind>
<name>Okada</name>
<date>20020900</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>348699</main-classification></classification-national>
</citation>
<citation>
<patcit num="00011">
<document-id>
<country>US</country>
<doc-number>6591064</doc-number>
<kind>B2</kind>
<name>Higashiyama et al.</name>
<date>20030700</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<patcit num="00012">
<document-id>
<country>US</country>
<doc-number>6727948</doc-number>
<kind>B1</kind>
<name>Silverbrook</name>
<date>20040400</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<patcit num="00013">
<document-id>
<country>US</country>
<doc-number>2002/0080257</doc-number>
<kind>A1</kind>
<name>Blank</name>
<date>20020600</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>348345</main-classification></classification-national>
</citation>
<citation>
<patcit num="00014">
<document-id>
<country>US</country>
<doc-number>2003/0071908</doc-number>
<kind>A1</kind>
<name>Sannoh et al.</name>
<date>20030400</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<patcit num="00015">
<document-id>
<country>US</country>
<doc-number>2005/0036688</doc-number>
<kind>A1</kind>
<name>Froeba et al.</name>
<date>20050200</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>382199</main-classification></classification-national>
</citation>
<citation>
<patcit num="00016">
<document-id>
<country>US</country>
<doc-number>2005/0063566</doc-number>
<kind>A1</kind>
<name>Beek et al.</name>
<date>20050300</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<patcit num="00017">
<document-id>
<country>EP</country>
<doc-number>1 128 316</doc-number>
<date>20010800</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<patcit num="00018">
<document-id>
<country>JP</country>
<doc-number>6-27537</doc-number>
<date>19940200</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<patcit num="00019">
<document-id>
<country>JP</country>
<doc-number>7-12980</doc-number>
<date>19950300</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<patcit num="00020">
<document-id>
<country>JP</country>
<doc-number>10-334213</doc-number>
<date>19981200</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<patcit num="00021">
<document-id>
<country>JP</country>
<doc-number>2001-51338</doc-number>
<date>20010200</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<patcit num="00022">
<document-id>
<country>JP</country>
<doc-number>2001-215403</doc-number>
<date>20010800</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<nplcit num="00023">
<othercit>Pending U.S. Appl. No. 10/120,369, filed Apr. 12, 2002.</othercit>
</nplcit>
<category>cited by other</category>
</citation>
<citation>
<nplcit num="00024">
<othercit>Pending U.S. Appl. No. 10/244,670, filed Sep. 17, 2002.</othercit>
</nplcit>
<category>cited by other</category>
</citation>
<citation>
<nplcit num="00025">
<othercit>U.S. Appl. No. 10/244,670, filed Sep. 17, 2002, Sannoh et al.</othercit>
</nplcit>
<category>cited by other</category>
</citation>
<citation>
<nplcit num="00026">
<othercit>U.S. Appl. No. 10/354,086, filed Jan. 30, 2003, Ojima et al.</othercit>
</nplcit>
<category>cited by other</category>
</citation>
<citation>
<nplcit num="00027">
<othercit>U.S. Appl. No. 10/623,556, filed Jul. 22, 2003, Shiraishi.</othercit>
</nplcit>
<category>cited by other</category>
</citation>
<citation>
<nplcit num="00028">
<othercit>U.S. Appl. No. 10/636,712, filed Aug. 8, 2003, Shinohara et al.</othercit>
</nplcit>
<category>cited by other</category>
</citation>
<citation>
<nplcit num="00029">
<othercit>U.S. Appl. No. 10/637,502, filed Aug. 11, 2003, Shiraishi.</othercit>
</nplcit>
<category>cited by other</category>
</citation>
<citation>
<nplcit num="00030">
<othercit>U.S. Appl. No. 10/764,438, filed Jan. 27, 2004, Shiraishi et al.</othercit>
</nplcit>
<category>cited by other</category>
</citation>
<citation>
<nplcit num="00031">
<othercit>U.S. Appl. No. 10/914,196, Aug. 10, 2004, Shiraishi.</othercit>
</nplcit>
<category>cited by other</category>
</citation>
<citation>
<nplcit num="00032">
<othercit>U.S. Appl. No. 11/028,307, filed Jan. 4, 2005, Shiraishi.</othercit>
</nplcit>
<category>cited by other</category>
</citation>
</references-cited>
<number-of-claims>12</number-of-claims>
<us-exemplary-claim>1</us-exemplary-claim>
<us-field-of-classification-search>
<classification-national>
<country>US</country>
<main-classification>348345</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>348349</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>348352</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>348348</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>348350</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>34833303</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>382118</main-classification>
</classification-national>
</us-field-of-classification-search>
<figures>
<number-of-drawing-sheets>36</number-of-drawing-sheets>
<number-of-figures>50</number-of-figures>
</figures>
<us-related-documents>
<related-publication>
<document-id>
<country>US</country>
<doc-number>20030071908</doc-number>
<kind>A1</kind>
<date>20030417</date>
</document-id>
</related-publication>
</us-related-documents>
<parties>
<applicants>
<applicant sequence="001" app-type="applicant-inventor" designation="us-only">
<addressbook>
<last-name>Sannoh</last-name>
<first-name>Masato</first-name>
<address>
<city>Tokyo</city>
<country>JP</country>
</address>
</addressbook>
<nationality>
<country>JP</country>
</nationality>
<residence>
<country>JP</country>
</residence>
</applicant>
<applicant sequence="002" app-type="applicant-inventor" designation="us-only">
<addressbook>
<last-name>Shiraishi</last-name>
<first-name>Kenji</first-name>
<address>
<city>Tokyo</city>
<country>JP</country>
</address>
</addressbook>
<nationality>
<country>JP</country>
</nationality>
<residence>
<country>JP</country>
</residence>
</applicant>
</applicants>
<agents>
<agent sequence="01" rep-type="attorney">
<addressbook>
<orgname>Oblon, Spivak, McClelland, Maier &amp; Neustadt, P.C.</orgname>
<address>
<country>unknown</country>
</address>
</addressbook>
</agent>
</agents>
</parties>
<assignees>
<assignee>
<addressbook>
<orgname>Ricoh Company, Limited</orgname>
<role>03</role>
<address>
<city>Tokyo</city>
<country>JP</country>
</address>
</addressbook>
</assignee>
</assignees>
<examiners>
<primary-examiner>
<last-name>Vu</last-name>
<first-name>Ngoc-Yen</first-name>
<department>2622</department>
</primary-examiner>
<assistant-examiner>
<last-name>Henderson</last-name>
<first-name>Adam L.</first-name>
</assistant-examiner>
</examiners>
</us-bibliographic-data-grant>
<abstract id="abstract">
<p id="p-0001" num="0000">There are provided an image memory which stores one image of a subject, an image fetch section which takes in an image from the image memory to another memory or register in a predetermined unit, a control section which takes charge of the overall control, a face characteristic storage section which stores a plurality of characteristics of a face, a recognition and judgment section which recognizes a face from the data from the image fetch section and the data from the face characteristic storage section and judges each portion, an edge detector which detects an edge detection value from the result data thereof, and an output section which outputs the final judgment result to the outside.</p>
</abstract>
<drawings id="DRAWINGS">
<figure id="Fig-EMI-D00000" num="00000">
<img id="EMI-D00000" he="74.42mm" wi="144.61mm" file="US07298412-20071120-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00001" num="00001">
<img id="EMI-D00001" he="251.97mm" wi="134.28mm" orientation="landscape" file="US07298412-20071120-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00002" num="00002">
<img id="EMI-D00002" he="231.90mm" wi="139.45mm" orientation="landscape" file="US07298412-20071120-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00003" num="00003">
<img id="EMI-D00003" he="267.63mm" wi="169.93mm" orientation="landscape" file="US07298412-20071120-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00004" num="00004">
<img id="EMI-D00004" he="97.20mm" wi="123.11mm" file="US07298412-20071120-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00005" num="00005">
<img id="EMI-D00005" he="267.55mm" wi="176.61mm" file="US07298412-20071120-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00006" num="00006">
<img id="EMI-D00006" he="179.41mm" wi="168.23mm" file="US07298412-20071120-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00007" num="00007">
<img id="EMI-D00007" he="199.47mm" wi="176.11mm" file="US07298412-20071120-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00008" num="00008">
<img id="EMI-D00008" he="197.44mm" wi="171.20mm" file="US07298412-20071120-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00009" num="00009">
<img id="EMI-D00009" he="140.72mm" wi="161.29mm" file="US07298412-20071120-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00010" num="00010">
<img id="EMI-D00010" he="264.08mm" wi="169.59mm" file="US07298412-20071120-D00010.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00011" num="00011">
<img id="EMI-D00011" he="235.37mm" wi="170.18mm" file="US07298412-20071120-D00011.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00012" num="00012">
<img id="EMI-D00012" he="245.19mm" wi="111.84mm" orientation="landscape" file="US07298412-20071120-D00012.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00013" num="00013">
<img id="EMI-D00013" he="212.85mm" wi="87.55mm" file="US07298412-20071120-D00013.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00014" num="00014">
<img id="EMI-D00014" he="263.99mm" wi="165.35mm" file="US07298412-20071120-D00014.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00015" num="00015">
<img id="EMI-D00015" he="175.85mm" wi="172.13mm" file="US07298412-20071120-D00015.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00016" num="00016">
<img id="EMI-D00016" he="224.28mm" wi="100.08mm" file="US07298412-20071120-D00016.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00017" num="00017">
<img id="EMI-D00017" he="234.70mm" wi="130.13mm" file="US07298412-20071120-D00017.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00018" num="00018">
<img id="EMI-D00018" he="245.96mm" wi="171.37mm" file="US07298412-20071120-D00018.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00019" num="00019">
<img id="EMI-D00019" he="148.00mm" wi="170.43mm" file="US07298412-20071120-D00019.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00020" num="00020">
<img id="EMI-D00020" he="94.57mm" wi="109.73mm" file="US07298412-20071120-D00020.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00021" num="00021">
<img id="EMI-D00021" he="235.80mm" wi="157.73mm" orientation="landscape" file="US07298412-20071120-D00021.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00022" num="00022">
<img id="EMI-D00022" he="251.04mm" wi="154.60mm" orientation="landscape" file="US07298412-20071120-D00022.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00023" num="00023">
<img id="EMI-D00023" he="260.27mm" wi="164.76mm" file="US07298412-20071120-D00023.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00024" num="00024">
<img id="EMI-D00024" he="258.32mm" wi="152.57mm" file="US07298412-20071120-D00024.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00025" num="00025">
<img id="EMI-D00025" he="227.50mm" wi="172.04mm" file="US07298412-20071120-D00025.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00026" num="00026">
<img id="EMI-D00026" he="222.42mm" wi="85.51mm" file="US07298412-20071120-D00026.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00027" num="00027">
<img id="EMI-D00027" he="222.67mm" wi="172.97mm" orientation="landscape" file="US07298412-20071120-D00027.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00028" num="00028">
<img id="EMI-D00028" he="196.77mm" wi="107.61mm" orientation="landscape" file="US07298412-20071120-D00028.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00029" num="00029">
<img id="EMI-D00029" he="170.69mm" wi="90.51mm" file="US07298412-20071120-D00029.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00030" num="00030">
<img id="EMI-D00030" he="171.20mm" wi="94.74mm" file="US07298412-20071120-D00030.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00031" num="00031">
<img id="EMI-D00031" he="246.38mm" wi="111.34mm" file="US07298412-20071120-D00031.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00032" num="00032">
<img id="EMI-D00032" he="174.24mm" wi="168.32mm" file="US07298412-20071120-D00032.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00033" num="00033">
<img id="EMI-D00033" he="171.70mm" wi="167.98mm" file="US07298412-20071120-D00033.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00034" num="00034">
<img id="EMI-D00034" he="198.71mm" wi="107.87mm" orientation="landscape" file="US07298412-20071120-D00034.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00035" num="00035">
<img id="EMI-D00035" he="266.87mm" wi="131.91mm" file="US07298412-20071120-D00035.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00036" num="00036">
<img id="EMI-D00036" he="259.67mm" wi="81.79mm" file="US07298412-20071120-D00036.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
</drawings>
<description id="description">
<?BRFSUM description="Brief Summary" end="lead"?>
<heading id="h-0001" level="1">BACKGROUND OF THE INVENTION</heading>
<p id="p-0002" num="0001">1) Field of the Invention</p>
<p id="p-0003" num="0002">The present invention relates to an image pickup device, an automatic focusing method, an automatic exposure method, an electronic flash control method and a computer program, which have a face detection function for detecting a face of a subject.</p>
<p id="p-0004" num="0003">2) Description of the Related Art</p>
<p id="p-0005" num="0004">Recently, with a development of personal computers, use of digital cameras has become popular as image input devices thereof. Particularly, in many cases, photographers who do not have special techniques use digital cameras. Therefore, in many digital cameras, the shutter speed, exposure and focusing are automatically set, matched with a subject, in order to avoid a failure in taking a picture. In many cases, the subject for photographing is generally a human object. However, there has not been proposed a digital camera, which specially has a setting mode peculiar to human objects. It is a main object of the present invention to photograph a human object appropriately.</p>
<p id="p-0006" num="0005">In order to always take a picture, which is in focus, while manually performing focus adjustment of the subject, the distance of which from the camera changes every moment, skilled camera work technique is required. Therefore, an automatic focusing function has been developed, so that even nonprofessionals can always photograph an image, which is in focus. In an image pickup device, which has a conventional automatic focusing function, it is common to perform automatic focusing, designating one or a plurality of areas in the center of a screen as a ranging area. In the above ranging method, however, when a human object is photographed, it is necessary to perform automatic focusing in a composition in which the human object and the ranging area are overlapped on each other, and after the focus is adjusted, to change the composition to a desired composition and photograph. Hence, there is a problem in that it takes time to photograph, with a human object being in focus.</p>
<p id="p-0007" num="0006">As a technique for determining exposure with a conventional image pickup device such as a digital camera, there are used, (1) a multifractionated photometric method in which a photometric range in a screen is divided into a plurality of photometric areas, and exposure is determined from the photometric result of each photometric area, (2) a center-weighted photometric method in which optical intensity is measured by giving weight to the center of the screen, (3) a partial photometric method in which optical intensity is measured in only several tens % of the center of the screen, and (4) a spot photometric method in which optical intensity is measured in only several % of the center of the screen.</p>
<p id="p-0008" num="0007">With the conventional photometric methods of (1) to (4), however, there is a problem in that a human object is not always exposed properly. More specifically, in the multifractionated photometric method (1), since the exposure is controlled so that the whole screen is exposed properly, there is a problem in that a human object in the screen may not be exposed properly. With the center-weighted photometric method (2), the partial photometric method (3) and the spot photometric method (4), a human object is exposed properly, only when the photometric area agrees with the position of the human object.</p>
<p id="p-0009" num="0008">Further, when a color photograph is to be taken, using an electronic flash device, a so-called red-eye phenomenon may occur, in which human eyes appear red or gold. Such red-eye phenomenon occurs because flash of light of a light emitter of an electronic flashing device, which has passed through the pupil of the eye, is reflected by the retinal portion, and the reflected light is revealed in the film. More specifically, many capillary vessels exist in the retinal portion of the eye, and hemoglobin in the blood is red, and hence the reflected light thereof appears reddish. In order to prevent the red-eye phenomenon, there has been heretofore known a technique in which after light for preventing the red-eye phenomenon is emitted so as to reduce the size of the pupil of a human object, the main light emission is performed.</p>
<p id="p-0010" num="0009">With a camera having a conventional electronic flash device, however, there are various problems described below. Firstly, there is such a problem that when a close-up of a face is to be shot, a place where the electronic flash light is strongly irradiated, such as forehead, reflects the electronic flash light, thereby causing blanking. Secondly, there is such a problem that since backlight judgment has been carried out by a difference between the brightness in the central portion of the screen and the ambient brightness, when a human object is not in the middle of the screen in the backlight situation, automatic electronic flashing for correcting the backlight is not performed, and it is necessary to switch to a forced emission mode in order to emit electronic flashlight. Thirdly, there is such a problem that in order to prevent the red-eye phenomenon and blanking due to the electronic flash light, a user has to carry out red-eye mode switchover and a weak light emitting operation of the electronic flash.</p>
<p id="p-0011" num="0010">In Japanese Patent Application Laid-Open No. 2001-51338, there is disclosed a camera in which the direction of the face of a subject can be recognized, and when the face turns to a predetermined direction, the subject recording operation can be performed. This camera has, a field image detection unit which detects image information of a field, a face recognition unit which recognizes the face of a subject, based on the image information from the field image detection unit, a judgment unit which judges whether the face is turning to a predetermined direction based on the information from the face recognition unit, and a control unit which allows to perform the subject recording operation according to the judgment result of the judgment unit.</p>
<p id="p-0012" num="0011">However, with the image pickup device such as a digital camera, comprising a mode in which the shutter speed, exposure and focusing are automatically set depending on a subject, pushing of a release button to photograph the subject depends on the photographer's intention. Therefore, when a human object is to be photographed, even if the photographer recognizes the human object, the release timing may be shifted, thereby losing the photographing chance. In Japanese Patent Application Laid-Open No. 2001-51338, such a technique is used that the direction of the face is recognized, and when the face turns to a predetermined direction, the subject recording operation is performed. Therefore, if the human object moves or the camera is blurred, the face may not be photographed clearly.</p>
<p id="p-0013" num="0012">In Japanese Patent Application Laid-Open No. H10-334213, there is disclosed a technique in which a face image of a target human object image is automatically recognized from color image data including the human object image, without being affected by the photographing background and illuminance, and an area corresponding to the size of the face image is cut out. According to this technique, the camera comprises, an image acquiring CPU which acquires a color image including a human object image, a color liquid crystal display section for displaying the acquired color image on a screen, a face recognition color memory which preliminarily stores a plurality of face recognition color data for recognizing a face image of the human object image from the color image, a recognition instructing section which instructs implementation of face image recognition processing, a face image recognizing CPU which compares color data in the entire color image area with the face recognition color data, in response to the instruction of implementation of the image recognition processing, to thereby recognize an area of color data which agrees with any of the face recognition color data as a face image, an area designator which designates an area to be cut out of the recognized face image, a frame forming CPU which forms a frame for the area corresponding to the size of the face image, in response to the designation of the cutout area, and a face image cutout CPU which cuts out the face image of an area enclosed by the frame.</p>
<p id="p-0014" num="0013">However, when a photograph for certificate is taken, there has been heretofore such a problem that the face in the photographed image approaches left or right, or up or down, or inclines. When the face inclines, an operator has to specify the cutout position manually, watching the image, or specify the position of the face, thereby making the operation troublesome. Further, Japanese Patent Application Laid-Open No. H10-334213 is for automatically recognizing the face image of a target human object image from color image data including the human object image, without being affected by the photographing background and illuminance, and cutting out the area corresponding to the size of the face image. Hence, it is not taken into consideration to correct the balance of the size to be cut out or an inclination of the image.</p>
<heading id="h-0002" level="1">SUMMARY OF THE INVENTION</heading>
<p id="p-0015" num="0014">It is a primary object of this invention to provide an image pickup device and an automatic focusing method, which can bring a human face in a screen into focus, regardless of the position and range of the human object in the screen, and photograph the human object without changing the composition between at the time of automatic focusing and at the time of photographing, and a program for a computer to execute the method.</p>
<p id="p-0016" num="0015">And it is a second object of this invention to provide an image pickup device and an automatic exposure method, which can properly expose a human object, regardless of the position and range of the human object in the screen, and a program for a computer to execute the method.</p>
<p id="p-0017" num="0016">And it is a third object of this invention to provide an image pickup device and an electronic flash control method, which can automatically judge whether a human object is to be photographed, without troubling the user, and can automatically emit light suitable for the human object photographing, and a program for a computer to execute the method.</p>
<p id="p-0018" num="0017">And it is a fourth object of this invention to provide an image pickup device, in which a release button is automatically pushed by recognizing and judging a face image, thereby enabling recording only a clear image, and preventing photographing an unclear image, without losing a shutter chance.</p>
<p id="p-0019" num="0018">And it is a fifth object of this invention to provide an image pickup device, in which a face image can be cut out, with the size and inclination being automatically corrected, by recognizing and judging the face image, and stored as another image file.</p>
<p id="p-0020" num="0019">According to one aspect of the invention, there is provided an image pickup device provided with an automatic focusing function, comprising, an image pickup unit which inputs image data corresponding to a subject, a face detection unit which detects a human face from the image data input from the image pickup unit, and an automatic focusing control unit which performs automatic focusing control, designating at least a part of the human face detected by the face detection unit as a ranging area.</p>
<p id="p-0021" num="0020">According to one aspect of the invention, there is provided an automatic focusing method comprising, an image input step of inputting image data corresponding to a subject, a face detection step of detecting a human face from the image data input at the image input step, and an automatic focusing control step of performing automatic focusing control, designating at least a part of the human face detected at the face detection step as a ranging area.</p>
<p id="p-0022" num="0021">According to one aspect of the invention, there is provided a program for a computer to execute, an image input step of inputting image data corresponding to a subject, a face detection step of detecting a human face from the image data input at the image input step, and an automatic focusing control step of performing automatic focusing control, designating at least a part of the human face detected at the face detection step as a ranging area.</p>
<p id="p-0023" num="0022">According to one aspect of the invention, there is provided an image pickup device provided with an automatic exposure function, comprising, an image pickup unit which inputs image data corresponding to a subject, a face detection unit which detects a human face from the image data input from the image pickup unit, a photometric unit which measures optical intensity, designating the human face detected by the face detection unit as a photometric area, and an exposure control unit which calculates the exposure based on the photometric result of the human face by the photometric unit, and performs exposure control based on the calculated exposure.</p>
<p id="p-0024" num="0023">According to one aspect of the invention, there is provided an automatic exposure method comprising, an image input step of inputting image data corresponding to a subject, a face detection step of detecting a human face from the image data input at the image input step, a photometric step of measuring optical intensity, designating the human face detected at the face detection step as a photometric area, and an exposure control step of calculating the exposure based on the photometric result of the human face at the photometric step, and performs exposure control based on the calculated exposure.</p>
<p id="p-0025" num="0024">According to one aspect of the invention, there is provided a program for a computer to execute, an image input step of inputting image data corresponding to a subject, a face detection step of detecting a human face from the image data input at the image input step, a photometric step of measuring optical intensity, designating the human face detected at the face detection step as a photometric area and an exposure control step of calculating the exposure based on the photometric result of the human face at the photometric step, and performing exposure control based on the calculated exposure.</p>
<p id="p-0026" num="0025">According to one aspect of the invention, there is provided an image pickup device provided with an electronic flash function, comprising, an image pickup unit which inputs image data corresponding to a subject, a face detection unit which detects a human face from the image data input from the image pickup unit, an electronic flashing unit for emitting electronic flash light, and an electronic flash control unit which controls the electronic flashing unit based on the detection result of the human face by the face detection unit.</p>
<p id="p-0027" num="0026">According to one aspect of the invention, there is provided an electronic flash control method comprising, an image input step of inputting image data corresponding to a subject, a face detection step of detecting a human face from the image data input by the image pickup unit and an electronic flash control step of controlling the electronic flashing unit for emitting the electronic flash light, based on the detection result of a human face at the face detection step.</p>
<p id="p-0028" num="0027">According to one aspect of the invention, there is provided a program for a computer to execute, an image input step of inputting image data corresponding to a subject, a face detection step of detecting a human face from the image data input by the image pickup unit, and an electronic flash control step of controlling the electronic flashing unit for emitting the electronic flash light, based on the detection result of a human face at the face detection step.</p>
<p id="p-0029" num="0028">According to one aspect of the invention, there is provided an image pickup device comprising, an image pickup unit which converts an optical image of a subject to image data and outputs this image data, a face image recognition unit which recognizes face image from the image data, and a face portion judgment unit which judges each portion in the face image recognized by the face image recognition unit, wherein when each portion in the face image can all be judged by the face portion judgment unit, a release button for executing photographing of the subject is automatically pressed, or the photographing operation is executed.</p>
<p id="p-0030" num="0029">According to one aspect of the invention, there is provided an image pickup device comprising, an image pickup unit which converts a subject image to image data and outputs this image data, and a face image recognition unit which recognizes face image from the image data, wherein when the face image is recognized by the face image recognition unit, the face image portion is cut out and stored as a separate image file.</p>
<p id="p-0031" num="0030">These and other objects, features and advantages of the present invention are specifically set forth in or will become apparent from the following detailed descriptions of the invention when read in conjunction with the accompanying drawings.</p>
<?BRFSUM description="Brief Summary" end="tail"?>
<?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?>
<description-of-drawings>
<heading id="h-0003" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading>
<p id="p-0032" num="0031"><figref idref="DRAWINGS">FIG. 1</figref> is a block diagram which shows an appearance of a digital camera according to a first embodiment;</p>
<p id="p-0033" num="0032"><figref idref="DRAWINGS">FIG. 2</figref> is a block diagram which shows a configuration of the digital camera according to the first embodiment;</p>
<p id="p-0034" num="0033"><figref idref="DRAWINGS">FIG. 3</figref> is a diagram which shows a user setup screen for recording conditions displayed on a liquid crystal monitor, when a SETUP mode is selected by a mode changing switch in an operation section in the digital camera according to the first embodiment;</p>
<p id="p-0035" num="0034"><figref idref="DRAWINGS">FIG. 4</figref> is a flowchart which explains a first example of the AF operation;</p>
<p id="p-0036" num="0035"><figref idref="DRAWINGS">FIG. 5</figref> is a diagram (<b>1</b>) which shows a display example on the liquid crystal monitor;</p>
<p id="p-0037" num="0036"><figref idref="DRAWINGS">FIG. 6</figref> is a diagram (<b>2</b>) which shows a display example on the liquid crystal monitor;</p>
<p id="p-0038" num="0037"><figref idref="DRAWINGS">FIG. 7</figref> is a diagram which shows a user setup screen for recording conditions displayed on a liquid crystal monitor, when a SETUP mode is selected by a mode changing switch in an operation section according to a second example;</p>
<p id="p-0039" num="0038"><figref idref="DRAWINGS">FIG. 8</figref> is a flowchart which explains the second example of the AF operation;</p>
<p id="p-0040" num="0039"><figref idref="DRAWINGS">FIG. 9</figref> is a diagram which shows a display example on the liquid crystal monitor;</p>
<p id="p-0041" num="0040"><figref idref="DRAWINGS">FIG. 10</figref> is a diagram which shows a user setup screen for recording conditions displayed on a liquid crystal monitor, when a SETUP mode is selected by a mode changing switch in an operation section in the digital camera according to the second embodiment;</p>
<p id="p-0042" num="0041"><figref idref="DRAWINGS">FIG. 11</figref> is a diagram which shows a user setup screen for recording conditions displayed on the liquid crystal monitor, when the SETUP mode is selected by the mode changing switch in the operation section in a digital camera according to a second embodiment;</p>
<p id="p-0043" num="0042"><figref idref="DRAWINGS">FIG. 12</figref> is a diagram which explains the AE operation;</p>
<p id="p-0044" num="0043"><figref idref="DRAWINGS">FIG. 13</figref> is a diagram which explains the AE operation;</p>
<p id="p-0045" num="0044"><figref idref="DRAWINGS">FIG. 14</figref> is a flowchart which explains the first example of the AE operation;</p>
<p id="p-0046" num="0045"><figref idref="DRAWINGS">FIG. 15</figref> is a diagram which shows a display example on the liquid crystal monitor;</p>
<p id="p-0047" num="0046"><figref idref="DRAWINGS">FIG. 16</figref> is a diagram which shows a display example on a sub-LCD;</p>
<p id="p-0048" num="0047"><figref idref="DRAWINGS">FIG. 17</figref> is a diagram which shows a display example on the liquid crystal monitor of a digital camera;</p>
<p id="p-0049" num="0048"><figref idref="DRAWINGS">FIG. 18</figref> is a flowchart which explains the second example of the AE operation;</p>
<p id="p-0050" num="0049"><figref idref="DRAWINGS">FIG. 19</figref> is a diagram which shows a display example on the liquid crystal monitor;</p>
<p id="p-0051" num="0050"><figref idref="DRAWINGS">FIG. 20</figref> is a diagram which shows a user setup screen for recording conditions displayed on a liquid crystal monitor, when a SETUP mode is selected by a mode changing switch in an operation section in a digital camera according to a third embodiment;</p>
<p id="p-0052" num="0051"><figref idref="DRAWINGS">FIG. 21</figref> is a block diagram which shows a detailed configuration of an electronic flash circuit in <figref idref="DRAWINGS">FIG. 2</figref>;</p>
<p id="p-0053" num="0052"><figref idref="DRAWINGS">FIG. 22</figref> is a flowchart which explains electronic flashing operation;</p>
<p id="p-0054" num="0053"><figref idref="DRAWINGS">FIG. 23</figref> is a diagram which shows a display example on the liquid crystal monitor;</p>
<p id="p-0055" num="0054"><figref idref="DRAWINGS">FIG. 24</figref> is a diagram which shows a display example on the liquid crystal monitor;</p>
<p id="p-0056" num="0055"><figref idref="DRAWINGS">FIG. 25</figref> is a block diagram which shows a digital camera according to a fourth embodiment;</p>
<p id="p-0057" num="0056"><figref idref="DRAWINGS">FIG. 26</figref> is a schematic block diagram which shows a face recognition processor in the digital camera in the first embodiment;</p>
<p id="p-0058" num="0057"><figref idref="DRAWINGS">FIG. 27</figref> is a flowchart which explains the first example;</p>
<p id="p-0059" num="0058"><figref idref="DRAWINGS">FIG. 28</figref> is a flowchart which explains the second example;</p>
<p id="p-0060" num="0059"><figref idref="DRAWINGS">FIG. 29</figref> is a flowchart which explains a third example;</p>
<p id="p-0061" num="0060"><figref idref="DRAWINGS">FIG. 30</figref> is a flowchart which explains a fourth example;</p>
<p id="p-0062" num="0061"><figref idref="DRAWINGS">FIG. 31</figref> is a flowchart which explains a fifth example;</p>
<p id="p-0063" num="0062"><figref idref="DRAWINGS">FIG. 32</figref> is a schematic block diagram which shows a face recognition processor in a digital camera in a fifth embodiment;</p>
<p id="p-0064" num="0063"><figref idref="DRAWINGS">FIG. 33</figref> is a flowchart which explains the operation of the digital camera according to the fifth embodiment;</p>
<p id="p-0065" num="0064"><figref idref="DRAWINGS">FIG. 34</figref> is a diagram (<b>1</b>) which explains a face image;</p>
<p id="p-0066" num="0065"><figref idref="DRAWINGS">FIG. 35</figref> is a diagram (<b>2</b>) which explains the face image; and</p>
<p id="p-0067" num="0066"><figref idref="DRAWINGS">FIG. 36</figref> is a diagram (<b>3</b>) which explains the face image;</p>
</description-of-drawings>
<?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?>
<?DETDESC description="Detailed Description" end="lead"?>
<heading id="h-0004" level="1">DETAILED DESCRIPTIONS</heading>
<p id="p-0068" num="0067">Preferred embodiments of a digital camera, which employs a image pickup device, an automatic focusing method, an automatic exposure method, an electronic flash control method and a program to be executed by a computer, will be explained in detail, in order of (a first embodiment), (a second embodiment), (a third embodiment), (a fourth embodiment) and (a fifth embodiment). The components, kinds, combinations, shapes and relative arrangements thereof described in each embodiment are not intended to limit the scope of the invention, and are only explanation examples, unless otherwise specified.</p>
<p id="h-0005" num="0000">(First Embodiment)</p>
<p id="p-0069" num="0068">A digital camera according to a first embodiment will be explained with reference to <figref idref="DRAWINGS">FIG. 1</figref> to <figref idref="DRAWINGS">FIG. 9</figref>. In the first embodiment, a digital camera which can bring a human face in a screen into focus, regardless of the position and range of the human object in the screen, and photograph the human object without changing the composition between at the time of automatic focusing and at the time of photographing will be explained.</p>
<p id="p-0070" num="0069"><figref idref="DRAWINGS">FIG. 1</figref> is a block diagram which shows an appearance of a digital camera according to the first embodiment, in which <figref idref="DRAWINGS">FIG. 1A</figref> shows a front elevation, <figref idref="DRAWINGS">FIG. 1B</figref> shows a side view, and <figref idref="DRAWINGS">FIG. 1C</figref> shows a top view. <figref idref="DRAWINGS">FIG. 1</figref> shows a state that an upper lid is opened. In this figure, reference number <b>201</b> denotes a camera body, <b>202</b> denotes an image pickup lens, <b>203</b> denotes an electronic flash unit, <b>204</b> denotes a viewfinder, <b>205</b> denotes the upper lid provided so as to be opened or closed, <b>206</b> denotes a hinge that supports the upper lid <b>205</b> so as to be opened or closed, <b>207</b> denotes a liquid crystal monitor provided inside of the upper lid <b>205</b>, <b>208</b> denotes a sub-LCD provided outside of the upper lid <b>205</b>, <b>209</b> denotes an upper lid open/close detection switch for detecting whether the upper lid <b>205</b> is opened or closed, <b>210</b> denotes a barrier opening/closing knob, <b>211</b> denotes a release switch for instructing operation such as photographing, <b>212</b> denotes a mode changing switch for selecting each mode, <b>213</b> denotes an electronic flash switch for performing electronic flashing, <b>214</b> denotes an image quality mode switch for setting photographing image quality, <b>215</b> denotes a cursor key, <b>216</b> denotes a DATE switch for inputting date, <b>217</b> denotes an ENTER key, and <b>218</b> denotes a SELECT key.</p>
<p id="p-0071" num="0070">The mode changing switch <b>212</b> is for selecting, (1) a SETUP mode for changing or confirming the setting of the camera, (2) a still picture mode for recording a still picture, (3) a motion picture mode for recording a motion picture, (4) a reproduction mode for reproducing a recorded file, (5) a PC mode for sending a file to a personal computer or operating the camera from the personal computer, by connecting the camera to the personal computer, and (6) a communication mode for communicating via a network.</p>
<p id="p-0072" num="0071"><figref idref="DRAWINGS">FIG. 2</figref> is a block diagram which shows a configuration of the digital camera according to the first embodiment. The digital camera shown in this figure comprises, image pickup lenses <b>202</b> (zoom lens <b>202</b><i>a, </i>focus lens <b>202</b><i>b</i>), a mechanical mechanism (a mechanical shutter, a diaphragm, a filter, etc.) <b>101</b>, a CCD <b>102</b>, F/E (a CDS circuit, an AGC circuit and A/D conversion circuit) <b>103</b>, an IPP <b>104</b>, a compression/expansion circuit <b>105</b>, a memory card <b>106</b>, a DRAM <b>107</b>, a liquid crystal monitor <b>207</b>, a sub-LCD <b>208</b>, an operation section <b>108</b>, motors (a zoom lens motor <b>109</b><i>a, </i>a focus motor <b>109</b><i>b, </i>a diaphragm/shutter motor <b>109</b><i>c</i>) <b>109</b>, drivers (a zoom driver <b>110</b><i>a, </i>a focus driver <b>110</b><i>b, </i>a diaphragm/shutter driver <b>110</b><i>c</i>) <b>110</b>, an SG <b>111</b>, a battery <b>112</b>, a DC/DC converter <b>113</b>, an AC adapter <b>114</b>, a controller (CPU <b>115</b><i>a, </i>a FLASH_ROM <b>115</b><i>b, </i>an SD_RAM <b>115</b><i>c</i>) <b>115</b>, a communication driver section <b>116</b>, a network communicator <b>117</b> and an electronic flash unit <b>203</b>.</p>
<p id="p-0073" num="0072">The image pickup lens <b>202</b> is for forming an image of a subject, and comprises a zoom lens <b>202</b><i>a </i>for realizing a zoom function, and a focus lens <b>2</b><i>b </i>for bringing the subject into focus. The mechanical mechanism <b>101</b> comprises a mechanical shutter, a diaphragm for adjusting the brightness of the subject, a filter and the like. The CCD <b>102</b> converts the subject image formed by the image pickup lens <b>202</b> into an electric signal, and outputs the signal as analog image data to the F/E <b>103</b>.</p>
<p id="p-0074" num="0073">The F/E <b>103</b> is for performing preprocessing with respect to the analog image data input from the CCD <b>102</b>, and comprises a CDS circuit for performing correlated double sampling of the analog image data input from the CCD <b>102</b>, an AGC circuit for adjusting the gain of the analog image data output from the CDS circuit, and an A/D conversion circuit for performing A/D conversion with respect to the analog image data output from the AGC circuit and outputting the digital image data to the IPP <b>104</b>.</p>
<p id="p-0075" num="0074">The IPP <b>104</b> is for performing various image processing with respect to the input image data, and for example, after performing interpolation processing with respect to the color image data input from the F/E <b>103</b>, it converts the color image data to YUV data (luminance Y data, color difference U and V data).</p>
<p id="p-0076" num="0075">The compression/expansion circuit <b>105</b> performs compression processing with respect to the YUV data, with a compression method conforming to the JPEG method, or performs expansion processing with respect to the compressed image data stored in a memory card, with an expansion method conforming to the JPEG method.</p>
<p id="p-0077" num="0076">The DRAM <b>107</b> is a frame memory which temporarily stores the picked up image data, the image data read from the memory card <b>106</b> and the like. The memory card <b>106</b> is provided detachably with respect to the digital camera body, and stores the compressed image data as an image file.</p>
<p id="p-0078" num="0077">The liquid crystal monitor <b>207</b> is for displaying image data and the like. The sub-LCD <b>208</b> is for displaying modes and operation contents set in the digital camera.</p>
<p id="p-0079" num="0078">The zoom driver <b>110</b><i>a </i>drives the zoom motor <b>109</b><i>a, </i>in response to a control signal input from the controller <b>115</b>, to thereby shift the zoom lens <b>202</b><i>a </i>in the direction of optical axis. The focus driver <b>110</b><i>b </i>drives the focus motor <b>109</b><i>b, </i>in response to a control signal input from the controller <b>115</b>, to thereby shift the focus lens <b>2</b><i>b </i>in the direction of optical axis. The diaphragm/shutter driver <b>110</b><i>c </i>drives the diaphragm/shutter motor <b>119</b><i>c, </i>in response to a control signal input from the controller <b>115</b>, to thereby set a diaphragm stop of the diaphragm in the mechanical mechanism <b>101</b> and open or close the mechanical shutter in the mechanical mechanism <b>101</b>.</p>
<p id="p-0080" num="0079">The SG <b>111</b> drives the CCD <b>10</b> and the <b>2</b>F/E <b>103</b>, based on a control signal input from the controller <b>115</b>. The battery <b>112</b> comprises, for example, a NiCd battery, a nickel hydrogen battery or a lithium battery, and supplies DC voltage to the DC/DC converter <b>113</b>. The AC adapter <b>114</b> converts the AC voltage to the DC voltage and supplies the DC voltage to the DC/DC converter <b>113</b>. The DC/DC converter <b>113</b> converts the DC voltage level input form the battery <b>112</b> and the AC adapter <b>114</b> to supply power to each section of the digital cameral.</p>
<p id="p-0081" num="0080">The communication driver <b>116</b> is connected to an external equipment (for example, a personal computer) for performing data communication. The network communicator <b>117</b> is for performing data communication with the network.</p>
<p id="p-0082" num="0081">The electronic flash unit <b>203</b> is a unit which emits electronic flash light, and emits light for preventing red-eye phenomenon or for main light emission, under control of the controller <b>115</b>.</p>
<p id="p-0083" num="0082">The operation section <b>108</b> comprises various keys and switches (a release switch <b>211</b>, a mode changing switch <b>212</b>, an electronic flash switch <b>213</b>, an image quality mode switch <b>214</b>, a cursor key <b>215</b>, a DATE switch <b>216</b>, an ENTER key <b>217</b> and a SELECT key <b>218</b>), as shown in <figref idref="DRAWINGS">FIG. 13</figref>, for a user to give various instructions to the digital camera, and detects operation of various keys and switches and outputs it to the controller <b>115</b>. The release switch <b>211</b> is a two-stage switch, such that a first SW is turned ON by half pushing (a first pushing stroke), and a second SW is turned ON by full pushing (a second pushing stroke). In the digital camera, when the first SW is turned ON, the AE operation and the AF operation are executed, and when the second SW is turned ON, the photographing operation is executed.</p>
<p id="p-0084" num="0083">In <figref idref="DRAWINGS">FIG. 3</figref>, there is shown a user setup screen for recording conditions displayed on the liquid crystal monitor <b>207</b>, when the SETUP mode is selected by the mode changing switch <b>212</b> in the operation section <b>108</b>. The user setup screen for recording conditions shown in this figure is a screen for a user to set setting condition for each item ([AF/Manual] in “Focus”, [ON/OFF] in “Face detection (face detection operation mode)”, [Auto/manual] in “Photometric point”, [ON/OFF] in “Long time exposure”, and [ON/OFF] in “With date”). In the screen shown in this figure, a desired item is selected by the cursor key <b>215</b>, a desired setting condition is selected by the SELECT key <b>218</b> in the selected item, and the selected setting condition is set by the ENTER key <b>217</b>. In the example shown in this figure, the setting is made such that “Focus” is [AF], “Face detection (face detection operation mode)” is [ON], “Photometric point” is [Auto], “Long time exposure” is [OFF], and “With date” is [OFF].</p>
<p id="p-0085" num="0084">The controller <b>115</b> is a unit which controls the overall operation of the digital camera, and comprises, the CPU <b>115</b><i>a </i>which controls the operation of each section in the digital camera, using the FLASH_RAM <b>115</b><i>c </i>as a work area, in accordance with a control program stored in the FLASH_ROM <b>115</b><i>b, </i>based on the instruction from the operation section <b>108</b>, an SD_ROM <b>115</b><i>b </i>which stores various control programs executed by the CPU <b>115</b><i>a, </i>and parameters and data necessary for the control, and an SD_RAM <b>115</b><i>c </i>used as the work area of the CPU <b>115</b><i>a. </i>The control program, parameters and data stored in the FLASH_ROM <b>115</b><i>b </i>can be distributed through the network, and can be also stored by downloading these through the network. The program, parameters and data can be written in the FLASH_ROM <b>115</b><i>b </i>from an external equipment connected via the communication driver section <b>116</b>.</p>
<p id="p-0086" num="0085">The CPU <b>115</b><i>a </i>controls the monitoring operation, the AF operation, the AE operation, the photographing operation, the electronic flashing operation and the like. The outline of the basic operation of (1) monitoring operation, (2) AF operation and (3) still picture photographing operation, to be executed under control of the CPU <b>115</b><i>a, </i>will be explained below.</p>
<p id="h-0006" num="0000">(1) Monitoring Operation</p>
<p id="p-0087" num="0086">When the still picture mode or the motion picture mode is selected by the mode changing switch <b>212</b> in the operation section <b>108</b>, the CPU <b>115</b><i>a </i>displays a monitored image of a subject on the liquid crystal monitor <b>207</b>. Specifically, the image data of the subject input from the CCD <b>102</b> is processed by the F/E (CDS, AGC, A/D) <b>103</b>, signal-processed by the IPP <b>104</b> and then written in the DRAM <b>107</b>. The CPU <b>115</b><i>a </i>displays the monitored image corresponding to the image data stored in the DRAM <b>107</b> on the liquid crystal monitor <b>207</b>. A user can determine the photographing composition, watching the monitored image of the subject displayed on this liquid crystal monitor <b>207</b>.</p>
<p id="h-0007" num="0000">(2) AF Operation</p>
<p id="p-0088" num="0087">After the still picture mode or the motion picture mode has been selected by the mode changing switch <b>212</b> in the operation section <b>108</b>, when the release switch <b>211</b> is half-pushed to turn on the SW <b>1</b>, the CPU <b>115</b><i>a </i>executes the AF operation. Specifically, in the AF operation, when the face detection mode is set, the face detection processing is first executed to thereby detect a human face. More specifically, the image data input from the CCD <b>102</b> is processed in the F/E (CDS, AGC, A/D) <b>103</b>, signal-processed by the IPP <b>104</b> and then written in the DRAM <b>107</b>. The CPU <b>115</b><i>a </i>detects a human face from image data stored in the DRAM <b>107</b>, by using the known face detection technique (Gabor Wavelet transform+ graph matching). The CPU <b>115</b><i>a </i>executes the automatic focusing processing, using a known automatic focusing method such as a mountaineering servo method, designating the detected human face as the ranging area (AF area), to shift the focus lens <b>202</b><i>b </i>to a focusing position, to thereby focus on the human face. In such automatic focusing processing, the focus lens <b>202</b><i>b </i>is moved, to sample an AF evaluation value indicating a contrast of the subject corresponding to the image data in the ranging area (AF area), of the image data input from the CCD <b>102</b>, and designates the peak position of the AF evaluation value as the focusing position.</p>
<p id="p-0089" num="0088">The CPU <b>115</b><i>a </i>executes the control program to thereby perform face detection by the software, but dedicated hardware for face detection may be provided.</p>
<p id="h-0008" num="0000">(3) Still Picture Photographing Operation</p>
<p id="p-0090" num="0089">When the still picture mode is selected by the mode changing switch <b>212</b> in the operation section <b>108</b>, and the release switch in the operation section <b>108</b> is fully pushed to turn on the SW <b>2</b>, the CPU <b>115</b><i>a </i>executes the still picture photographing operation. The imaged data taken in from the CCD <b>102</b> is signal-processed by the IPP <b>104</b>, and subjected to the compression processing by the compression/expansion circuit <b>105</b>, and then written in the DRAM <b>107</b>. The CPU <b>115</b><i>a </i>then writes the compressed image data written in the DRAM <b>107</b> into the memory card <b>106</b> as a still picture file.</p>
<p id="h-0009" num="0000">[First Example]</p>
<p id="p-0091" num="0090"><figref idref="DRAWINGS">FIG. 4</figref> is a flowchart which explains a first example of the AF operation of the digital camera. <figref idref="DRAWINGS">FIG. 5</figref> and <figref idref="DRAWINGS">FIG. 6</figref> are diagrams which show display examples on the liquid crystal monitor <b>207</b>. <figref idref="DRAWINGS">FIG. 5</figref> shows a display example when the human face of the subject is one, and <figref idref="DRAWINGS">FIG. 6</figref> shows a display example when the human face of the subject is plural (two). The AF operation of the digital camera will be explained in accordance with the flowchart in <figref idref="DRAWINGS">FIG. 4</figref>, with reference to the display examples on the liquid crystal monitor <b>207</b> shown in <figref idref="DRAWINGS">FIG. 5</figref> and <figref idref="DRAWINGS">FIG. 6</figref>. The CPU <b>115</b><i>a </i>executes the AF processing, when the still picture mode or the motion picture mode is selected by the mode changing switch <b>212</b> in the operation section <b>108</b>, and then the release switch <b>213</b> is half-pushed to turn on the SW <b>1</b>.</p>
<p id="p-0092" num="0091">In <figref idref="DRAWINGS">FIG. 4</figref>, the CPU <b>115</b><i>a </i>first judges whether the face detection operation mode is set, by which human face detection is to be executed (step S<b>201</b>). The face detection operation mode is set on the user setup screen for recording conditions (see <figref idref="DRAWINGS">FIG. 3</figref>). As a result of this judgment, when it is judged that the face detection operation mode has been set, the CPU <b>115</b><i>a </i>executes the face detection processing to detect a face (step S<b>202</b>).</p>
<p id="p-0093" num="0092">Methods (1) to (3) shown below are known as the method of detecting a face in the subject image, and in a third embodiment, either one of these methods is used,
<ul id="ul0001" list-style="none">
    <li id="ul0001-0001" num="0093">(1) A method of forming a mosaic image of a color image, and extracting a face area by paying attention to the flesh-colored area, as shown in “Proposal of modified HSV color system effective in face area extraction”, Television Society Magazine Vol. 49, No. 6, pp. 787–797 (1995),</li>
    <li id="ul0001-0002" num="0094">(2) A method of using geometrical shape characteristic relating to each portion which constitutes a head of front human object image, such as hair, eyes and the mouth, to extract a head area of the front human object image, as shown in “Method of extracting face area from still grayscale scene picture”, Electronic Information Communication Society Magazine, Vol. 74-D-II, No. 11, pp. 1625–1627 (1991), and</li>
    <li id="ul0001-0003" num="0095">(3) A method of extracting a front human object image by using a profile edge of the human object image, which appears due to a delicate movement of the human object between the frames, in a case of a motion picture, as shown in “Face area detection for videophone and effect thereof”, Image Laboratory 1991-11 (1991).</li>
</ul>
</p>
<p id="p-0094" num="0096">When the human face is detected, the CPU <b>115</b><i>a </i>detects the eyes, the nose, the mouth and the ears. Detection of the eyes, the nose, the mouth and the ears in the face can be performed, using a known method. For example, detection of eyes can be performed, using techniques disclosed in Japanese Patent Application Laid-Open No. H3-17696, Japanese Patent Application Laid-Open No. H4-255015, Japanese Patent Application Laid-Open No. H5-300601, Japanese Patent Application Laid-Open No. H9-21611 and Japanese Patent Application Laid-Open No. H9-251342.</p>
<p id="p-0095" num="0097">The CPU <b>115</b><i>a </i>then judges whether the face has been detected (step S<b>203</b>). As a result of this judgment, when the face has been detected, the CPU <b>115</b><i>a </i>displays a rectangular frame on the face portion of the human object (displays face detection execution result OK), in the image data displayed on the liquid crystal monitor <b>207</b>, to thereby notify the user of the detection of the face (step S<b>204</b>) <figref idref="DRAWINGS">FIG. 5A</figref> shows a display example (when the human face is one) of the image data (monitored image) displayed on the liquid crystal monitor <b>207</b>. <figref idref="DRAWINGS">FIG. 5B</figref> shows a display example, when a rectangular frame A is displayed on the human face portion in the image data in <figref idref="DRAWINGS">FIG. 5A</figref>. <figref idref="DRAWINGS">FIG. 6A</figref> shows a display example of the image data (monitored image) displayed on the liquid crystal monitor <b>207</b>, when there is a plurality of (two) human faces. <figref idref="DRAWINGS">FIG. 6B</figref> shows a display example, when rectangular frames A<b>1</b> and A<b>2</b> are displayed on the human face portions in the image data in <figref idref="DRAWINGS">FIG. 6A</figref>.</p>
<p id="p-0096" num="0098">The CPU <b>115</b><i>a </i>judges whether there is a plurality of faces (step S<b>205</b>). As a result of this judgment, when a plurality of faces are detected, the CPU <b>115</b><i>a </i>judges whether the ranging area selection mode has been set, by which a user selects the ranging area (step S<b>206</b>). The ranging area selection mode is set on the user setup screen for recording conditions (see <figref idref="DRAWINGS">FIG. 3</figref>). As a result of this judgment, when it is judged that the ranging area selection mode has not been set, the CPU <b>115</b><i>a </i>sets a face close to the center as the ranging area (step S<b>207</b>), and then executes the automatic focusing processing (step S<b>208</b>).</p>
<p id="p-0097" num="0099">On the other hand, at step S<b>206</b>, when it is judged that the ranging area selection mode has been set, the CPU <b>115</b><i>a </i>displays characters or the like for the user to select the ranging area on the liquid crystal monitor <b>207</b> (step S<b>211</b>). <figref idref="DRAWINGS">FIG. 6C</figref> shows a display example in which characters or the like for the user to select the ranging area (guidance display for selecting the ranging area) on the screen in <figref idref="DRAWINGS">FIG. 6B</figref>.</p>
<p id="p-0098" num="0100">Subsequently, by the user operation of the operation section <b>108</b>, a desired position is selected as the ranging area, among face-recognized positions displayed on the liquid crystal monitor <b>207</b> (step S<b>212</b>). In <figref idref="DRAWINGS">FIG. 6C</figref>, the user selects either one of the rectangular frames A<b>1</b> and A<b>2</b> by the operation of the cursor key <b>215</b>, and the selected rectangular frame is determined as the ranging area by the ENTER key <b>217</b>. <figref idref="DRAWINGS">FIG. 6D</figref> shows an example in which the rectangular frame A<b>2</b> is selected as the ranging area in <figref idref="DRAWINGS">FIG. 6C</figref>.</p>
<p id="p-0099" num="0101">After having set the inside of the selected rectangular frame as the ranging area (step S<b>213</b>), the CPU <b>115</b><i>a </i>executes the automatic focusing processing (step S<b>208</b>).</p>
<p id="p-0100" num="0102">On the other hand, at step S<b>205</b>, if a plurality of faces are not detected, that is, only one face is detected, the CPU <b>115</b><i>a </i>judges whether eyes, nose, mouth and ears in the face are detected in the face (step S<b>214</b>). As a result of this judgment, when the eyes, nose, mouth and ears in the face cannot be detected, the CPU <b>115</b><i>a </i>sets the central position of the face as the ranging area (step S<b>217</b>), to perform the automatic focusing processing (step S<b>208</b>).</p>
<p id="p-0101" num="0103">On the other hand, at step S<b>214</b>, when the eyes, nose, mouth and ears in the face cannot be detected, the CPU <b>115</b><i>a </i>judges whether the ranging area selection mode has been set, by which the user selects the ranging area (step S<b>215</b>). As a result of this judgment, when the CPU <b>115</b><i>a </i>judges that the ranging area selection mode has not been set, the CPU <b>115</b><i>a </i>sets the position of eyes as the ranging area (step S<b>218</b>), and executes the automatic focusing processing (step S<b>208</b>).</p>
<p id="p-0102" num="0104">At step S<b>215</b>, when it is judged that the ranging area selection mode has been set, the CPU <b>115</b><i>a </i>displays a rectangular frame on the eyes, the nose, the mouth and the ears, respectively, on the liquid crystal monitor <b>207</b> (step S<b>16</b>). <figref idref="DRAWINGS">FIG. 5C</figref> is a diagram which shows a display example, when rectangular frames al to a<b>4</b> are displayed respectively on the eyes, the nose and the mouth, in the image data of the subject in <figref idref="DRAWINGS">FIG. 5A</figref>.</p>
<p id="p-0103" num="0105">From the positions where the eyes, the nose, the mouth and the ears are recognized, which are displayed on the liquid crystal monitor <b>207</b>, a desired position is selected as the ranging area by the user operation of the operation section <b>108</b> (step S<b>212</b>). In <figref idref="DRAWINGS">FIG. 5C</figref>, the user selects any one of the rectangular frames a<b>1</b> to a<b>4</b> by operating the cursor key <b>215</b>, and determines a selected rectangular frame as the ranging area by the ENTER key <b>217</b>. <figref idref="DRAWINGS">FIG. 5D</figref> shows an example in which a rectangular frame a<b>3</b> has been selected as the ranging area in <figref idref="DRAWINGS">FIG. 5C</figref>.</p>
<p id="p-0104" num="0106">After having set the face portion in the rectangular frame determined by the user (step S<b>213</b>) as the ranging area, the CPU <b>115</b><i>a </i>executes the automatic focusing processing (step S<b>8</b>).</p>
<p id="p-0105" num="0107">On the other hand, at step S<b>201</b>, when it is judged that the face detection operation mode has not been set, or at step S<b>203</b>, when a face has not been detected, the CPU <b>115</b><i>a </i>sets the normal ranging area position as the ranging area (step S<b>210</b>), and then executes the automatic focusing processing (step S<b>208</b>). When the release switch <b>211</b> is fully pushed, photographing is carried out.</p>
<p id="p-0106" num="0108">As described above, according to the first example, the CPU <b>115</b><i>a </i>detects a human face in the image data input by the CCD <b>102</b>, and carries out the automatic focusing control, designating at least a part of the detected human face as the ranging area. Thereby, the CPU <b>115</b><i>a </i>executes the automatic focusing operation, designating the human face in the screen as the ranging area. Therefore, the human face can be brought into focus, regardless of the position of the human object in the screen. As a result, the human object can be photographed, without changing the composition between at the time of the automatic focusing operation and at the time of photographing.</p>
<p id="p-0107" num="0109">According to the first example, when having detected a human face, the CPU <b>115</b><i>a </i>displays a rectangular frame (see <figref idref="DRAWINGS">FIG. 5</figref>) on the face in the image data displayed on the liquid crystal monitor <b>207</b>. Therefore, the user is notified of the detection of the face, so that the user can recognize that the face has been detected.</p>
<p id="p-0108" num="0110">According to the first example, when a plurality of faces are detected, the CPU <b>115</b><i>a </i>carries out automatic focusing control, designating a face close to the central portion as the ranging area. Thereby, the main subject can be brought into focus at a high probability. In an aggregate photograph, it is possible to increase the possibility that faces at the front, back, right, and left positions of the face in the center are also brought into focus at a high probability.</p>
<p id="p-0109" num="0111">According to the first example, when a plurality of faces are detected, the user selects the ranging area from the faces (see <figref idref="DRAWINGS">FIG. 6</figref>), and the CPU <b>115</b><i>a </i>performs automatic focusing control based on the ranging area selected by the user. As a result, the user can select which human object is brought into focus.</p>
<p id="p-0110" num="0112">According to the first example, the user can set permission/inhibition of the face detection operation (see <figref idref="DRAWINGS">FIG. 3</figref>). Therefore, the user can omit unnecessary face detection operation by setting permission or inhibition of the face detection operation, thereby enabling more prompt photographing.</p>
<p id="p-0111" num="0113">According to the first example, when eyes of a human face are detected, and when the ranging area selection mode is not set, the CPU <b>115</b><i>a </i>carries out the automatic focusing control, designating the eyes as the ranging area. As a result, photographing can be performed by focusing on the eyes.</p>
<p id="p-0112" num="0114">According to the first example, when any of the eyes, the mouth, the nose, and the ears of a human face is detected, and when the ranging area selection mode is set, the user selects the ranging area from the detected eyes, nose, mouth and ears, and the CPU <b>115</b><i>a </i>performs automatic focusing control based on the ranging area selected by the user. As a result, the user can focus the lens on a portion in the face on which the user wants to focus, and hence user's photographing intention can be reflected.</p>
<p id="h-0010" num="0000">[Second Example]</p>
<p id="p-0113" num="0115">A second example of the AF operation will be explained, with reference to <figref idref="DRAWINGS">FIG. 7</figref> to <figref idref="DRAWINGS">FIG. 9</figref>. In the second example of the AF operation, the size of the ranging area is changed depending on the size of the detected human face.</p>
<p id="p-0114" num="0116"><figref idref="DRAWINGS">FIG. 7</figref> shows a user setup screen for recording conditions displayed on the liquid crystal monitor <b>207</b>, when a SETUP mode is selected by the mode changing switch <b>12</b> in the operation section <b>108</b> in the second example. The user setup screen for recording conditions shown in this figure is a screen for a user to set setting condition for each item ([AF/Manual] in “Focus”, [ON/OFF] in “Face detection (face detection operation mode)”, [ON/OFF] in “Ranging area automatic setting (ranging area automatic setting mode)”, [Auto/manual] in “Photometric point”, [ON/OFF] in“Long time exposure”, and [ON/OFF] in “With date”) In the screen shown in this figure, a desired item is selected by the cursor key <b>215</b>, a desired setting condition is selected by the SELECT key <b>218</b> in the selected item, and the selected setting condition is set by the ENTER key <b>217</b>. In the example shown in this figure, the setting is made such that “Focus” is [AF], “Face detection (face detection operation mode)” is [ON], “Ranging area automatic setting (ranging area automatic setting mode)” is [ON], “Photometric point” is [Auto], “Long time exposure” is [OFF], and “With date” is [OFF].</p>
<p id="p-0115" num="0117"><figref idref="DRAWINGS">FIG. 8</figref> is a flowchart which explains the second example of the AF operation. <figref idref="DRAWINGS">FIG. 9</figref> is a diagram which shows a display example on the liquid crystal monitor <b>207</b>. The second example of the AF operation will be explained, with reference to the display example on the liquid crystal monitor <b>207</b> shown in <figref idref="DRAWINGS">FIG. 9</figref>, in accordance with the flowchart shown in <figref idref="DRAWINGS">FIG. 8</figref>. After the still picture mode or the motion picture mode has been selected by the mode changing switch <b>212</b> in the operation section <b>108</b>, the CPU <b>115</b><i>a </i>executes the AF processing when the release switch <b>213</b> is half-pushed to turn on the SW <b>1</b>.</p>
<p id="p-0116" num="0118">In <figref idref="DRAWINGS">FIG. 8</figref>, the CPU <b>115</b><i>a </i>first judges whether the face detection operation mode in which face detection of a human object is executed, has been set (step S<b>231</b>). The face detection operation mode is set on the user setup screen for recording conditions (see <figref idref="DRAWINGS">FIG. 7</figref>). As a result of this judgment, when it is judged to be the face detection operation mode, the CPU <b>115</b><i>a </i>executes the similar face detection processing to that of the first embodiment (step S<b>232</b>).</p>
<p id="p-0117" num="0119">The CPU <b>115</b><i>a </i>then judges whether the face has been detected (step S<b>233</b>). As a result of this judgment, when the face has been detected, the CPU <b>115</b><i>a </i>displays a rectangular frame on the face portion of the human object (displays face detection execution result OK), in the image data displayed on the liquid crystal monitor <b>207</b>, to thereby notify the user of the detection of the face (step S<b>234</b>).</p>
<p id="p-0118" num="0120">The CPU <b>115</b><i>a </i>judges whether the ranging area automatic setting mode has been set, in which the size of the ranging area is automatically set (step S<b>235</b>). The ranging area automatic setting mode is set on the user setup screen for the recording conditions (see <figref idref="DRAWINGS">FIG. 7</figref>). As a result of this judgment, when it is judged that the ranging area automatic setting mode has been set, the CPU <b>115</b><i>a </i>compares the size of the detected face and a predetermined value <b>1</b> (step S<b>136</b>), and when the face size is not larger than the predetermined value <b>1</b>, the CPU <b>115</b><i>a </i>sets size <b>1</b> of the ranging area (step S<b>241</b>). On the other hand, if the face size is larger than the predetermined value <b>1</b>, the CPU <b>115</b><i>a </i>compares the face size with a predetermined value <b>2</b> (however, predetermined value <b>2</b>&gt;predetermined value <b>1</b>) (step S<b>237</b>), and when the face size is larger than the predetermined value <b>2</b>, the CPU <b>115</b><i>a </i>sets size <b>3</b> of the ranging area (step S<b>238</b>). When the face size is not larger than the predetermined value <b>2</b>, the CPU <b>115</b><i>a </i>sets size <b>2</b> of the ranging area (step S<b>242</b>).</p>
<p id="p-0119" num="0121">On the other hand, at step S<b>235</b>, if the ranging area automatic setting mode has not been set, the user sets the size <b>1</b>, <b>2</b> or <b>3</b> of the ranging area (step S<b>243</b>). <figref idref="DRAWINGS">FIG. 5A</figref> shows a display example of the image data (monitored image) displayed on the liquid crystal monitor <b>207</b>. In <figref idref="DRAWINGS">FIG. 9</figref>, there is displayed a screen for a user to select the size of the ranging area, on the screen shown in <figref idref="DRAWINGS">FIG. 5A</figref>. In <figref idref="DRAWINGS">FIG. 9</figref>, there are displayed three sizes (size <b>1</b>, size <b>2</b> and size <b>3</b>) of ranging areas having different sizes, and the user operates the cursor key <b>215</b> to select one of size <b>1</b>, size <b>2</b> and size <b>3</b>, and determines the selected size as the size of the ranging area, by the ENTER key <b>217</b>. In the example shown in <figref idref="DRAWINGS">FIG. 9</figref>, the size <b>2</b> of the ranging area has been selected.</p>
<p id="p-0120" num="0122">After having set the ranging area of the size, automatically set or selected by the user, on the portion of the detected human face, the CPU <b>115</b><i>a </i>executes the automatic focusing processing (step S<b>239</b>).</p>
<p id="p-0121" num="0123">On the other hand, when it is judged that the face detection operation mode has not been set at step S<b>231</b>, or when the face has not been detected at step S<b>233</b>, the CPU <b>115</b><i>a </i>sets the normal ranging area position as the ranging area (step S<b>240</b>), and then executes the automatic focusing processing (step S<b>239</b>).</p>
<p id="p-0122" num="0124">As explained above, according to the second example, when the ranging area automatic judgment mode is set, the CPU <b>115</b><i>a </i>automatically sets the size of the ranging area based on the size of the detected human face, to perform automatic focusing control. As a result, the focusing accuracy can be improved.</p>
<p id="p-0123" num="0125">Further, according to the second example, when the ranging area automatic judgment mode is not set, the user selects the size of the ranging area (see <figref idref="DRAWINGS">FIG. 9</figref>), the CPU <b>115</b><i>a </i>sets the ranging area of a size selected by the user, to perform automatic focusing control. As a result, the focusing accuracy can be improved, when a specific portion in the face is brought into focus.</p>
<p id="p-0124" num="0126">The present invention is not limited to the first example and the second example, and various modification is possible within the range that does not change the essential point of the invention, and for example, modifications described below are possible.</p>
<p id="h-0011" num="0000">[First Modification Example]</p>
<p id="p-0125" num="0127">A human object photographing mode for taking a picture of a human object (first photographing mode) and a landscape photographing mode for taking a picture of landscape (second photographing mode) are provided in the digital camera of the present invention, such that the mode can be selected by, for example, the mode changing switch <b>212</b>. When the human object photographing mode is selected, the CPU <b>115</b><i>a </i>performs automatic focusing control accompanied with the face detection processing (processing after “YES” at step S<b>201</b> in <figref idref="DRAWINGS">FIG. 4</figref>). When the landscape photographing mode is selected, the CPU <b>115</b><i>a </i>does not perform the face detection processing, and may perform the automatic focusing control (processing after “NO” at step S<b>201</b> in <figref idref="DRAWINGS">FIG. 4</figref>), based on the normal ranging area. This is because, in the human object photographing mode in which it is necessary to focus the lens on the face, the face detection processing is necessary, but in the landscape photographing mode in which it is not necessary to focus the lens on the face, the face detection processing is not necessary.</p>
<p id="h-0012" num="0000">[Second Modification Example]</p>
<p id="p-0126" num="0128">Every time the CPU <b>115</b><i>a </i>performs the automatic focusing control, designating the face detected by the face detection processing as the ranging area, the CPU <b>115</b><i>a </i>stores the ranging result (focusing position) in the FLASH_ROM <b>115</b><i>b, </i>and the next focusing position may be predicted from the past ranging results stored in the FLASH_ROM <b>115</b><i>b. </i>Thereby, the next focusing position can be predicted from the past focusing positions to thereby focus the lens on the face quickly, thereby enabling prompt photographing.</p>
<p id="h-0013" num="0000">(Second Embodiment)</p>
<p id="p-0127" num="0129">A digital camera according to a second embodiment will be explained, with reference to <figref idref="DRAWINGS">FIG. 2</figref> and <figref idref="DRAWINGS">FIG. 10</figref> to <figref idref="DRAWINGS">FIG. 19</figref>. In the second embodiment, a digital camera that can properly expose a human object, regardless of the position and range of the human object in the screen will be explained. The visual configuration and the block configuration of the digital camera according to the second embodiment are the same as those in the first embodiment (<figref idref="DRAWINGS">FIG. 1</figref> and <figref idref="DRAWINGS">FIG. 2</figref>), and hence explanation of the common parts is omitted, and only the different part will be explained.</p>
<p id="p-0128" num="0130"><figref idref="DRAWINGS">FIG. 10</figref> and <figref idref="DRAWINGS">FIG. 11</figref> show the user setup screen for recording conditions displayed on the liquid crystal monitor <b>207</b>, when the SETUP mode is selected by the mode changing switch <b>212</b> in the operation section <b>108</b> in <figref idref="DRAWINGS">FIG. 2</figref>. The user setup screen for recording conditions shown in this figure is a screen for a user to set setting condition for each item ([AF/Manual] in “Focus”, [ON/OFF] in “Face detection (face detection operation mode)”, [Auto/manual (face (first photometric method)/face+ background (second photometric method)] in “Photometric method”, [ON/OFF] in “Long time exposure”, and [ON/OFF] in “With date”). In the screen shown in this figure, a desired item is selected by the cursor key <b>215</b>, a desired setting condition is selected by the SELECT key <b>218</b> in the selected item, and the selected setting condition is set by the ENTER key <b>217</b>.</p>
<p id="p-0129" num="0131">As described above, when the user selects [Auto] in the “photometric method”, the CPU <b>115</b><i>a </i>selects either the first photometric method (face) or the second photometric method (face+background). On the other hand, when the user selects [Manual] in the “photometric method”, the user then selects either [face (the first photometric method)] or [face+background (the second photometric method)]. In the example shown in <figref idref="DRAWINGS">FIG. 10</figref> and <figref idref="DRAWINGS">FIG. 11</figref>, the setting is made such that “Focus” is [AF], “Face detection (face detection processing)” is [ON], “Photometric method” is [Auto], “Long time exposure” is [OFF], and “With date” is [OFF].</p>
<p id="p-0130" num="0132">The CPU <b>115</b><i>a </i>in <figref idref="DRAWINGS">FIG. 2</figref> controls the monitoring operation, the AF operation, the AE operation, the photographing operation, the electronic flashing operation and the like. The outline of the AE operation which is executed under control of the CPU <b>115</b><i>a </i>will be explained below. The monitoring operation and the photographing operation are the same as in the third embodiment.</p>
<p id="h-0014" num="0000">[AE Operation]</p>
<p id="p-0131" num="0133">After the still picture mode or the motion picture mode has been selected by the mode changing switch <b>12</b> in the operation section <b>108</b>, when the release switch <b>11</b> is half-pushed to turn on the SW <b>1</b>, the CPU <b>115</b><i>a </i>executes the AE processing. Specifically, in the AE operation, when the face detection operation mode has been set (see <figref idref="DRAWINGS">FIG. 10</figref> and <figref idref="DRAWINGS">FIG. 11</figref>), the CPU <b>115</b><i>a </i>first executes the face detection processing to detect a human face. Specifically, the image data input from the CCD <b>102</b> is processed by the F/E (CDS, AGC, A/D) <b>103</b> and input in the IPP <b>104</b>. This image data is signal-processed by the IPP <b>104</b> and then written in the DRAM <b>107</b>. The CPU <b>115</b><i>a </i>detects a human face from image data stored in the DRAM <b>107</b>, by using the known face detection technique (Gabor Wavelet transform+graph matching). The CPU <b>115</b><i>a </i>calculates an AE evaluation value (photometric result), designating the detected human face as the photometric area (AE area), and calculates the exposure based on this AE evaluation value. Then the CPU <b>115</b><i>a </i>sets the exposure conditions (electronic shutter speed of the CCD <b>3</b>, diaphragm stop of the diaphragm of the mechanical mechanism <b>101</b>, gain of the AGC circuit in the F/E <b>103</b>, and the like) corresponding to the exposure, to thereby perform exposure control.</p>
<p id="p-0132" num="0134">The calculation method of the AE evaluation value will be specifically explained. <figref idref="DRAWINGS">FIG. 12</figref> shows a division example when the screen of the CCD <b>102</b> (image frame) is divided into a plurality of areas Y<b>1</b> to Y<b>35</b>. <figref idref="DRAWINGS">FIG. 13</figref> is a diagram which shows one example of the image data of the subject in <figref idref="DRAWINGS">FIG. 12</figref>.</p>
<p id="p-0133" num="0135">The IPP <b>104</b> calculates luminance data Y<b>1</b> to Y<b>35</b> for each area Y<b>1</b> to Y<b>35</b>, whose optical intensity is to be measured, based on the image data (each pixel data (R, G, B)) input from the CCD <b>102</b>. The calculation equation of the luminance data is shown below. In the calculation equation, Rn, Gn and Bn indicate a mean value of each pixel data (R, G, B) in each area, respectively.
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?>Area 1 <i>Y</i>1=0.299<i>R</i>1+0.587<i>G</i>1+0.114<i>B</i>1<?in-line-formulae description="In-line Formulae" end="tail"?>
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?>Area 2 <i>Y</i>2=0.299<i>R</i>2+0.587<i>G</i>2+0.114<i>B</i>2<?in-line-formulae description="In-line Formulae" end="tail"?>
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?>Area <i>n Yn=</i>0.299<i>Rn+</i>0.587<i>Gn+</i>0.114<i>Bn</i><?in-line-formulae description="In-line Formulae" end="tail"?>
</p>
<p id="p-0134" num="0136">When the face detection operation mode has been set, and when the photometric processing is to be performed using only the face (first photometric method), the luminance data of the area including the face is respectively calculated, and a mean value of the luminance data of the calculated area is designated as luminance data Yy (AE evaluation value). In the example shown in <figref idref="DRAWINGS">FIG. 13</figref>, a mean value of the luminance data of areas Y<b>14</b>, Y<b>20</b>, Y<b>21</b>, Y<b>26</b> and Y<b>27</b> is designated as luminance data Yy (AE evaluation value).</p>
<p id="p-0135" num="0137">When the face detection operation mode has been set, and when the photometric processing is to be performed using the face and the whole screen, the luminance data of the area including the face is respectively calculated, and a mean value of the luminance data of the calculated area is designated as luminance data Yface. The luminance data of areas Y<b>1</b> to Y<b>25</b> on the whole screen is respectively calculated, and a mean value of the luminance data of the calculated whole area is designated as luminance data Yall. The luminance data Yy (AE evaluation value)=Yface×n<b>1</b>+luminance data Yall×n<b>2</b> is then calculated, designating weighting to the face portion as n<b>1</b> and weighting to the whole screen as n<b>2</b> (provided that, n<b>1</b>&gt;n<b>2</b>, n<b>1</b>+n<b>2</b>=1).</p>
<p id="p-0136" num="0138">At the time of normal photometric processing, any one of the following methods is used to calculate the luminance data Yy (AE evaluation value), (1) center-weighted photometric method (a method of determining luminance data by placing emphasis on the brightness in the center and adding the ambient brightness), (2) partial photometric method (a method of determining luminance data only by a mean value of luminance data only within an area in the screen), (3) spot photometric method (having the same idea as the partial photometric method, but the photometric area is as small as about 1 to 2%), and (4) divided photometric method (a method of dividing a screen into a plurality of areas, to obtain a mean value of luminance for each area, and determining luminance data from the distribution pattern of the luminance).</p>
<p id="p-0137" num="0139">When the value of target luminance data Yx stored in the FLASH_ROM <b>115</b><i>b </i>in advance agrees with the value of luminance data Yy of the subject, being a result of measuring optical intensity of the subject, the CPU <b>115</b><i>a </i>judges it to be a proper exposure (brightness), calculates the exposure so that the luminance data Yy of the subject becomes the target luminance data Yx, and sets the exposure conditions (shutter speed of the CCD <b>102</b>, diaphragm stop (opening degree) of the diaphragm of the mechanical mechanism <b>101</b>, sensitivity of the CCD <b>102</b> (gain of an amplifier circuit in the F/E <b>103</b>)) based on the calculated exposure.</p>
<p id="p-0138" num="0140">Specifically, the CPU <b>115</b><i>a </i>carries out any one (or may be two or more) of “slowing down the shutter speed of the CCD <b>102</b>”, “increasing the size of the diaphragm”, and “increasing the sensitivity of the CCD <b>102</b>”, in order to increase the value of the luminance data Yy, and carries out the opposite processing in order to decrease the value of the luminance data Yy.</p>
<p id="p-0139" num="0141">The CPU <b>115</b><i>a </i>executes the control program to carry out face detection, but hardware dedicated for face detection may be provided.</p>
<p id="h-0015" num="0000">[First Example]</p>
<p id="p-0140" num="0142"><figref idref="DRAWINGS">FIG. 14</figref> is a flowchart which explains a first example of the AE operation. <figref idref="DRAWINGS">FIG. 15</figref> is a diagram which shows a display example on the liquid crystal monitor <b>207</b>. The first example of the AE operation will be explained in accordance with the flowchart shown in <figref idref="DRAWINGS">FIG. 14</figref>, with reference to the display example of the liquid crystal monitor <b>207</b> in <figref idref="DRAWINGS">FIG. 15</figref>. After the still picture mode or the motion picture mode has been selected by the mode changing switch <b>212</b> in the operation section <b>108</b>, when the release switch <b>211</b> is half-pushed to turn on the SW <b>1</b>, the CPU <b>115</b><i>a </i>in <figref idref="DRAWINGS">FIG. 2</figref> executes the AE processing.</p>
<p id="p-0141" num="0143">In <figref idref="DRAWINGS">FIG. 14</figref>, CPU <b>115</b><i>a </i>in <figref idref="DRAWINGS">FIG. 2</figref> first judges if the face detection operation mode for executing face detection of a human object has been set (step S<b>301</b>). The face detection operation mode is set on the user setup screen for recording conditions (see <figref idref="DRAWINGS">FIG. 10</figref> and <figref idref="DRAWINGS">FIG. 11</figref>). As a result of this judgment, when it is judged to be the face detection operation mode, the CPU <b>115</b><i>a </i>executes the face detection processing (step S<b>302</b>). The method of detecting the human object image from the subject image is the same as that in the first embodiment, and hence the detailed explanation thereof is omitted.</p>
<p id="p-0142" num="0144">The CPU <b>115</b><i>a </i>then judges whether the face has been detected (step S<b>303</b>). As a result of this judgment, when the face has been detected, the CPU <b>115</b><i>a </i>displays a rectangular frame on the face portion of the human object (displays face detection execution result OK), in the image data displayed on the liquid crystal monitor <b>207</b>, to thereby notify the user of the detection of the face (step S<b>304</b>). <figref idref="DRAWINGS">FIG. 15A</figref> shows a display example of the image data (monitored image) displayed on the liquid crystal monitor <b>207</b>. <figref idref="DRAWINGS">FIG. 15B</figref> shows a display example when the rectangular frame is displayed on the face portion of the human object in the image data shown in <figref idref="DRAWINGS">FIG. 15A</figref>.</p>
<p id="p-0143" num="0145">Subsequently, the CPU <b>115</b><i>a </i>judges if the photometric method has been set (step S<b>305</b>). The photometric method is set on the user setup screen for recording conditions (see <figref idref="DRAWINGS">FIG. 10</figref> and <figref idref="DRAWINGS">FIG. 11</figref>). As a result of this judgment, when the photometric method has been set, control proceeds to step S<b>307</b>.</p>
<p id="p-0144" num="0146">When the photometric method has not been set at step S<b>305</b>, the CPU <b>115</b><i>a </i>executes the photometric automatic judgment processing (step S<b>306</b>), and control proceeds to step S<b>307</b>. In this photometric automatic judgment processing, selection is carried out, based on the ratio of the face occupying in the screen or the position of the face or both, either of a “first photometric method in which only the face portion is optically measured, to calculate the luminance data Yy (AE evaluation value), to calculate the exposure”, or a “second photometric method in which, based on the photometric result of the face and the photometric result of the whole screen, the luminance data Yy (AE evaluation value) is calculated, by placing emphasis on the photometric result of the face (by giving more weight to the photometric result of the face), to thereby calculate the exposure”. Specifically, when the ratio of the face occupying in the screen is large or the face is in the central position of the screen or both, the first photometric method is selected, and in other cases, the second photometric method is selected.</p>
<p id="p-0145" num="0147">When the selected photometric method is the first photometric method, the CPU <b>115</b><i>a </i>carries out the photometric processing, designating the face portion as the photometric area (AE area) (step S<b>308</b>). The CPU <b>115</b><i>a </i>calculates the exposure based on the photometric result in the photometric area (luminance data Yy (AE evaluation value)) (step S<b>309</b>), and executes the exposure setup processing, to thereby set the exposure conditions based on the calculated exposure (step S<b>310</b>).</p>
<p id="p-0146" num="0148">At step S<b>307</b>, when the selected photometric method is the second photometric method, the CPU <b>115</b><i>a </i>executes the photometric processing for the whole screen, to optically measure the optical intensity on the whole screen (step S<b>311</b>) The CPU <b>115</b><i>a </i>carries out the photometric processing, designating the face portion as the photometric area (AE area) (step S<b>312</b>). Based on the photometric result of the face and the photometric result of the whole screen, the CPU <b>115</b><i>a </i>calculates the luminance data Yy (AE evaluation value), by placing emphasis on the photometric result of the face (by giving more weight to the photometric result of the face), corresponding to the ratio of the face occupying in the screen or the position of the face or both, to thereby calculate the exposure (step S<b>313</b>) based on the calculated luminance data Yy (AE evaluation value) The CPU <b>115</b><i>a </i>then executes the exposure setup processing, to thereby set the exposure conditions based on the calculated exposure (step S<b>310</b>).</p>
<p id="p-0147" num="0149">When it is judged that the face detection operation mode has not been set at step S<b>301</b>, or when the face has not been detected at step S<b>303</b>, the CPU <b>115</b><i>a </i>executes the normal photometric processing (step S<b>314</b>), calculates the exposure based on the photometric result (step S<b>315</b>), and carries out the exposure setup processing to thereby set the exposure conditions based on the calculated exposure (step S<b>310</b>).</p>
<p id="p-0148" num="0150">As explained above, according to the first example, the CPU <b>115</b><i>a </i>detects a human face in the subject, and carries out exposure control based on the photometric result for the detected human face. As a result, regardless of the range and position of the human object, the human object can be properly exposed.</p>
<p id="p-0149" num="0151">According to the first example, the CPU <b>115</b><i>a </i>selects either of the first photometric method in which the exposure is calculated based on the photometric result of the human face, and the second photometric method in which weighting to the photometric result of the human face is increased compared with the photometric result of the whole screen, based on the photometric result of the human face and the photometric result of the whole screen, to thereby calculate the exposure, corresponding to the ratio of the face occupying in the screen or the position of the face or both. As a result, when the face occupying in the screen is large, or when the face is in the center, the first photometric method in which the exposure is calculated based on the photometric result of the human face is adopted, thereby priority can be given to the proper exposure of the human object.</p>
<p id="p-0150" num="0152">According to the first example, since a user can select the first photometric method and the second photometric (see <figref idref="DRAWINGS">FIG. 10</figref> and <figref idref="DRAWINGS">FIG. 11</figref>), the user can select either the photometric result of the whole screen or the photometric result of the face, to which much weight is given, thereby the user's photographing intention can be reflected.</p>
<p id="p-0151" num="0153">According to the first example, when the second photometric method is to be executed, the CPU <b>115</b><i>a </i>sets weighting to the photometric result of the face with respect to the photometric result of the whole screen, according to the ratio of the face occupying in the screen or the position of the face or both. Therefore, if the ratio of the face occupying in the screen is large, weighting to the photometric result of the face portion is increased, and if the ratio of the face occupying in the screen is small, weighting to the photometric result of the face portion is decreased, thereby enabling exposure well balanced between the face and the background.</p>
<p id="p-0152" num="0154">According to the first example, since the user can set permission/inhibition of the face detection (face detection operation mode) (see <figref idref="DRAWINGS">FIG. 10</figref> and <figref idref="DRAWINGS">FIG. 11</figref>), the user can select whether a human object is to be properly exposed.</p>
<p id="p-0153" num="0155">According to the first example, since a rectangular frame is displayed on the detected face on the liquid crystal monitor <b>207</b> to display for the user that the CPU <b>115</b><i>a </i>has succeeded in the face detection, the user can confirm whether the face detection is a success or not.</p>
<p id="p-0154" num="0156">According to the first example, on the user setup screen for recording conditions (see <figref idref="DRAWINGS">FIG. 10</figref> and <figref idref="DRAWINGS">FIG. 11</figref>) displayed on the liquid crystal monitor <b>207</b>, either the first photometric method or the second photometric method is selected. However, for example, on the screen where the monitored image is displayed on the liquid crystal monitor <b>207</b>, either the first photometric method or the second photometric method maybe selected. <figref idref="DRAWINGS">FIG. 17</figref> shows a display example of the screen for selecting the first photometric method or the second photometric method. In this figure, E<b>1</b> indicates the first selection method and E<b>2</b> indicates the second selection method, and the user selects one of those.</p>
<p id="h-0016" num="0000">[Second Example]</p>
<p id="p-0155" num="0157">In a second example of the AE operation, in the second photometric method (a method of calculating the exposure based on the photometric result of the human face and the photometric result of the whole screen, by giving more weight to the photometric result of the human face as compared with the photometric result of the whole screen), the user can set the weighting ratio between the face and the background.</p>
<p id="p-0156" num="0158"><figref idref="DRAWINGS">FIG. 18</figref> is a flowchart which explains the second example of the AE operation. <figref idref="DRAWINGS">FIG. 19</figref> is a diagram which shows a display example on the liquid crystal monitor <b>207</b>. The second example of the AE operation will be explained in accordance with the flowchart shown in <figref idref="DRAWINGS">FIG. 18</figref>, with reference to the display example of the liquid crystal monitor <b>207</b> in <figref idref="DRAWINGS">FIG. 19</figref>. After the still picture mode or the motion picture mode has been selected by the mode changing switch <b>212</b> in the operation section <b>108</b>, when the release switch <b>211</b> is half-pushed to turn on the SW <b>1</b>, the CPU <b>115</b><i>a </i>executes the AE processing.</p>
<p id="p-0157" num="0159">In <figref idref="DRAWINGS">FIG. 18</figref>, CPU <b>115</b><i>a </i>in <figref idref="DRAWINGS">FIG. 2</figref> first judges if the face detection operation mode for executing face detection of a human object has been set (step S<b>321</b>). The face detection operation mode is set on the user setup screen for recording conditions described above (see <figref idref="DRAWINGS">FIG. 10</figref> and <figref idref="DRAWINGS">FIG. 11</figref>). As a result of this judgment, when it is judged that the face detection operation mode has been set, the CPU <b>115</b><i>a </i>executes the face detection processing as in the first embodiment (step S<b>322</b>).</p>
<p id="p-0158" num="0160">The CPU <b>115</b><i>a </i>then judges whether the face has been detected (step S<b>323</b>). As a result of this judgment, when the face has been detected, the CPU <b>115</b><i>a </i>displays a rectangular frame on the face portion of the human object (displays face detection execution result OK), in the image data displayed on the liquid crystal monitor <b>207</b>, to thereby notify the user of the detection of the face (step S<b>24</b>). <figref idref="DRAWINGS">FIG. 15A</figref> shows a display example of the image data (monitored image) displayed on the liquid crystal monitor <b>207</b>. <figref idref="DRAWINGS">FIG. 15B</figref> shows a display example when the rectangular frame is displayed on the face portion of the human object in the image data shown in <figref idref="DRAWINGS">FIG. 15A</figref>.</p>
<p id="p-0159" num="0161">Subsequently, the CPU <b>115</b><i>a </i>executes weighting user setup processing for the face portion (step S<b>325</b>). Specifically, in the weighting user setup processing of the face portion, the CPU <b>115</b><i>a </i>displays a screen for setting a weighting ratio between the face and the whole screen (background). <figref idref="DRAWINGS">FIG. 19</figref> shows a display example of a screen for setting the weighting ratio between the face and the whole screen (background). The user moves the position of an arrow displayed on the screen to a desired position by the cursor key <b>215</b>, and presses the ENTER key <b>217</b>, thereby the weighting ratio between the face and the whole screen (background) is set.</p>
<p id="p-0160" num="0162">Thereafter, the CPU <b>115</b><i>a </i>executes the photometric processing for the whole screen, to thereby measure the optical intensity on the whole screen (step S<b>326</b>). The CPU <b>115</b><i>a </i>carries out the photometric processing, designating the face portion as the photometric area (AE area) (step S<b>327</b>). Based on the photometric result of the face portion and the photometric result of the whole screen, the CPU <b>115</b><i>a </i>calculates the luminance data Yy (AE evaluation value), by putting weight set by the user, to thereby calculate the exposure (step S<b>328</b>) based on the calculated luminance data Yy (AE evaluation value). The CPU <b>115</b><i>a </i>then executes the exposure setup processing, to thereby set the exposure conditions based on the calculated exposure (step S<b>329</b>).</p>
<p id="p-0161" num="0163">When it is judged that the face detection operation mode has not been set at step S<b>321</b>, or when the face has not been detected at step S<b>323</b>, the CPU <b>115</b><i>a </i>executes the normal photometric processing (step S<b>330</b>), calculates the exposure based on the photometric result, to set the exposure conditions based on the calculated exposure (step S<b>329</b>).</p>
<p id="p-0162" num="0164">As explained above, according to the second example, a user can set weighting to the photometric result of the human face with respect to the photometric result of the whole screen. Therefore, when the exposure is adjusted based on the photometric result of the human face and the photometric result of the whole screen, it is possible to reflect the user's photographing intention.</p>
<p id="p-0163" num="0165">The present invention is not limited to the first example and the second example, various modifications are possible within the range that does not change the essential point of the invention, and for example, modifications described below are possible.</p>
<p id="h-0017" num="0000">[First Modification Example]</p>
<p id="p-0164" num="0166">A human object photographing mode for taking a picture of a human object (first photographing mode) and a landscape photographing mode for taking a picture of landscape (second photographing mode) are provided in the digital camera of the present invention, such that the mode can be selected by, for example, the mode changing switch <b>212</b>. When the human object photographing mode is selected, the CPU <b>115</b><i>a </i>performs exposure control accompanied with the face detection processing (processing after “YES” at step S<b>1</b> in <figref idref="DRAWINGS">FIG. 14</figref>). When the landscape photographing mode is selected, the CPU <b>115</b><i>a </i>does not perform the face detection processing, and may perform the exposure control (processing after “NO” at step S<b>1</b> in <figref idref="DRAWINGS">FIG. 14</figref>) by the normal photometric method. This is because, in the human object photographing mode in which it is necessary to properly expose the face, the face detection processing is necessary, but in the landscape photographing mode in which it is not necessary to properly expose the face, the face detection processing is not necessary.</p>
<p id="h-0018" num="0000">[Second Modification Example]</p>
<p id="p-0165" num="0167">In the first example and the second example, the face detection execution result is displayed on the liquid crystal monitor <b>207</b>, but it may be displayed on the sub-LCD <b>208</b>. <figref idref="DRAWINGS">FIG. 16</figref> shows a display example on the sub-LCD <b>208</b>, when the CPU <b>115</b><i>a </i>has succeeded in the face detection. An LED may be provided for displaying the face detection execution result, instead of the sub-LCD <b>208</b>.</p>
<p id="h-0019" num="0000">(Third Embodiment)</p>
<p id="p-0166" num="0168">A digital camera according to a third embodiment will be explained, with reference to <figref idref="DRAWINGS">FIG. 2</figref> and <figref idref="DRAWINGS">FIG. 20</figref> to <figref idref="DRAWINGS">FIG. 26</figref>. In the third embodiment, a digital camera that can automatically judge if photographing of a human object is to be performed, and automatically performs electronic flashing suitable for photographing of a human object, without troubling the user, will be explained. The visual configuration and the block configuration of the digital camera according to the third embodiment are the same as those in the first embodiment (<figref idref="DRAWINGS">FIG. 1</figref> and <figref idref="DRAWINGS">FIG. 2</figref>), and hence explanation of the common parts is omitted, and only the different part will be explained.</p>
<p id="p-0167" num="0169"><figref idref="DRAWINGS">FIG. 20</figref> shows the user setup screen for recording conditions displayed on the liquid crystal monitor <b>207</b>, when the SETUP mode is selected by the mode changing switch <b>212</b> in the operation section <b>108</b> in <figref idref="DRAWINGS">FIG. 2</figref>. The user setup screen for recording conditions shown in this figure is a screen for a user to set setting condition for each item ([AF/Manual] in “Focus”, [ON/OFF] in “Face detection (face detection operation mode)”, [Auto/manual (weak user preset value, large, medium, small) in “Face detection automatic weak emission (setting of weak preset value”, [ON/OFF] in “Red-eye preventing emission (red-eye emission mode)”, [ON/OFF] in “Face detection backlight correction (permission of electronic flashing at the time of judging backlight)”, [Auto/manual] in “Photometric point”, [ON/OFF] in “Long time exposure”, and [ON/OFF] in “With date”). In the screen shown in this figure, a desired item is selected by the cursor key <b>215</b>, a desired setting condition is selected by the SELECT key <b>218</b> in the selected item, and the selected setting condition is set by the ENTER key <b>217</b>.</p>
<p id="p-0168" num="0170">As described below, when the user selects [Auto] in “Face detection automatic weak emission (setting of weak preset value”, the CPU <b>115</b><i>a </i>sets the weak preset value. On the other hand, when the user selects [Manual] in “Face detection automatic weak emission (setting of weak preset value”, the user further sets either one of [Large, Medium, Small] in the weak preset value (weak preset value set by user).</p>
<p id="p-0169" num="0171">In the example shown in this figure, the setting is made such that “Focus” is [AF], “Face detection (face detection operation mode)” is [ON], “Face detection automatic weak emission (setting of weak preset value” is [Auto], “Red-eye preventing emission (red-eye emission mode)” is [ON], “Face detection backlight correction (permission of electronic flashing at the time of judging backlight)” is [ON], “Photometric point” is [Auto], “Long time exposure” is [OFF], and “With date” is [OFF].</p>
<p id="p-0170" num="0172">The CPU <b>115</b><i>a </i>also controls the monitoring operation, the AF operation, the AE operation, the photographing operation, the electronic flashing operation and the like. Further, the CPU <b>115</b><i>a </i>comprises a still picture mode for recording still pictures and a motion picture mode for recording motion pictures and speech, and controls execution of each mode.</p>
<p id="p-0171" num="0173"><figref idref="DRAWINGS">FIG. 21</figref> is a block diagram which shows a detailed configuration of the electronic flash unit <b>203</b> in <figref idref="DRAWINGS">FIG. 2</figref>. The electronic flash unit <b>203</b> has an xenon tube, and comprises a light emitter <b>301</b> which emits electronic flash light, a main capacitor <b>302</b> which accumulates electric charge for light emission of the xenon tube in the light emitter <b>301</b>, a charging circuit (booster circuit) <b>303</b> for charging the main capacitor <b>302</b>, a light emission starting (trigger) circuit <b>304</b> which starts light emission of the xenon tube in the light emitter <b>301</b>, a light emission stopping circuit <b>305</b> which stops light emission of the xenon tube in the light emitter <b>301</b>, and a reflected light receiver <b>306</b> which receives the reflected light from a subject and outputs the light to the light emission stopping circuit <b>305</b>.</p>
<p id="p-0172" num="0174">The operation of the electronic flash unit <b>203</b> will be explained below. The CPU <b>115</b><i>a </i>outputs a charging start signal to the charging circuit <b>303</b> at the time of charging. When the charging start signal is input from the CPU <b>115</b><i>a, </i>the charging circuit <b>303</b> raises the pressure of the output voltage input from the DC/DC converter <b>113</b>, to thereby start charging of the main capacitor <b>302</b>. When charging is started, the CPU <b>115</b><i>a </i>regularly monitors the charging voltage of the main capacitor <b>302</b> via the charging circuit <b>303</b>. When the main capacitor <b>302</b> has been charged up to an electronic flashing available voltage, which has been preset, the CPU <b>115</b><i>a </i>outputs a charging stop signal to the charging circuit <b>303</b>. The charging circuit <b>303</b> stops charging of the main capacitor <b>302</b>, when the charging stop signal is input from the CPU <b>115</b><i>a. </i></p>
<p id="p-0173" num="0175">When the CPU <b>115</b><i>a </i>allows the xenon tube in the light emitter <b>301</b> to emit light, the CPU <b>115</b><i>a </i>outputs a light emission signal to the light emission starting circuit <b>304</b>. When the light emission signal is input from the CPU <b>115</b><i>a, </i>the light emission starting circuit <b>304</b> starts light emission from the xenon tube in the light emitter <b>301</b>. The reflected light receiver <b>306</b> receives the reflected light from the subject, and outputs the light to the light emission stopping circuit <b>305</b>. The light emission stopping circuit <b>305</b> integrates the reflected light input from the reflected light receiver <b>306</b>. When the integral quantity of the reflected light becomes a predetermined value, the light emission stopping circuit <b>305</b> stops light emission from the xenon tube in the light emitter <b>301</b> (normal electronic flashing (main light emission)). When the integral quantity of the reflected light does not reach the predetermined value, even if a certain period of time has passed, or when the xenon tube is made to emit light only for very short time, like red-eye preventing emission, the CPU <b>115</b><i>a </i>outputs a light emission stop signal to the light emission stopping circuit <b>305</b>, and the light emission stopping circuit <b>305</b> stops light emission from the xenon tube in response to this signal.</p>
<p id="p-0174" num="0176">The outline of the electronic flashing operation which is executed under control of the CPU <b>115</b><i>a </i>will be explained below.</p>
<p id="h-0020" num="0000">[Electronic Flashing Operation]</p>
<p id="p-0175" num="0177">After the still picture mode or the motion picture mode has been selected by the mode changing switch <b>212</b> in the operation section <b>108</b> in <figref idref="DRAWINGS">FIG. 2</figref>, and in the state with the electronic flash switch <b>213</b> turned ON, when the release switch <b>211</b> is fully pushed to turn on the SW <b>2</b>, the CPU <b>115</b><i>a </i>executes the electronic flashing operation. Specifically, in the electronic flashing operation, when the face detection operation mode has been set (see <figref idref="DRAWINGS">FIG. 20</figref>), the face detection processing is executed to detect a human face. Specifically, the image data input from the CCD <b>102</b> is processed by the F/E (CDS, AGC, A/D) <b>103</b>, and input to the IPP <b>104</b>. After signal processing has been carried out in the IPP <b>104</b>, it is written in the DRAM <b>107</b>. The CPU <b>115</b><i>a </i>detects a human face from image data stored in the DRAM <b>107</b>, by using the known face recognition technique (Gabor Wavelet transform+graph matching). When the human face is detected, it can be judged that photographing of a human object is to be carried out. The CPU <b>115</b><i>a </i>controls the electronic flash unit <b>203</b>, based on the face detection result. In other words, in the digital camera in this embodiment, a human face is detected to automatically judge that photographing of a human object is to be carried out, the electronic flash unit <b>203</b> is automatically controlled without troubling a user, and the human object is photographed with proper brightness. Here, the CPU <b>115</b><i>a </i>executes the control program to perform face detection, but hardware dedicated for face detection may be provided.</p>
<p id="p-0176" num="0178"><figref idref="DRAWINGS">FIG. 22</figref> is a flowchart which explains the electronic flashing operation of the digital camera according to this embodiment. The electronic flashing operation of the digital camera shown in <figref idref="DRAWINGS">FIG. 2</figref> will be explained, with reference to the flowchart in <figref idref="DRAWINGS">FIG. 22</figref>. A case that the face detection operation mode has been set will be explained.</p>
<p id="h-0021" num="0000">[Light Emission Request Initializing Processing]</p>
<p id="p-0177" num="0179">In <figref idref="DRAWINGS">FIG. 22</figref>, the CPU <b>115</b><i>a </i>in <figref idref="DRAWINGS">FIG. 2</figref> clears the electronic flashing request (step S<b>401</b>), and then clears the red-eye emission request (step S<b>402</b>). The CPU <b>115</b><i>a </i>then executes the face detection processing (step S<b>403</b>).</p>
<p id="p-0178" num="0180">The face detection processing will be explained below. The method of detecting a human object image facing the front from a subject image is the same as in the first embodiment, and hence the explanation thereof is omitted.</p>
<p id="p-0179" num="0181">The CPU <b>115</b><i>a </i>performs AE calculation processing for exposure, and performs AE calculation processing from the image data taken in from the CCD <b>102</b> (step S<b>404</b>). The AE calculation processing will be explained specifically. <figref idref="DRAWINGS">FIG. 12</figref> shows a division example when the screen of the CCD <b>102</b> (image frame) is divided into a plurality of areas Y<b>1</b> to Y<b>35</b>. <figref idref="DRAWINGS">FIG. 13</figref> is a diagram which shows one example of the image data of the subject in <figref idref="DRAWINGS">FIG. 12</figref>.</p>
<p id="p-0180" num="0182">The IPP <b>104</b> calculates luminance data Y<b>1</b> to Y<b>35</b> for each area Y<b>1</b> to Y<b>35</b>, whose optical intensity is to be measured, based on the image data (each pixel data (R, G, B)) input from the CCD <b>102</b>. The calculation equation of the luminance data is shown below. In the calculation equation, Rn, Gn and Bn indicate a mean value of each pixel data (R, G, B) in each area, respectively.
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?>Area 1<i>Y</i>1=0.299<i>R</i>1+0.587<i>G</i>1+0.114<i>B</i>1<?in-line-formulae description="In-line Formulae" end="tail"?>
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?>Area 2<i>Y</i>2=0.299<i>R</i>2+0.587<i>G</i>2+0.114<i>B</i>2<?in-line-formulae description="In-line Formulae" end="tail"?>
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?>Area <i>n Yn=</i>0.299<i>Rn+</i>0.587<i>Gn+</i>0.114<i>Bn</i><?in-line-formulae description="In-line Formulae" end="tail"?>
</p>
<p id="p-0181" num="0183">The IPP <b>104</b> calculates the luminance data Yy (photometric value), by using any one of the following methods, (1) center-weighted photometric method (a method of determining luminance data by placing emphasis on the brightness in the center and adding the ambient brightness), (2) partial photometric method (a method of determining luminance data only by a mean value of luminance data only within the area on the screen), (3) spot photometric method (having the same idea as the partial photometric method, but the photometric area is as small as about <b>1</b> to 2%), and (4) divided photometric method (a method of dividing a screen into a plurality of areas, to obtain a mean value of luminance for each area, and determining luminance data from the distribution pattern of the luminance).</p>
<p id="p-0182" num="0184">When the face detection operation mode has been set, the IPP <b>104</b> respectively calculates the luminance data of the area including the face, and calculates the mean value of the luminance data of the calculated area as luminance data YFace (brightness in the face portion). In the example shown in <figref idref="DRAWINGS">FIG. 13</figref>, a mean value of the luminance data of areas Y<b>14</b>, Y<b>20</b>, Y<b>21</b>, Y<b>26</b> and Y<b>27</b> is designated as luminance data Yface (brightness in the face portion).</p>
<p id="p-0183" num="0185">When the face detection operation mode has been set, the IPP <b>104</b> also respectively calculates the luminance data of the other area excluding the area including the face (areas around the face), and calculates the mean value of the luminance data of the calculated other areas as luminance data YA (ambient brightness). In the example shown in <figref idref="DRAWINGS">FIG. 13</figref>, a mean value of the luminance data of areas Y<b>1</b> to Y<b>13</b>, Y<b>15</b> to Y<b>19</b>, Y<b>22</b> to Y<b>25</b> and Y<b>28</b> to Y<b>35</b> is designated as luminance data YA (ambient brightness).</p>
<p id="h-0022" num="0000">[Electronic Flash Mode Confirmation Processing]</p>
<p id="p-0184" num="0186">The CPU <b>115</b><i>a </i>judges whether the electronic flash inhibiting mode has been set (step S<b>405</b>). Such an electronic flash inhibiting mode is set when the electronic flash switch is turned OFF. As a result of this judgment, when the electronic flash inhibiting mode has been set, the flow is terminated, and on the other hand, when the electronic flash inhibiting mode has not been set, control proceeds to step <b>406</b>.</p>
<p id="p-0185" num="0187">At step S<b>406</b>, the CPU <b>115</b><i>a </i>judges whether the forced light emission mode has been set. Such a forced light emission mode is set on the user setup screen for recording conditions described above (see <figref idref="DRAWINGS">FIG. 20</figref>). As a result of this judgment, when the forced light emission mode has been set, control proceeds to step S<b>408</b>, and on the other hand, when the forced light emission mode has not been set, control proceeds to step S<b>407</b>.</p>
<p id="p-0186" num="0188">At step S<b>407</b>, the CPU <b>115</b><i>a </i>refers to the photometric result of the AE calculation at step S<b>404</b> to judge whether the photometric value is less than a predetermined threshold (predetermined brightness). As a result of this judgment, if the photometric value is not less than the predetermined threshold (predetermined brightness), control proceeds to step S<b>409</b>, and on the other hand, if the photometric value is less than the predetermined threshold (predetermined brightness), the CPU <b>115</b><i>a </i>sets the electronic flashing request (step S<b>408</b>).</p>
<p id="h-0023" num="0000">[Backlight Judgment Processing]</p>
<p id="p-0187" num="0189">The CPU <b>115</b><i>a </i>judges whether the electronic flashing is permitted at the time of backlight judgment (step S<b>409</b>). Permission/inhibition of the electronic flashing at the time of backlight judgment is set on the user setup screen for recording conditions described above (see <figref idref="DRAWINGS">FIG. 20</figref>). As a result of this judgment, if electronic flashing is not permitted at the time of backlight judgment, control proceeds to step S<b>413</b>. On the other hand, if electronic flashing is permitted at the time of backlight judgment, the CPU <b>115</b><i>a </i>then judges whether a face is detected in the screen in the above face detection processing (step S<b>410</b>).</p>
<p id="p-0188" num="0190">As a result of this judgment, when the face has not been detected, control proceeds to step S<b>413</b>. On the other hand, at step S<b>410</b>, when the face has been detected, the CPU <b>115</b><i>a </i>refers to the photometric result of the AE calculation at step S<b>404</b> to judge whether the face portion is darker than the ambient portion by a predetermined value or more (step S<b>411</b>). As a result of this judgment, if the face portion is not darker than the ambient portion by a predetermined value or more, control proceeds to step S<b>413</b>. On the other hand, if the face portion is darker than the ambient portion by a predetermined value or more, the electronic flashing request is set (step S<b>412</b>) and control proceeds to step S<b>413</b>.</p>
<p id="p-0189" num="0191">Subsequently, the CPU <b>115</b><i>a </i>judges whether the electronic flashing request has been set in the processing up to now (step S<b>413</b>). As a result of this judgment, if the electronic flashing request has not been set, the flow is terminated, and on the other hand, if the electronic flashing request has been set, control proceeds to step S<b>414</b>.</p>
<p id="h-0024" num="0000">[Red-eye Emission Processing]</p>
<p id="p-0190" num="0192">At step S<b>414</b>, the CPU <b>115</b><i>a </i>judges whether the red-eye emission mode has been set. This red-eye emission mode is set on the user setup screen for recording conditions described above (see <figref idref="DRAWINGS">FIG. 20</figref>). As a result of this judgment, if the red-eye emission mode has been set, the CPU <b>115</b><i>a </i>sets the red-eye emission request (step S<b>417</b>), and control proceeds to step S<b>418</b>. On the other hand, if the red-eye emission mode has not been set, the CPU <b>115</b><i>a </i>judges whether a face has been detected in the screen (step S<b>415</b>). As a result of this judgment, if a face has not been detected in the screen, control proceeds to step S<b>418</b>. On the other hand, when a face has been detected in the screen, the CPU <b>115</b><i>a </i>judges whether permission of automatic red-eye emission has been set (step S<b>416</b>). This permission/inhibition of the automatic red-eye emission is set on the user setup screen for recording conditions described above (see <figref idref="DRAWINGS">FIG. 20</figref>). As a result of this judgment, if permission of automatic red-eye emission has not been set, control proceeds to step S<b>418</b>. On the other hand, if permission of automatic red-eye emission has been set, the CPU <b>115</b><i>a </i>sets the red-eye emission request (step S<b>417</b>) and control proceeds to step S<b>418</b>. At step S<b>418</b>, the CPU <b>115</b><i>a </i>calculates the electronic flashlight quantity.</p>
<p id="h-0025" num="0000">[Weak Emission Judgment Processing]</p>
<p id="p-0191" num="0193">At step S<b>419</b>, the CPU <b>115</b><i>a </i>judges whether a face has been detected in the screen. As a result of this judgment, if a face has not been detected in the screen, control proceeds to step S<b>424</b>. On the other hand, if a face has been detected in the screen, it is judged whether the area of a portion judged to be a face is larger than a predetermined value (step S<b>420</b>). As a result of this judgment, if the area of the portion judged to be a face is not larger than the predetermined value, control proceeds to step S<b>424</b>. On the other hand, if the area of the portion judged to be a face is larger than the predetermined value, the CPU <b>115</b><i>a </i>judges whether a weak preset value has been set (step S<b>421</b>). This weak preset value is set on the user setup screen for recording conditions described above (see <figref idref="DRAWINGS">FIG. 20</figref>). As a result of this judgment, when the weak preset value has been set, the CPU <b>115</b><i>a </i>subtracts the weak preset value set by the user from the calculated electronic flashing quantity (step S<b>422</b>) When the weak preset value has not been set, the CPU <b>115</b><i>a </i>subtracts a predetermined weak preset value from the calculated electronic flashing quantity (step S<b>423</b>).</p>
<p id="p-0192" num="0194">Subsequently, at step S<b>424</b>, the CPU <b>115</b><i>a </i>judges whether the red-eye emission request has been set. As a result of this judgment, if the red-eye emission request has been set, the CPU <b>115</b><i>a </i>performs the red-eye emission processing (step S<b>425</b>), and then performs the main emission processing (step S<b>426</b>). On the other hand, if the red-eye emission request has not been set, the CPU <b>115</b><i>a </i>performs the main emission processing (step S<b>426</b>) without performing the red-eye emission processing.</p>
<p id="p-0193" num="0195">As explained above, according to the third embodiment, the CPU <b>115</b><i>a </i>judges whether photographing of a human object is to be carried out, by detecting a human face, and when photographing of a human object is to be carried out and electronic flash is used, after the red-eye preventing emission has been performed, the main emission is to be carried out. As a result, it can be judged automatically that photographing of a human object is to be carried out, and red-eye emission can be performed automatically, thereby time and energy of a user for changing over to the red-eye emission mode can be omitted.</p>
<p id="p-0194" num="0196">According to the third embodiment, it is judged whether photographing of a human object is to be carried out by detecting a face. When photographing of a human object is to be carried out, and electronic flash is used, a user can set whether the red-eye preventing emission is to be carried out automatically. As a result, if the user does not want to use the red-eye emission, the user can inhibit the red-eye emission, thereby enabling photographing using a good shutter chance.</p>
<p id="p-0195" num="0197">According to the third embodiment, when the ratio of the face occupying in the screen is not smaller than a predetermined value, the CPU <b>115</b><i>a </i>judges that a close-up picture of a face is to be taken, and the electronic flash light quantity is set weak. Therefore, blanking of aport ion where the electronic flash light is strongly irradiated can be prevented. In this case, a user can set the quantity of the electronic flash light quantity to be weakened (user preset value), and hence photographing using the electronic flash can be performed depending on the situation.</p>
<p id="p-0196" num="0198">According to the third embodiment, the CPU <b>115</b><i>a </i>compares the brightness at a position which has been judged as a face with the ambient brightness, and when the brightness at the position which has been judged as a face is darker than the ambient brightness by a predetermined value or more, it is judged to be backlight, and electronic flash is automatically used. As a result, even if a human object is not at the center of the screen, it becomes possible to use the electronic flash for correcting the backlight.</p>
<p id="p-0197" num="0199">According to the third embodiment, when it is judged to be backlight, the user can set whether to use the electronic flash. Therefore, the user can select if backlight correction by the electronic flash is to be performed, thereby enabling photographing corresponding to the user's photographing intention and the situation.</p>
<p id="p-0198" num="0200">According to the third embodiment, the user can set whether face detection operation is to be performed. Hence, the user can select whether face detection operation is to be performed, according to if photographing of a human object is to be performed or not. As a result, convenience of the user can be improved.</p>
<p id="p-0199" num="0201">In the third embodiment, the face detection result of the face detection processing may be informed to a user. For example, when a face has been detected, the CPU <b>115</b><i>a </i>displays a rectangular frame on the face portion of a human object, in the image data displayed on the liquid crystal monitor <b>207</b>, to thereby notify the user that a face has been detected. <figref idref="DRAWINGS">FIG. 23</figref> shows a display example of image data (monitored image) displayed on the liquid crystal monitor <b>207</b>, and <figref idref="DRAWINGS">FIG. 24</figref> shows a display example when a rectangular frame A is displayed on the face portion of the human object in the image data shown in <figref idref="DRAWINGS">FIG. 23</figref>.</p>
<p id="h-0026" num="0000">(Fourth Embodiment)</p>
<p id="p-0200" num="0202">A digital camera according to a fourth embodiment will be explained below, with reference to <figref idref="DRAWINGS">FIG. 25</figref> to <figref idref="DRAWINGS">FIG. 31</figref>. In the fourth embodiment, a digital camera that performs photographing when a face image of a subject is recognized and each part of the face is judged, will be explained.</p>
<p id="p-0201" num="0203"><figref idref="DRAWINGS">FIG. 25</figref> is a block diagram of a digital camera according to the fourth embodiment. A digital camera <b>100</b> comprises, a lens <b>1</b> which collects an optical image of a subject, a diaphragm section <b>2</b> which converges beams of light collected by the lens <b>1</b>, a motor driver <b>11</b> which shifts a plurality of lenses (not shown) for focusing or drives the diaphragm section <b>2</b>, a CCD (Charge Coupled Device) <b>3</b> which photoelectrically exchanges an optical image which has passed through the lens <b>1</b> and the diaphragm section <b>2</b>, a CDS (Correlated Double Sampling) <b>4</b> which reduces noise included in the CCD <b>3</b>, an A/D converter <b>5</b> which converts an analog signal from the CCD <b>3</b> to a digital signal, a timing generator <b>13</b> which generates timing for the CCD <b>3</b>, the CDS <b>4</b> and the A/D converter <b>5</b>, a digital signal processing circuit <b>7</b> which carries out image processing in accordance with the image processing parameter, a frame memory <b>6</b> which stores the record of picked up pixels and the image-processed image, a display section <b>8</b> which displays the picked up image on an LCD (Liquid Crystal Display), an image compression and expansion circuit <b>9</b> which compresses image data having been processed by the digital signal processing circuit <b>7</b> or expands the image data to the original image data, a memory card <b>10</b> which stores image data compressed by the image compression and expansion circuit <b>9</b>, a microcomputer <b>14</b> (hereinafter, referred to as an MC) which executes a predetermined control based on the control program, an EEPROM (Electrically Erasable and Programmable Read Only Memory) <b>16</b> which stores parameters, a camera operation section <b>17</b> which has a release button or the like for an operator to operate the camera body, an OSD (On-Screen Display) <b>15</b> which monitors the number of films and the emission state of the electronic flash, an electronic flash <b>12</b> for illuminating the subject, and a face recognition processor <b>18</b> which performs recognition of a face image and judgment of each portion in the face image. The lens <b>1</b>, the diaphragm <b>2</b> and the CCD <b>3</b> mainly constitute an image pickup unit, and the display section <b>8</b>, the digital signal processing circuit <b>7</b> and the MC <b>14</b> mainly constitute a monitoring unit, and the camera operation section <b>17</b> mainly constitutes a selection unit.</p>
<p id="p-0202" num="0204">The outline of the operation of the digital camera <b>100</b> in this configuration will be explained, with reference to <figref idref="DRAWINGS">FIG. 25</figref>. When an operator looks at a subject through a finder (not shown), and pushes the release button in the camera operation section <b>17</b>, photographing is carried out. Specifically, the MC <b>14</b> detects the signal, and allows the motor driver <b>11</b> to drive the lens <b>1</b> and the diaphragm section <b>2</b>, to focus the CCD <b>3</b> on the image of the subject. If necessary, the electronic flash <b>12</b> is emitted. The series of operation is automatically carried out by the MC <b>14</b> based on the information from the sensor (not shown). The image brought into focus of the CCD <b>3</b> is taken out sequentially by a clock generated by the timing generator <b>13</b>, and the noise included in the data is reduced by the CDS<b>4</b>. Here, as the noise included in the output signal of the CCD <b>3</b>, reset noise is predominant. In order to reduce this noise, by subtracting a picture signal and reset noise included in the signal period from each other, and a reset noise included only in the field through period, the reset noises are cancelled. The analog signal is converted to a 10-bit digital signal by the AID converter <b>5</b>, and the digital signal is input to the digital signal processing circuit <b>7</b>, and temporarily stored in the frame memory <b>6</b>. In response to the instruction from the MC <b>14</b>, the data temporarily stored in the frame memory <b>6</b> is processed by parameters stored in the EEPROM <b>16</b>, and the processed image is stored again in the frame memory <b>6</b>. The white balance processing is included in this processing. Further, the data written in the frame memory <b>6</b> is transmitted to the control section in the display section <b>8</b>, and the contents are displayed on the LCD.</p>
<p id="p-0203" num="0205">The frame memory <b>6</b> is an image memory that can accumulate image data of at least one screen of the picked up image, and it uses a commonly used memory, for example, VRAM (Video Random Access Memory), SRAM (Static Random Access Memory), DRAM (Dynamic Random Access Memory) or SDRAM (Synchronous DRAM). When it is desired to record the image in the memory card <b>10</b>, due to the intention of the operator, the MC <b>14</b> executes control of transferring the image to the memory card <b>10</b> with respect to the digital signal processing circuit <b>7</b>, in response to the instruction of the camera operation section <b>17</b>. That is to say, the image-processed image is read out from the frame memory <b>6</b>, and transmitted to the image compression and expansion circuit <b>9</b>. Here, the image is compressed by the JPEG (Joint Photographic Experts Group) method, and stored in the memory card <b>10</b>. ADCT (adaptive discrete cosine) is used for this coding algorithm, and hierarchical coding is also employed in which an image having low resolution is first coded, and gradually the resolution becomes high. As described above, the memory card <b>10</b> is for compressing the image data stored in the frame memory <b>6</b>, and storing the compressed data. Alternatively, for example, the configuration may be such that the data is recorded in an internal memory of about 8 MB or a smart media compact flash memory.</p>
<p id="p-0204" num="0206">When the contents of the memory card <b>10</b> are displayed on the display section <b>8</b>, or when it is desired to transfer the image data by connecting with another PC by an external terminal, a desired memory card is inserted to a connector (not shown) of the camera body, and an instruction is given from the camera operation section <b>17</b>. The MC <b>14</b> then instructs the digital signal processing circuit <b>7</b> to read out the compressed image data in the memory card, and the image data is input to the image compression and expansion circuit <b>9</b>, and expanded in accordance with the expansion algorithm to be recovered, and the image is displayed on the display section <b>8</b>.</p>
<p id="p-0205" num="0207"><figref idref="DRAWINGS">FIG. 26</figref> is a schematic block diagram which shows a face recognition processor <b>18</b> in <figref idref="DRAWINGS">FIG. 25</figref>. The face recognition processor <b>18</b> comprises an image memory <b>20</b> which stores one image of the subject, an image fetch section <b>21</b> which takes an image from the image memory <b>20</b> to an other memory or register in a predetermined unit, a control section <b>22</b> which takes charge of the overall control, a face characteristic storage section <b>24</b> which stores a plurality of characteristics of a face, a recognition and judgment section <b>23</b> which recognizes a face from the data from the image fetch section <b>21</b> and the data from the face characteristic storage section <b>24</b> and judges each portion, an edge detector <b>26</b> which detects an edge detection value from the result data thereof, and an output section <b>25</b> which outputs the final judgment result to the outside. The image memory <b>20</b> may use the frame memory <b>6</b> in <figref idref="DRAWINGS">FIG. 25</figref>. The other portions may be realized by the MC <b>14</b> in <figref idref="DRAWINGS">FIG. 25</figref>, or may be realized by a dedicated LSI. The recognition and judgment section <b>23</b> and the face characteristic storage section <b>24</b> mainly constitute the face image recognition unit and the face portion judgment unit, and the edge detector <b>26</b> mainly constitutes the edge detection unit.</p>
<p id="p-0206" num="0208">Before explaining detection of the face image, known face image recognition technique will be explained schematically. Face recognition in the present invention needs not to be a level that can reliably recognize an individual face as in the conventional technique, and an either-or recognition level for recognizing that the subject is a face or other objects is sufficient. The object items in the face image recognition are largely divided into the following two items,
<ul id="ul0002" list-style="none">
    <li id="ul0002-0001" num="0209">(1) Identification of human object, To identify who the human object is, and</li>
    <li id="ul0002-0002" num="0210">(2) Identification of expression, To identify how the expression of the human object is.</li>
</ul>
</p>
<p id="p-0207" num="0211">In other words, it can be said that the identification of human object of (1) is structural recognition of the face and statistic identification. It can be said that the identification of expression is recognition of shape change of the face, and dynamic identification. It can be said that the present invention is simpler identification than the above two. As these methods of identification, there are (1) two-dimensional method and (2) three-dimensional method, and in the computers, (1) two-dimensional method is mainly used. Detailed contents thereof are omitted herein. As a method of identification of expression, there is a concept by means of the FACS (Facial Action unit System), and by using this, the expression can be expressed by using the position of a characteristic point of the expression component.</p>
<p id="p-0208" num="0212">Application of these techniques to an admission control system, a human object image database system, a recognition communication system and the like can be considered in the future.</p>
<p id="h-0027" num="0000">[First Example]</p>
<p id="p-0209" num="0213"><figref idref="DRAWINGS">FIG. 27</figref> is a flowchart which explains a first example of the digital camera. Explanation will be made, with reference to <figref idref="DRAWINGS">FIG. 27</figref>, together with <figref idref="DRAWINGS">FIG. 25</figref> and <figref idref="DRAWINGS">FIG. 26</figref>. The image data of the subject is photoelectrically exchanged by the CCD <b>3</b> in <figref idref="DRAWINGS">FIG. 25</figref> and image processed, and then stored in the image memory <b>20</b> in the face recognition processor <b>18</b> in <figref idref="DRAWINGS">FIG. 26</figref>. The image data is taken into the image fetch section <b>21</b> in a predetermined unit (frame, byte) (step S<b>1</b>). The image data is input to the recognition and judgment section <b>23</b>, and subjected to face recognition and judgment of each portion of the face, from the data in the face characteristic storage section <b>24</b> which stores various face characteristic data stored in advance. This recognition method is carried out based on various algorithms. The result thereof is transmitted in detail to the control section <b>22</b>, and the face image is recognized (step S<b>2</b>), and each portion is judged from the data (step S<b>3</b>). As a result, when the face image is recognized and each portion is judged (YES route), the control section <b>22</b> outputs a result signal to the output section <b>25</b>, thereby the release button (not shown) in the camera operation section <b>17</b> is automatically pushed (step S<b>4</b>). Thereby, photographing of the subject is carried out. On the other hand, when the face image is not detected at step S<b>2</b> and step S<b>3</b>, or when each portion is not judged (NO route), the control section <b>22</b> outputs a result signal to the output section <b>25</b>, and returns to step <b>51</b> to continue the face image recognition and judgment operation.</p>
<p id="p-0210" num="0214">As described above, most probable subject of the camera is a human object. Therefore, when a human object is to be photographed, it is important to photograph without losing a shutter chance. For this purpose, it has to be recognized that the subject is a human object. The most suitable portion to identify a human object is a face. A clear image can be photographed only when the face image is extracted from the subject, and each part such as the eyes, the mouth, the nose and the eyebrows can be judged from the face image. Particularly, since the digital camera stores the digitalized image data, analysis by the computer is easy. As a result, the operability is improved, and a clear image can be photographed without losing a shutter chance.</p>
<p id="p-0211" num="0215">Judgment of the face image is carried out by catching the outline of the face and features of shapes of the eyes, the nose, the eyebrows and the ears. If it is to be judged simply as a face, the face can be judged just by catching the outline. However, in order to clearly photograph the whole face, it is necessary to continue the judgment operation until all of the respective portions can be judged. Thereby, it is possible to photograph a clear image.</p>
<p id="h-0028" num="0000">[Second Example]</p>
<p id="p-0212" num="0216"><figref idref="DRAWINGS">FIG. 28</figref> is a flowchart which explains a second example of the digital camera. Explanation will be made, with reference to <figref idref="DRAWINGS">FIG. 28</figref>, together with <figref idref="DRAWINGS">FIG. 25</figref> and <figref idref="DRAWINGS">FIG. 26</figref>. The image data of the subject is photoelectrically exchanged by the CCD <b>3</b> in <figref idref="DRAWINGS">FIG. 1</figref> and image processed, and then stored in the image memory <b>20</b> in the face recognition processor <b>18</b>. The image data is taken into the image fetch section <b>21</b> in a predetermined unit (frame, byte) (step S<b>10</b>). The image data is input to the recognition and judgment section <b>23</b>, and subjected to face recognition and judgment of each portion of the face, from the data in the face characteristic storage section <b>24</b> which stores various face characteristic data stored in advance. This recognition method is carried out based on various algorithms. The result thereof is transmitted in detail to the control section <b>22</b>, and the face image is recognized (step S<b>11</b>), and each portion is judged from the data (step S<b>12</b>). As a result, when the face image is recognized and each portion is judged (YES route), the control section <b>22</b> outputs a result signal to the output section <b>25</b>, compresses the image data and stores the image data in the memory card <b>10</b> (step S<b>13</b>). On the other hand, when the face image is not detected at step S<b>11</b> and step S<b>12</b>, or when each portion is not judged (NO route), the control section <b>22</b> outputs a result signal to the output section <b>25</b>, and returns to step S<b>10</b> to continue the face image recognition and judgment operation.</p>
<p id="p-0213" num="0217">The memory capacity of the memory is limited. Further, a photographer may select a necessary image from the image recorded in the memory or delete an unnecessary image. At this time, when an unclear image is recorded in the memory, this operation becomes complicated, and the memory is used wastefully. However, by storing the image data only when all of the respective portions of the face image can be judged, the operability is improved and wasteful use of the memory can be avoided.</p>
<p id="h-0029" num="0000">[Third Example]</p>
<p id="p-0214" num="0218"><figref idref="DRAWINGS">FIG. 29</figref> is a flowchart which explains a third example of the digital camera. Explanation will be made, with reference to <figref idref="DRAWINGS">FIG. 29</figref>, together with <figref idref="DRAWINGS">FIG. 25</figref> and <figref idref="DRAWINGS">FIG. 26</figref>. The image data of the subject is photoelectrically exchanged by the CCD in <figref idref="DRAWINGS">FIG. 1</figref> and image processed, and then stored in the image memory <b>20</b> in the face recognition processor <b>18</b>. The image data is taken into the image fetch section <b>21</b> in a predetermined unit (frame, byte) (step S<b>15</b>). The image data is input to the recognition and judgment section <b>23</b>, and subjected to face recognition and judgment of each portion of the face, from the data in the face characteristic storage section <b>24</b> which stores various face characteristic data stored in advance. This recognition method is carried out based on various algorithms. The result thereof is transmitted in detail to the control section <b>22</b>, and the face image is recognized (step S<b>16</b>), and each portion is judged from the data (step S<b>17</b>). As a result, when the face image is recognized and each portion is judged (YES route), the area is divided into each part of the face (step S<b>18</b>), and the data is transmitted to the edge detector <b>26</b>. The edge detector <b>26</b> detects an edge for each area (step S<b>19</b>), and judges whether the edge in the eye area is not smaller than an edge threshold A (step S<b>20</b>), judges whether the edge in the nose area is not smaller than an edge threshold B (step S<b>21</b>), and judges whether the edge in the mouth area is not smaller than an edge threshold C (step S<b>22</b>). If the results of all of these judgments are YES, the control section <b>22</b> outputs a result signal to the output section <b>25</b>, thereby the release button (not shown) in the camera operation section <b>17</b> is automatically pushed (step S<b>23</b>). Thereby, photographing of the subject is carried out. On the other hand, when the edges are less than the threshold, respectively, at step S<b>20</b>, step S<b>21</b> and step S<b>22</b> (NO route), the control returns to step S<b>15</b> to continue the face image recognition and judgment operation and the edge detection operation.</p>
<p id="p-0215" num="0219">As described above, features of respective portions of the face are characterized by the edge. Therefore, the edge in each portion has a peculiar threshold, and it is possible to judge by comparing the edge detection value from the edge detector <b>26</b> with these thresholds. Thereby, control waits until the edge detection value exceeds the threshold, or the operation of the edge detector <b>26</b> is continued. As a result, a clear picture of the image of each portion can be reliably taken.</p>
<p id="p-0216" num="0220">In the above explanation, the edge detection operation is continued until the edge detection value exceeds the threshold. However, in this case, time may be required for the detection. Therefore, allowable ranges may be given to the detection time and the threshold, and when the tolerance is exceeded, the image data may be fetched, or the release button may be pushed to carry out photographing. Thereby, when the edge detection value from the edge detector <b>26</b> exceeds a predetermined threshold for a predetermined period of time, the image data is fetched, or the release button is pushed, and hence the operation time is accelerated.</p>
<p id="h-0030" num="0000">[Fourth Example]</p>
<p id="p-0217" num="0221"><figref idref="DRAWINGS">FIG. 30</figref> is a flowchart which explains a fourth example of the digital camera. Explanation will be made, with reference to <figref idref="DRAWINGS">FIG. 30</figref>, together with <figref idref="DRAWINGS">FIG. 25</figref> and <figref idref="DRAWINGS">FIG. 26</figref>. The image data of the subject is photoelectrically exchanged by the CCD in FIG. land image processed, and then stored in the image memory <b>20</b> in the face recognition processor. The image data is taken into the image fetch section <b>21</b> in a predetermined unit (frame, byte) (step S<b>30</b>). The image data is input to the recognition and judgment section <b>23</b>, and subjected to face recognition and judgment of each portion of the face, from the data in the face characteristic storage section <b>24</b> which stores various face characteristic data stored in advance. This recognition method is earned out based on various algorithms. The result thereof is transmitted in detail to the control section <b>22</b>, and the face image is recognized (step S<b>31</b>), and each portion is judged from the data (step S<b>32</b>). As a result, when the face image is recognized and each portion is judged (YES route), it is displayed on the LCD in the display section <b>8</b> in <figref idref="DRAWINGS">FIG. 1</figref> that photographing is possible (step S<b>33</b>). When it is judged NO at step S<b>31</b> or step S<b>32</b>, it is judged whether predetermined time has passed (step S<b>34</b>), and if the predetermined time has passed (YES route), it is displayed on the LCD in the display section <b>8</b> in <figref idref="DRAWINGS">FIG. 25</figref> that photographing is not possible (step S<b>35</b>).</p>
<p id="p-0218" num="0222">As described above, it is necessary for a photographer to recognize when judgment of the face image has finished. Therefore, it is preferable that a liquid crystal display unit or the like be provided in the camera. Thereby, when the conditions for photographing are satisfied, this matter is displayed on the display section <b>8</b>. As a result, the shutter chance can be caught precisely, thereby improving the operability. If the conditions for photographing are not satisfied, this matter is displayed on the display section <b>8</b>. As a result, the photographer can carry out the subsequent operation quickly.</p>
<p id="h-0031" num="0000">[Fifth Example]</p>
<p id="p-0219" num="0223"><figref idref="DRAWINGS">FIG. 31</figref> is a flowchart which explains a fifth example of the digital camera. Explanation will be made, with reference to <figref idref="DRAWINGS">FIG. 31</figref>, together with <figref idref="DRAWINGS">FIG. 25</figref> and <figref idref="DRAWINGS">FIG. 26</figref>. The image data of the subject is photoelectrically exchanged by the CCD <b>3</b> in <figref idref="DRAWINGS">FIG. 1</figref> and image processed, and then stored in the image memory <b>20</b> in the face recognition processor <b>18</b>. The image data is taken into the image fetch section <b>21</b> in a predetermined unit (frame, byte) (step S<b>40</b>). The image data is input to the recognition and judgment section <b>23</b>, and subjected to face recognition and judgment of each portion of the face, from the data in the face characteristic storage section <b>24</b> which stores various face characteristic data stored in advance. This recognition method is carried out based on various algorithms. The result thereof is transmitted in detail to the control section <b>22</b>, and the face image is recognized (step S<b>41</b>), and each portion is judged from the data (step S<b>42</b>). As a result, when the face image is recognized and each portion is judged (YES route), a first warning sound is generated by a sound producing device (not shown) in the camera operation section <b>17</b> in <figref idref="DRAWINGS">FIG. 25</figref> (step S<b>43</b>). If the judgment result is NO at step S<b>41</b> or step S<b>42</b>, it is judged if predetermined time has passed (step S<b>44</b>), and when predetermined time has passed (YES route), a second warning sound is generated by the sound producing device (not shown) in the camera operation section <b>17</b> in <figref idref="DRAWINGS">FIG. 25</figref> (step S<b>45</b>).</p>
<p id="p-0220" num="0224">As described above, it is necessary for a photographer to recognize when judgment of the face image has finished. But the photographer does not always watch the display section. Therefore, by judging by sound, it becomes easy for the photographer to recognize. As a result, when photographing conditions are satisfied, sound is generated by the sound producing device, and hence the user will not fail to notice the display, thereby further improving the operability. After the predetermined time has passed, when the photographing conditions are not satisfied, the image pickup device generates sound by the sound producing device. Thereby, the photographer can carry out the subsequent operation quickly. If the sound producing device generates only one type of sound, regardless of the warning content, it cannot be judged which warning it is. Therefore, it is preferable to change the sound depending on the warning content. As a result, the warning content can be judged, thereby further improving the operability.</p>
<p id="p-0221" num="0225">In the explanation above, the photographer does not always photograph a human object, or there is a human object which the photographer does not want to photograph. Therefore, if the photographer can select either of automatic pressing of the release button and manual pressing thereof, the operability of the photographer can be further improved.</p>
<p id="h-0032" num="0000">(Fifth Embodiment)</p>
<p id="p-0222" num="0226">A digital camera according to a fifth embodiment will be explained below, with reference to <figref idref="DRAWINGS">FIG. 25</figref> and <figref idref="DRAWINGS">FIG. 32</figref> to <figref idref="DRAWINGS">FIG. 36</figref>. In the fifth embodiment, such a digital camera that face image is judged, to thereby cut out a face image whose size and inclination are automatically corrected, and the cut out face image is stored as a separate image file will be explained. The block configuration of the digital camera according to the fifth embodiment is the same as that of the fourth embodiment (<figref idref="DRAWINGS">FIG. 25</figref>), and the common portions will not be explained here, and only a different portion will be explained.</p>
<p id="p-0223" num="0227"><figref idref="DRAWINGS">FIG. 32</figref> is a schematic block diagram of the face recognition processor <b>18</b> in <figref idref="DRAWINGS">FIG. 25</figref>. As shown in <figref idref="DRAWINGS">FIG. 32</figref>, the face recognition processor <b>18</b> comprises, an image memory <b>20</b> which stores one image of the subject, an image fetch section <b>21</b> which takes an image from the image memory <b>20</b> to another memory or register in a predetermined unit, a control section <b>22</b> which takes charge of the overall control, a face characteristic storage section <b>24</b> which stores a plurality of characteristics of a face, a recognition and judgment section <b>23</b> which recognizes a face from the data from the image fetch section <b>21</b> and the data from the face characteristic storage section <b>24</b> and judges each portion, a face image cutting section <b>30</b> which cuts out a predetermined face image from the result data, and an output section <b>25</b> which outputs the final judgment result to the outside. The image memory <b>20</b> may use the frame memory <b>6</b> in <figref idref="DRAWINGS">FIG. 25</figref>. The other portions may be realized by the MC <b>14</b> in <figref idref="DRAWINGS">FIG. 25</figref>, or may be realized by a dedicated LSI. The recognition and judgment section <b>23</b> and the face characteristic storage section <b>24</b> mainly constitute the face image recognition unit and the face portion judgment unit.</p>
<p id="p-0224" num="0228"><figref idref="DRAWINGS">FIG. 33</figref> is a flowchart which explains the operation of the digital camera according to the fifth embodiment. <figref idref="DRAWINGS">FIG. 34</figref> to <figref idref="DRAWINGS">FIG. 36</figref> are diagrams which show a face image. The operation of the digital camera according to the fifth embodiment will be explained based on the flowchart in <figref idref="DRAWINGS">FIG. 33</figref>, with reference to <figref idref="DRAWINGS">FIG. 34</figref> to <figref idref="DRAWINGS">FIG. 36</figref>.</p>
<p id="p-0225" num="0229">In <figref idref="DRAWINGS">FIG. 33</figref>, the image data of the subject is photoelectrically exchanged by the CCD <b>3</b> in <figref idref="DRAWINGS">FIG. 25</figref> and image processed, and then stored in the image memory <b>20</b> in the face recognition processor <b>18</b> shown in <figref idref="DRAWINGS">FIG. 32</figref>. The image data is taken into the image fetch section <b>21</b> in a predetermined unit (frame, byte) (step S<b>101</b>). The image data is input to the recognition and judgment section <b>23</b>, and subjected to face recognition and judgment of each portion of the face, from the data in the face characteristic storage section <b>24</b> which stores various face characteristic data stored in advance. This recognition method is carried out based on various algorithms. The result thereof is transmitted in detail to the control section <b>22</b>, and the face image is recognized (step S<b>102</b>), and each portion (eyes, nose and mouth) is judged from the data (step S<b>103</b>). As a result, when the face image is recognized and each portion is judged (YES route), control proceeds to step S<b>104</b>.</p>
<p id="p-0226" num="0230">The control section <b>22</b> divides the area of a face image <b>1</b> for each part. Specifically, as shown in <figref idref="DRAWINGS">FIG. 34</figref>, the control section <b>22</b> divides the face image into three parts, that is, mouth part <b>3</b>, nose part <b>4</b>, and eye part <b>5</b> (step S<b>104</b>). .The control unit <b>22</b> then figures out center A of the mouth, center B of the nose and center C of the eyes (step S<b>105</b>), and calculates a straight line <b>2</b> closest to the three points A, B and C (step S<b>106</b>). The straight line <b>2</b> is cut out so that it becomes parallel with the vertical direction of a frame <b>14</b> of the cut out image. As described above, the most suitable portion for identifying a human object is a face, and hence a face image <b>1</b> is extracted from the subject image, and only when the respective parts such as the mouth <b>3</b>, the nose <b>4</b> and the eyes <b>5</b> are judged from the face image, a clear image can be photographed. The respective parts are arranged substantially symmetrically with respect to the central line <b>2</b> of the face. Therefore, if the straight line <b>2</b> connecting the central points A, B and C of the respective parts is cut out so as to be parallel with the vertical direction of the frame <b>14</b> of the face image <b>1</b>, the face can be cut out without being inclined.</p>
<p id="p-0227" num="0231">As shown in <figref idref="DRAWINGS">FIG. 35</figref>, right and left blank spaces <b>8</b> and <b>9</b> are determined from the size of the face image <b>1</b>, to figure out vertical cutout borders <b>6</b> and <b>7</b> (step S<b>107</b>). Subsequently, as shown in <figref idref="DRAWINGS">FIG. 36</figref>, top and bottom blank spaces <b>10</b> and <b>11</b> are determined from the size of the face image <b>1</b>, to figure out horizontal cutout borders <b>12</b> and <b>13</b> (step S<b>108</b>). The face image <b>1</b> is then cut out along the cutout borders <b>6</b>, <b>13</b>, <b>7</b> and <b>12</b> figured out at steps S<b>107</b> and S<b>108</b> (step S<b>109</b>).</p>
<p id="p-0228" num="0232">The most important point in the photograph for certificate is that the image quality is high and it is well balanced between the background and the face in the image face. The balance is determined by a ratio between the blank spaces right and left and top and bottom and the face on the face image face to be cut out. In general, the right and left blank spaces <b>8</b> and <b>9</b> are made equal, and the top and bottom blank spaces <b>10</b> and <b>11</b> are determined such that the bottom blank space <b>11</b> is made larger than the top blank space <b>10</b>. This is because there are a neck and a breast at the bottom. As described above, since the blank spaces right and left and top and bottom in the face image face to be cut out are at a certain ratio with respect to the size of the face photographed on the face image face to be cut out, a well-balanced photograph can be produced.</p>
<p id="p-0229" num="0233">A file of the cut out face image is prepared (step S<b>110</b>), and a file of the original image (photographed image) is prepared (step S<b>111</b>). When the cut out face image and the original image are filed, it is desired to associate these files with each other. As a method therefor, for example, there can be used a method in which the same file name is given thereto, but the type or extension is changed, such that the original image is named as Rimg0001.jpg, and the face image is named as Rimg0001.tif. The image file name is formed by 8 characters, and the upper 4 characters maybe formed by optional alphabets or figures, and the lower 4 characters may be formed by figures. Further, there may be adopted a method in which the upper 4 characters may be different between the original image and the face image, and the lower 4 characters may be the same value (original image, Rimg0001.jpg, face image, Rpic0001.jpg). As a result, the data of the recognized face image face and the data of the face image face cut out from the face image face are stored as files based on a predetermined relation, and hence organizing the original data and the cut out face data becomes easy.</p>
<p id="p-0230" num="0234">The feature of the photographs for certificate is that the background is painted in a particular color. This is because the human object becomes remarkable thereby. However, when preparing a photograph for certificate from a normal photograph, in many cases, some background is photographed. Therefore, the background may be painted in a predetermined color. The face image referred to herein indicates also a body portion integral with the face.</p>
<p id="p-0231" num="0235">If the image area to be cut out on the display section <b>8</b> can be made visual, the cut out area can be recognized most clearly. Therefore, it is desired to display the face image area to be cut out by a frame. As a result, the cut out area can be clearly recognized, and the operability is improved.</p>
<p id="p-0232" num="0236">The photographed image and the cut out image are not always necessary images. Therefore, the configuration may be such that at this time, a user observes these images on the display section <b>8</b>, and can select recording or deletion by the camera operation section <b>17</b>. As a result, the operability can be improved, and the effective use of the memory becomes possible.</p>
<p id="p-0233" num="0237">As described above, the most important point in the photograph for certificate is that the image quality is high and it is well balanced between the background and the face in the image face. Particularly, one index for judging the quality of the image is the area of the face. That is, it can be said that as this area increases, the image quality becomes better. Since the pixel density per unit area of the CCD, which photoelectrically exchanges the image, is constant, as this area increases, the sum total of number of pixels increases. Therefore, since the area and the image quality are in a proportional relation, the area of the face can be determined by the sum total of number of pixels which are occupied by the area, and hence the image quality can be judged accurately by a simple control. Therefore, when the area of the face in the image (image for one frame) displayed on the display section <b>8</b> is smaller than a predetermined area, this image may not be recorded. Thereby, wasteful use of the memory can be prevented, thereby it can be prevented to take a picture for certificate of poor quality.</p>
<p id="p-0234" num="0238">According to the image pickup device of a first aspect, in an image pickup device which has an automatic focusing function, the image pickup device comprises, an image pickup unit which inputs image data corresponding to a subject, a face detection unit which detects a human face from the image data input from the image pickup unit, and an automatic focusing control unit which performs automatic focusing control, designating at least a part of the human face detected by the face detection unit as a ranging area. Therefore, the automatic focusing operation can be executed by designating the human face in the screen as a ranging area, to bring the human face into focus, regardless of the position of the human object in the screen. As a result, photographing of the human object becomes possible, without changing the composition between at the time of the automatic focusing operation and at the time of photographing.</p>
<p id="p-0235" num="0239">According to the image pickup device of a second aspect, since a notification unit for notifying a user that a human face has been detected by the face detection unit is provided, it becomes possible to notify the user of the judgment result of face detection, so that the user can confirm whether the face detection has a malfunction, and hence photographing error by the user can be prevented, in addition to the effect of the invention according to the first aspect.</p>
<p id="p-0236" num="0240">According to the image pickup device of a third aspect, when a plurality of faces are detected by the face detection unit, the automatic focusing control unit carries out the automatic focusing control, designating at least a part of the face closest to the central portion as the ranging area. Hence, a main subject can be brought into focus at a high probability. Even in an aggregate photograph, it is possible to increase the possibility that faces at the front, back, right, and left of the face in the center are also brought into focus at a high probability.</p>
<p id="p-0237" num="0241">According to the image pickup device of a fourth aspect, the image pickup device comprises a ranging area selection unit by which, when a plurality of faces are detected by the face detection unit, a user selects the ranging area from the faces, and the automatic focusing control unit carries out the automatic focusing control, based on the ranging area selected by the ranging area selection unit. As a result, it is possible for a user to select which human object is to be focused.</p>
<p id="p-0238" num="0242">According to the image pickup device of a fifth aspect, the image pickup device comprises a face detection operation setup unit, by which a user sets permission/inhibition of the face detection operation of the face detection unit. When permission of the face detection operation of the face detection unit is set by the face detection operation setup unit, the automatic focusing control unit allows the face detection unit to execute the face detection operation, to thereby perform the automatic focusing control by designating at least a part of the detected human face as the ranging area. On the other hand, when inhibition of the face detection operation of the face detection unit is set by the face detection operation setup unit, the automatic focusing control unit carries out the automatic focusing control, without allowing the face detection unit to execute the face detection operation. As a result, the user sets permission/inhibition of the face detection operation, and hence unnecessary face detection operation can be omitted, thereby enabling prompt photographing.</p>
<p id="p-0239" num="0243">According to the image pickup device of a sixth aspect, the image pickup device comprises a mode setup unit which sets a first photographing mode for performing automatic focusing by executing the face detection operation of the face detection unit, or a second photographing mode for performing automatic focusing without executing the face detection operation of the face detection unit. When the first photographing mode is set by the mode setup unit, the automatic focusing control unit allows the face detection unit to execute the face detection operation, to thereby perform automatic focusing control by designating at least a part of the detected human face as the ranging area. On the other hand, when the second photographing mode is set by the mode setup unit, the automatic focusing control unit carries out the automatic focusing control, without allowing the face detection unit to execute the face detection operation. As a result, when the automatic focusing is to be performed, the human object photographing and the landscape photographing can be easily discriminated, by dividing the mode into a photographing mode in which face recognition is performed (first photographing mode) and a photographing mode in which face recognition is not performed (second photographing mode).</p>
<p id="p-0240" num="0244">According to the image pickup device of a seventh aspect, when a human face is detected, the face detection unit further detects eyes, and when the eyes are detected by the face detection unit, the automatic focusing control unit carries out the automatic focusing control, designating at least a part of the eyes as the ranging area. As a result, it is possible to take a picture in which the eyes are focused.</p>
<p id="p-0241" num="0245">According to the image pickup device of an eighth aspect, when a human face is detected, the face detection unit further detects the eyes, a nose and a mouth, and the image pickup device further comprises a second ranging area selection unit, by which when the eyes, the nose and the mouth are detected by the face detection unit, a user selects the ranging area from the eyes, the nose and the mouth. The automatic focusing control unit carries out the automatic focusing control, based on the ranging area selected by the second ranging area selection unit. Therefore, the user can bring a portion of a face into focus, where the user wants to focus, and hence the user's intention for photographing can be reflected.</p>
<p id="p-0242" num="0246">According to the image pickup device of a ninth aspect, the automatic focusing control unit performs automatic focusing control by setting the size of the ranging area, based on the size of the human face detected by the face detection unit. As a result, it is possible to improve the focusing accuracy.</p>
<p id="p-0243" num="0247">According to the image pickup device of a tenth aspect, the image pickup device further comprises a ranging area size selection unit for a user to select the size of the ranging area. The automatic focusing control unit performs the automatic focusing control by setting the ranging area of a size selected by the ranging area size selection unit. As a result, the focusing accuracy can be improved, when a particular portion in the face is brought into focus.</p>
<p id="p-0244" num="0248">According to the image pickup device of an eleventh aspect, every time the automatic focusing control unit performs the automatic focusing control, designating at least a part of the human face detected by the face detection unit as the ranging area, the automatic focusing control unit stores the ranging result in a memory, and predicts the next focusing position from the past ranging results stored in the memory. As a result, the next focusing position is predicted from the past focusing position, and focusing can be performed promptly, thereby enabling prompt photographing.</p>
<p id="p-0245" num="0249">According to an automatic focusing method of a twelfth aspect, the method comprises, an image input step of inputting image data corresponding to a subject, a face detection step of detecting a human face from the image data input at the image input step, and an automatic focusing control step of performing automatic focusing control, designating at least a part of the human face detected at the face detection step as a ranging area. Therefore, by executing the automatic focusing operation, designating the human face in the screen as the ranging area, the human face can be brought into focus, regardless of the position of the human object in the screen. As a result, photographing of a human object becomes possible, without changing the composition between at the time of the automatic focusing operation and at the time of photographing.</p>
<p id="p-0246" num="0250">According to a program to be executed by a computer according to a thirteenth aspect, a computer executes the program to realize an image input step of inputting image data corresponding to a subject, a face detection step of detecting a human face from the image data input at the image input step, and an automatic focusing control step of performing automatic focusing operation, designating at least a part of the human face detected at the face detection step as a ranging area. Therefore, by executing the program by a computer, automatic focusing operation is executed, designating the human face in the screen as the ranging area, the human face can be brought into focus, regardless of the position of a human object in the screen. As a result, photographing of the human object becomes possible, without changing the composition between at the time of the automatic focusing operation and at the time of photographing.</p>
<p id="p-0247" num="0251">According to an image pickup device of a fourteenth aspect, in an image pickup device which has an automatic exposure function, the device comprises, an image pickup unit which inputs image data corresponding to a subject, a face detection unit which detects a human face from the image data input from the image pickup unit, a photometric unit which measures optical intensity, designating the human face detected by the face detection unit as a photometric area, and an exposure control unit which calculates the exposure based on the photometric result of the human face by the photometric unit, and performs exposure control based on the calculated exposure. As a result, it is possible to properly expose the human object, regardless of the position and range of the human object in the screen.</p>
<p id="p-0248" num="0252">According to the image pickup device of a fifteenth aspect, the photometric unit further measures the optical intensity by designating the whole screen as the photometric area, and the exposure control unit calculates the exposure by giving more weight to the photometric result of the human face compared with the photometric result of the whole screen, based on the photometric result of the human face and the photometric result of the whole screen by the photometric unit. As a result, it is possible to properly expose the background as well as the human object.</p>
<p id="p-0249" num="0253">According to the image pickup device of a sixteenth aspect, the exposure control unit selects either of a first photometric method in which the exposure is calculated based on the photometric result of the human face, and a second photometric method in which the exposure is calculated by giving more weight to the photometric result of the human face compared with the photometric result of the whole screen, based on the photometric result of the human face and the photometric result of the whole screen, corresponding to the ratio of the face occupying in the screen or the position of the face or both. As a result, it becomes possible to prevent insufficient exposure for both the human object and the background.</p>
<p id="p-0250" num="0254">According to the image pickup device of a seventeenth aspect, the device further comprises a photometric method selection unit, by which a user can select either of the first photometric method in which the exposure is calculated based on the photometric result of the human face, and the second photometric method in which the exposure is calculated by giving more weight to the photometric result of the human face compared with the photometric result of the whole screen, based on the photometric result of the human face and the photometric result of the whole screen. Therefore, the user can select which is given much weight to either the photometric result of the whole screen or the photometric result of the face portion. As a result, the user's intention for photographing can be reflected.</p>
<p id="p-0251" num="0255">According to the image pickup device of an eighteenth aspect, the device comprises a weighting setup unit, by which a user sets weighting to the photometric result of the human face with respect to the photometric result of the whole screen. Therefore, when the exposure is adjusted based on the photometric result of the human face and the photometric result of the whole screen, the user's intention for photographing can be reflected.</p>
<p id="p-0252" num="0256">According to the image pickup device of a nineteenth aspect, the exposure control unit sets weighting to the photometric result of the human face with respect to the photometric result of the whole screen, based on the ratio of the face occupying in the screen or the position of the face or both. Therefore, if the ratio of the face occupying in the screen is large, weighting to the photometric result of the face portion is increased, and if the ratio of the face occupying in the screen is small, weighting to the photometric result of the face portion is decreased, thereby enabling well-balanced exposure between the face and the background.</p>
<p id="p-0253" num="0257">According to the image pickup device of a twentieth aspect, the device comprises a face detection operation setup unit, by which a user sets permission/inhibition of the face detection operation of the face detection unit. When permission of the face detection operation of the face detection unit is set, the exposure control unit allows the face detection unit to execute the face detection operation, to thereby perform the exposure control based on the photometric result of the detected human face. When inhibition of the face detection operation of the face detection unit is set, the exposure control unit performs the exposure control, without allowing the face detection unit to execute the face detection operation. As a result, the user can inhibit face detection, thereby the user can prevent execution of unnecessary face detection.</p>
<p id="p-0254" num="0258">According to the image pickup device of a twenty-first aspect, the device comprises a mode setup unit for setting a first photographing mode in which exposure control is performed by executing the face detection operation of the face detection unit, or a second photographing mode in which exposure control is performed without executing the face detection operation of the face detection unit. When the first photographing mode is set, the exposure control unit allows the face detection unit to execute the face detection operation, to thereby perform exposure control based on the photometric result of the detected human face. When the second photographing mode is set, the exposure control unit performs exposure control without allowing the face detection unit to execute the face detection operation. As a result, by changing the photographing mode, the user can easily select acceptance or refusal of the face detection, and hence photographing suitable for the application of the user can be performed.</p>
<p id="p-0255" num="0259">According to the image pickup device of a twenty-second aspect, since a notification unit for notifying a user of the detection result of the human face by the face detection unit is provided, the user can judge whether the face detection has been operated normally. When a malfunction occurs, photographing is stopped, thereby a photographing failure can be prevented.</p>
<p id="p-0256" num="0260">According to the image pickup device of a twenty-third aspect, the notification unit is a monitor screen for displaying the image data. Therefore, it becomes possible to notify the user of the result of the face detection operation, such as the face in the screen and the position and range of the detected face, on the monitor screen.</p>
<p id="p-0257" num="0261">According to the image pickup device of a twenty-fourth aspect, the notification unit is a sub-LCD for displaying the operation condition or the like of the image pickup device. As a result, in the image pickup device according to the ninth aspect, it becomes possible to notify a user of the face detection result in a cheap configuration.</p>
<p id="p-0258" num="0262">According to the image pickup device of a twenty-fifth aspect, since the notification unit is an LED, in an image pickup device which has no monitor screen or even when the user does not use the monitor screen in an image pickup device which has a monitor screen, it becomes possible to notify the user of the face detection result.</p>
<p id="p-0259" num="0263">According to the image pickup device of a twenty-sixth aspect, when a human face cannot be detected by the face detection unit, the exposure control unit performs exposure control based on the photometric result of another photometric method. Therefore, in addition to the effect of the image pickup device according to the first aspect, even if a face cannot be detected, photographing as good as the conventional image pickup device is made possible, without causing a big failure.</p>
<p id="p-0260" num="0264">According to the exposure control method of a twenty-seventh aspect, there are provided, an image input step of inputting image data corresponding to a subject, a face detection step of detecting a human face from the image data input at the image input step, a photometric step of measuring optical intensity, designating the human face detected at the face detection step as a photometric area, and an exposure control step of calculating the exposure based on the photometric result of the human face at the photometric step, and performs exposure control based on the calculated exposure. As a result, it becomes possible to properly expose a human object, regardless of the position and range of the human object in the screen.</p>
<p id="p-0261" num="0265">According to the program to be executed by a computer according to a twenty-eighth aspect, the computer executes the program to realize, an image input step of inputting image data corresponding to a subject, a face detection step of detecting a human face from the image data input at the image input step, a photometric step of measuring optical intensity, designating the human face detected at the face detection step as a photometric area, and an exposure control step of calculating the exposure based on the photometric result of the human face at the photometric step, and performing exposure control based on the calculated exposure. As a result, it becomes possible to properly expose a human object, regardless of the position and range of the human object in the screen.</p>
<p id="p-0262" num="0266">According to the image pickup device of a twenty-ninth aspect, in an image pickup device provided with an electronic flash function, the device comprises, an image pickup unit which inputs image data corresponding to a subject, a face detection unit which detects a human face from the image data input from the image pickup unit, an electronic flashing unit for emitting electronic flash light, and an electronic flash control unit which controls the electronic flashing unit based on the detection result of the human face by the face detection unit. As a result, there is the effect that it is judged whether a human object is to be photographed, and light emission suitable for the human object photographing can be automatically carried out.</p>
<p id="p-0263" num="0267">According to the image pickup device of a thirtieth aspect, when a human face is detected by the face detection unit, the electronic flash control unit allows the electronic flashing unit to perform red-eye preventing emission, and then to perform the main emission. As a result, there is the effect that it is automatically judged whether a human object is to be photographed, to automatically perform the red-eye preventing emission, and hence it is not necessary for a user to change to the red-eye emission mode.</p>
<p id="p-0264" num="0268">According to the image pickup device of a thirty-first aspect, the device comprises a red-eye preventing emission mode setup unit, by which a user sets a red-eye preventing emission mode in which red-eye preventing emission is carried out. When the red-eye preventing emission mode is set by the red-eye preventing emission mode setup unit, and when a human face is detected by the face detection unit, the electronic flash control unit allows the electronic flashing unit to perform red-eye preventing emission, and then to perform the main emission. When the red-eye preventing emission mode is not set by the red-eye preventing emission mode setup unit, and when a human face is detected by the face detection unit, the electronic flash control unit allows the electronic flashing unit to perform only the main emission, without allowing the electronic flashing unit to perform the red-eye preventing emission. Therefore, the user can select whether the red-eye photographing is to be performed or not, and when the user does not want to carry out the red-eye emission, the user can inhibit the red-eye emission. As a result, there is the effect that photographing using a good shutter chance becomes possible.</p>
<p id="p-0265" num="0269">According to the image pickup device of a thirty-second aspect, when a human face is detected by the face detection unit, and when a ratio of the face occupying in the screen is not smaller than a predetermined value, the electronic flash control unit sets the electronic flash light quantity of the electronic flashing unit to a weak level. Therefore, there is the effect that when a ratio of the face occupying in the screen is not smaller than a predetermined value, it is judged that close-up of a face is to be shot, and the electronic flash light quantity is set to a weak level, thereby preventing a blanking which occurs because the electronic flash light is irradiated on the face strongly.</p>
<p id="p-0266" num="0270">According to the image pickup device of a thirty-third aspect, since the device comprises a weak emission quantity setup unit, by which a user sets a quantity to weaken the electronic flash light quantity. Therefore, there is the effect that the user can set the quantity to weaken the electronic flash light, thereby enabling photographing corresponding to the situation.</p>
<p id="p-0267" num="0271">According to the image pickup device of a thirty-fourth aspect, the device comprises a backlight judgment unit which compares the brightness of the face portion detected by the face detection unit with the ambient brightness, and judges to be backlight when the brightness of the face portion is darker than the ambient brightness by at least a predetermined value. When it is judged to be backlight by the backlight judgment unit, the electronic flash control unit allows the electronic flashing unit to emit electronic flash light. Therefore, even if a human object is not in the center of the screen, it can be judged to be backlight, and hence there is the effect that electronic flash light can be emitted for correcting the backlight.</p>
<p id="p-0268" num="0272">According to the image pickup device of a thirty-fifth aspect, the device comprises a backlight electronic flash setup unit, by which a user sets whether the electronic flashing unit is allowed to emit electronic flash light, when it is judged to be backlight by the backlight judgment unit. As a result, there is the effect that the user can select whether backlight correction is to be performed by the electronic flash light, and hence it becomes possible to correspond to the user s intention for photographing and the situation.</p>
<p id="p-0269" num="0273">According to the image pickup device of a thirty-sixth aspect, the device comprises a face detection operation setup unit, by which a user sets whether the electronic flashing unit is to be controlled, based on the detection result of the human face by the face detection unit. Therefore, there is the effect that the user can inhibit the face detection operation, thereby enabling photographing corresponding to the situation.</p>
<p id="p-0270" num="0274">According to the image pickup device of a thirty-seventh aspect, the device comprises a notification unit for notifying a user of the detection result of a human face by the face detection unit. Therefore, the user can judge whether the face recognition processing has been operated normally, and if a malfunction occurs, photographing is stopped. As a result, there is the effect that a photographing failure can be prevented.</p>
<p id="p-0271" num="0275">According to an electronic flash control method of a thirty-eighth aspect, the method comprises an image input step of inputting image data corresponding to a subject, a face detection step of detecting a human face from the image data input by the image pickup unit, and an electronic flash control step of controlling the electronic flashing unit for emitting the electronic flash light, based on the detection result of a human face at the face detection step. As a result, there is the effect that it is automatically judged whether a human object photographing is to be performed, to automatically perform light emission suitable for human object photographing.</p>
<p id="p-0272" num="0276">According to a program to be executed by a computer according to a thirty-ninth aspect, the computer executes the program to realize an image input step of inputting image data corresponding to a subject, a face detection step of detecting a human face from the image data input by the image pickup unit, and an electronic flash control step of controlling the electronic flashing unit for emitting the electronic flash light, based on the detection result of a human face at the face detection step. As a result, there is the effect that it is automatically judged whether a object photographing is to be performed, to automatically perform light emission suitable for human object photographing.</p>
<p id="p-0273" num="0277">According to the image pickup device of a fortieth aspect, when each portion in the face image can be judged by the face portion judgment unit, a release button for executing photographing of the subject is automatically pressed, or the photographing operation is carried out. As a result, the operability is improved, and a clear image can be photographed, without losing a shutter chance.</p>
<p id="p-0274" num="0278">According to the image pickup device of a forty-first aspect, when all of the respective portions in the face image cannot be judged, the operation of the face portion judgment unit is continued until all of the respective portions in the face image can be judged. As a result, a clear image can be photographed.</p>
<p id="p-0275" num="0279">According to the image pickup device of a forty-second aspect, only when all of the respective portions in the face image can be judged, the image data is stored. As a result, the operability is improved, and wasteful use of the memory can be eliminated.</p>
<p id="p-0276" num="0280">According to the image pickup device of a forty-third aspect, control waits until the edge detection value exceeds the threshold, or the operation of the edge detection unit is continued, until the edge detection value exceeds the threshold. As a result, an image of each portion can be clearly photographed reliably.</p>
<p id="p-0277" num="0281">According to the image pickup device of a forty-fourth aspect, when the edge detection value of the edge detection unit exceeds a predetermined threshold for a predetermined period of time, the image data is taken in, or the release button is pressed. As a result, the operation time becomes fast.</p>
<p id="p-0278" num="0282">According to the image pickup device of a forty-fifth aspect, when photographing conditions are satisfied, this matter is displayed on the display unit. As a result, a shutter chance can be caught precisely, thereby improving the operability.</p>
<p id="p-0279" num="0283">According to the image pickup device of a forty-sixth aspect, when the photographing conditions are not satisfied, this matter is displayed on the display unit. As a result, the operation to be taken next by the photographer can be taken quickly.</p>
<p id="p-0280" num="0284">According to the image pickup device of a forty-seventh aspect, when the photographing conditions are satisfied, sound is generated by the warning unit. As a result, a user does not fail to notice the display, thereby further improving the operability.</p>
<p id="p-0281" num="0285">According to the image pickup device of a forty-eighth aspect, the warning unit generates sound, when the photographing conditions are not satisfied after a predetermined period of time has passed. As a result, the operation to be taken next by the photographer can be taken quickly.</p>
<p id="p-0282" num="0286">According to the image pickup device of a forty-ninth aspect, a frequency or a melody of the sound is changed when the image pickup device satisfies the photographing conditions or does not satisfy the photographing conditions. As a result, the warning contents can be judged, thereby further improving the operability.</p>
<p id="p-0283" num="0287">According to the image pickup device of a fiftieth aspect, when it is selected not to press the release button by the selection unit, the release button is to be pressed manually. Therefore, the operability of the operator is further improved.</p>
<p id="p-0284" num="0288">According to the image pickup device of a fifty-first aspect, when the face image is recognized by the face image recognition unit, the face image portion is cutout and stored as a separate image file. As a result, even if the face is at the edge of the image at the time of photographing, a photograph for certificate can be produced therefrom.</p>
<p id="p-0285" num="0289">According to the image pickup device of a fifty-second aspect, the face image portion is cut out so that a straight line connecting the substantial central points of each portion of the face becomes parallel with the vertical direction of an image frame to be cut out. As a result, the image can be cut out without the face being inclined.</p>
<p id="p-0286" num="0290">According to the image pickup device of a fifty-third aspect, since the blank spaces right and left and top and bottom in the cut out face image face are at a certain ratio with respect to the size of the face picked up on the face image face to be cut out, a well-balanced photograph can be produced.</p>
<p id="p-0287" num="0291">According to the image pickup device of a fifty-fourth aspect, the data of the recognized face image face and the data of the face image face cut out from the face image face are stored as a file, based on a predetermined relation. As a result, organizing the original data and the cut out face data becomes easy.</p>
<p id="p-0288" num="0292">According to the image pickup device of a fifty-fifth aspect, since portions other than the cut out face image are painted over by a predetermined color, the human object can be made more remarkable.</p>
<p id="p-0289" num="0293">According to the image pickup device of a fifty-sixth aspect, when the face image face is to be cut out, the face image face area to be cut out is displayed by a frame. As a result, the area to be cut out can be recognized clearly, thereby improving the operability.</p>
<p id="p-0290" num="0294">According to the image pickup device of a fifty-seventh aspect, since recording or deletion of the image can be selected by the selection unit, the operability can be improved and the effective use of the memory can be realized.</p>
<p id="p-0291" num="0295">According to the image pickup device of a fifty-eighth aspect, when the area of the face occupying in the photographed image is smaller than a predetermined area, the image is not recorded. As a result, wasteful use of the memory can be prevented, and a photograph for certificate having a poor quality can be prevented from being taken.</p>
<p id="p-0292" num="0296">According to the image pickup device of a fifty-ninth aspect, the area of the face is determined by a sum total of the number of pixels occupied by the area. As a result, the image quality can be accurately judged by simple control.</p>
<p id="p-0293" num="0297">The present document incorporates by reference the entire content s of Japanese priority documents, 2001-284162 filed in Japan on Sep. 18, 2001, 2001-284163 filed in Japan on Sep. 18, 2001, 2001-304342 filed in Japan on Sep. 28, 2001, 2001-304638 filed in Japan on Sep. 28, 2001 and 2001-304343 filed in Japan on Sep. 28, 2001.</p>
<p id="p-0294" num="0298">Although the invention has been described with respect to a specific embodiment for a complete and clear disclosure, the appended claims are not to be thus limited but are to be construed as embodying all modifications and alternative constructions that may occur to one skilled in the art which fairly fall within the basic teaching herein set forth.</p>
<?DETDESC description="Detailed Description" end="tail"?>
</description>
<us-claim-statement>What is claimed is:</us-claim-statement>
<claims id="claims">
<claim id="CLM-00001" num="00001">
<claim-text>1. An image pickup device provided with an automatic focusing function, comprising:
<claim-text>an image pickup unit configured to input image data corresponding to a subject;</claim-text>
<claim-text>a face detection unit configured to separately detect eyes, a nose, and a mouth of a human face from the image data input from the image pickup unit;</claim-text>
<claim-text>a ranging area selection unit, by which when the eyes, the nose, and the mouth are separately detected by the face detection unit, a user selects any one of the eyes, the nose, or the mouth as a ranging area; and</claim-text>
<claim-text>an automatic focusing control unit configured to perform automatic focusing control based on the ranging area selected by the ranging area selection unit.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00002" num="00002">
<claim-text>2. The image pickup device according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising:
<claim-text>a notification unit configured to notify a user that a human face has been detected by the face detection unit.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00003" num="00003">
<claim-text>3. The image pickup device according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein when a plurality of faces are detected by the face detection unit, the automatic focusing control unit carries out the automatic focusing control, designating at least a part of the face closest to the central portion as the ranging area.</claim-text>
</claim>
<claim id="CLM-00004" num="00004">
<claim-text>4. The image pickup device according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising:
<claim-text>a ranging area selection unit configured to receive a user input selecting a ranging area from a plurality of faces when a plurality of faces are detected by the face detection unit, wherein</claim-text>
<claim-text>the automatic focusing control unit is configured to perform the automatic focusing control, based on the ranging area selected by the ranging area selection unit.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00005" num="00005">
<claim-text>5. The image pickup device according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising:
<claim-text>a face detection operation setup unit configured to receive a user input setting permission/inhibition of the face detection operation of the face detection unit, wherein</claim-text>
<claim-text>when permission of the face detection operation of the face detection unit is set by the face detection operation setup unit, the automatic focusing control unit allows the face detection unit to execute the face detection operation, to thereby perform the automatic focusing control by designating at least a part of the detected human face as the ranging area, and when inhibition of the face detection operation of the face detection unit is set by the face detection operation setup unit, the automatic focusing control unit carries out the automatic focusing control, without allowing the face detection unit to execute the face detection operation.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00006" num="00006">
<claim-text>6. The image pickup device according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising:
<claim-text>a mode setup unit configured to set a first photographing mode for performing automatic focusing by executing the face detection operation of the face detection unit, or a second photographing mode for performing automatic focusing without executing the face detection operation of the face detection unit, wherein</claim-text>
<claim-text>when the first photographing mode is set by the mode setup unit, the automatic focusing control unit allows the face detection unit to execute the face detection operation, to thereby perform automatic focusing control by designating at least a part of the detected human face as the ranging area, and when the second photographing mode is set by the mode setup unit, the automatic focusing control unit carries out the automatic focusing control, without allowing the face detection unit to execute the face detection operation.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00007" num="00007">
<claim-text>7. The image pickup device according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein when a human face is detected, the face detection unit is further configured to detect the eyes separately, and
<claim-text>when the eyes are separately detected by the face detection unit, the ranging area selection unit is configured to receive from the user a selection of one of the eyes separately as the ranging area.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00008" num="00008">
<claim-text>8. The image pickup device according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the automatic focusing control unit is configured to perform automatic focusing control by setting the size of the ranging area, based on the size of the human face detected by the face detection unit.</claim-text>
</claim>
<claim id="CLM-00009" num="00009">
<claim-text>9. The image pickup device according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising a ranging area size selection unit configured to receive a user input selecting a size of the ranging area, wherein
<claim-text>the automatic focusing control unit is configured to perform the automatic focusing control by setting the ranging area of a size selected by the ranging area size selection unit.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00010" num="00010">
<claim-text>10. The image pickup device according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein when the automatic focusing control unit performs the automatic focusing control, designating at least a part of the human face detected by the face detection unit as the ranging area, the automatic focusing control unit stores the ranging result in a memory, and predicts the next focusing position from the past ranging results stored in the memory.</claim-text>
</claim>
<claim id="CLM-00011" num="00011">
<claim-text>11. An automatic focusing method comprising:
<claim-text>inputting image data corresponding to a subject;</claim-text>
<claim-text>separately detecting eyes, a nose, and a mouth of a human face from the image data input at the image input step;</claim-text>
<claim-text>providing a plurality of markers on a screen, one of the plurality of markers corresponding to each of the eyes, the nose, and the mouth;</claim-text>
<claim-text>receiving from a user a selection of any one of the plurality of markers corresponding to the eyes, the nose, or the mouth as a ranging area; and</claim-text>
<claim-text>performing automatic focusing control based on the ranging area selected by the user.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00012" num="00012">
<claim-text>12. A program configured to be stored on a computer readable medium and when executed by a computer, the program configured to perform an automatic focusing method, comprising:
<claim-text>separately detecting eyes, a nose, and a mouth of a human face from the image data input at the image input step;</claim-text>
<claim-text>providing a plurality of markers on a screen, one of the plurality of markers corresponding to each of the eyes, the nose, and the mouth;</claim-text>
<claim-text>receiving from a user a selection of any one of the plurality of markers corresponding to the eyes, the nose, or the mouth as a ranging area; and</claim-text>
<claim-text>performing automatic focusing control based on the ranging area selected by the user.</claim-text>
</claim-text>
</claim>
</claims>
</us-patent-grant>
