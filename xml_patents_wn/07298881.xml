<us-patent-grant lang="EN" dtd-version="v4.2 2006-08-23" file="US07298881-20071120.XML" status="PRODUCTION" id="us-patent-grant" country="US" date-produced="20071106" date-publ="20071120">
<us-bibliographic-data-grant>
<publication-reference>
<document-id>
<country>US</country>
<doc-number>07298881</doc-number>
<kind>B2</kind>
<date>20071120</date>
</document-id>
</publication-reference>
<application-reference appl-type="utility">
<document-id>
<country>US</country>
<doc-number>11056368</doc-number>
<date>20050214</date>
</document-id>
</application-reference>
<us-application-series-code>11</us-application-series-code>
<us-term-of-grant>
<us-term-extension>162</us-term-extension>
</us-term-of-grant>
<classifications-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>G</section>
<class>06</class>
<subclass>K</subclass>
<main-group>9</main-group>
<subgroup>00</subgroup>
<symbol-position>F</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20071120</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
</classifications-ipcr>
<classification-national>
<country>US</country>
<main-classification>382128</main-classification>
<further-classification>128922</further-classification>
<further-classification>378 21</further-classification>
<further-classification>378 37</further-classification>
<further-classification>378 62</further-classification>
<further-classification>382219</further-classification>
<further-classification>600407</further-classification>
</classification-national>
<invention-title id="d0e53">Method, system, and computer software product for feature-based correlation of lesions from multiple images</invention-title>
<references-cited>
<citation>
<patcit num="00001">
<document-id>
<country>US</country>
<doc-number>5828774</doc-number>
<kind>A</kind>
<name>Wang</name>
<date>19981000</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>382128</main-classification></classification-national>
</citation>
<citation>
<patcit num="00002">
<document-id>
<country>US</country>
<doc-number>6075879</doc-number>
<kind>A</kind>
<name>Roehrig et al.</name>
<date>20000600</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>382132</main-classification></classification-national>
</citation>
<citation>
<patcit num="00003">
<document-id>
<country>US</country>
<doc-number>6138045</doc-number>
<kind>A</kind>
<name>Kupinski et al.</name>
<date>20001000</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>600425</main-classification></classification-national>
</citation>
<citation>
<patcit num="00004">
<document-id>
<country>US</country>
<doc-number>6151417</doc-number>
<kind>A</kind>
<name>Florent</name>
<date>20001100</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>382265</main-classification></classification-national>
</citation>
<citation>
<patcit num="00005">
<document-id>
<country>US</country>
<doc-number>6310967</doc-number>
<kind>B1</kind>
<name>Heine et al.</name>
<date>20011000</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>382128</main-classification></classification-national>
</citation>
<citation>
<patcit num="00006">
<document-id>
<country>US</country>
<doc-number>6373998</doc-number>
<kind>B2</kind>
<name>Thirion et al.</name>
<date>20020400</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>382294</main-classification></classification-national>
</citation>
<citation>
<patcit num="00007">
<document-id>
<country>US</country>
<doc-number>6631204</doc-number>
<kind>B1</kind>
<name>Smith</name>
<date>20031000</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>382130</main-classification></classification-national>
</citation>
<citation>
<patcit num="00008">
<document-id>
<country>US</country>
<doc-number>6683973</doc-number>
<kind>B2</kind>
<name>Li et al.</name>
<date>20040100</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>382132</main-classification></classification-national>
</citation>
<citation>
<patcit num="00009">
<document-id>
<country>US</country>
<doc-number>6735745</doc-number>
<kind>B2</kind>
<name>Sarig</name>
<date>20040500</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>716  4</main-classification></classification-national>
</citation>
<citation>
<patcit num="00010">
<document-id>
<country>US</country>
<doc-number>6855114</doc-number>
<kind>B2</kind>
<name>Drukker et al.</name>
<date>20050200</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>600443</main-classification></classification-national>
</citation>
<citation>
<patcit num="00011">
<document-id>
<country>US</country>
<doc-number>6907102</doc-number>
<kind>B1</kind>
<name>Sauer et al.</name>
<date>20050600</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>378 19</main-classification></classification-national>
</citation>
<citation>
<patcit num="00012">
<document-id>
<country>US</country>
<doc-number>7162061</doc-number>
<kind>B1</kind>
<name>Takeo</name>
<date>20070100</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>382128</main-classification></classification-national>
</citation>
<citation>
<patcit num="00013">
<document-id>
<country>US</country>
<doc-number>2003/0007598</doc-number>
<kind>A1</kind>
<name>Wang et al.</name>
<date>20030100</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<patcit num="00014">
<document-id>
<country>US</country>
<doc-number>2005/0234570</doc-number>
<kind>A1</kind>
<name>Horsch et al.</name>
<date>20051000</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>700 93</main-classification></classification-national>
</citation>
<citation>
<patcit num="00015">
<document-id>
<country>US</country>
<doc-number>2005/0265606</doc-number>
<kind>A1</kind>
<name>Nakamura</name>
<date>20051200</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>382218</main-classification></classification-national>
</citation>
<citation>
<patcit num="00016">
<document-id>
<country>US</country>
<doc-number>2006/0004278</doc-number>
<kind>A1</kind>
<name>Giger et al.</name>
<date>20060100</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>600408</main-classification></classification-national>
</citation>
<citation>
<patcit num="00017">
<document-id>
<country>WO</country>
<doc-number>WO 9515537</doc-number>
<kind>A1</kind>
<date>19950600</date>
</document-id>
</patcit>
<category>cited by examiner</category>
</citation>
<citation>
<nplcit num="00018">
<othercit>Josien P. W. Pluim, et al., Mutual-Information-Based Registration of Medical Images: a Survey, Aug. 2003, IEEE Transactions on Medical Imaging, vol. 22, No. 8.</othercit>
</nplcit>
<category>cited by other</category>
</citation>
<citation>
<nplcit num="00019">
<othercit>Lubomir Hadjiiski, et al., Automated Registration of Breast Lesions in Temporal Pairs of Mammograms for Interval Change an Alysis—Local Affine Transformation for Improved Localization, Apr. 9, 2001, Med. Phys. 28 (6).</othercit>
</nplcit>
<category>cited by other</category>
</citation>
</references-cited>
<number-of-claims>30</number-of-claims>
<us-exemplary-claim>1</us-exemplary-claim>
<us-field-of-classification-search>
<classification-national>
<country>US</country>
<main-classification>128922</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>345 32</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>378 19</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>378 21</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>378 37</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>378 62</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>382128</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>382130</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>382131</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>382132</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>382219</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>600407</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>600425</main-classification>
</classification-national>
</us-field-of-classification-search>
<figures>
<number-of-drawing-sheets>10</number-of-drawing-sheets>
<number-of-figures>15</number-of-figures>
</figures>
<us-related-documents>
<us-provisional-application>
<document-id>
<country>US</country>
<doc-number>60544237</doc-number>
<kind>00</kind>
<date>20040213</date>
</document-id>
</us-provisional-application>
<related-publication>
<document-id>
<country>US</country>
<doc-number>20060004278</doc-number>
<kind>A1</kind>
<date>20060105</date>
</document-id>
</related-publication>
</us-related-documents>
<parties>
<applicants>
<applicant sequence="001" app-type="applicant-inventor" designation="us-only">
<addressbook>
<last-name>Giger</last-name>
<first-name>Maryellen L</first-name>
<address>
<city>Elmhurst</city>
<state>IL</state>
<country>US</country>
</address>
</addressbook>
<nationality>
<country>omitted</country>
</nationality>
<residence>
<country>US</country>
</residence>
</applicant>
<applicant sequence="002" app-type="applicant-inventor" designation="us-only">
<addressbook>
<last-name>Wen</last-name>
<first-name>HuiHua</first-name>
<address>
<city>Wilmette</city>
<state>IL</state>
<country>US</country>
</address>
</addressbook>
<nationality>
<country>omitted</country>
</nationality>
<residence>
<country>US</country>
</residence>
</applicant>
<applicant sequence="003" app-type="applicant-inventor" designation="us-only">
<addressbook>
<last-name>Lan</last-name>
<first-name>Li</first-name>
<address>
<city>Naperville</city>
<state>IL</state>
<country>US</country>
</address>
</addressbook>
<nationality>
<country>omitted</country>
</nationality>
<residence>
<country>US</country>
</residence>
</applicant>
</applicants>
<agents>
<agent sequence="01" rep-type="attorney">
<addressbook>
<orgname>Oblon, Spivak, McClelland, Maier &amp; Neustadt, P.C.</orgname>
<address>
<country>unknown</country>
</address>
</addressbook>
</agent>
</agents>
</parties>
<assignees>
<assignee>
<addressbook>
<orgname>University of Chicago</orgname>
<role>02</role>
<address>
<city>Chicago</city>
<state>IL</state>
<country>US</country>
</address>
</addressbook>
</assignee>
</assignees>
<examiners>
<primary-examiner>
<last-name>Desire</last-name>
<first-name>Gregory M</first-name>
<department>2624</department>
</primary-examiner>
</examiners>
</us-bibliographic-data-grant>
<abstract id="abstract">
<p id="p-0001" num="0000">A method, system, and computer software product for correlating medical images, comprising: obtaining first image data representative of a first medical image including a first abnormality; obtaining second image data representative of a second medical image including a second abnormality; determining at least one feature value for each of the first and second abnormalities using the first and second image data; calculating, based on the determined feature values, a likelihood value indicative of a likelihood that the first and second abnormalities are a same abnormality; and outputting the determined likelihood value.</p>
</abstract>
<drawings id="DRAWINGS">
<figure id="Fig-EMI-D00000" num="00000">
<img id="EMI-D00000" he="55.96mm" wi="65.45mm" file="US07298881-20071120-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00001" num="00001">
<img id="EMI-D00001" he="205.74mm" wi="164.00mm" file="US07298881-20071120-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00002" num="00002">
<img id="EMI-D00002" he="110.91mm" wi="150.54mm" file="US07298881-20071120-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00003" num="00003">
<img id="EMI-D00003" he="226.74mm" wi="203.37mm" file="US07298881-20071120-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00004" num="00004">
<img id="EMI-D00004" he="228.35mm" wi="144.61mm" file="US07298881-20071120-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00005" num="00005">
<img id="EMI-D00005" he="147.40mm" wi="153.16mm" file="US07298881-20071120-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00006" num="00006">
<img id="EMI-D00006" he="156.04mm" wi="149.52mm" file="US07298881-20071120-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00007" num="00007">
<img id="EMI-D00007" he="151.98mm" wi="129.54mm" file="US07298881-20071120-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00008" num="00008">
<img id="EMI-D00008" he="147.49mm" wi="117.77mm" file="US07298881-20071120-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00009" num="00009">
<img id="EMI-D00009" he="175.60mm" wi="168.74mm" file="US07298881-20071120-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00010" num="00010">
<img id="EMI-D00010" he="219.29mm" wi="178.56mm" file="US07298881-20071120-D00010.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
</drawings>
<description id="description">
<?RELAPP description="Other Patent Relations" end="lead"?>
<heading id="h-0001" level="1">CROSS-REFERENCE TO RELATED APPLICATIONS</heading>
<p id="p-0002" num="0001">This application claims the benefit under 35 U.S.C. § 119(e) of the filing date of Provisional Application Ser. No. 60/544,237, filed Feb. 13, 2004, the contents of which are incorporated herein by reference.</p>
<?RELAPP description="Other Patent Relations" end="tail"?>
<?GOVINT description="Government Interest" end="lead"?>
<heading id="h-0002" level="1">STATEMENT REGARDING FEDERALLY SPONSORED RESEARCH</heading>
<p id="p-0003" num="0002">The present invention was made in part with U.S. Government support under USPHS Grant No. T32 CA09649. The U.S. Government may have certain rights to this invention.</p>
<?GOVINT description="Government Interest" end="tail"?>
<?BRFSUM description="Brief Summary" end="lead"?>
<heading id="h-0003" level="1">BACKGROUND OF THE INVENTION</heading>
<heading id="h-0004" level="1">Field of the Invention</heading>
<p id="p-0004" num="0003">The present invention is directed to feature-based correlation of lesions from multiple images. More precisely, the present invention relates to the correlation of lesions observed on breast images acquired from different modalities, which may include, e.g., mammography, breast sonography, and magnetic resonance imaging (MRI). The present invention also relates to the correlation of lesions observed on breast images acquired from different views, times, or protocols, of a single modality for a given patient.</p>
<p id="p-0005" num="0004">The present invention also generally relates to computerized techniques for automated analysis of digital images, for example, as disclosed in one or more of U.S. Pat. Nos. 4,839,807; 4,841,555; 4,851,984; 4,875,165; 4,918,534; 5,072,384; 5,150,292; 5,224,177; 5,289,374; 5,319,549; 5,343,390; 5,359,513; 5,452,367; 5,463,548; 5,491,627; 5,537,485; 5,598,481; 5,622,171; 5,638,458; 5,657,362; 5,666,434; 5,673,332; 5,668,888; 5,732,697; 5,740,268; 5,790,690; 5,873,824; 5,881,124; 5,931,780; 5,974,165; 5,982,915; 5,984,870; 5,987,345; 6,011,862; 6,058,322; 6,067,373; 6,075,878; 6,078,680; 6,088,473; 6,112,112; 6,141,437; 6,185,320; 6,205,348; 6,240,201; 6,282,305; 6,282,307; 6,317,617 as well as U.S. patent application Ser. Nos. 08/173,935; 08/398,307 PCT Publication WO 96/27846); 08/536,149; 08/900,189; 09/027,468; 09/141,535; 09/471,088; 09/692,218; 09/716,335; 09/759,333; 09/760,854; 09/773,636; 09/816,217; 09/830,562; 09/818,831; 09/842,860; 09/860,574; 60/160,790; 60/176,304; 60/329,322; 09/990,311; 09/990,310; 09/990,377; 10/360,814; and 60/331,995; and PCT patent applications PCT/US98/15165; PCT/US98/24933; PCT/US99/03287; PCT/US00/41299; PCT/JS01/00680; PCT/US01/01478 and PCT/US01/01479, all of which are incorporated herein by reference.</p>
<p id="p-0006" num="0005">The present invention includes the use of various technologies referenced and described in the documents identified in the following LIST OF REFERENCES:</p>
<heading id="h-0005" level="1">LIST OF REFERENCES</heading>
<p id="p-0007" num="0000">
<ul id="ul0001" list-style="none">
    <li id="ul0001-0001" num="0006">[1] E. A. Sickles, W. N. Weber, H. B. Galvin, S. H. Ominsky, and R. A. Sollitto, “Baseline screening mammography: one vs two views per breast,” Am. J. Roentgenol., vol. 147(6), pp. 1149-53, 1986.</li>
    <li id="ul0001-0002" num="0007">[2] R. G. Blanks, S. M. Moss, and M. G. Wallis, “Use of two view mammography compared with one view in the detection of small invasive cancers: further results from the National Health Service breast screening programme,” J. Med. Screen., vol. 4(2), pp. 98-101, 1997.</li>
    <li id="ul0001-0003" num="0008">[3] N. Vujovic and D. Brzakovic, “Control points in pairs of mammographic images,” IEEE IP, vol. 6(10), pp. 1388-1399, 1997.</li>
    <li id="ul0001-0004" num="0009">[4] Y. Kita, R. P. Highnam, and J. M. Brady, “Correspondence between Different View Breast X-Rays Using a Simulation of Breast Deformation,” in Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition, June 23-25, pp. 700, 1998.</li>
    <li id="ul0001-0005" num="0010">[5] S. L. Kok-Wiles, J. M. Brady, and R. P. Highnam, “Comparing mammogram pairs in the detection of lesions,” in Proceedings of the 4th International Workshop on Digital Mammography, Nijmegen, The Netherlands, June 1998.</li>
    <li id="ul0001-0006" num="0011">[6] F. F. Yin, M. L. Giger, K. Doi, et al., “Computerized detection of masses in digital mammograms: automated alignment of breast images and its effect on bilateral-subtraction technique,” Med. Phys. vol. 21(3), pp. 445-452, 1994.</li>
    <li id="ul0001-0007" num="0012">[7] N. Karssemeijer and G. M. te Brake, “Combining single view features and asymmetry for detection of mass lesions,” in Proceedings of the 4th International Workshop on Digital Mammography, Nijmegen, The Netherlands, June 1998, pp. 95-102.</li>
    <li id="ul0001-0008" num="0013">[8] R. Chandrasekhar and Y. Attikiouzel, “A Simple Method for Automatically Locating the Nipple on Mammograms,” IEEE Trans. Med. Imaging, vol. 16(5), pp. 483-494, 1997.</li>
    <li id="ul0001-0009" num="0014">[9] S. van Engeland and N. Karssemeijer, “Matching Breast Lesions in Multiple Mammographic Views,” in Proceedings of MICCAI 2001: 4th International Conference, Utrecht, The Netherlands, October 14-17, pp. 1172, 2001.</li>
    <li id="ul0001-0010" num="0015">[10] F. Richard and L. Cohen, “A new image registration technique with free boundary constraints: application to mammography,” Computer Vision and Image Understanding, vol. 89, pp. 166-196, 2003.</li>
    <li id="ul0001-0011" num="0016">[11] L. Hadjiiski, H. Chan, B. Sahiner, N. Petrick, and M. A. Helvie, “Automated registration of breast lesions in temporal pairs of mammograms for interval change analysis local affine transformation for improved localization,” Med. Phys., vol. 28(6), pp. 1070-1079, 2001.</li>
    <li id="ul0001-0012" num="0017">[12] S. van Engeland, P. Snoeren, J. Hendriks, and N. Karssemeijer, “A Comparison of Methods for Mammogram Registration,” IEEE Trans. Med. Imaging, vol. 22(11), pp. 1436-1444, 2003.</li>
    <li id="ul0001-0013" num="0018">[13] G. Xiao, J. M. Brady, J. A. Noble, M. Burcher, and R. English, “Non-rigid registration of 3-D free-hand ultrasound images of the breast,” IEEE Trans. Med. Imaging, vol. 21, pp. 405-412, 2002.</li>
    <li id="ul0001-0014" num="0019">[14] J. F. Krucker, G. L. LeCarpentier, J. B. Fowlkes, and P. L. Carson, “Rapid Elastic Image Registration for 3D Ultrasound,” IEEE Trans. Med. Imaging, vol. 21, pp. 1384-1394, 2002.</li>
    <li id="ul0001-0015" num="0020">[15] Z. Huo, M. L. Giger, C. J. Vyborny, U. Bick, P. Lu, D. E. Wolverton, and R. A. Schmidt, “Analysis of spiculation in the computerized classification of mammographic masses,” Med. Phys., vol. 22, pp. 1569-1579, 1995.</li>
    <li id="ul0001-0016" num="0021">[16] M. A. Kupinski and M. L. Giger, “Automated seeded lesion segmentation on digital mammograms,” IEEE Trans. Med. Imaging, vol. 17, pp. 510-517, 1998.</li>
    <li id="ul0001-0017" num="0022">[17] M. L. Giger, H. Al-Hallaq, Z. Huo, C. Moran, D. E. Wolverton, C. W. Chan, and W. Zhong, “Computerized analysis of lesions in US images of the Breast,” Acad. Radiol., vol. 6, pp. 665-674, 1999.</li>
    <li id="ul0001-0018" num="0023">[18] K. Horsch, M. L. Giger, L. A. Venta, and C. J. Vyborny, “Automatic segmentation of breast lesions on ultrasound,” Med. Phys., vol. 28, pp. 1652-1659, 2001.</li>
    <li id="ul0001-0019" num="0024">[19] K. Horsch, M. L. Giger, L. A. Venta, and C. J. Vyborny, “Computerized diagnosis of breast lesions on ultrasound,” Med. Phys., vol. 29, pp. 157-164, 2002.</li>
    <li id="ul0001-0020" num="0025">[20] Z. Huo, M. L. Giger, and C. J. Vyborny, “Computerized analysis of multiple-mammographic views: potential usefulness of special view mammograms in computer-aided diagnosis,” IEEE Trans. Med. Imaging, vol. 20, pp. 1285-1292, 2001.</li>
    <li id="ul0001-0021" num="0026">[21] Z. Huo, M. L. Giger, D. E. Wolverton, W. Zhong, S. Cumming, and O. I. Olopade, “Computerized analysis of mammographic parenchymal patterns for breast cancer risk assessment: feature selection,” Med. Phys., vol. 27, pp. 4-12, 2000.</li>
    <li id="ul0001-0022" num="0027">[22] M. A. Kupinski, D. C. Edwards, M. L. Giger, and C. E. Metz, “Ideal Observer Approximation Using Bayesian Classification Neural Networks,” IEEE Trans. Med. Imaging, vol. 20, pp. 886-899, 2001.</li>
</ul>
</p>
<p id="p-0008" num="0028">[23] C. E. Metz and X. Pan, “Proper binormal ROC curves: theory and maximum-likelihood estimation,” J. Math. Psych., vol. 43, pp. 1-33, 1999.</p>
<p id="p-0009" num="0029">The entire contents of each reference listed in the LIST OF REFERENCES are incorporated herein by reference.</p>
<heading id="h-0006" level="1">DISCUSSION OF THE BACKGROUND</heading>
<p id="p-0010" num="0030">The merging of information from images obtained from mammography, ultrasound, and MRI is an important problem in breast cancer diagnosis. Breast images of a given patient can provide complementary information regarding an abnormality. Moreover, comparing the images also improves their interpretation.</p>
<p id="p-0011" num="0031">Radiologists frequently compare breast images taken at different times for identifying and evaluating lesions. Also, diagnostic mammography and breast sonography are routinely performed to evaluate breast abnormalities that are initially detected with screening mammography or on physical examination.</p>
<p id="p-0012" num="0032">However, comparing multiple images taken by the same modality, but in different views or according to different configurations of parameters, is a nontrivial task that requires considerable expertise and experience on the part of radiologists. Indeed, there is considerable difficulty in comparing multiple breast images or fusing information from multiple breast images. This difficulty is due in part to the images themselves, which provide non-identical, but complementary information concerning the lesion. In addition, whereas more than one lesion may be present in a patient in some cases, the totality of the lesions may not be represented on all images obtained in a study. Furthermore, lesions of the same abnormality can exhibit different characteristics in different images due to differences in view projection, imaging time, and/or contrast mechanism.</p>
<p id="p-0013" num="0033">In light of these difficulties, computer-based techniques are needed to correlate lesions observed on breast images acquired for a given patient either using different modalities, such as mammography, breast sonography, and MRI, or using a single modality, but different views, times, or protocols for that modality. In fact, such techniques are acutely needed since such correlations arise in numerous applications, including applications using multi-view mammograms, sonograms, or multi-modal images for breast cancer diagnosis. Such correlations also arise in applications comparing breast images in longitudinal studies for evaluation of disease diagnosis, prognosis, or patient management for both human interpretation as well as CAD methods.</p>
<p id="p-0014" num="0034">It is known that using at least two views of mammograms allows radiologists to better detect and evaluate breast abnormalities [1,2]. Studies have attempted to develop computer algorithms determining geometrical transformations that can establish a one-to-one mapping between two mammographic images.</p>
<p id="p-0015" num="0035">Image registration techniques that have been considered for matching mammogram pairs include identifying and matching control points [3] and identifying and matching inherent image landmarks, such as curves [4], regions [5], breast skin line [6,7] and nipple position [8,9] to minimize energy functions defined by intensities and contours of regions of interest [10].</p>
<p id="p-0016" num="0036">Several image registration techniques have been applied to detect lesions in temporal pairs of mammograms [11]. The purpose of such registration methods [12], often used to compensate for differences in breast compression, positioning and acquisition protocols on mammograms, is to aid radiologist in detecting and analyzing changes in the breast that may have occurred between the mammograms as well as new asymmetries observed between the left and right breast.</p>
<p id="p-0017" num="0037">Recent ultrasound-to-ultrasound breast image registration studies [13,14] have concentrated on 3-D compound images. The volumes reconstructed therein typically have deteriorated spatial resolution because of the presence of refraction artifacts and tissue mis-registration between sequential 2-D scans. Both studies use voxel-based intensity correlation coefficients between two sub-volumes to implement a deformation energy function and mutual information.</p>
<p id="p-0018" num="0038">There is also a need for automated classification and computer-aided diagnosis of mass lesions in breast. That is, one would like to determine as reliably as possible whether breast lesions are benign or malignant using computer-extracted features of the lesions found on mammograms or sonograms [15,16,17,18].</p>
<p id="p-0019" num="0039">Further, correlating lesions from two different breast mammogram views of a given modality using CAD is generally accepted as a second opinion to that of radiologists. It is highly desirable, when attempting to differentiate between benign and malignant breast lesions using a two-view mammogram or sonogram analysis, to have corresponding lesions arranged in pairs to merge information. Due to the elasticity of the breast tissue, deformation in the positioning and compression of the examination procedure vary from one examination to the other. As a result, finding the geometric relationship between breast images poses a rather daunting task.</p>
<p id="p-0020" num="0040">Consequently, deformable registration techniques typically require a sophisticated model to obtain consistent deformations both locally and globally. Iterative optimization techniques are often needed to estimate geometric transformation parameters. However, the similarity measurement or fitness function defined to that end is often highly non-linear and contains many local maxima. Further, these optimization techniques do not guarantee convergence to the global maximum. In addition, these techniques are often computationally very expensive and thus difficult to use in a clinical setting.</p>
<heading id="h-0007" level="1">SUMMARY OF THE INVENTION</heading>
<p id="p-0021" num="0041">Accordingly, to overcome the problems of the related art, an embodiment of the present invention provides a method, system, and medium for performing feature-based correlation of lesions in multiple images.</p>
<p id="p-0022" num="0042">According to an aspect of the present invention, there is provided a method, system, and medium for correlating medical images, comprising: (1) obtaining a first medical image including a first lesion; (2) obtaining a second medical image including a second lesion; (3) determining at least one feature value for each of the first and second lesions using image data of the first and second medical images; (4) calculating, based on the determined feature values, a likelihood value indicative of a likelihood that the first and second lesions are a same lesion; and (5) outputting the determined likelihood value.</p>
<p id="p-0023" num="0043">According to an aspect of the present invention, there is provided a method, system, and medium for performing a feature-based correlation of lesions obtained from different modalities including mammography, sonography, and magnetic resonance imaging.</p>
<p id="p-0024" num="0044">According to another aspect of the present invention, there is provided a method, system, and medium for performing a feature-based correlation of lesions obtained from different views, times, or protocols for a single modality.</p>
<p id="p-0025" num="0045">According to another aspect of the present invention, there is provided a method, system, and medium for performing a correlation of multiple lesions based on a single feature.</p>
<p id="p-0026" num="0046">According to another aspect of the present invention, there is provided a method, system, and medium for performing a correlation of multiple lesions based on a plurality of features.</p>
<p id="p-0027" num="0047">According to another aspect of the present invention, there is provided a method, system, and medium for characterizing whether lesions from a plurality of images correspond to the same lesion.</p>
<p id="p-0028" num="0048">According to another aspect of the present invention, there is provided a method, system, and medium for automatically characterizing abnormalities using features that are traditionally used by radiologists in the clinical evaluation of breast masses as well as lower-level features that may not be as intuitive to the radiologist's eye-brain system. These radiographic image features emerge within embodiments of the present invention as mathematical descriptors of characteristics of mass lesions and automatically correlate, i.e., match, lesions observed on multiple breast images of the same patient.</p>
<p id="p-0029" num="0049">Embodiments of the present invention provide new automated methods, systems, and media employing an intelligent computer system/workstation for computer-assisted interpretation of breast magnetic resonance imaging medical images.</p>
<p id="p-0030" num="0050">Embodiments of the present invention provide a classification scheme for an automated matching of lesions from multiple breast images, thereby aiding radiologists or oncologists, who are presented with matching likelihood values in a human-readable form, to compare and combine information from mammography, breast ultrasound, or other image modalities to improve diagnostic accuracy and overall patient outcome.</p>
<p id="p-0031" num="0051">Embodiments of the present invention identify specific features that achieve automatic lesion matching, thereby aiding radiologists or oncologists to compare and combine information from multi-modality breast images, and improve diagnostic accuracy and overall patient outcome. Moreover, the features can be used without any information regarding the location of a lesion characterized by those features.</p>
<p id="p-0032" num="0052">Embodiments of the present invention also facilitate identification of incorrectly matched lesions that belong to different patients.</p>
<p id="p-0033" num="0053">Other methods, systems, and media of the present invention will become apparent to one of ordinary skill in the art upon examination of the following drawings and detailed description of the invention.</p>
<?BRFSUM description="Brief Summary" end="tail"?>
<?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?>
<description-of-drawings>
<heading id="h-0008" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading>
<p id="p-0034" num="0054">A more complete appreciation of the invention and many of the attendant advantages thereof will be readily obtained as the same becomes better understood by reference to the following detailed description when considered in connection with the accompanying drawings, wherein:</p>
<p id="p-0035" num="0055"><figref idref="DRAWINGS">FIGS. 1A-1D</figref> illustrate four ultrasound images of two lesions in the right breast of a patient;</p>
<p id="p-0036" num="0056"><figref idref="DRAWINGS">FIGS. 2A and 2B</figref> illustrate two mammographic views, mediolateral oblique (MLO) and craniocaudal (CC), of two lesions in the same breast;</p>
<p id="p-0037" num="0057"><figref idref="DRAWINGS">FIG. 3</figref> illustrates a method for feature-based automated classification and matching;</p>
<p id="p-0038" num="0058"><figref idref="DRAWINGS">FIGS. 4A and 4B</figref> illustrate two-dimensional distributions of an image feature generated from images taken in two different views for corresponding and non-corresponding datasets;</p>
<p id="p-0039" num="0059"><figref idref="DRAWINGS">FIG. 5</figref> illustrates the distribution of likelihood for corresponding and non-corresponding datasets for a posterior acoustic behavior feature;</p>
<p id="p-0040" num="0060"><figref idref="DRAWINGS">FIG. 6</figref> illustrates a likelihood ratio histogram for corresponding and non-corresponding datasets for a posterior acoustic behavior feature;</p>
<p id="p-0041" num="0061"><figref idref="DRAWINGS">FIG. 7</figref> illustrates a ROC curve for round-robin analysis and a sonographic feature (posterior acoustic behavior);</p>
<p id="p-0042" num="0062"><figref idref="DRAWINGS">FIG. 8</figref> illustrates a ROC curve for round-robin analysis and a mammographic feature (average grey value);</p>
<p id="p-0043" num="0063"><figref idref="DRAWINGS">FIG. 9</figref> illustrates a canonical correlation analysis for multiple features; and</p>
<p id="p-0044" num="0064"><figref idref="DRAWINGS">FIG. 10</figref> illustrates a system for correlating medical images.</p>
</description-of-drawings>
<?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?>
<?DETDESC description="Detailed Description" end="lead"?>
<heading id="h-0009" level="1">DETAILED DESCRIPTION OF THE PREFERRED EMBODIMENTS</heading>
<p id="p-0045" num="0065">The aforementioned difficulties are mitigated by using image features associated with the lesions and developing a classification scheme for establishing lesion correspondence. That is, given two breast images, lesions are first automatically segmented from the surrounding breast tissues and a set of features (feature vector) is automatically extracted from each identified lesion. Subsequently, for every two indicated lesions that are not in the same image, a classifier will examine their features and yield the likelihood that the two lesions correspond to the same physical lesion.</p>
<p id="p-0046" num="0066">Image features produced by automatic segmentation that have been successful in developing computer-aided diagnosis (CAD) of breast cancers are also useful for the task of lesion matching. An appropriate subset of features useful for discriminating corresponding and non-corresponding lesion pairs will need to be determined. The features will then be used to develop discrimination methods for automated correlating of lesions from multiple breast images.</p>
<p id="p-0047" num="0067">Canonical correlation is a statistical analysis for identification and quantification of associations between two sets of variables. Canonical correlation works by finding linear combinations of variables from each set, called canonical variables, such that the correlation between the two canonical variables is maximized. The advantage of this technique is to concentrate a high-dimensional relationship between the two sets of variables into a few pairs of canonical variables. According to an embodiment of the present invention, canonical correlation analysis is performed to identify a subset of features for the discrimination task.</p>
<p id="p-0048" num="0068">To identify useful features for discrimination, one selects features that have a higher canonical correlation coefficient generated by the same physical lesion and have a lower correlation generated by different lesions.</p>
<p id="h-0010" num="0000">Database</p>
<p id="p-0049" num="0069">Embodiments of the present invention will be discussed using certain databases of medical images. These databases will now be described to facilitate an understanding of the present invention. However, it is to be understood that the use of these databases does not limit the scope of the invention in any way and the correlation analysis can be implemented for other types of medical images, such as chest radiography, magnetic resonance imaging, etc.</p>
<p id="p-0050" num="0070">First, consider a sonographic database of 262 biopsy-proven lesions. Specifically, each sonographic biopsy-proven lesion includes both transverse and longitudinal views, and the 262 lesions include 81 complicated cysts, 115 benign lesions, and 66 malignant lesions.</p>
<p id="p-0051" num="0071">Second, consider a mammography database of 230 biopsy-proven lesions. Specifically, each mammography biopsy-proven lesion includes craniocaudal (CC) and mediolateral oblique (MLO) views, and the lesions include 112 complicated cysts, 8 benign solid lesions, and 110 malignant lesions.</p>
<p id="p-0052" num="0072">Further, in order to mimic a lesion mismatch scenario that might occur in clinical practice, consider also three “non-corresponding” datasets denoted by A, B, and C.</p>
<p id="p-0053" num="0073">Dataset A contains lesion pairs obtained from patients who had two or more lesions shown on different image views. One lesion from one view was paired with the other lesion in another view, of the same patient, as a non-corresponding lesion pair. The sonographic database comprises 35 patients who had two or more lesions shown on transverse and longitudinal views, which leads to 53 non-corresponding lesion pairs. The mammographic database comprises 11 patients who had two or more lesions shown on transverse and longitudinal views, which leads to 25 non-corresponding lesion pairs.</p>
<p id="p-0054" num="0074"><figref idref="DRAWINGS">FIGS. 1A-1D</figref> illustrate four ultrasound images. The images represent two lesions, taken in both transverse and longitudinal views, from the right breast of a patient. <figref idref="DRAWINGS">FIG. 1A</figref> shows a transverse image for the lesion at location 10:00 B. <figref idref="DRAWINGS">FIG. 1B</figref> shows a longitudinal image for the lesion at location 1:00 A. <figref idref="DRAWINGS">FIG. 1C</figref> shows a transverse image for the lesion at location 1:00 A. <figref idref="DRAWINGS">FIG. 1D</figref> shows a longitudinal image for the lesion at location 10:00 B. The same lesion, shown in different views, has different shape and orientation. The arrow indicates the correspondence between scans belonging to the same physical lesion.</p>
<p id="p-0055" num="0075"><figref idref="DRAWINGS">FIGS. 2A and 2B</figref> illustrate two mammographic lesions in the same breast. <figref idref="DRAWINGS">FIG. 2A</figref> shows a MLO view and <figref idref="DRAWINGS">FIG. 2B</figref> shows a CC view. Again, the same lesion, shown in different views, has a different shape and orientation since the images are taken from different angles. The arrow indicates the correspondence between scans belonging to the same physical lesion.</p>
<p id="p-0056" num="0076">There are few patients having two or more lesions in the same breast in the non-corresponding database. The non-corresponding dataset must therefore be extended by generating lesion pairs across patients. To that end, dataset B contains lesion pairs obtained from different patients who have lesions with the same pathology and dataset C contains lesion pairs obtained from different patients who have lesions with different pathology.</p>
<p id="p-0057" num="0077">One approach to constitute these “non-matching lesion” datasets is to generate all possible lesion combinations across patients with the same lesion pathology, or with the different pathology, and then randomly select 300 pairs among all possible pairs.</p>
<p id="p-0058" num="0078">Table I summarizes the number of lesion pairs contained in each dataset for both databases.</p>
<p id="p-0059" num="0079">
<tables id="TABLE-US-00001" num="00001">
<table frame="none" colsep="0" rowsep="0">
<tgroup align="left" colsep="0" rowsep="0" cols="3">
<colspec colname="offset" colwidth="56pt" align="left"/>
<colspec colname="1" colwidth="49pt" align="center"/>
<colspec colname="2" colwidth="112pt" align="center"/>
<tbody valign="top">
<row>
<entry/>
<entry namest="offset" nameend="2" align="center" rowsep="1"/>
</row>
<row>
<entry/>
<entry>Corresponding</entry>
<entry>Non-corresponding Datasets (pairs)</entry>
</row>
</tbody>
</tgroup>
<tgroup align="left" colsep="0" rowsep="0" cols="5">
<colspec colname="1" colwidth="56pt" align="left"/>
<colspec colname="2" colwidth="49pt" align="center"/>
<colspec colname="3" colwidth="42pt" align="center"/>
<colspec colname="4" colwidth="35pt" align="center"/>
<colspec colname="5" colwidth="35pt" align="center"/>
<tbody valign="top">
<row>
<entry>Database</entry>
<entry>Dataset (pairs)</entry>
<entry>A</entry>
<entry>B</entry>
<entry>C</entry>
</row>
<row>
<entry namest="1" nameend="5" align="center" rowsep="1"/>
</row>
<row>
<entry>Sonography</entry>
<entry>262</entry>
<entry>53</entry>
<entry>300</entry>
<entry>300</entry>
</row>
<row>
<entry>Mammography</entry>
<entry>230</entry>
<entry>25</entry>
<entry>300</entry>
<entry>300</entry>
</row>
<row>
<entry namest="1" nameend="5" align="center" rowsep="1"/>
</row>
</tbody>
</tgroup>
</table>
</tables>
<br/>
A Classification Framework for Lesion Matching
</p>
<p id="p-0060" num="0080">Conventional approaches for matching lesions generally begin with an attempt to spatially align two images in order to establish a one-to-one pixel correspondence between the images. Once the images are aligned, establishing lesion correspondence can be relatively straightforward. Unfortunately, an adequate spatial alignment between two breast images is often very difficult to achieve due to the non-rigid morphology of the breast and the differences in imaging protocols. For example, breast compression is applied in obtaining mammograms but not in breast ultrasound. Moreover, it is also rather impractical to exactly reproduce the compression applied in a mammography. Therefore, a breast can exhibit significantly different shapes on two images. In addition, while an entire breast is typically imaged in mammography, only a portion of the breast is usually scanned in ultrasound, further complicating the task of spatially aligning two such breast images. These difficulties associated with the spatial alignment of breast images can be mitigated by using local image features associated with the lesions for establishing lesion correspondence. To achieve this, one can use image features produced by automatic classification schemes. These image features have been successful used in developing computer-aided diagnosis (CAD) of breast cancers and are also suitable for the task of lesion matching.</p>
<p id="p-0061" num="0081">Classification has been widely applied in CAD and image analysis. Typically, once a suspicious region is detected, features characterizing the region are extracted. These features, or a subset of them, are then employed in an automatic classifier to yield an estimate of the probability of malignancy.</p>
<p id="p-0062" num="0082"><figref idref="DRAWINGS">FIG. 3</figref> illustrates generally a method for lesion matching. Instead of a geometric transformation establishing the correct one-to-one mapping between (x, y) locations in two image, an embodiment the method uses lesion characteristics automatically extracted from lesions in the two images to distinguish between corresponding and non-corresponding lesion pairs. In <figref idref="DRAWINGS">FIG. 3</figref>, two images are acquired in steps <b>301</b><i>a </i>and <b>301</b><i>b</i>, respectively. In steps <b>302</b><i>a </i>and <b>302</b><i>b</i>, lesions are extracted from the images by automatic segmentation from the surrounding breast tissues. This can be done using any conventional segmentation technique. Naturally, any kind of segmentation approach could also be used within the scope of the present invention. In steps <b>303</b><i>a </i>and <b>303</b><i>b</i>, feature vectors are extracted corresponding to the automatically segmented lesions. Subsequently, in step <b>304</b>, a classifier evaluates the similarity between the features of the lesions. In step <b>305</b>, the likelihood that the two lesions correspond to the same physical lesion is determined.</p>
<p id="p-0063" num="0083">The lesion-matching step performed in step <b>304</b> will now be described in more depth. Let {right arrow over (x)}<sub>1 </sub>denote the feature vector extracted for a lesion seen in one image, and let {right arrow over (x)}<sub>2 </sub>denote the feature vector extracted for a lesion seen in another image. A classifier examines the “similarity” between the feature vectors {right arrow over (x)}<sub>1 </sub>and {right arrow over (x)}<sub>2 </sub>to yield an estimate of the likelihood that the two lesions correspond to the same physical lesion seen on two different images. An appropriate “similarity” measure is learned from two datasets of the feature pairs, one derived from the same physical lesions seen in different images, the “corresponding dataset,” and the other derived from physically different lesions seen in different images, the “non-corresponding dataset.”</p>
<p id="p-0064" num="0084">In general, the two images can be generated either using the same imaging modality or using different imaging modalities. In both cases, the images can be taken in different views and/or using different imaging parameters. In addition, the classification task can consider multiple image features. Consider a single image feature for each image, denoted x<sub>1 </sub>and x<sub>2</sub>, respectively, in designing the classifier. Extensions to multi-element feature vectors and multi-modality lesion matching naturally fall within the scope of the present invention. The two-dimensional (2D) distributions of the feature pair (x<sub>1</sub>, x<sub>2</sub>) show that the feature pair derived from corresponding datasets exhibit a linear relationship. Therefore, in designing a classifier, applying linear regression is appropriate. Subsequently, one obtains the likelihoods for a given feature pair based on the distance from the derived regression line for pairs corresponding to the same physical lesions and for those from physically different lesions. Classification of whether two lesions seen on two images correspond to the same physical lesion is based, for example, on a likelihood ratio test. The performance of the proposed classification scheme is evaluated by receiver operating characteristics (ROC) analysis [23].</p>
<p id="h-0011" num="0000">Modeling with Linear Regression and Feature Selection</p>
<p id="p-0065" num="0085"><figref idref="DRAWINGS">FIGS. 4A and 4B</figref> illustrate the two-dimensional distribution of an image feature generated from sonographic images taken in the transverse and longitudinal views for corresponding and non-corresponding datasets. <figref idref="DRAWINGS">FIG. 4A</figref> relates to the sonographic image feature posterior acoustic behavior extracted from lesions seen on the transverse and longitudinal views. <figref idref="DRAWINGS">FIG. 4B</figref> relates to the mammographic image feature average gray value extracted from lesions seen on the MLO and CC views.</p>
<p id="p-0066" num="0086">A linear relationship is observed for feature pairs extracted from the corresponding dataset. In comparison, feature pairs derived from the non-corresponding dataset show a much wider spread in distribution.</p>
<p id="p-0067" num="0087">This difference between the spread of the distribution for the two populations can be utilized for classification. To that end, one can compute a correlation coefficient r for the 2D feature distribution generated from the corresponding dataset and a correlation coefficient r′ for the distribution generated from the non-corresponding datasets. The correlation coefficient may be, for example, Pearson's coefficient.</p>
<p id="p-0068" num="0088">One can also obtain p-values of the derived correlation coefficients. This procedure allows the identification of a number of candidate features that may be useful for the proposed lesion-matching task. For example, one may select features that yield r≧0.5 with p≦0.05 while r′≦0.5.</p>
<p id="p-0069" num="0089">Principal component analysis (PCA) provides a useful means for identifying the linear relationship observed in <figref idref="DRAWINGS">FIGS. 4A and 4B</figref>. Given the joint distribution of n zero-mean random variables, z<sub>1</sub>, . . . , z<sub>n</sub>, PCA identifies the orthonormal vectors ê<sub>1</sub>, . . . , ê<sub>n </sub>(called the principal axes) so that the distributions of {circumflex over (z)}<sub>i</sub>={right arrow over (z)}·ê<sub>1</sub>, i=2, . . . , n, where {right arrow over (z)}=(z<sub>1</sub>, . . . , z<sub>n</sub>), are mutually uncorrelated. Furthermore, the distribution of {circumflex over (z)}<sub>1 </sub>has the largest variance among all unitary transforms of {right arrow over (z)}, i.e., the 1D subspace spanned by ê<sub>1 </sub>contains the most significant statistical variations of the distribution.</p>
<p id="p-0070" num="0090">Similarly, for i=2, . . . , n, {circumflex over (z)}<sub>1 </sub>is the unitary transform of {right arrow over (z)} that has the largest variance while are statistically uncorrelated to {circumflex over (z)}<sub>1</sub>, . . . , {circumflex over (z)}<sub>i−1</sub>; i.e., the 1D subspace spanned by ê<sub>1 </sub>contains the most statistical variations of the distribution in the subspace orthogonal to the subspace spanned by ê<sub>1</sub>, . . . , ê<sub>i−1</sub>. Consider now the application of PCA to the 2D distributions of the feature pair (x<sub>1</sub>, x<sub>2</sub>) derived from the corresponding database.</p>
<p id="p-0071" num="0091">Before applying PCA, the mean of the distribution ( <o ostyle="single">x</o><sub>1</sub>, <o ostyle="single">x</o><sub>2</sub>) is calculated and subtracted from the data thereby yielding a zero-mean population ({tilde over (x)}<sub>1</sub>, {tilde over (x)}<sub>2</sub>), where {tilde over (x)}<sub>1</sub>=x<sub>1</sub>− <o ostyle="single">x</o><sub>1 </sub>and {tilde over (x)}<sub>2</sub>=x<sub>2</sub>− <o ostyle="single">x</o><sub>2</sub>. The most significant statistical variations of the distribution of ({tilde over (x)}<sub>1</sub>, {tilde over (x)}<sub>2</sub>) will therefore appear in the first principal axis ê<sub>1</sub>. Conversely, the spread of the distribution of the feature pair derived from the corresponding dataset along the second principal axis ê<sub>2 </sub>is minimized among all possible axes in the {tilde over (x)}<sub>1</sub>−{tilde over (x)}<sub>2 </sub>space. However, the spread of the distribution of the feature pair derived from the non-corresponding dataset is not necessarily minimized along this axis.</p>
<p id="p-0072" num="0092">Therefore, a large difference in the spreads of the distributions of the two populations is likely to be observed along ê<sub>2</sub>. Let ê<sub>1</sub>=(e<sub>11</sub>, e<sub>12</sub>), e<sub>11</sub>≠0. The regression line in the x<sub>1</sub>−x<sub>2 </sub>space corresponding to the ê<sub>1 </sub>axis in the {tilde over (x)}<sub>1</sub>−{tilde over (x)}<sub>2 </sub>space is given by</p>
<p id="p-0073" num="0093">
<maths id="MATH-US-00001" num="00001">
<math overflow="scroll">
<mrow>
  <mrow>
    <mi>L</mi>
    <mo>⁢</mo>
    <mstyle>
      <mtext>:</mtext>
    </mstyle>
    <mo>⁢</mo>
    <mstyle>
      <mspace width="0.8em" height="0.8ex"/>
    </mstyle>
    <mo>⁢</mo>
    <mrow>
      <mo>{</mo>
      <mrow>
        <mrow>
          <mrow>
            <mo>(</mo>
            <mrow>
              <msub>
                <mi>x</mi>
                <mn>1</mn>
              </msub>
              <mo>,</mo>
              <msub>
                <mi>x</mi>
                <mn>2</mn>
              </msub>
            </mrow>
            <mo>)</mo>
          </mrow>
          <mo>⁢</mo>
          <mstyle>
            <mtext>:</mtext>
          </mstyle>
          <mo>⁢</mo>
          <mstyle>
            <mspace width="0.8em" height="0.8ex"/>
          </mstyle>
          <mo>⁢</mo>
          <msub>
            <mi>x</mi>
            <mn>2</mn>
          </msub>
        </mrow>
        <mo>=</mo>
        <mrow>
          <msub>
            <mi>β</mi>
            <mn>0</mn>
          </msub>
          <mo>+</mo>
          <mrow>
            <msub>
              <mi>β</mi>
              <mn>1</mn>
            </msub>
            <mo>⁢</mo>
            <msub>
              <mi>x</mi>
              <mn>1</mn>
            </msub>
          </mrow>
        </mrow>
      </mrow>
      <mo>}</mo>
    </mrow>
  </mrow>
  <mo>,</mo>
  <mrow>
    <mrow>
      <mi>where</mi>
      <mo>⁢</mo>
      <mstyle>
        <mspace width="0.8em" height="0.8ex"/>
      </mstyle>
      <mo>⁢</mo>
      <msub>
        <mi>β</mi>
        <mn>0</mn>
      </msub>
    </mrow>
    <mo>=</mo>
    <mrow>
      <mrow>
        <msub>
          <mover>
            <mi>x</mi>
            <mi>_</mi>
          </mover>
          <mn>2</mn>
        </msub>
        <mo>+</mo>
        <mrow>
          <mrow>
            <mo>(</mo>
            <mfrac>
              <msub>
                <mi>e</mi>
                <mn>12</mn>
              </msub>
              <msub>
                <mi>e</mi>
                <mn>11</mn>
              </msub>
            </mfrac>
            <mo>)</mo>
          </mrow>
          <mo>⁢</mo>
          <msub>
            <mover>
              <mi>x</mi>
              <mi>_</mi>
            </mover>
            <mn>1</mn>
          </msub>
          <mo>⁢</mo>
          <mstyle>
            <mspace width="0.8em" height="0.8ex"/>
          </mstyle>
          <mo>⁢</mo>
          <mi>and</mi>
          <mo>⁢</mo>
          <mstyle>
            <mspace width="0.8em" height="0.8ex"/>
          </mstyle>
          <mo>⁢</mo>
          <msub>
            <mi>β</mi>
            <mn>1</mn>
          </msub>
        </mrow>
      </mrow>
      <mo>=</mo>
      <mrow>
        <mfrac>
          <msub>
            <mi>e</mi>
            <mn>12</mn>
          </msub>
          <msub>
            <mi>e</mi>
            <mn>11</mn>
          </msub>
        </mfrac>
        <mo>.</mo>
      </mrow>
    </mrow>
  </mrow>
</mrow>
</math>
</maths>
<br/>
Likelihood Ratio Test
</p>
<p id="p-0074" num="0094">The task of a two-group automated classifier is to map a multidimensional observation variable into a scalar decision variable from which a threshold can be derived to determine which group an observation belongs to. A good candidate variable for discriminating feature pairs generated from the corresponding dataset against those from the non-corresponding dataset can be achieved by considering a function of the distance of a feature pair to the regression line L. In particular, one can consider the respective likelihood of a feature pair being at a given distance from the regression line when the pair is derived from the corresponding and the non-corresponding datasets. Discrimination can then be based on a likelihood-ratio criterion [22].</p>
<p id="p-0075" num="0095">Let H<sub>1 </sub>denote the hypothesis that a given feature pair (x<sub>1</sub>, x<sub>2</sub>) is derived from the same physical lesion and H<sub>0 </sub>the hypothesis that it is derived from physically different lesions. Let d denote the distance of this feature pair to the regression line established from the corresponding dataset, as described above. The likelihood of H<sub>1 </sub>being true given the feature pair is the conditional probability for obtaining the feature pair given the hypothesis, i.e., l<sub>1</sub>(d)=prob(d|H<sub>1</sub>). Similarly, the likelihood of H<sub>0 </sub>being true given the feature pair is l<sub>0</sub>(d)=prob(d|H<sub>0</sub>). The likelihood ratio is given</p>
<p id="p-0076" num="0096">
<maths id="MATH-US-00002" num="00002">
<math overflow="scroll">
<mrow>
  <mrow>
    <mi>by</mi>
    <mo>⁢</mo>
    <mstyle>
      <mspace width="0.8em" height="0.8ex"/>
    </mstyle>
    <mo>⁢</mo>
    <mrow>
      <mi>Λ</mi>
      <mo>⁡</mo>
      <mrow>
        <mo>(</mo>
        <mi>d</mi>
        <mo>)</mo>
      </mrow>
    </mrow>
  </mrow>
  <mo>=</mo>
  <mrow>
    <mfrac>
      <mrow>
        <msub>
          <mi>l</mi>
          <mn>1</mn>
        </msub>
        <mo>⁡</mo>
        <mrow>
          <mo>(</mo>
          <mo>ⅆ</mo>
          <mo>)</mo>
        </mrow>
      </mrow>
      <mrow>
        <msub>
          <mi>l</mi>
          <mn>0</mn>
        </msub>
        <mo>⁡</mo>
        <mrow>
          <mo>(</mo>
          <mo>ⅆ</mo>
          <mo>)</mo>
        </mrow>
      </mrow>
    </mfrac>
    <mo>=</mo>
    <mrow>
      <mfrac>
        <mrow>
          <mi>prob</mi>
          <mo>⁡</mo>
          <mrow>
            <mo>(</mo>
            <mrow>
              <mo>ⅆ</mo>
              <mrow>
                <mo>❘</mo>
                <msub>
                  <mi>H</mi>
                  <mn>1</mn>
                </msub>
              </mrow>
            </mrow>
            <mo>)</mo>
          </mrow>
        </mrow>
        <mrow>
          <mi>prob</mi>
          <mo>⁡</mo>
          <mrow>
            <mo>(</mo>
            <mrow>
              <mo>ⅆ</mo>
              <mrow>
                <mo>❘</mo>
                <msub>
                  <mi>H</mi>
                  <mn>0</mn>
                </msub>
              </mrow>
            </mrow>
            <mo>)</mo>
          </mrow>
        </mrow>
      </mfrac>
      <mo>.</mo>
    </mrow>
  </mrow>
</mrow>
</math>
</maths>
</p>
<p id="p-0077" num="0097">This test statistic is compared against a pre-determined threshold Ω for discrimination. The hypothesis H<sub>1 </sub>is declared true, i.e., the given feature pair is declared to derive from the same physical lesion, when Λ(d)≧Ω; otherwise, the hypothesis H<sub>0 </sub>is declared true and the feature pair is considered to derive from two physically different lesions.</p>
<p id="p-0078" num="0098">To use this likelihood ratio test, the likelihood functions need to be determined. Let (x<sub>1</sub><sup>(i)</sup>, x<sub>2</sub><sup>(i)</sup>), i=1, . . . , N, denote a feature pair in the corresponding dataset and d<sup>(i)</sup>, i=1, . . . , N, denote the distance of the pair to the regression line L established from the corresponding dataset.</p>
<p id="p-0079" num="0099">By definition, l<sub>1</sub>(d) is the distribution of d<sup>(i)</sup>. Therefore, it can be estimated from the histogram of d<sup>(i) </sup>provided that the population size N is sufficiently large. Given a relatively small population, one can apply gaussian smoothing to reduce sampling errors. That is, one estimates l<sub>1</sub>(d) by</p>
<p id="p-0080" num="0100">
<maths id="MATH-US-00003" num="00003">
<math overflow="scroll">
<mrow>
  <mrow>
    <mrow>
      <msub>
        <mi>l</mi>
        <mn>1</mn>
      </msub>
      <mo>⁡</mo>
      <mrow>
        <mo>(</mo>
        <mi>d</mi>
        <mo>)</mo>
      </mrow>
    </mrow>
    <mo>=</mo>
    <mrow>
      <mfrac>
        <mn>1</mn>
        <mrow>
          <msqrt>
            <mrow>
              <mn>2</mn>
              <mo>⁢</mo>
              <mi>πσ</mi>
            </mrow>
          </msqrt>
          <mo>⁢</mo>
          <mi>N</mi>
        </mrow>
      </mfrac>
      <mo>⁢</mo>
      <mrow>
        <munderover>
          <mo>∑</mo>
          <mrow>
            <mi>i</mi>
            <mo>=</mo>
            <mn>1</mn>
          </mrow>
          <mi>N</mi>
        </munderover>
        <mo>⁢</mo>
        <mrow>
          <mi>exp</mi>
          <mo>⁢</mo>
          <mrow>
            <mo>{</mo>
            <mrow>
              <mrow>
                <mrow>
                  <mo>-</mo>
                  <msup>
                    <mrow>
                      <mo>(</mo>
                      <mrow>
                        <mi>d</mi>
                        <mo>-</mo>
                        <msup>
                          <mi>d</mi>
                          <mrow>
                            <mo>(</mo>
                            <mi>i</mi>
                            <mo>)</mo>
                          </mrow>
                        </msup>
                      </mrow>
                      <mo>)</mo>
                    </mrow>
                    <mn>2</mn>
                  </msup>
                </mrow>
                <mo>/</mo>
                <mn>2</mn>
              </mrow>
              <mo>⁢</mo>
              <msup>
                <mi>σ</mi>
                <mn>2</mn>
              </msup>
            </mrow>
            <mo>}</mo>
          </mrow>
        </mrow>
      </mrow>
    </mrow>
  </mrow>
  <mo>,</mo>
</mrow>
</math>
</maths>
<br/>
wherein σ&gt;0 is a parameter controlling a level of smoothing. The likelihood function l<sub>0</sub>(d) is similarly estimated by using feature pairs from the non-corresponding dataset.
<br/>
The Correlation of Features Between Views
</p>
<p id="p-0081" num="0101">Fifteen features were automatically extracted to represent the lesion in the sonographic database [19] to characterize the lesion's shape, margin sharpness, echogenic texture, and posterior acoustic behavior. A total of fifteen mammographic features [21] were also extracted from each lesion to quantitatively characterize the lesion's spiculation, margin sharpness, mass density, and texture. In order to validate the assumption of a linear relation of features between views, correlation of features were calculated between image views for corresponding dataset and non-corresponding dataset A in both databases. To that effect, Table II lists the top four correlation coefficients r for the corresponding datasets and r′ for the non-corresponding datasets in the sonographic database. Similarly, Table II lists the top four correlation coefficients r for the corresponding datasets and r′ for the non-corresponding datasets in the mammographic database. The associated p-values for image features extracted from two views are also listed in Tables II and III.</p>
<p id="p-0082" num="0102">
<tables id="TABLE-US-00002" num="00002">
<table frame="none" colsep="0" rowsep="0" pgwide="1">
<tgroup align="left" colsep="0" rowsep="0" cols="3">
<colspec colname="offset" colwidth="56pt" align="left"/>
<colspec colname="1" colwidth="56pt" align="center"/>
<colspec colname="2" colwidth="147pt" align="center"/>
<thead>
<row>
<entry/>
<entry namest="offset" nameend="2" rowsep="1">TABLE IV</entry>
</row>
</thead>
<tbody valign="top">
<row>
<entry/>
<entry namest="offset" nameend="2" align="center" rowsep="1"/>
</row>
<row>
<entry/>
<entry>Corresponding</entry>
<entry>Non-corresponding Datasets</entry>
</row>
</tbody>
</tgroup>
<tgroup align="left" colsep="0" rowsep="0" cols="5">
<colspec colname="offset" colwidth="56pt" align="left"/>
<colspec colname="1" colwidth="56pt" align="center"/>
<colspec colname="2" colwidth="49pt" align="center"/>
<colspec colname="3" colwidth="49pt" align="center"/>
<colspec colname="4" colwidth="49pt" align="center"/>
<tbody valign="top">
<row>
<entry/>
<entry>Dataset</entry>
<entry>A</entry>
<entry>B</entry>
<entry>C</entry>
</row>
</tbody>
</tgroup>
<tgroup align="left" colsep="0" rowsep="0" cols="9">
<colspec colname="1" colwidth="56pt" align="left"/>
<colspec colname="2" colwidth="21pt" align="center"/>
<colspec colname="3" colwidth="35pt" align="center"/>
<colspec colname="4" colwidth="21pt" align="center"/>
<colspec colname="5" colwidth="28pt" align="center"/>
<colspec colname="6" colwidth="21pt" align="center"/>
<colspec colname="7" colwidth="28pt" align="center"/>
<colspec colname="8" colwidth="21pt" align="center"/>
<colspec colname="9" colwidth="28pt" align="center"/>
<tbody valign="top">
<row>
<entry>Feature</entry>
<entry>r</entry>
<entry>p-value</entry>
<entry>r′</entry>
<entry>p-value</entry>
<entry>r′</entry>
<entry>p-value</entry>
<entry>r′</entry>
<entry>p-value</entry>
</row>
<row>
<entry namest="1" nameend="9" align="center" rowsep="1"/>
</row>
</tbody>
</tgroup>
<tgroup align="left" colsep="0" rowsep="0" cols="9">
<colspec colname="1" colwidth="56pt" align="left"/>
<colspec colname="2" colwidth="21pt" align="center"/>
<colspec colname="3" colwidth="35pt" align="center"/>
<colspec colname="4" colwidth="21pt" align="center"/>
<colspec colname="5" colwidth="28pt" align="center"/>
<colspec colname="6" colwidth="21pt" align="center"/>
<colspec colname="7" colwidth="28pt" align="char" char="."/>
<colspec colname="8" colwidth="21pt" align="char" char="."/>
<colspec colname="9" colwidth="28pt" align="center"/>
<tbody valign="top">
<row>
<entry>Shape: Filtered</entry>
<entry>0.75</entry>
<entry>&lt;0.00001</entry>
<entry>0.38</entry>
<entry>0.004</entry>
<entry>0.24</entry>
<entry>&lt;0.001</entry>
<entry>−0.08</entry>
<entry>0.166</entry>
</row>
<row>
<entry>ARD</entry>
</row>
<row>
<entry>Filtered margin</entry>
<entry>0.77</entry>
<entry>&lt;0.00001</entry>
<entry>0.36</entry>
<entry>0.007</entry>
<entry>0.17</entry>
<entry>0.003</entry>
<entry>0.01</entry>
<entry>0.864</entry>
</row>
<row>
<entry>sharpness</entry>
</row>
<row>
<entry>Posterior</entry>
<entry>0.82</entry>
<entry>&lt;0.00001</entry>
<entry>0.14</entry>
<entry>0.311</entry>
<entry>0.24</entry>
<entry>&lt;0.001</entry>
<entry>−0.10</entry>
<entry>0.095</entry>
</row>
<row>
<entry>acoustic</entry>
</row>
<row>
<entry>behavior</entry>
</row>
<row>
<entry>Texture: conY4</entry>
<entry>0.62</entry>
<entry>&lt;0.00001</entry>
<entry>0.20</entry>
<entry>0.143</entry>
<entry>0.14</entry>
<entry>0.02</entry>
<entry>0.02</entry>
<entry>0.784</entry>
</row>
<row>
<entry namest="1" nameend="9" align="center" rowsep="1"/>
</row>
</tbody>
</tgroup>
</table>
</tables>
</p>
<p id="p-0083" num="0103">
<tables id="TABLE-US-00003" num="00003">
<table frame="none" colsep="0" rowsep="0">
<tgroup align="left" colsep="0" rowsep="0" cols="3">
<colspec colname="offset" colwidth="35pt" align="left"/>
<colspec colname="1" colwidth="56pt" align="center"/>
<colspec colname="2" colwidth="126pt" align="center"/>
<thead>
<row>
<entry/>
<entry namest="offset" nameend="2" rowsep="1">TABLE V</entry>
</row>
</thead>
<tbody valign="top">
<row>
<entry/>
<entry namest="offset" nameend="2" align="center" rowsep="1"/>
</row>
<row>
<entry/>
<entry>Corresponding</entry>
<entry>Non-corresponding Datasets</entry>
</row>
</tbody>
</tgroup>
<tgroup align="left" colsep="0" rowsep="0" cols="5">
<colspec colname="offset" colwidth="35pt" align="left"/>
<colspec colname="1" colwidth="56pt" align="center"/>
<colspec colname="2" colwidth="42pt" align="center"/>
<colspec colname="3" colwidth="42pt" align="center"/>
<colspec colname="4" colwidth="42pt" align="center"/>
<tbody valign="top">
<row>
<entry/>
<entry>Dataset</entry>
<entry>A</entry>
<entry>B</entry>
<entry>C</entry>
</row>
</tbody>
</tgroup>
<tgroup align="left" colsep="0" rowsep="0" cols="9">
<colspec colname="1" colwidth="35pt" align="left"/>
<colspec colname="2" colwidth="21pt" align="center"/>
<colspec colname="3" colwidth="35pt" align="center"/>
<colspec colname="4" colwidth="21pt" align="center"/>
<colspec colname="5" colwidth="21pt" align="center"/>
<colspec colname="6" colwidth="21pt" align="center"/>
<colspec colname="7" colwidth="21pt" align="center"/>
<colspec colname="8" colwidth="21pt" align="center"/>
<colspec colname="9" colwidth="21pt" align="center"/>
<tbody valign="top">
<row>
<entry/>
<entry/>
<entry>p-</entry>
<entry/>
<entry>p-</entry>
<entry/>
<entry>p-</entry>
<entry/>
<entry>p-</entry>
</row>
<row>
<entry>Feature</entry>
<entry>r</entry>
<entry>value</entry>
<entry>r′</entry>
<entry>value</entry>
<entry>r′</entry>
<entry>value</entry>
<entry>r′</entry>
<entry>value</entry>
</row>
<row>
<entry namest="1" nameend="9" align="center" rowsep="1"/>
</row>
</tbody>
</tgroup>
<tgroup align="left" colsep="0" rowsep="0" cols="9">
<colspec colname="1" colwidth="35pt" align="left"/>
<colspec colname="2" colwidth="21pt" align="center"/>
<colspec colname="3" colwidth="35pt" align="center"/>
<colspec colname="4" colwidth="21pt" align="center"/>
<colspec colname="5" colwidth="21pt" align="center"/>
<colspec colname="6" colwidth="21pt" align="center"/>
<colspec colname="7" colwidth="21pt" align="center"/>
<colspec colname="8" colwidth="21pt" align="char" char="."/>
<colspec colname="9" colwidth="21pt" align="center"/>
<tbody valign="top">
<row>
<entry>NRG<sub>ROI</sub></entry>
<entry>0.47</entry>
<entry>&lt;0.00001</entry>
<entry>0.31</entry>
<entry>0.144</entry>
<entry>0.15</entry>
<entry>0.011</entry>
<entry>−0.12</entry>
<entry>0.043</entry>
</row>
<row>
<entry>Average</entry>
<entry>0.66</entry>
<entry>&lt;0.00001</entry>
<entry>0.07</entry>
<entry>0.731</entry>
<entry>0.07</entry>
<entry>0.233</entry>
<entry>0.01</entry>
<entry>0.917</entry>
</row>
<row>
<entry>gray</entry>
</row>
<row>
<entry>value</entry>
</row>
<row>
<entry>Contrast</entry>
<entry>0.56</entry>
<entry>&lt;0.00001</entry>
<entry>0.29</entry>
<entry>0.165</entry>
<entry>0.03</entry>
<entry>0.608</entry>
<entry>0.07</entry>
<entry>0.225</entry>
</row>
<row>
<entry>Diameter</entry>
<entry>0.50</entry>
<entry>&lt;0.00001</entry>
<entry>0.05</entry>
<entry>0.796</entry>
<entry>0.05</entry>
<entry>0.359</entry>
<entry>−0.10</entry>
<entry>0.088</entry>
</row>
<row>
<entry namest="1" nameend="9" align="center" rowsep="1"/>
</row>
</tbody>
</tgroup>
</table>
</tables>
<br/>
Training the Classifier for Lesion Matching
</p>
<p id="p-0084" num="0104">After identifying the correlated feature vectors, one first uses the features from the corresponding dataset to determine β<sub>0 </sub>and β<sub>1 </sub>for the linear regression model. The corresponding and non-corresponding datasets are used to train the classifier.</p>
<p id="p-0085" num="0105"><figref idref="DRAWINGS">FIGS. 5 and 6</figref> illustrate the relationship between a feature obtained from two different views for the three datasets in the mammographic database and the sonographic database. <figref idref="DRAWINGS">FIG. 5</figref> shows the likelihood distribution of the corresponding and non-corresponding pairs in terms of the sonographic feature posterior acoustic behavior. From these two distributions, one can calculate the likelihood ratio as a function of the distance to the regression line. Pairs of the corresponding dataset tend to have a larger likelihood ratio, yielding a likelihood ratio histogram skewed to the right, whereas pairs of the non-corresponding dataset tend to have a smaller likelihood ratio, yielding a likelihood ratio histogram skewed to the left. One can then classify whether a pair of images corresponds to the same actual lesion by performing the discrimination according to this likelihood ratio. By varying the decision variable threshold Ω, an ROC curve can be computed from the training process for the discrimination task. From the ROC curve, one can determine an optimal threshold value of the classifier to yield some prescribed sensitivity and specificity in the task of distinguishing between corresponding and non-corresponding pairs.</p>
<p id="p-0086" num="0106">The performance of this lesion-matching method can be evaluated for individual computer-extracted features by calculating the area under the curve (AUC) value of the ROC curve. ROC curves were generated for both re-substitution and round-robin analysis. The round-robin evaluation was performed to provide a more realistic estimate of the performance. In a round-robin evaluation, one of the patients is excluded from the database. That is, lesions from this patient are removed from the database and classified according to a classifier trained with the remaining lesions. This process is then repeated for each patient. For datasets generated by lesions across patients, one assumes lesions have been corresponding to a lesion and its counter-part are from the same patient and eliminates them from the training process.</p>
<p id="p-0087" num="0107">Tests using the corresponding dataset paired with the non-corresponding datasets A, B, and C were conducted to assess the performance of the method in distinguishing between corresponding and non-corresponding pairs. The results of the four features that yielded the four highest AUC values are summarized in Tables IV and V which displays the performance in terms of AUC of individual features for the sonographic and mammographic databases, respectively, in the task of distinguishing between corresponding and non-corresponding pairs. Independent validation was performed 11 times by randomly selecting 300 non-corresponding pairs for datasets B and C. The performance lists for these validations are average AUC values resulting from ROC analysis of each of the 11 independent trials. The standard deviations on the AUC values are given in parentheses. The posterior acoustic behavior outperformed the other features in differentiating corresponding and non-corresponding lesions identified on sonograms, yielding an AUC value ranging from 0.73 to 0.79 (0.72 to 0.78 in round robin). The average gray value outperformed others for the mammographic database, yielding an AUC value ranging from 0.68 to 0.71 (0.66 to 0.70 in round robin).</p>
<p id="p-0088" num="0108"><figref idref="DRAWINGS">FIG. 7</figref> illustrates the performance of the method in terms of ROC curves for the posterior acoustic behavior feature for each test in the task of distinguishing corresponding from non-corresponding lesion pairs in the sonographic database. Similarly, <figref idref="DRAWINGS">FIG. 8</figref> illustrates the performance for the average gray value feature in the mammographic database.</p>
<p id="p-0089" num="0109">
<tables id="TABLE-US-00004" num="00004">
<table frame="none" colsep="0" rowsep="0" pgwide="1">
<tgroup align="left" colsep="0" rowsep="0" cols="2">
<colspec colname="offset" colwidth="49pt" align="left"/>
<colspec colname="1" colwidth="273pt" align="center"/>
<tbody valign="top">
<row>
<entry/>
<entry namest="offset" nameend="1" align="center" rowsep="1"/>
</row>
<row>
<entry/>
<entry>US Database: 262 Corresponding lesion pairs</entry>
</row>
</tbody>
</tgroup>
<tgroup align="left" colsep="0" rowsep="0" cols="4">
<colspec colname="offset" colwidth="49pt" align="left"/>
<colspec colname="1" colwidth="77pt" align="center"/>
<colspec colname="2" colwidth="98pt" align="center"/>
<colspec colname="3" colwidth="98pt" align="center"/>
<tbody valign="top">
<row>
<entry/>
<entry>Data set A</entry>
<entry>Dataset B</entry>
<entry>Dataset C</entry>
</row>
<row>
<entry/>
<entry>Non-Corresponding</entry>
<entry>Non-Corresponding</entry>
<entry>Non-Corresponding</entry>
</row>
<row>
<entry/>
<entry>lesion pairs (53)</entry>
<entry>lesion pairs (300)</entry>
<entry>lesion pairs (300)</entry>
</row>
</tbody>
</tgroup>
<tgroup align="left" colsep="0" rowsep="0" cols="7">
<colspec colname="1" colwidth="49pt" align="left"/>
<colspec colname="2" colwidth="49pt" align="center"/>
<colspec colname="3" colwidth="28pt" align="center"/>
<colspec colname="4" colwidth="49pt" align="center"/>
<colspec colname="5" colwidth="49pt" align="center"/>
<colspec colname="6" colwidth="49pt" align="center"/>
<colspec colname="7" colwidth="49pt" align="center"/>
<tbody valign="top">
<row>
<entry/>
<entry/>
<entry>Round</entry>
<entry>Resubsituation</entry>
<entry>Round</entry>
<entry>Resubsituation</entry>
<entry>Round</entry>
</row>
<row>
<entry>Features</entry>
<entry>Resubsituation</entry>
<entry>robin</entry>
<entry>(σ)</entry>
<entry>robin (σ)</entry>
<entry>(σ)</entry>
<entry>robin (σ)</entry>
</row>
<row>
<entry namest="1" nameend="7" align="center" rowsep="1"/>
</row>
</tbody>
</tgroup>
<tgroup align="left" colsep="0" rowsep="0" cols="11">
<colspec colname="1" colwidth="49pt" align="left"/>
<colspec colname="2" colwidth="49pt" align="center"/>
<colspec colname="3" colwidth="28pt" align="char" char="."/>
<colspec colname="4" colwidth="21pt" align="center"/>
<colspec colname="5" colwidth="28pt" align="center"/>
<colspec colname="6" colwidth="21pt" align="center"/>
<colspec colname="7" colwidth="28pt" align="center"/>
<colspec colname="8" colwidth="21pt" align="center"/>
<colspec colname="9" colwidth="28pt" align="center"/>
<colspec colname="10" colwidth="21pt" align="center"/>
<colspec colname="11" colwidth="28pt" align="center"/>
<tbody valign="top">
<row>
<entry>Shape: Filtered</entry>
<entry>0.59</entry>
<entry>0.57</entry>
<entry>0.68</entry>
<entry>(0.01)</entry>
<entry>0.67</entry>
<entry>(0.01)</entry>
<entry>0.74</entry>
<entry>(0.01)</entry>
<entry>0.73</entry>
<entry>(0.01)</entry>
</row>
<row>
<entry>ARD</entry>
</row>
<row>
<entry>Filtered margin</entry>
<entry>0.61</entry>
<entry>0.58</entry>
<entry>0.71</entry>
<entry>(0.02)</entry>
<entry>0.70</entry>
<entry>(0.02)</entry>
<entry>0.76</entry>
<entry>(0.02)</entry>
<entry>0.74</entry>
<entry>(0.02)</entry>
</row>
<row>
<entry>sharpness</entry>
</row>
<row>
<entry>Posterior</entry>
<entry>0.75</entry>
<entry>0.72</entry>
<entry>0.72</entry>
<entry>(0.01)</entry>
<entry>0.70</entry>
<entry>(0.02)</entry>
<entry>0.79</entry>
<entry>(0.02)</entry>
<entry>0.78</entry>
<entry>(0.02)</entry>
</row>
<row>
<entry>acoustic</entry>
</row>
<row>
<entry>behavior</entry>
</row>
<row>
<entry>Texture: conY4</entry>
<entry>0.67</entry>
<entry>0.6</entry>
<entry>0.69</entry>
<entry>(0.02)</entry>
<entry>0.67</entry>
<entry>(0.02)</entry>
<entry>0.70</entry>
<entry>(0.01)</entry>
<entry>0.68</entry>
<entry>(0.01)</entry>
</row>
<row>
<entry namest="1" nameend="11" align="center" rowsep="1"/>
</row>
</tbody>
</tgroup>
</table>
</tables>
</p>
<p id="p-0090" num="0110">
<tables id="TABLE-US-00005" num="00005">
<table frame="none" colsep="0" rowsep="0" pgwide="1">
<tgroup align="left" colsep="0" rowsep="0" cols="2">
<colspec colname="offset" colwidth="49pt" align="left"/>
<colspec colname="1" colwidth="273pt" align="center"/>
<tbody valign="top">
<row>
<entry/>
<entry namest="offset" nameend="1" align="center" rowsep="1"/>
</row>
<row>
<entry/>
<entry>Mammogram Database: 230 Corresponding lesion pairs</entry>
</row>
</tbody>
</tgroup>
<tgroup align="left" colsep="0" rowsep="0" cols="4">
<colspec colname="offset" colwidth="49pt" align="left"/>
<colspec colname="1" colwidth="77pt" align="center"/>
<colspec colname="2" colwidth="98pt" align="center"/>
<colspec colname="3" colwidth="98pt" align="center"/>
<tbody valign="top">
<row>
<entry/>
<entry>Dataset A</entry>
<entry>Dataset B</entry>
<entry>Dataset C</entry>
</row>
<row>
<entry/>
<entry>Non-Corresponding</entry>
<entry>Non-Corresponding</entry>
<entry>Non-Corresponding</entry>
</row>
<row>
<entry/>
<entry>lesion pairs (25)</entry>
<entry>lesion pairs (300)</entry>
<entry>lesion pairs (300)</entry>
</row>
</tbody>
</tgroup>
<tgroup align="left" colsep="0" rowsep="0" cols="7">
<colspec colname="1" colwidth="49pt" align="left"/>
<colspec colname="2" colwidth="49pt" align="center"/>
<colspec colname="3" colwidth="28pt" align="center"/>
<colspec colname="4" colwidth="49pt" align="center"/>
<colspec colname="5" colwidth="49pt" align="center"/>
<colspec colname="6" colwidth="49pt" align="center"/>
<colspec colname="7" colwidth="49pt" align="center"/>
<tbody valign="top">
<row>
<entry/>
<entry/>
<entry>Round</entry>
<entry>Resubsituation</entry>
<entry>Round</entry>
<entry>Resubsituation</entry>
<entry>Round</entry>
</row>
<row>
<entry>Features</entry>
<entry>Resubsituation</entry>
<entry>robin</entry>
<entry>(σ)</entry>
<entry>robin (σ)</entry>
<entry>(σ)</entry>
<entry>robin (σ)</entry>
</row>
<row>
<entry namest="1" nameend="7" align="center" rowsep="1"/>
</row>
</tbody>
</tgroup>
<tgroup align="left" colsep="0" rowsep="0" cols="11">
<colspec colname="1" colwidth="49pt" align="left"/>
<colspec colname="2" colwidth="49pt" align="center"/>
<colspec colname="3" colwidth="28pt" align="center"/>
<colspec colname="4" colwidth="21pt" align="center"/>
<colspec colname="5" colwidth="28pt" align="center"/>
<colspec colname="6" colwidth="21pt" align="center"/>
<colspec colname="7" colwidth="28pt" align="center"/>
<colspec colname="8" colwidth="21pt" align="center"/>
<colspec colname="9" colwidth="28pt" align="center"/>
<colspec colname="10" colwidth="21pt" align="center"/>
<colspec colname="11" colwidth="28pt" align="center"/>
<tbody valign="top">
<row>
<entry>NRG<sub>ROI</sub></entry>
<entry>0.60</entry>
<entry>0.58</entry>
<entry>0.60</entry>
<entry>(0.01)</entry>
<entry>0.59</entry>
<entry>(0.01)</entry>
<entry>0.66</entry>
<entry>(0.01)</entry>
<entry>0.64</entry>
<entry>(0.01)</entry>
</row>
<row>
<entry>Average gray</entry>
<entry>0.68</entry>
<entry>0.66</entry>
<entry>0.70</entry>
<entry>(0.01)</entry>
<entry>0.69</entry>
<entry>(0.01)</entry>
<entry>0.72</entry>
<entry>(0.01)</entry>
<entry>0.71</entry>
<entry>(0.01)</entry>
</row>
<row>
<entry>value</entry>
</row>
<row>
<entry>Contrast</entry>
<entry>0.68</entry>
<entry>0.53</entry>
<entry>0.62</entry>
<entry>(0.01)</entry>
<entry>0.60</entry>
<entry>(0.02)</entry>
<entry>0.63</entry>
<entry>(0.02)</entry>
<entry>0.61</entry>
<entry>(0.02)</entry>
</row>
<row>
<entry>Diameter</entry>
<entry>0.68</entry>
<entry>0.54</entry>
<entry>0.66</entry>
<entry>(0.02)</entry>
<entry>0.63</entry>
<entry>(0.02)</entry>
<entry>0.67</entry>
<entry>(0.01)</entry>
<entry>0.65</entry>
<entry>(0.02)</entry>
</row>
<row>
<entry namest="1" nameend="11" align="center" rowsep="1"/>
</row>
</tbody>
</tgroup>
</table>
</tables>
</p>
<p id="p-0091" num="0111">Furthermore, sonograms from 35 patients who have two or more lesions in the same breast were used to conduct a test for 48 corresponding lesion pairs and 53 non-corresponding lesion pairs. In this test, the posterior acoustic behavior feature performed well with an AUC of 0.81 (0.77 in round robin), which indicates that the non-corresponding datasets B and C comprised by lesions across patients can provide a good estimation in clinical practice.</p>
<p id="p-0092" num="0112"><figref idref="DRAWINGS">FIG. 9</figref> illustrates that the calculated canonical correlation coefficient for four sonographic features (filtered ARD, filtered margin sharpness, posterior acoustic behavior, and texture) derived from the two lesions corresponding to the same physical lesion is 0.85. In comparison, the calculated canonical correlation is 0.37 for the same features when derived from different lesions. A linear relationship can be observed for the canonical variables for features extracted from the two lesions correspond to the same physical lesion. On the other hand, the canonical variables for features derived from different lesions show a much wider spread in distribution. The observed difference in the spreads of the distributions for the two populations can be utilized for classification.</p>
<p id="p-0093" num="0113"><figref idref="DRAWINGS">FIG. 10</figref> illustrates a system for carrying out embodiments of the present invention. An imaging device <b>1001</b> is used to acquire medical images. The images can be stored using a storage unit <b>1002</b>. The images can be processed by a computing unit <b>1003</b> comprising a lesion segmentation device <b>1004</b>, which automatically segments lesions from the background, a feature extraction device <b>1005</b>, which automatically extracts at least one feature corresponding to the lesions, a similarity evaluation device <b>1006</b>, which determines the similarity between the features corresponding to the lesions, and a likelihood estimation device <b>1007</b> which determines a likelihood that the lesions all correspond to the same lesion. The system can also comprise a computer-aided diagnosis device <b>1008</b>, a display device <b>1009</b>, and/or a multimodality device <b>1010</b>, all configured to receive and use the likelihood that the lesions all correspond to the same lesion.</p>
<p id="p-0094" num="0114">Alternatively, the image data of interest may be stored in an image archiving system, such as Picture Archiving Communications System (PACS), and retrieved therefrom for processing according to the present invention. Either way, the present invention obtains the image data for subsequent processing as described before.</p>
<p id="p-0095" num="0115">Embodiments of the invention can modify and improve upon a system for a multimodality display workstation that displays the images as well as information derived from the images. The novel workstation can incorporate multimodality images of the same patient, automatically assess whether the same lesion is being considered across the images, and automatically relate the information calculated from one modality to that from another. For example, the lesion seen on multiple images may be characterized by mammographic feature, sonographic features, MRI features, as well as combination features. Similar lesions across modalities can be automatically retrieved from on-line multimodality reference atlases. In addition, the physical lesion can be represented on single and multi-modality distributions of malignant, benign, and other states. Further, besides presentation of computer calculated and/or generated data, novel means to display image data can also be incorporated.</p>
<p id="p-0096" num="0116">All embodiments of the present invention conveniently may be implemented using a conventional general purpose computer or micro-processor programmed according to the teachings of the present invention, as will be apparent to those skilled in the computer art. Appropriate software may readily be prepared by programmers of ordinary skill based on the teachings of the present disclosure, as will be apparent to those skilled in the software art. In particular, the computer housing may house a motherboard that contains a CPU, memory (e.g., DRAM, ROM, EPROM, EEPROM, SRAM, SDRAM, and Flash RAM), and other optional special purpose logic devices (e.g., ASICS) or configurable logic devices (e.g., GAL and reprogrammable FPGA). The computer also includes plural input devices, (e.g., keyboard and mouse), and a display card for controlling a monitor. Additionally, the computer may include a floppy disk drive; other removable media devices (e.g. compact disc, tape, and removable magneto-optical media); and a hard disk or other fixed high density media drives, connected using an appropriate device bus (e.g., a SCSI bus, an Enhanced IDE bus, or an Ultra DMA bus). The computer may also include a compact disc reader, a compact disc reader/writer unit, or a compact disc jukebox, which may be connected to the same device bus or to another device bus.</p>
<p id="p-0097" num="0117">Examples of computer program products associated with the present invention include compact discs, hard disks, floppy disks, tape, magneto-optical disks, PROMs (e.g., EPROM, EEPROM, Flash EPROM), DRAM, SRAM, SDRAM, etc. Stored on any one or on a combination of these computer readable media, the present invention includes software for controlling both the hardware of the computer and for enabling the computer to interact with a human user. Such software may include, but is not limited to, device drivers, operating systems and user applications, such as development tools. Computer program products of the present invention include any computer readable medium which stores computer program instructions (e.g., computer code devices) which when executed by a computer causes the computer to perform the method of the present invention. The computer code devices of the present invention may be any interpretable or executable code mechanism, including but not limited to, scripts, interpreters, dynamic link libraries, Java classes, and complete executable programs. Moreover, parts of the processing of the present invention may be distributed (e.g., between (1) multiple CPUs or (2) at least one CPU and at least one configurable logic device) for better performance, reliability, and/or cost. For example, an outline or image may be selected on a first computer and sent to a second computer for remote diagnosis.</p>
<p id="p-0098" num="0118">The invention may also be implemented by the preparation of application specific integrated circuits or by interconnecting an appropriate network of conventional component circuits, as will be readily apparent to those skilled in the art.</p>
<p id="p-0099" num="0119">The present method of feature-based correlation can also be implemented more generally by one of ordinary skill in the art for the correlation of other abnormalities of other organs. In particular, the present method is applicable to any type of abnormalities in N dimensions (N&gt;1). Thus, an embodiment of the present method can be readily applied to 2D/3D aneurysms, embolisms, lung cancer, stomach cancer, etc.</p>
<p id="p-0100" num="0120">For the purposes of this description, an image is defined to be a representation of a physical scene, in which the image has been generated by some imaging technology. Examples of imaging technology include television or CCD cameras, or X-ray, sonar, nuclear, or ultrasound imaging devices. The initial medium on which an image is recorded could be an electronic solid-state device, a photographic film, or some other device such as a photostimulable phosphor. That recorded image could then be converted into digital form by a combination of electronic (as in the case of a CCD signal) or mechanical/optical means (as in the case of digitizing a photographic film or digitizing the data from a photostimulable phosphor). The number of dimensions that an image could have could be one (e.g., acoustic signals), two (e.g., X-ray radiological images), or more (e.g., tomosynthesis or nuclear magnetic resonance images).</p>
<p id="p-0101" num="0121">Numerous modifications and variations of the present invention are possible in light of the above teachings. It is therefore to be understood that within the scope of the appended claims, the invention may be practiced otherwise than as specifically described herein.</p>
<?DETDESC description="Detailed Description" end="tail"?>
</description>
<us-math idrefs="MATH-US-00001" nb-file="US07298881-20071120-M00001.NB">
<img id="EMI-M00001" he="6.01mm" wi="76.20mm" file="US07298881-20071120-M00001.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-math idrefs="MATH-US-00002" nb-file="US07298881-20071120-M00002.NB">
<img id="EMI-M00002" he="6.69mm" wi="76.20mm" file="US07298881-20071120-M00002.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-math idrefs="MATH-US-00003" nb-file="US07298881-20071120-M00003.NB">
<img id="EMI-M00003" he="8.81mm" wi="76.20mm" file="US07298881-20071120-M00003.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-claim-statement>The invention claimed is:</us-claim-statement>
<claims id="claims">
<claim id="CLM-00001" num="00001">
<claim-text>1. A computer-implemented method for correlating medical images, comprising:
<claim-text>obtaining first image data representative of a first medical image including a first abnormality;</claim-text>
<claim-text>obtaining second image data representative of a second medical image including a second abnormality;</claim-text>
<claim-text>determining at least one feature value for each of the first and second abnormalities using the first and second image data;</claim-text>
<claim-text>calculating, based on the determined feature values, a likelihood value indicative of a likelihood that the first and second abnormalities are a same abnormality; and</claim-text>
<claim-text>outputting the determined likelihood value,</claim-text>
<claim-text>wherein the step of obtaining the first image data comprises obtaining first image data representative of the first medical image derived using a first modality, the first modality being one of mammography, sonography, and magnetic resonance imaging; and</claim-text>
<claim-text>the step of obtaining the second image data comprises obtaining second image data representative of the second medical image derived using a second modality different from the first modality.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00002" num="00002">
<claim-text>2. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the step of obtaining the first image data comprises obtaining first image data representative of the first medical image using a first modality, in a given view; and
<claim-text>the step of obtaining the second image data comprises obtaining second image data representative of the second medical image derived using a second modality, in a view different from the given view.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00003" num="00003">
<claim-text>3. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the step of obtaining the first image data comprises obtaining first image data representative of the first medical image using a first modality, in a given time; and
<claim-text>the step of obtaining the second image data comprises obtaining second image data representative of the second medical image using the first modality, at a time different from the given time.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00004" num="00004">
<claim-text>4. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the step of obtaining the first image data comprises obtaining first image data representative of the first medical image using a first protocol of a first modality; and
<claim-text>the step of obtaining the second image data comprises obtaining second image data representative of the second medical image using a second protocol of the first modality, the second protocol being different from the first protocol.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00005" num="00005">
<claim-text>5. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the determining step comprises: automatically segmenting the first and second abnormalities.</claim-text>
</claim>
<claim id="CLM-00006" num="00006">
<claim-text>6. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the determining step comprises:
<claim-text>identifying at least one maximally correlated feature using a canonical correlation analysis; and</claim-text>
<claim-text>determining a value for each of the at least one maximally correlated feature for each of the first and second abnormalities.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00007" num="00007">
<claim-text>7. The method of <claim-ref idref="CLM-00006">claim 6</claim-ref>, wherein the identifying step comprises:
<claim-text>selecting the at least one maximally correlated feature from a list of candidate features including filtered ARD, filtered margin sharpness, posterior acoustic behavior, texture, NRG, average gray value, contrast, and diameter.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00008" num="00008">
<claim-text>8. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the determining step comprises:
<claim-text>evaluating a measure of similarity between maximally correlated features to determine feature-conditioned likelihoods that the first and second abnormalities are the same abnormality.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00009" num="00009">
<claim-text>9. The method of <claim-ref idref="CLM-00008">claim 8</claim-ref>, wherein the calculating step comprises:
<claim-text>using a likelihood ratio test based on the feature-conditioned likelihoods that the first and second abnormalities are the same abnormality.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00010" num="00010">
<claim-text>10. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the calculating step comprises:
<claim-text>applying the determined feature values to a classifier to obtain the likelihood value.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00011" num="00011">
<claim-text>11. A system for correlating medical images, comprising:
<claim-text>a mechanism configured to obtain first image data representative of a first medical image including a first abnormality;</claim-text>
<claim-text>a mechanism configured to obtain second image data representative of a second medical image including a second abnormality;</claim-text>
<claim-text>a mechanism configured to determine at least one feature value for each of the first and second abnormalities using the first and second image data;</claim-text>
<claim-text>a mechanism configured to calculate, based on the determined feature values, a likelihood value indicative of a likelihood that the first and second abnormalities are a same abnormality; and</claim-text>
<claim-text>a mechanism configured to output the determined likelihood value,</claim-text>
<claim-text>wherein the mechanism configured to obtain the first image data comprises a mechanism configured to obtain first image data representative of the first medical image using a first modality, the first modality being one of mammography, sonography, and magnetic resonance imaging; and</claim-text>
<claim-text>the mechanism configured to obtain the second image data comprises a mechanism configured to obtain second image data representative of the second medical image using a second modality different from the first modality.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00012" num="00012">
<claim-text>12. The system of <claim-ref idref="CLM-00011">claim 11</claim-ref>, wherein the mechanism configured to obtain the first image data comprises a mechanism configured to obtain first image data representative of the first medical image using a first modality, in a given view; and
<claim-text>the mechanism configured to obtain the second image data comprises a mechanism configured to obtain second image data representative of the second medical image using the first modality, in a view different from the given view.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00013" num="00013">
<claim-text>13. The system of <claim-ref idref="CLM-00011">claim 11</claim-ref>, wherein the mechanism configured to obtain the first image data comprises a mechanism configured to obtain first image data representative of the first medical image using a first modality, at a given time; and
<claim-text>the mechanism configured to obtain the second image data comprises a mechanism configured to obtain second image data representative of the second medical image using the first modality, at a time different from the given time.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00014" num="00014">
<claim-text>14. The system of <claim-ref idref="CLM-00011">claim 11</claim-ref>, wherein the mechanism configured to obtain the first image data comprises a mechanism configured to obtain first image data representative of the first medical image using a first protocol of a first modality; and
<claim-text>the mechanism configured to obtain the second image data comprises a mechanism configured to obtain second image data representative of the second medical image using a second protocol of the first modality, the second protocol being different from the first protocol.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00015" num="00015">
<claim-text>15. The system of <claim-ref idref="CLM-00011">claim 11</claim-ref>, wherein the mechanism configured to determine comprises:
<claim-text>a mechanism configured to automatically segment the first and second abnormalities.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00016" num="00016">
<claim-text>16. The system of <claim-ref idref="CLM-00011">claim 11</claim-ref>, wherein the mechanism configured to determine comprises:
<claim-text>a mechanism configured to identify at least one maximally correlated feature use a canonical correlation analysis; and</claim-text>
<claim-text>a mechanism configured to determine a value for each of the at least one maximally correlated feature for each of the first and second abnormalities.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00017" num="00017">
<claim-text>17. The system of <claim-ref idref="CLM-00016">claim 16</claim-ref>, wherein the mechanism configured to identify the at least one maximally correlated feature comprises:
<claim-text>a mechanism configured to select the at least one maximally correlated feature from a list of candidate features including filtered ARD, filtered margin sharpness, posterior acoustic behavior, texture, NRG, average gray value, contrast, and diameter.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00018" num="00018">
<claim-text>18. The system of <claim-ref idref="CLM-00011">claim 11</claim-ref>, wherein the mechanism configured to determine comprises:
<claim-text>a mechanism configured to evaluate a measure of similarity between maximally correlated features to determine feature-conditioned likelihoods that the first and second abnormalities are the same abnormality.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00019" num="00019">
<claim-text>19. The system of <claim-ref idref="CLM-00018">claim 18</claim-ref>, wherein the mechanism configured to calculate comprises:
<claim-text>a mechanism configured to use a likelihood ratio test based on the feature-conditioned likelihoods that the first and second abnormalities are the same abnormality.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00020" num="00020">
<claim-text>20. The system of <claim-ref idref="CLM-00011">claim 11</claim-ref>, wherein the mechanism configured to calculate comprises:
<claim-text>a mechanism configured to apply the determined feature values to a classifier to obtain the likelihood value.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00021" num="00021">
<claim-text>21. A computer program product which stores, on a computer-readable medium, instructions for execution on a computer system, which when executed by the computer system, causes the computer system to perform the steps of:
<claim-text>obtaining first image data representative of a first medical image including a first abnormality;</claim-text>
<claim-text>obtaining second image data representative of a second medical image including a second abnormality;</claim-text>
<claim-text>determining at least one feature value for each of the first and second abnormalities using the first and second image data;</claim-text>
<claim-text>calculating, based on the determined feature values, a likelihood value indicative of a likelihood that the first and second abnormalities are a same abnormality; and</claim-text>
<claim-text>outputting the determined likelihood values,</claim-text>
<claim-text>wherein the step of obtaining the first image data comprises obtaining first image data representative of the first medical image using a first modality, the first modality being one of mammography, sonography, and magnetic resonance imaging; and</claim-text>
<claim-text>the step of obtaining the second image data comprises obtaining second image data representative of the second medical image using a second modality different from the first modality.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00022" num="00022">
<claim-text>22. The computer program product of <claim-ref idref="CLM-00021">claim 21</claim-ref>, wherein the step of obtaining the first image data comprises obtaining first image data representative of the first medical image using a first modality, in a given view; and
<claim-text>the step of obtaining the second image data comprises obtaining second image data representative of the second medical image using the first modality, in a view different from the given view.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00023" num="00023">
<claim-text>23. The computer program product of <claim-ref idref="CLM-00021">claim 21</claim-ref>, wherein the step of obtaining the first image data comprises obtaining first image data representative of the first medical image using a first modality, at a given time; and
<claim-text>the step of obtaining the second image data comprises obtaining second image data representative of the second medical image using the first modality, at a time different from the given time.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00024" num="00024">
<claim-text>24. The computer program product of <claim-ref idref="CLM-00021">claim 21</claim-ref>, wherein the step of obtaining the first image data comprises obtaining first image data representative of the first medical image using a first protocol of a first modality; and
<claim-text>the step of obtaining the second image data comprises obtaining second image data representative of the second medical image using a second protocol of the first modality, the second protocol being different from the first protocol.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00025" num="00025">
<claim-text>25. The computer program product of <claim-ref idref="CLM-00021">claim 21</claim-ref>, wherein the determining step comprises:
<claim-text>automatically segmenting the first and second abnormalities.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00026" num="00026">
<claim-text>26. The computer program product of <claim-ref idref="CLM-00021">claim 21</claim-ref>, wherein the determining step comprises:
<claim-text>identifying at least one maximally correlated feature using a canonical correlation analysis; and</claim-text>
<claim-text>determining a value for each of the at least one maximally correlated feature for each of the first and second abnormalities.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00027" num="00027">
<claim-text>27. The computer program product of <claim-ref idref="CLM-00026">claim 26</claim-ref>, wherein the identifying step comprises:
<claim-text>selecting the at least one maximally correlated feature from a list of candidate features including filtered ARD, filtered margin sharpness, posterior acoustic behavior, texture, NRG, average gray value, contrast, and diameter.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00028" num="00028">
<claim-text>28. The computer program product of <claim-ref idref="CLM-00021">claim 21</claim-ref>, wherein the determining step comprises:
<claim-text>evaluating a measure of similarity between maximally correlated features to determine feature-conditioned likelihoods that the first and second abnormalities are the same abnormality.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00029" num="00029">
<claim-text>29. The computer program product of <claim-ref idref="CLM-00028">claim 28</claim-ref>, wherein the calculating step comprises:
<claim-text>using a likelihood ratio test based on the feature-conditioned likelihoods that the first and second abnormalities are the same abnormality.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00030" num="00030">
<claim-text>30. The computer program product of <claim-ref idref="CLM-00021">claim 21</claim-ref>, wherein the calculating step comprises:
<claim-text>applying the determined feature values to a classifier to obtain the likelihood value.</claim-text>
</claim-text>
</claim>
</claims>
</us-patent-grant>
