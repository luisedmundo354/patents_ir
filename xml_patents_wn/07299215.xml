<us-patent-grant lang="EN" dtd-version="v4.2 2006-08-23" file="US07299215-20071120.XML" status="PRODUCTION" id="us-patent-grant" country="US" date-produced="20071106" date-publ="20071120">
<us-bibliographic-data-grant>
<publication-reference>
<document-id>
<country>US</country>
<doc-number>07299215</doc-number>
<kind>B2</kind>
<date>20071120</date>
</document-id>
</publication-reference>
<application-reference appl-type="utility">
<document-id>
<country>US</country>
<doc-number>10419761</doc-number>
<date>20030422</date>
</document-id>
</application-reference>
<us-application-series-code>10</us-application-series-code>
<us-term-of-grant>
<us-term-extension>739</us-term-extension>
</us-term-of-grant>
<classifications-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>G</section>
<class>06</class>
<subclass>F</subclass>
<main-group>17</main-group>
<subgroup>00</subgroup>
<symbol-position>F</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20071120</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>G</section>
<class>06</class>
<subclass>N</subclass>
<main-group>5</main-group>
<subgroup>00</subgroup>
<symbol-position>L</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20071120</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
</classifications-ipcr>
<classification-national>
<country>US</country>
<main-classification>706 45</main-classification>
<further-classification>706 14</further-classification>
<further-classification>706 12</further-classification>
</classification-national>
<invention-title id="d0e53">Cross-validation for naive bayes data mining model</invention-title>
<references-cited>
<citation>
<patcit num="00001">
<document-id>
<country>US</country>
<doc-number>6182058</doc-number>
<kind>B1</kind>
<name>Kohavi</name>
<date>20010100</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>706 45</main-classification></classification-national>
</citation>
<citation>
<patcit num="00002">
<document-id>
<country>US</country>
<doc-number>6278464</doc-number>
<kind>B1</kind>
<name>Kohavi et al.</name>
<date>20010800</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>345440</main-classification></classification-national>
</citation>
<citation>
<patcit num="00003">
<document-id>
<country>US</country>
<doc-number>6460049</doc-number>
<kind>B1</kind>
<name>Becker et al.</name>
<date>20021000</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>7071041</main-classification></classification-national>
</citation>
<citation>
<patcit num="00004">
<document-id>
<country>US</country>
<doc-number>6807491</doc-number>
<kind>B2</kind>
<name>Pavlovic et al.</name>
<date>20041000</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>702 20</main-classification></classification-national>
</citation>
<citation>
<patcit num="00005">
<document-id>
<country>US</country>
<doc-number>2003/0077586</doc-number>
<kind>A1</kind>
<name>Pavlovic et al.</name>
<date>20030400</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>435  6</main-classification></classification-national>
</citation>
<citation>
<patcit num="00006">
<document-id>
<country>US</country>
<doc-number>2003/0176931</doc-number>
<kind>A1</kind>
<name>Pednault et al.</name>
<date>20030900</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>700 31</main-classification></classification-national>
</citation>
</references-cited>
<number-of-claims>33</number-of-claims>
<us-exemplary-claim>1</us-exemplary-claim>
<us-field-of-classification-search>
<classification-national>
<country>US</country>
<main-classification>706 45</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>706 14</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>706 12</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>702 20</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>700 31</main-classification>
</classification-national>
</us-field-of-classification-search>
<figures>
<number-of-drawing-sheets>5</number-of-drawing-sheets>
<number-of-figures>5</number-of-figures>
</figures>
<us-related-documents>
<us-provisional-application>
<document-id>
<country>US</country>
<doc-number>60379110</doc-number>
<kind>00</kind>
<date>20020510</date>
</document-id>
</us-provisional-application>
<related-publication>
<document-id>
<country>US</country>
<doc-number>20030212851</doc-number>
<kind>A1</kind>
<date>20031113</date>
</document-id>
</related-publication>
</us-related-documents>
<parties>
<applicants>
<applicant sequence="001" app-type="applicant-inventor" designation="us-only">
<addressbook>
<last-name>Drescher</last-name>
<first-name>Gary L.</first-name>
<address>
<city>Cambridge</city>
<state>MA</state>
<country>US</country>
</address>
</addressbook>
<nationality>
<country>omitted</country>
</nationality>
<residence>
<country>US</country>
</residence>
</applicant>
<applicant sequence="002" app-type="applicant-inventor" designation="us-only">
<addressbook>
<last-name>Kuntala</last-name>
<first-name>Pavani</first-name>
<address>
<city>Nashua</city>
<state>NH</state>
<country>US</country>
</address>
</addressbook>
<nationality>
<country>omitted</country>
</nationality>
<residence>
<country>US</country>
</residence>
</applicant>
</applicants>
<agents>
<agent sequence="01" rep-type="attorney">
<addressbook>
<orgname>Bingham McCutchen LLP</orgname>
<address>
<country>unknown</country>
</address>
</addressbook>
</agent>
</agents>
</parties>
<assignees>
<assignee>
<addressbook>
<orgname>Oracle International Corporation</orgname>
<role>02</role>
<address>
<city>Redwood Shores</city>
<state>CA</state>
<country>US</country>
</address>
</addressbook>
</assignee>
</assignees>
<examiners>
<primary-examiner>
<last-name>Hirl</last-name>
<first-name>Joseph P</first-name>
<department>2129</department>
</primary-examiner>
</examiners>
</us-bibliographic-data-grant>
<abstract id="abstract">
<p id="p-0001" num="0000">A system, method, and computer program product provides a useful measure of the accuracy of a Naïve Bayes predictive model and reduced computational expense relative to conventional techniques. A method for measuring accuracy of a Naïve Bayes predictive model comprises the steps of receiving a training dataset comprising a plurality of rows of data, building a Naïve Bayes predictive model using the training dataset, for each of at least a portion of the plurality of rows of data in the training dataset incrementally untraining the Naïve Bayes predictive model using the row of data and determining an accuracy of the incrementally untrained Naïve Bayes predictive model, and determining an aggregate accuracy of the Naïve Bayes predictive model.</p>
</abstract>
<drawings id="DRAWINGS">
<figure id="Fig-EMI-D00000" num="00000">
<img id="EMI-D00000" he="214.29mm" wi="94.06mm" file="US07299215-20071120-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00001" num="00001">
<img id="EMI-D00001" he="215.90mm" wi="155.96mm" orientation="landscape" file="US07299215-20071120-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00002" num="00002">
<img id="EMI-D00002" he="229.95mm" wi="168.66mm" file="US07299215-20071120-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00003" num="00003">
<img id="EMI-D00003" he="234.36mm" wi="96.18mm" file="US07299215-20071120-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00004" num="00004">
<img id="EMI-D00004" he="212.34mm" wi="126.66mm" orientation="landscape" file="US07299215-20071120-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00005" num="00005">
<img id="EMI-D00005" he="231.39mm" wi="96.35mm" file="US07299215-20071120-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
</drawings>
<description id="description">
<?RELAPP description="Other Patent Relations" end="lead"?>
<heading id="h-0001" level="1">CROSS-REFERENCE TO RELATED APPLICATIONS</heading>
<p id="p-0002" num="0001">The benefit of provisional application 60/379,110, filed May 10, 2002, under 35 U.S.C. § 119(e), is hereby claimed.</p>
<?RELAPP description="Other Patent Relations" end="tail"?>
<?BRFSUM description="Brief Summary" end="lead"?>
<heading id="h-0002" level="1">FIELD OF THE INVENTION</heading>
<p id="p-0003" num="0002">The present invention relates to a system, method, and computer program product for measuring accuracy of a Naïve Bayes predictive model using cross-validation.</p>
<heading id="h-0003" level="1">BACKGROUND OF THE INVENTION</heading>
<p id="p-0004" num="0003">Data mining is a technique by which hidden patterns may be found in a group of data. True data mining doesn't just change the presentation of data, but actually discovers previously unknown relationships among the data. Data mining is typically implemented as software in association with database systems. Data mining includes several major steps. First, data mining models are generated by based on one or more data analysis algorithms. Initially, the models are “untrained”, but are “trained” by processing training data and generating information that defines the model. The generated information is then deployed for use in data mining, for example, by providing predictions of future behavior or recommendations for actions to be taken based on specific past behavior.</p>
<p id="p-0005" num="0004">One particularly useful type of data mining model is based on the Bayesian classification technique. Bayesian classifiers are statistical classifiers. They can predict class membership probabilities, such as the probability that a given sample belongs to a particular class. Bayesian classification is based on Bayes theorem. Studies comparing classification algorithms have found a simple Bayesian classifier known as the naive Bayesian classifier to be comparable in performance with decision tree and neural network classifiers. Bayesian classifiers have also exhibited high accuracy and speed when applied to large databases.</p>
<p id="p-0006" num="0005">Users of a data mining predictive model benefit from knowing in advance how accurate a model's predictions will be. Cross-validation is one technique for measuring the accuracy of a predictive model. Leave-one-out cross-validation is an especially accurate special case of cross-validation, but it is ordinarily computationally expensive. Thus, a need arises for a technique by which leave-one-out cross-validation may be performed that provides a useful measure of the accuracy of a predictive model, but that provides reduced computational expense relative to conventional techniques.</p>
<heading id="h-0004" level="1">SUMMARY OF THE INVENTION</heading>
<p id="p-0007" num="0006">The present invention is a system, method, and computer program product that provides a useful measure of the accuracy of a Naïve Bayes predictive model, but that provides reduced computational expense relative to conventional techniques.</p>
<p id="p-0008" num="0007">In one embodiment of the present invention, a method for measuring accuracy of a Naïve Bayes predictive model comprises the steps of defining code executable by a database management system for performing cross-validation of the Naïve Bayes predictive model, executing the defined code so as to perform cross-validation of the Naïve Bayes predictive model, and outputting a an indication of the accuracy of the Naïve Bayes predictive model. The executing step may comprise the steps of receiving a training dataset comprising a plurality of rows of data, building a Naïve Bayes predictive model using the training dataset, for each of at least a portion of the plurality of rows of data in the training dataset, incrementally untraining the Naïve Bayes predictive model using the row of data, and determining an accuracy of the incrementally untrained Naïve Bayes predictive model, and determining an aggregate accuracy of the Naïve Bayes predictive model.</p>
<p id="p-0009" num="0008">The step of building the Naïve Bayes predictive model using the training dataset may comprise the step of computing probabilities of target values based on counts of occurrences of target values in training dataset. The step of incrementally untraining the Naïve Bayes predictive model may comprise the steps of if a target value of the row of data equals a target value being computed, computing a probability of the target value based on a count of occurrence of the target value minus one and if the target value of the row of data does not equal the target value being computed, computing a probability of the target value based on the count of occurrence of the target value. The step of determining an accuracy of the incrementally untrained Naïve Bayes predictive model may comprise the steps of applying the incrementally untrained Naïve Bayes predictive model to the row of data to generate an output and determining an error between the model output and the row of data. The step of determining an aggregate accuracy of the Naïve Bayes predictive model may comprise the step of determining an average of the determined errors between the model output and the row of data.</p>
<p id="p-0010" num="0009">In one embodiment of the present invention, a method for measuring accuracy of a Naïve Bayes predictive model comprises the steps of receiving a training dataset comprising a plurality of partitions of rows of data, building a Naïve Bayes predictive model using the training dataset, for each of at least a portion of the plurality of partitions of data in the training dataset, incrementally untraining the Naïve Bayes predictive model using rows of data in the partition, and determining an accuracy of the incrementally untrained Naïve Bayes predictive model, and determining an aggregate accuracy of the Naïve Bayes predictive model. The step of building the Naïve Bayes predictive model using the training dataset may comprise the step of computing probabilities of target values based on counts of occurrences of target values in training dataset. The step of incrementally untraining the Naïve Bayes predictive model may comprise the steps of if a target value of a row of data in the partition equals a target value being computed, computing a probability of the target value based on a count of occurrence of the target value minus one, and if the target value of the row of data in the partition does not equal the target value being computed, computing a probability of the target value based on the count of occurrence of the target value. The step of determining an accuracy of the incrementally untrained Naïve Bayes predictive model may comprise the steps of applying the incrementally untrained Naïve Bayes predictive model to the row of data to generate an output, and determining an error between the model output and the row of data. The step of determining an aggregate accuracy of the Naïve Bayes predictive model may comprise the step of determining an average of the determined errors between the model output and the row of data.</p>
<?BRFSUM description="Brief Summary" end="tail"?>
<?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?>
<description-of-drawings>
<heading id="h-0005" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading>
<p id="p-0011" num="0010">The details of the present invention, both as to its structure and operation, can best be understood by referring to the accompanying drawings, in which like reference numbers and designations refer to like elements.</p>
<p id="p-0012" num="0011"><figref idref="DRAWINGS">FIG. 1</figref> is an exemplary data flow diagram of a data mining process, including building and scoring of models and generation of predictions/recommendations.</p>
<p id="p-0013" num="0012"><figref idref="DRAWINGS">FIG. 2</figref> is an exemplary block diagram of a data mining system, in which the present invention may be implemented.</p>
<p id="p-0014" num="0013"><figref idref="DRAWINGS">FIG. 3</figref> is an exemplary flow diagram of a process of leave-one-out cross-validation of a Naïve Bayes model, according to the present invention.</p>
<p id="p-0015" num="0014"><figref idref="DRAWINGS">FIG. 4</figref> is an exemplary data flow diagram of the processing shown in <figref idref="DRAWINGS">FIG. 3</figref> and <figref idref="DRAWINGS">FIG. 5</figref>.</p>
<p id="p-0016" num="0015"><figref idref="DRAWINGS">FIG. 5</figref> is an exemplary flow diagram of a process of n-fold cross-validation of a Naïve Bayes model, according to the present invention.</p>
</description-of-drawings>
<?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?>
<?DETDESC description="Detailed Description" end="lead"?>
<heading id="h-0006" level="1">DETAILED DESCRIPTION OF THE INVENTION</heading>
<p id="p-0017" num="0016">An exemplary data flow diagram of a data mining process, including building and scoring of models and generation of predictions/recommendations, is shown in <figref idref="DRAWINGS">FIG. 1</figref>. The training/model building step <b>102</b> involves generating the models that are used to perform data mining recommendation and prediction. The inputs to training/model building step <b>102</b> include training parameters <b>104</b>, training data <b>106</b>, and untrained models <b>108</b>. Untrained models <b>108</b> include algorithms that process the training data <b>106</b> in order to actually build the models. Training parameters <b>104</b> are parameters that are input to the data-mining model building algorithms to control how the algorithms build the models. Training data <b>106</b> is data that is input to the algorithms and which is used to actually build the models.</p>
<p id="p-0018" num="0017">Training/model building step <b>102</b> invokes the data mining model building algorithms included in untrained models <b>108</b>, initializes the algorithms using the training parameters <b>104</b>, processes training data <b>106</b> using the algorithms to build the model, and generates trained model <b>110</b>. Trained model <b>110</b> may also be evaluated and adjusted in order to improve the quality, i.e. prediction accuracy, of the model. Trained model <b>110</b> is then encoded in an appropriate format and deployed for use in making predictions or recommendations.</p>
<p id="p-0019" num="0018">Scoring step <b>112</b> involves using the deployed trained model <b>110</b> to make predictions or recommendations based on new data that is received. Trained model <b>110</b>, prediction parameters <b>114</b>, and prediction data <b>116</b> are input to scoring step <b>112</b>. Trained models <b>110</b> include information defining the model that was generated by model building step <b>102</b>. Prediction parameters <b>114</b> are parameters that are input to the scoring step <b>118</b> to control the scoring of scoring data <b>116</b> against trained model <b>110</b> and are input to the selection and prediction/recommendation step <b>120</b> to control the selection of the scored data and the generation of predictions and recommendations</p>
<p id="p-0020" num="0019">Scoring data <b>116</b> is processed according trained model <b>110</b>, as controlled by prediction parameters <b>114</b>, to generate one or more scores for each row of data in scoring data <b>116</b>. The scores for each row of data indicate how closely the row of data matches attributes of the model, how much confidence may be placed in the prediction, how likely each output prediction/recommendation to be true, and other statistical indicators. Scored data <b>118</b> is output from scoring step <b>112</b> and includes predictions or recommendations, along with corresponding probabilities for the scored data.</p>
<p id="p-0021" num="0020">Scored data <b>118</b> is input to selection and prediction/recommendation generation step, which evaluates the probabilities associated with the predictions/recommendations and selects at least a portion of the predictions/recommendations. The selected predictions/recommendations are those having probabilities meeting the selection criteria. The selection criteria may be defined by desired results data and/or by predefined or default criteria included in selection/generation step <b>120</b>. In addition, the selection criteria may include a limit on the number of predictions/recommendations that are to be selected, or may indicate that the predictions/recommendations are to be sorted based on their associated probabilities. The selected predictions/recommendations are output <b>122</b> from step <b>120</b> for use in data mining.</p>
<p id="p-0022" num="0021">An exemplary block diagram of a data mining system <b>200</b>, in which the present invention may be implemented, is shown in <figref idref="DRAWINGS">FIG. 2</figref>. System <b>200</b> is typically a programmed general-purpose computer system, such as a personal computer, workstation, server system, and minicomputer or mainframe computer. System <b>200</b> includes one or more processors (CPUs) <b>202</b>A-<b>202</b>N, input/output circuitry <b>204</b>, network adapter <b>206</b>, and memory <b>208</b>. CPUs <b>202</b>A-<b>202</b>N execute program instructions in order to carry out the functions of the present invention. Typically, CPUs <b>202</b>A-<b>202</b>N are one or more microprocessors, such as an INTEL PENTIUM® processor. <figref idref="DRAWINGS">FIG. 2</figref> illustrates an embodiment in which system <b>200</b> is implemented as a single multi-processor computer system, in which multiple processors <b>202</b>A-<b>202</b>N share system resources, such as memory <b>208</b>, input/output circuitry <b>204</b>, and network adapter <b>206</b>. However, the present invention also contemplates embodiments in which system <b>200</b> is implemented as a plurality of networked computer systems, which may be single-processor computer systems, multi-processor computer systems, or a mix thereof.</p>
<p id="p-0023" num="0022">Input/output circuitry <b>204</b> provides the capability to input data to, or output data from, system <b>200</b>. For example, input/output circuitry may include input devices, such as keyboards, mice, touchpads, trackballs, scanners, etc., output devices, such as video adapters, monitors, printers, etc., and input/output devices, such as, modems, etc. Network adapter <b>206</b> interfaces system <b>200</b> with Internet/intranet <b>210</b>. Internet/intranet <b>210</b> may include one or more standard local area network (LAN) or wide area network (WAN), such as Ethernet, Token Ring, the Internet, or a private or proprietary LAN/WAN.</p>
<p id="p-0024" num="0023">Memory <b>208</b> stores program instructions that are executed by, and data that are used and processed by, CPU <b>202</b> to perform the functions of system <b>200</b>. Memory <b>208</b> may include electronic memory devices, such as random-access memory (RAM), read-only memory (ROM), programmable read-only memory (PROM), electrically erasable programmable read-only memory (EEPROM), flash memory, etc., and electromechanical memory, such as magnetic disk drives, tape drives, optical disk drives, etc., which may use an integrated drive electronics (IDE) interface, or a variation or enhancement thereof, such as enhanced IDE (EIDE) or ultra direct memory access (UDMA), or a small computer system interface (SCSI) based interface, or a variation or enhancement thereof, such as fast-SCSI, wide-SCSI, fast and wide-SCSI, etc, or a fiber channel-arbitrated loop (FC-AL) interface.</p>
<p id="p-0025" num="0024">In the example shown in <figref idref="DRAWINGS">FIG. 2</figref>, memory <b>208</b> includes training parameters <b>212</b>, untrained Naïve Bayes model <b>214</b>, training dataset <b>216</b>, trained model <b>218</b>, accuracy determination results <b>220</b>, training/model building routines <b>224</b>, untraining routines <b>226</b>, accuracy determination routines <b>228</b>, aggregate accuracy determination routines <b>230</b>, and operating system <b>232</b>. Training parameters <b>212</b> are parameters that are input to the data-mining model building algorithms to control how the algorithms build the models. Untrained model <b>214</b> includes one or more untrained Naïve Bayes models that are used to build the models. Training dataset <b>216</b> includes data that is input to the algorithms and which is used to actually build the models. Trained model <b>218</b> includes representations of the Naïve Bayes model that are used to score data. Accuracy determination results <b>220</b> include entries, each representing the accuracy of the incrementally untrained model using a row of data from training dataset <b>216</b> as determined by accuracy determination routines <b>228</b>. Aggregate accuracy determination results <b>222</b> is an aggregate indicator of the accuracy of trained model <b>218</b>, which is generated from accuracy determination results <b>220</b> by aggregate accuracy determination routines <b>230</b>. Training/model building routines <b>224</b> build the trained model using untrained model <b>214</b>, training parameters <b>212</b>, and training data <b>216</b>. Untraining routines incrementally untrain trained model <b>218</b> for each set of rows of data in training dataset <b>216</b>. Accuracy determination routines <b>228</b> determine the accuracy of the incrementally untrained model for each set of rows of data from training dataset <b>216</b>. Aggregate accuracy determination routines <b>230</b> generate aggregate accuracy determination result <b>222</b>. Operating system <b>226</b> provides overall system functionality.</p>
<p id="p-0026" num="0025">As shown in <figref idref="DRAWINGS">FIG. 2</figref>, the present invention contemplates implementation on a system or systems that provide multi-processor, multi-tasking, multi-process, and/or multi-thread computing, as well as implementation on systems that provide only single processor, single thread computing. Multi-processor computing involves performing computing using more than one processor. Multi-tasking computing involves performing computing using more than one operating system task. A task is an operating system concept that refers to the combination of a program being executed and bookkeeping information used by the operating system. Whenever a program is executed, the operating system creates a new task for it. The task is like an envelope for the program in that it identifies the program with a task number and attaches other bookkeeping information to it. Many operating systems, including UNIX®, OS/2®, and WINDOWS®, are capable of running many tasks at the same time and are called multitasking operating systems. Multi-tasking is the ability of an operating system to execute more than one executable at the same time. Each executable is running in its own address space, meaning that the executables have no way to share any of their memory. This has advantages, because it is impossible for any program to damage the execution of any of the other programs running on the system. However, the programs have no way to exchange any information except through the operating system (or by reading files stored on the file system). Multi-process computing is similar to multi-tasking computing, as the terms task and process are often used interchangeably, although some operating systems make a distinction between the two.</p>
<p id="p-0027" num="0026">The most straightforward way to determine accuracy is to build a model using a portion of the available training data, and compute the model's error rate when applied to the remainder of the data. If the same data were used both for building the model and for scoring, the model would be given an unfair advantage that would artificially inflate its apparent accuracy. When working with a limited amount of training data, however, setting aside enough data to support an accurate scoring measure might seriously detract from the quality of the model, which generally improves as more data is available. Cross-validation is a way to mitigate this problem.</p>
<p id="p-0028" num="0027">With leave-n-out cross-validation, the training data is divided into n partitions, each containing approximately 1/n of the data's records. Next, n models are built; for each model, all but one of the partitions are used for training, and the remaining one is used for scoring the model's accuracy. Typically, the accuracy measures are then averaged together.</p>
<p id="p-0029" num="0028">Leave-one-out cross-validation is a special case of leave-n-out cross-validation. In leave-one-out cross-validation, the number of partitions is equal to the number of training records, and each partition consists of a single record. Thus, the number of models equals the number of training records, with each model being built from almost all the training data. Building so many models is computationally expensive. But with Naive Bayes models, there is a shortcut: it is possible to build a single model, using all the training data, and then quickly modify the model to make it as though a particular record had not been used when building the model. This process can be called “incrementally untraining” the model for that record. By measuring the model's accuracy on each training record, first temporarily incrementally untraining the model for that record, we obtain the same result as by building many models, but without incurring the greatly multiplied expense of actually building them.</p>
<p id="p-0030" num="0029">Naive Bayes uses Bayes' Theorem, combined with a (“naive”) presumption of conditional independence, to predict, for each record (a set of values, one for each field), the value of a target (output) field, from evidence given by one or more predictor (input) fields.</p>
<p id="p-0031" num="0030">Given target field T with possible values T<b>1</b>, . . . Tm, and predictor fields I<b>1</b>, . . . In, with values (in the current record) of I<b>1</b>*, . . . In*, the probability that the target T has value T<sub>i</sub>, given the values of the predictors, is derived as follows:</p>
<p id="p-0032" num="0031">
<maths id="MATH-US-00001" num="00001">
<math overflow="scroll">
<mtable>
  <mtr>
    <mtd>
      <mrow>
        <mi>P</mi>
        <mo>(</mo>
        <mrow>
          <msub>
            <mi>T</mi>
            <mi>i</mi>
          </msub>
          <mo>⁢</mo>
          <mrow>
            <mo></mo>
            <mrow>
              <msub>
                <mi>I</mi>
                <msup>
                  <mn>1</mn>
                  <mo>*</mo>
                </msup>
              </msub>
              <mo>,</mo>
              <mstyle>
                <mspace width="0.3em" height="0.3ex"/>
              </mstyle>
              <mo>⁢</mo>
              <mrow>
                <mi>…</mi>
                <mo>⁢</mo>
                <mstyle>
                  <mspace width="0.8em" height="0.8ex"/>
                </mstyle>
                <mo>⁢</mo>
                <msub>
                  <mi>I</mi>
                  <msup>
                    <mi>n</mi>
                    <mo>*</mo>
                  </msup>
                </msub>
              </mrow>
            </mrow>
            <mo>)</mo>
          </mrow>
        </mrow>
      </mrow>
    </mtd>
  </mtr>
  <mtr>
    <mtd>
      <mrow>
        <mrow>
          <mrow>
            <mrow>
              <mrow>
                <mrow>
                  <mo>=</mo>
                  <mrow>
                    <mrow>
                      <mi>P</mi>
                      <mo>⁡</mo>
                      <mrow>
                        <mo>(</mo>
                        <msub>
                          <mi>T</mi>
                          <mi>i</mi>
                        </msub>
                        <mo>)</mo>
                      </mrow>
                    </mrow>
                    <mo>⁢</mo>
                    <mrow>
                      <mi>P</mi>
                      <mo>(</mo>
                      <mrow>
                        <msub>
                          <mi>I</mi>
                          <msup>
                            <mn>1</mn>
                            <mo>*</mo>
                          </msup>
                        </msub>
                        <mo>,</mo>
                        <mstyle>
                          <mspace width="0.3em" height="0.3ex"/>
                        </mstyle>
                        <mo>⁢</mo>
                        <mrow>
                          <mi>…</mi>
                          <mo>⁢</mo>
                          <mstyle>
                            <mspace width="0.8em" height="0.8ex"/>
                          </mstyle>
                          <mo>⁢</mo>
                          <msub>
                            <mi>I</mi>
                            <msup>
                              <mi>n</mi>
                              <mo>*</mo>
                            </msup>
                          </msub>
                        </mrow>
                      </mrow>
                      <mo></mo>
                    </mrow>
                    <mo>⁢</mo>
                    <msub>
                      <mi>T</mi>
                      <mi>i</mi>
                    </msub>
                  </mrow>
                </mrow>
                <mo>)</mo>
              </mrow>
              <mo>/</mo>
              <mrow>
                <mi>P</mi>
                <mo>⁡</mo>
                <mrow>
                  <mo>(</mo>
                  <mrow>
                    <msub>
                      <mi>I</mi>
                      <msup>
                        <mn>1</mn>
                        <mo>*</mo>
                      </msup>
                    </msub>
                    <mo>,</mo>
                    <mstyle>
                      <mspace width="0.3em" height="0.3ex"/>
                    </mstyle>
                    <mo>⁢</mo>
                    <mrow>
                      <mi>…</mi>
                      <mo>⁢</mo>
                      <mstyle>
                        <mspace width="0.8em" height="0.8ex"/>
                      </mstyle>
                      <mo>⁢</mo>
                      <msub>
                        <mi>I</mi>
                        <msup>
                          <mi>n</mi>
                          <mo>*</mo>
                        </msup>
                      </msub>
                    </mrow>
                  </mrow>
                  <mo>)</mo>
                </mrow>
              </mrow>
            </mrow>
            <mo>,</mo>
            <mrow>
              <mi>by</mi>
              <mo>⁢</mo>
              <mstyle>
                <mspace width="0.8em" height="0.8ex"/>
              </mstyle>
              <mo>⁢</mo>
              <mi>Bayes</mi>
            </mrow>
          </mrow>
          <mo>’</mo>
        </mrow>
        <mo>⁢</mo>
        <mstyle>
          <mspace width="0.8em" height="0.8ex"/>
        </mstyle>
        <mo>⁢</mo>
        <mi>theorem</mi>
      </mrow>
    </mtd>
  </mtr>
  <mtr>
    <mtd>
      <mrow>
        <mrow>
          <mrow>
            <mrow>
              <mrow>
                <mrow>
                  <mrow>
                    <msup>
                      <mo> </mo>
                      <mo>∼</mo>
                    </msup>
                    <mo>⁢</mo>
                    <mi>P</mi>
                  </mrow>
                  <mo>⁡</mo>
                  <mrow>
                    <mo>(</mo>
                    <msub>
                      <mi>T</mi>
                      <mi>i</mi>
                    </msub>
                    <mo>)</mo>
                  </mrow>
                </mrow>
                <mo>⁢</mo>
                <msub>
                  <mo>?</mo>
                  <mi>j</mi>
                </msub>
                <mo>⁢</mo>
                <mi>P</mi>
              </mrow>
              <mo>⁢</mo>
              <mrow>
                <mo>(</mo>
                <msub>
                  <mi>I</mi>
                  <msup>
                    <mi>j</mi>
                    <mo>*</mo>
                  </msup>
                </msub>
                <mo></mo>
              </mrow>
              <mo>⁢</mo>
              <msub>
                <mi>T</mi>
                <mi>i</mi>
              </msub>
            </mrow>
            <mo>)</mo>
          </mrow>
          <mo>/</mo>
          <mrow>
            <mi>P</mi>
            <mo>⁡</mo>
            <mrow>
              <mo>(</mo>
              <mrow>
                <msub>
                  <mi>I</mi>
                  <msup>
                    <mn>1</mn>
                    <mo>*</mo>
                  </msup>
                </msub>
                <mo>,</mo>
                <mstyle>
                  <mspace width="0.3em" height="0.3ex"/>
                </mstyle>
                <mo>⁢</mo>
                <mrow>
                  <mi>…</mi>
                  <mo>⁢</mo>
                  <mstyle>
                    <mspace width="0.8em" height="0.8ex"/>
                  </mstyle>
                  <mo>⁢</mo>
                  <msub>
                    <mi>I</mi>
                    <msup>
                      <mi>n</mi>
                      <mo>*</mo>
                    </msup>
                  </msub>
                </mrow>
              </mrow>
              <mo>)</mo>
            </mrow>
          </mrow>
        </mrow>
        <mo>,</mo>
        <mrow>
          <mi>by</mi>
          <mo>⁢</mo>
          <mstyle>
            <mspace width="0.8em" height="0.8ex"/>
          </mstyle>
          <mo>⁢</mo>
          <mi>the</mi>
          <mo>⁢</mo>
          <mstyle>
            <mspace width="0.8em" height="0.8ex"/>
          </mstyle>
          <mo>⁢</mo>
          <mi>conditional</mi>
        </mrow>
      </mrow>
    </mtd>
  </mtr>
  <mtr>
    <mtd>
      <mrow>
        <mi>independence</mi>
        <mo>⁢</mo>
        <mstyle>
          <mspace width="0.8em" height="0.8ex"/>
        </mstyle>
        <mo>⁢</mo>
        <mi>assumption</mi>
      </mrow>
    </mtd>
  </mtr>
  <mtr>
    <mtd>
      <mrow>
        <mrow>
          <mrow>
            <mrow>
              <mo>=</mo>
              <mrow>
                <mi>P</mi>
                <mo>⁢</mo>
                <mrow>
                  <mrow>
                    <mo>(</mo>
                    <msub>
                      <mi>T</mi>
                      <mi>i</mi>
                    </msub>
                    <mo>)</mo>
                  </mrow>
                  <mo>⁢</mo>
                  <msub>
                    <mo>?</mo>
                    <mi>j</mi>
                  </msub>
                  <mo>⁢</mo>
                  <mi>P</mi>
                </mrow>
                <mo>⁢</mo>
                <mrow>
                  <mo>(</mo>
                  <msub>
                    <mi>I</mi>
                    <msup>
                      <mi>j</mi>
                      <mo>*</mo>
                    </msup>
                  </msub>
                  <mo></mo>
                </mrow>
                <mo>⁢</mo>
                <msub>
                  <mi>T</mi>
                  <mi>i</mi>
                </msub>
              </mrow>
            </mrow>
            <mo>)</mo>
          </mrow>
          <mo>/</mo>
          <mrow>
            <msub>
              <mi>S</mi>
              <mi>k</mi>
            </msub>
            <mo>⁡</mo>
            <mrow>
              <mo>(</mo>
              <mrow>
                <mrow>
                  <mrow>
                    <mi>P</mi>
                    <mo>⁡</mo>
                    <mrow>
                      <mo>(</mo>
                      <msub>
                        <mi>T</mi>
                        <mi>k</mi>
                      </msub>
                      <mo>)</mo>
                    </mrow>
                  </mrow>
                  <mo>⁢</mo>
                  <msub>
                    <mo>?</mo>
                    <mi>j</mi>
                  </msub>
                  <mo>⁢</mo>
                  <mi>P</mi>
                </mrow>
                <mo>⁢</mo>
                <mrow>
                  <mo>(</mo>
                  <msub>
                    <mi>I</mi>
                    <msup>
                      <mi>j</mi>
                      <mo>*</mo>
                    </msup>
                  </msub>
                  <mo></mo>
                </mrow>
                <mo>⁢</mo>
                <msub>
                  <mi>T</mi>
                  <mi>k</mi>
                </msub>
              </mrow>
              <mo>)</mo>
            </mrow>
          </mrow>
        </mrow>
        <mo>)</mo>
      </mrow>
    </mtd>
  </mtr>
  <mtr>
    <mtd>
      <mrow>
        <mrow>
          <mrow>
            <mrow>
              <mo>=</mo>
              <mrow>
                <mrow>
                  <msub>
                    <mi>L</mi>
                    <mi>i</mi>
                  </msub>
                  <mo>/</mo>
                  <msub>
                    <mi>S</mi>
                    <mi>k</mi>
                  </msub>
                </mrow>
                <mo>⁢</mo>
                <msub>
                  <mi>L</mi>
                  <mi>k</mi>
                </msub>
              </mrow>
            </mrow>
            <mo>,</mo>
            <mrow>
              <mrow>
                <mi>defining</mi>
                <mo>⁢</mo>
                <mstyle>
                  <mspace width="0.8em" height="0.8ex"/>
                </mstyle>
                <mo>⁢</mo>
                <mi>likelihood</mi>
                <mo>⁢</mo>
                <mstyle>
                  <mspace width="1.1em" height="1.1ex"/>
                </mstyle>
                <mo>⁢</mo>
                <msub>
                  <mi>L</mi>
                  <mi>k</mi>
                </msub>
              </mrow>
              <mo>=</mo>
              <mstyle>
                <mspace width="0.3em" height="0.3ex"/>
              </mstyle>
              <mo>⁢</mo>
              <mrow>
                <mrow>
                  <mrow>
                    <mi>P</mi>
                    <mo>⁡</mo>
                    <mrow>
                      <mo>(</mo>
                      <msub>
                        <mi>T</mi>
                        <mi>k</mi>
                      </msub>
                      <mo>)</mo>
                    </mrow>
                  </mrow>
                  <mo>⁢</mo>
                  <msub>
                    <mo>?</mo>
                    <mi>j</mi>
                  </msub>
                  <mo>⁢</mo>
                  <mi>P</mi>
                </mrow>
                <mo>⁢</mo>
                <mrow>
                  <mo>(</mo>
                  <msub>
                    <mi>I</mi>
                    <msup>
                      <mi>j</mi>
                      <mo>*</mo>
                    </msup>
                  </msub>
                  <mo></mo>
                </mrow>
                <mo>⁢</mo>
                <msub>
                  <mi>T</mi>
                  <mi>k</mi>
                </msub>
              </mrow>
            </mrow>
          </mrow>
          <mo>)</mo>
        </mrow>
        <mo>⁢</mo>
        <mstyle>
          <mspace width="0.3em" height="0.3ex"/>
        </mstyle>
      </mrow>
    </mtd>
  </mtr>
  <mtr>
    <mtd>
      <msub>
        <mi>L</mi>
        <mi>i</mi>
      </msub>
    </mtd>
  </mtr>
  <mtr>
    <mtd>
      <mrow>
        <mrow>
          <mo>=</mo>
          <mrow>
            <mi>P</mi>
            <mo>⁢</mo>
            <mrow>
              <mrow>
                <mo>(</mo>
                <msub>
                  <mi>T</mi>
                  <mi>i</mi>
                </msub>
                <mo>)</mo>
              </mrow>
              <mo>⁢</mo>
              <msub>
                <mo>?</mo>
                <mi>j</mi>
              </msub>
              <mo>⁢</mo>
              <mi>P</mi>
            </mrow>
            <mo>⁢</mo>
            <mrow>
              <mo>(</mo>
              <msub>
                <mi>I</mi>
                <msup>
                  <mi>j</mi>
                  <mo>*</mo>
                </msup>
              </msub>
              <mo></mo>
            </mrow>
            <mo>⁢</mo>
            <msub>
              <mi>T</mi>
              <mi>i</mi>
            </msub>
          </mrow>
        </mrow>
        <mo>)</mo>
      </mrow>
    </mtd>
  </mtr>
  <mtr>
    <mtd>
      <mrow>
        <mrow>
          <mo>=</mo>
          <mrow>
            <mrow>
              <mrow>
                <mo>(</mo>
                <mrow>
                  <mrow>
                    <mrow>
                      <mi>count</mi>
                      <mo>⁢</mo>
                      <mstyle>
                        <mspace width="0.3em" height="0.3ex"/>
                      </mstyle>
                      <mo>[</mo>
                      <msub>
                        <mi>T</mi>
                        <mi>i</mi>
                      </msub>
                      <mo>]</mo>
                    </mrow>
                    <mo>/</mo>
                    <msub>
                      <mi>S</mi>
                      <mi>k</mi>
                    </msub>
                  </mrow>
                  <mo>⁢</mo>
                  <mstyle>
                    <mspace width="0.3em" height="0.3ex"/>
                  </mstyle>
                  <mo>⁢</mo>
                  <mrow>
                    <mi>count</mi>
                    <mo>⁢</mo>
                    <mstyle>
                      <mspace width="0.3em" height="0.3ex"/>
                    </mstyle>
                    <mo>[</mo>
                    <msub>
                      <mi>T</mi>
                      <mi>k</mi>
                    </msub>
                    <mo>]</mo>
                  </mrow>
                </mrow>
                <mo>)</mo>
              </mrow>
              <mo>⁢</mo>
              <msub>
                <mo>?</mo>
                <mi>j</mi>
              </msub>
            </mrow>
            <mo>⁢</mo>
            <mrow>
              <mrow>
                <mo>(</mo>
                <mrow>
                  <mrow>
                    <mrow>
                      <mi>count</mi>
                      <mo>⁢</mo>
                      <mstyle>
                        <mspace width="0.3em" height="0.3ex"/>
                      </mstyle>
                      <mo>[</mo>
                      <mrow>
                        <msub>
                          <mi>I</mi>
                          <msup>
                            <mi>j</mi>
                            <mo>*</mo>
                          </msup>
                        </msub>
                        <mo>⁢</mo>
                        <msub>
                          <mi>T</mi>
                          <mi>i</mi>
                        </msub>
                      </mrow>
                      <mo>]</mo>
                    </mrow>
                    <mo>/</mo>
                    <msub>
                      <mi>S</mi>
                      <mi>k</mi>
                    </msub>
                  </mrow>
                  <mo>⁢</mo>
                  <mstyle>
                    <mspace width="0.8em" height="0.8ex"/>
                  </mstyle>
                  <mo>⁢</mo>
                  <mrow>
                    <mi>count</mi>
                    <mo>⁢</mo>
                    <mstyle>
                      <mspace width="0.3em" height="0.3ex"/>
                    </mstyle>
                    <mo>[</mo>
                    <msub>
                      <mi>T</mi>
                      <mi>k</mi>
                    </msub>
                    <mo>]</mo>
                  </mrow>
                </mrow>
                <mo>)</mo>
              </mrow>
              <mo>/</mo>
            </mrow>
          </mrow>
        </mrow>
        <mo>⁢</mo>
        <mstyle>
          <mspace width="0.3em" height="0.3ex"/>
        </mstyle>
      </mrow>
    </mtd>
  </mtr>
  <mtr>
    <mtd>
      <mrow>
        <mrow>
          <mo>(</mo>
          <mrow>
            <mrow>
              <mrow>
                <mi>count</mi>
                <mo>⁢</mo>
                <mstyle>
                  <mspace width="0.3em" height="0.3ex"/>
                </mstyle>
                <mo>[</mo>
                <msub>
                  <mi>T</mi>
                  <mi>i</mi>
                </msub>
                <mo>]</mo>
              </mrow>
              <mo>/</mo>
              <msub>
                <mi>S</mi>
                <mi>k</mi>
              </msub>
            </mrow>
            <mo>⁢</mo>
            <mstyle>
              <mspace width="0.6em" height="0.6ex"/>
            </mstyle>
            <mo>⁢</mo>
            <mrow>
              <mi>count</mi>
              <mo>⁢</mo>
              <mstyle>
                <mspace width="0.3em" height="0.3ex"/>
              </mstyle>
              <mo>[</mo>
              <msub>
                <mi>T</mi>
                <mi>k</mi>
              </msub>
              <mo>]</mo>
            </mrow>
          </mrow>
          <mo>)</mo>
        </mrow>
        <mo>∼</mo>
        <mrow>
          <mrow>
            <mrow>
              <mi>count</mi>
              <mo>⁢</mo>
              <mstyle>
                <mspace width="0.3em" height="0.3ex"/>
              </mstyle>
              <mo>[</mo>
              <msub>
                <mi>T</mi>
                <mi>i</mi>
              </msub>
              <mo>]</mo>
            </mrow>
            <mo>⁢</mo>
            <msub>
              <mo>?</mo>
              <mi>j</mi>
            </msub>
          </mrow>
          <mo>⁢</mo>
          <mrow>
            <mo>(</mo>
            <mrow>
              <mrow>
                <mi>count</mi>
                <mo>⁢</mo>
                <mstyle>
                  <mspace width="0.3em" height="0.3ex"/>
                </mstyle>
                <mo>[</mo>
                <mrow>
                  <msub>
                    <mi>I</mi>
                    <msup>
                      <mi>j</mi>
                      <mo>*</mo>
                    </msup>
                  </msub>
                  <mo>⁢</mo>
                  <msub>
                    <mi>T</mi>
                    <mi>i</mi>
                  </msub>
                </mrow>
                <mo>]</mo>
              </mrow>
              <mo>/</mo>
            </mrow>
          </mrow>
        </mrow>
      </mrow>
    </mtd>
  </mtr>
  <mtr>
    <mtd>
      <mrow>
        <mrow>
          <mrow>
            <mi>count</mi>
            <mo>⁢</mo>
            <mstyle>
              <mspace width="0.3em" height="0.3ex"/>
            </mstyle>
            <mo>[</mo>
            <msub>
              <mi>T</mi>
              <mi>i</mi>
            </msub>
            <mo>]</mo>
          </mrow>
          <mo>)</mo>
        </mrow>
        <mo>,</mo>
        <mrow>
          <mi>removing</mi>
          <mo>⁢</mo>
          <mstyle>
            <mspace width="0.8em" height="0.8ex"/>
          </mstyle>
          <mo>⁢</mo>
          <mi>factors</mi>
          <mo>⁢</mo>
          <mstyle>
            <mspace width="0.8em" height="0.8ex"/>
          </mstyle>
          <mo>⁢</mo>
          <mi>of</mi>
          <mo>⁢</mo>
          <mstyle>
            <mspace width="0.8em" height="0.8ex"/>
          </mstyle>
          <mo>⁢</mo>
          <msub>
            <mi>S</mi>
            <mi>k</mi>
          </msub>
          <mo>⁢</mo>
          <mrow>
            <mi>count</mi>
            <mo>⁢</mo>
            <mstyle>
              <mspace width="0.3em" height="0.3ex"/>
            </mstyle>
            <mo>[</mo>
            <msub>
              <mi>T</mi>
              <mi>k</mi>
            </msub>
            <mo>]</mo>
          </mrow>
          <mo>⁢</mo>
          <mstyle>
            <mspace width="0.8em" height="0.8ex"/>
          </mstyle>
          <mo>⁢</mo>
          <mi>common</mi>
          <mo>⁢</mo>
          <mstyle>
            <mspace width="0.8em" height="0.8ex"/>
          </mstyle>
          <mo>⁢</mo>
          <mi>to</mi>
          <mo>⁢</mo>
          <mstyle>
            <mspace width="0.8em" height="0.8ex"/>
          </mstyle>
          <mo>⁢</mo>
          <mi>all</mi>
          <mo>⁢</mo>
          <mstyle>
            <mspace width="0.8em" height="0.8ex"/>
          </mstyle>
          <mo>⁢</mo>
          <mi>L</mi>
        </mrow>
      </mrow>
    </mtd>
  </mtr>
</mtable>
</math>
</maths>
<br/>
Thus, the probability of each target value is straightforwardly computed by multiplying and dividing several counts; these counts are part of the Naive Bayes model itself. Incremental untraining in support of leave-one-out cross-validation is accomplished simply by multiplying or dividing by one less than the specified count (provided that the current training record's target value equals the value whose probability is being computed; otherwise, the specified count is used without modification). Likewise, incremental untraining in support of leave-n-out cross-validation is accomplished simply by multiplying or dividing by n less than the specified count (provided that the current training record's target value equals the value whose probability is being computed; otherwise, the specified count is used without modification).
</p>
<p id="p-0033" num="0032">An exemplary flow diagram of a process <b>300</b> of leave-n-out cross-validation of a Naïve Bayes model is shown in <figref idref="DRAWINGS">FIG. 3</figref>. It is best viewed in conjunction with <figref idref="DRAWINGS">FIG. 2</figref> and with <figref idref="DRAWINGS">FIG. 4</figref>, which is an exemplary data flow diagram of the processing performed by process <b>300</b>. The process begins with step <b>302</b>, in which training parameters <b>212</b>, untrained Naïve Bayes model <b>214</b>, and training dataset <b>216</b> are received and/or specified. Untrained Naïve Bayes model <b>214</b> includes algorithms that process the training data <b>216</b> in order to actually build the model. Training parameters <b>212</b> are parameters that are input to the data-mining model building algorithms to control how the algorithms build the models. Training data <b>216</b> is data that is input to the algorithms and which is used to actually build the models.</p>
<p id="p-0034" num="0033">In step <b>303</b>, in a preferred embodiment of the present invention, database queries that perform the leave-n-out cross-validation of steps <b>504</b>-<b>512</b> are generated based on the received and/or specified training parameters <b>212</b>, untrained Naïve Bayes model <b>214</b>, and training dataset <b>216</b>. The database queries may be generated in any query language that can be understood by the selected database management system, but typically, Structured Query Language (SQL) is used.</p>
<p id="p-0035" num="0034">In step <b>304</b>, the data mining model building algorithms included in untrained Naïve Bayes model <b>214</b> are invoked by training/model building routines <b>224</b>. The algorithms are initialized using the training parameters <b>212</b>, training data <b>216</b> is processed using the algorithms to build the model, and trained model <b>218</b> is generated.</p>
<p id="p-0036" num="0035">In step <b>306</b>, for each row of data in training dataset <b>216</b>, steps <b>308</b> and <b>310</b> are performed. In step <b>308</b>, trained model <b>218</b> is incrementally untrained for the row of data from training dataset <b>216</b> that is currently being processed by untraining routines <b>226</b>. In step <b>310</b>, the accuracy of the incrementally untrained model is determined using the row of data from training dataset <b>216</b> that is currently being processed by accuracy determination routines <b>228</b>. In particular, the model is applied to the current row of data and the error between the model output and the row of data is determined. The output of the accuracy determination of step <b>310</b> is one entry in accuracy determination results <b>220</b>.</p>
<p id="p-0037" num="0036">When all rows in training dataset <b>216</b> have been processed in steps <b>308</b> and <b>310</b>, and entries in accuracy determination results <b>220</b> generated for each such row, then in step <b>312</b>, aggregate accuracy determination result <b>222</b>, which is an aggregate indicator of the accuracy of trained model <b>218</b>, is generated from accuracy determination results <b>220</b> by aggregate accuracy determination routines <b>230</b>. Typically, the aggregate accuracy determination result <b>222</b> is determined by averaging the individual accuracy determination results <b>220</b>, but the present invention also contemplates other methods of determining aggregate accuracy.</p>
<p id="p-0038" num="0037">An exemplary flow diagram of a process <b>500</b> of leave-n-out cross-validation of a Naïve Bayes model is shown in <figref idref="DRAWINGS">FIG. 5</figref>. It is best viewed in conjunction with <figref idref="DRAWINGS">FIG. 2</figref> and with <figref idref="DRAWINGS">FIG. 4</figref>, which is also an exemplary data flow diagram of the processing performed by process <b>500</b>. The process begins with step <b>502</b>, in which training parameters <b>212</b>, untrained Naïve Bayes model <b>214</b>, and training dataset <b>216</b> are received and/or specified. Untrained Naïve Bayes model <b>214</b> includes algorithms that process the training data <b>216</b> in order to actually build the model. Training parameters <b>212</b> are parameters that are input to the data-mining model building algorithms to control how the algorithms build the models. Training data <b>216</b> is data that is input to the algorithms and which is used to actually build the models.</p>
<p id="p-0039" num="0038">In step <b>503</b>, in a preferred embodiment of the present invention, database queries that perform the leave-n-out cross-validation of steps <b>504</b>-<b>512</b> are generated based on the received and/or specified training parameters <b>212</b>, untrained Naïve Bayes model <b>214</b>, and training dataset <b>216</b>. The database queries may be generated in any query language that can be understood by the selected database management system, but typically, Structured Query Language (SQL) is used.</p>
<p id="p-0040" num="0039">In step <b>504</b>, the data mining model building algorithms included in untrained Naïve Bayes model <b>214</b> are invoked by training/model building routines <b>224</b>. The algorithms are initialized using the training parameters <b>212</b>, training data <b>216</b> is processed using the algorithms to build the model, and trained model <b>218</b> is generated.</p>
<p id="p-0041" num="0040">In step <b>506</b>, for each partition of the data in training dataset <b>216</b>, steps <b>508</b> and <b>510</b> are performed. In step <b>508</b>, trained model <b>218</b> is incrementally untrained for each row of data in the partition of training dataset <b>216</b> that is currently being processed by untraining routines <b>226</b>. This cumulatively modifies the model based on all rows in the partition. In step <b>510</b>, the accuracy of the incrementally untrained model is determined using the partition of data from training dataset <b>216</b> that is currently being processed by accuracy determination routines <b>228</b>. In particular, the model is applied to the rows of data in the partition and the error between the model output and the rows of data is determined. The output of the accuracy determination of step <b>510</b> is one entry in accuracy determination results <b>220</b>.</p>
<p id="p-0042" num="0041">When all partitions in training dataset <b>216</b> have been processed in steps <b>508</b> and <b>510</b>, and entries in accuracy determination results <b>220</b> generated for each such partition, then in step <b>512</b>, aggregate accuracy determination result <b>222</b>, which is an aggregate indicator of the accuracy of trained model <b>218</b>, is generated from accuracy determination results <b>220</b> by aggregate accuracy determination routines <b>230</b>. Typically, the aggregate accuracy determination result <b>222</b> is determined by averaging the individual accuracy determination results <b>220</b>, but the present invention also contemplates other methods of determining aggregate accuracy.</p>
<p id="p-0043" num="0042">Thus, the model (or a copy thereof) is trained once and untrained once for each training record, merely doubling the amount of work, instead of requiring n times as much work to build n models conventionally.</p>
<p id="p-0044" num="0043">It is important to note that while the present invention has been described in the context of a fully functioning data processing system, those of ordinary skill in the art will appreciate that the processes of the present invention are capable of being distributed in the form of a computer readable medium of instructions and a variety of forms and that the present invention applies equally regardless of the particular type of signal bearing media actually used to carry out the distribution. Examples of computer readable media include recordable-type media such as floppy disc, a hard disk drive, RAM, and CD-ROM's, as well as transmission-type media, such as digital and analog communications links.</p>
<p id="p-0045" num="0044">Although specific embodiments of the present invention have been described, it will be understood by those of skill in the art that there are other embodiments that are equivalent to the described embodiments. Accordingly, it is to be understood that the invention is not to be limited by the specific illustrated embodiments, but only by the scope of the appended claims.</p>
<?DETDESC description="Detailed Description" end="tail"?>
</description>
<us-math idrefs="MATH-US-00001" nb-file="US07299215-20071120-M00001.NB">
<img id="EMI-M00001" he="44.45mm" wi="76.20mm" file="US07299215-20071120-M00001.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-claim-statement>What is claimed is:</us-claim-statement>
<claims id="claims">
<claim id="CLM-00001" num="00001">
<claim-text>1. A method for measuring accuracy of a Naïve Bayes predictive model for creating a recommendation or prediction of a tangible result comprising the steps of:
<claim-text>defining code executable by a database management system for performing cross-validation by incremental untraining of the Naïve Bayes predictive model;</claim-text>
<claim-text>executing the defined code so as to perform cross-validation of the Naïve Bayes predictive model;</claim-text>
<claim-text>outputting an indication of the accuracy of the Naïve Bayes predictive model; and</claim-text>
<claim-text>creating a recommendation or prediction of a tangible result using the Naïve Bayes predictive model.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00002" num="00002">
<claim-text>2. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the executing step comprises the steps of:
<claim-text>receiving a training dataset comprising a plurality of rows of data;</claim-text>
<claim-text>building a Naïve Bayes predictive model using the training dataset;</claim-text>
<claim-text>for each of at least a portion of the plurality of rows of data in the training dataset:</claim-text>
<claim-text>incrementally untraining the Naïve Bayes predictive model using the row of data, and</claim-text>
<claim-text>determining an accuracy of the incrementally untrained Naïve Bayes predictive model; and</claim-text>
<claim-text>determining an aggregate accuracy of the Naïve Bayes predictive model.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00003" num="00003">
<claim-text>3. The method of <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein the step of building the Naïve Bayes predictive model using the training dataset comprises the step of:
<claim-text>computing probabilities of target values based on counts of occurrences of target values in training dataset.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00004" num="00004">
<claim-text>4. The method of <claim-ref idref="CLM-00003">claim 3</claim-ref>, wherein the step of incrementally untraining the Naïve Bayes predictive model comprises the steps of:
<claim-text>if a target value of the row of data equals a target value being computed, computing a probability of the target value based on a count of occurrence of the target value minus one; and</claim-text>
<claim-text>if the target value of the row of data does not equal the target value being computed, computing a probability of the target value based on the count of occurrence of the target value.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00005" num="00005">
<claim-text>5. The method of <claim-ref idref="CLM-00004">claim 4</claim-ref>, wherein the step of determining an accuracy of the incrementally untrained Naïve Bayes predictive model comprises the steps of:
<claim-text>applying the incrementally untrained Naïve Bayes predictive model to the row of data to generate an output; and</claim-text>
<claim-text>determining an error between the model output and the row of data.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00006" num="00006">
<claim-text>6. The method of <claim-ref idref="CLM-00005">claim 5</claim-ref>, wherein the step of determining an aggregate accuracy of the Naïve Bayes predictive model comprises the step of:
<claim-text>determining an average of the determined errors between the model output and the row of data.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00007" num="00007">
<claim-text>7. A system for measuring accuracy of a Naïve Bayes predictive model for creating a recommendation or prediction of a tangible result comprising:
<claim-text>a processor operable to execute computer program instructions;</claim-text>
<claim-text>a memory operable to store computer program instructions executable by the processor; and</claim-text>
<claim-text>computer program instructions stored in the memory and executable to perform the steps of:</claim-text>
<claim-text>defining code executable by a database management system for performing cross-validation by incremental untraining of the Naïve Bayes predictive model;</claim-text>
<claim-text>executing the defined code so as to perform cross-validation of the Naïve Bayes predictive model;</claim-text>
<claim-text>outputting an indication of the accuracy of the Naïve Bayes predictive model; and</claim-text>
<claim-text>creating a recommendation or prediction of a tangible result using the Naïve Bayes predictive model.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00008" num="00008">
<claim-text>8. The system of <claim-ref idref="CLM-00007">claim 7</claim-ref>, wherein the executing step comprises the steps of:
<claim-text>receiving a training dataset comprising a plurality of rows of data;</claim-text>
<claim-text>building a Naïve Bayes predictive model using the training dataset;</claim-text>
<claim-text>for each of at least a portion of the plurality of rows of data in the training dataset:</claim-text>
<claim-text>incrementally untraining the Naïve Bayes predictive model using the row of data, and</claim-text>
<claim-text>determining an accuracy of the incrementally untrained Naïve Bayes predictive model; and</claim-text>
<claim-text>determining an aggregate accuracy of the Naïve Bayes predictive model.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00009" num="00009">
<claim-text>9. The system of <claim-ref idref="CLM-00008">claim 8</claim-ref>, wherein the step of building the Naïve Bayes predictive model using the training dataset comprises the step of:
<claim-text>computing probabilities of target values based on counts of occurrences of target values in training dataset.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00010" num="00010">
<claim-text>10. The system of <claim-ref idref="CLM-00009">claim 9</claim-ref>, wherein the step of incrementally untraining the Naïve Bayes predictive model comprises the steps of:
<claim-text>if a target value of the row of data equals a target value being computed, computing a probability of the target value based on a count of occurrence of the target value minus one; and</claim-text>
<claim-text>if the target value of the row of data does not equal the target value being computed, computing a probability of the target value based on the count of occurrence of the target value.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00011" num="00011">
<claim-text>11. The system of <claim-ref idref="CLM-00010">claim 10</claim-ref>, wherein the step of determining an accuracy of the incrementally untrained Naïve Bayes predictive model comprises the steps of:
<claim-text>applying the incrementally untrained Naïve Bayes predictive model to the row of data to generate an output; and</claim-text>
<claim-text>determining an error between the model output and the row of data.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00012" num="00012">
<claim-text>12. The system of <claim-ref idref="CLM-00011">claim 11</claim-ref>, wherein the step of determining an aggregate accuracy of the Naïve Bayes predictive model comprises the step of:
<claim-text>determining an average of the determined errors between the model output and the row of data.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00013" num="00013">
<claim-text>13. A computer program product for measuring accuracy of a Naïve Bayes predictive model for creating a recommendation or prediction of a tangible result comprising:
<claim-text>a computer readable medium;</claim-text>
<claim-text>computer program instructions, recorded on the computer readable medium, executable by a processor, for performing the steps of:</claim-text>
<claim-text>defining code executable by a database management system for performing cross-validation by incremental untraining of the Naïve Bayes predictive model;</claim-text>
<claim-text>executing the defined code so as to perform cross-validation of the Naïve Bayes predictive model;</claim-text>
<claim-text>outputting an indication of the accuracy of the Naïve Bayes predictive model; and</claim-text>
<claim-text>creating a recommendation or prediction of a tangible result using the Naïve Bayes predictive model.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00014" num="00014">
<claim-text>14. The computer program product of <claim-ref idref="CLM-00013">claim 13</claim-ref>, wherein the executing step comprises the steps of:
<claim-text>receiving a training dataset comprising a plurality of rows of data;</claim-text>
<claim-text>building a Naïve Bayes predictive model using the training dataset;</claim-text>
<claim-text>for each of at least a portion of the plurality of rows of data in the training dataset:</claim-text>
<claim-text>incrementally untraining the Naïve Bayes predictive model using the row of data, and</claim-text>
<claim-text>determining an accuracy of the incrementally untrained Naïve Bayes predictive model; and</claim-text>
<claim-text>determining an aggregate accuracy of the Naïve Bayes predictive model.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00015" num="00015">
<claim-text>15. The computer program product of <claim-ref idref="CLM-00014">claim 14</claim-ref>, wherein the step of building the Naïve Bayes predictive model using the training dataset comprises the step of:
<claim-text>computing probabilities of target values based on counts of occurrences of target values in training dataset.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00016" num="00016">
<claim-text>16. The computer program product of <claim-ref idref="CLM-00015">claim 15</claim-ref>, wherein the step of incrementally untraining the Naïve Bayes predictive model comprises the steps of:
<claim-text>if a target value of the row of data equals a target value being computed, computing a probability of the target value based on a count of occurrence of the target value minus one; and</claim-text>
<claim-text>if the target value of the row of data does not equal the target value being computed, computing a probability of the target value based on the count of occurrence of the target value.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00017" num="00017">
<claim-text>17. The computer program product of <claim-ref idref="CLM-00016">claim 16</claim-ref>, wherein the step of determining an accuracy of the incrementally untrained Naïve Bayes predictive model comprises the steps of:
<claim-text>applying the incrementally untrained Naïve Bayes predictive model to the row of data to generate an output; and</claim-text>
<claim-text>determining an error between the model output and the row of data.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00018" num="00018">
<claim-text>18. The computer program product of <claim-ref idref="CLM-00017">claim 17</claim-ref>, wherein the step of determining an aggregate accuracy of the Naïve Bayes predictive model comprises the step of:
<claim-text>determining an average of the determined errors between the model output and the row of data.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00019" num="00019">
<claim-text>19. A method for measuring accuracy of a Naïve Bayes predictive model for creating a recommendation or prediction of a tangible result comprising the steps of:
<claim-text>receiving a training dataset comprising a plurality of partitions of rows of data;</claim-text>
<claim-text>building a Naïve Bayes predictive model using the training dataset;</claim-text>
<claim-text>for each of at least a portion of the plurality of partitions of data in the training dataset:</claim-text>
<claim-text>incrementally untraining the Naïve Bayes predictive model using rows of data in the partition, and</claim-text>
<claim-text>determining an accuracy of the incrementally untrained Naïve Bayes predictive model;</claim-text>
<claim-text>determining an aggregate accuracy of the Naïve Bayes predictive model; and</claim-text>
<claim-text>creating a recommendation or prediction of a tangible result using the Naïve Bayes predictive model.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00020" num="00020">
<claim-text>20. The method of <claim-ref idref="CLM-00019">claim 19</claim-ref>, wherein the step of building the Naïve Bayes predictive model using the training dataset comprises the step of:
<claim-text>computing probabilities of target values based on counts of occurrences of target values in training dataset.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00021" num="00021">
<claim-text>21. The method of <claim-ref idref="CLM-00020">claim 20</claim-ref>, wherein the step of incrementally untraining the Naïve Bayes predictive model comprises the steps of:
<claim-text>if a target value of a row of data in the partition equals a target value being computed, computing a probability of the target value based on a count of occurrence of the target value minus one; and</claim-text>
<claim-text>if the target value of the row of data in the partition does not equal the target value being computed, computing a probability of the target value based on the count of occurrence of the target value.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00022" num="00022">
<claim-text>22. The method of <claim-ref idref="CLM-00021">claim 21</claim-ref>, wherein the step of determining an accuracy of the incrementally untrained Naïve Bayes predictive model comprises the steps of:
<claim-text>applying the incrementally untrained Naïve Bayes predictive model to the row of data to generate an output; and</claim-text>
<claim-text>determining an error between the model output and the row of data.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00023" num="00023">
<claim-text>23. The method of <claim-ref idref="CLM-00022">claim 22</claim-ref>, wherein the step of determining an aggregate accuracy of the Naïve Bayes predictive model comprises the step of:
<claim-text>determining an average of the determined errors between the model output and the row of data.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00024" num="00024">
<claim-text>24. A system for measuring accuracy of a Naïve Bayes predictive for creating a recommendation or prediction of a tangible result model comprising:
<claim-text>a processor operable to execute computer program instructions;</claim-text>
<claim-text>a memory operable to store computer program instructions executable by the processor; and</claim-text>
<claim-text>computer program instructions stored in the memory and executable to perform the steps of:</claim-text>
<claim-text>receiving a training dataset comprising a plurality of partitions of rows of data;</claim-text>
<claim-text>building a Naïve Bayes predictive model using the training dataset;</claim-text>
<claim-text>for each of at least a portion of the plurality of partitions of data in the training dataset:</claim-text>
<claim-text>incrementally untraining the Naïve Bayes predictive model using rows of data in the partition, and</claim-text>
<claim-text>determining an accuracy of the incrementally untrained Naïve Bayes predictive model;</claim-text>
<claim-text>determining an aggregate accuracy of the Naïve Bayes predictive model; and</claim-text>
<claim-text>creating a recommendation or prediction of a tangible result using the Naïve Bayes predictive model.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00025" num="00025">
<claim-text>25. The system of <claim-ref idref="CLM-00024">claim 24</claim-ref>, wherein the step of building the Naïve Bayes predictive model using the training dataset comprises the step of:
<claim-text>computing probabilities of target values based on counts of occurrences of target values in training dataset.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00026" num="00026">
<claim-text>26. The system of <claim-ref idref="CLM-00025">claim 25</claim-ref>, wherein the step of incrementally untraining the Naïve Bayes predictive model comprises the steps of:
<claim-text>if a target value of a row of data in the partition equals a target value being computed, computing a probability of the target value based on a count of occurrence of the target value minus one; and</claim-text>
<claim-text>if the target value of the row of data in the partition does not equal the target value being computed, computing a probability of the target value based on the count of occurrence of the target value.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00027" num="00027">
<claim-text>27. The system of <claim-ref idref="CLM-00026">claim 26</claim-ref>, wherein the step of determining an accuracy of the incrementally untrained Naïve Bayes predictive model comprises the steps of:
<claim-text>applying the incrementally untrained Naïve Bayes predictive model to the row of data to generate an output; and</claim-text>
<claim-text>determining an error between the model output and the row of data.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00028" num="00028">
<claim-text>28. The system of <claim-ref idref="CLM-00027">claim 27</claim-ref>, wherein the step of determining an aggregate accuracy of the Naïve Bayes predictive model comprises the step of:
<claim-text>determining an average of the determined errors between the model output and the row of data.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00029" num="00029">
<claim-text>29. A computer program product for measuring accuracy of a Naïve Bayes predictive model for creating a recommendation or prediction of a tangible result comprising:
<claim-text>a computer readable storage medium;</claim-text>
<claim-text>computer program instructions, recorded on the computer readable medium, executable by a processor, for performing the steps of:</claim-text>
<claim-text>receiving a training dataset comprising a plurality of partitions of rows of data;</claim-text>
<claim-text>building a Naïve Bayes predictive model using the training dataset;</claim-text>
<claim-text>for each of at least a portion of the plurality of partitions of data in the training dataset:</claim-text>
<claim-text>incrementally untraining the Naïve Bayes predictive model using rows of data in the partition, and</claim-text>
<claim-text>determining an accuracy of the incrementally untrained Naïve Bayes predictive model;</claim-text>
<claim-text>determining an aggregate accuracy of the Naïve Bayes predictive model; and</claim-text>
<claim-text>creating a recommendation or prediction of a tangible result using the Naïve Bayes predictive model.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00030" num="00030">
<claim-text>30. The computer program product of <claim-ref idref="CLM-00029">claim 29</claim-ref>, wherein the step of building the Naïve Bayes predictive model using the training dataset comprises the step of:
<claim-text>computing probabilities of target values based on counts of occurrences of target values in training dataset.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00031" num="00031">
<claim-text>31. The computer program product of <claim-ref idref="CLM-00030">claim 30</claim-ref>, wherein the step of incrementally untraining the Naïve Bayes predictive model comprises the steps of:
<claim-text>if a target value of a row of data in the partition equals a target value being computed, computing a probability of the target value based on a count of occurrence of the target value minus one; and</claim-text>
<claim-text>if the target value of the row of data in the partition does not equal the target value being computed, computing a probability of the target value based on the count of occurrence of the target value.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00032" num="00032">
<claim-text>32. The computer program product of <claim-ref idref="CLM-00031">claim 31</claim-ref>, wherein the step of determining an accuracy of the incrementally untrained Naïve Bayes predictive model comprises the steps of:
<claim-text>applying the incrementally untrained Naïve Bayes predictive model to the row of data to generate an output; and</claim-text>
<claim-text>determining an error between the model output and the row of data.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00033" num="00033">
<claim-text>33. The computer program product of <claim-ref idref="CLM-00032">claim 32</claim-ref>, wherein the step of determining an aggregate accuracy of the Naïve Bayes predictive model comprises the step of:
<claim-text>determining an average of the determined errors between the model output and the row of data.</claim-text>
</claim-text>
</claim>
</claims>
</us-patent-grant>
