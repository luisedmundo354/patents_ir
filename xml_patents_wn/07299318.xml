<us-patent-grant lang="EN" dtd-version="v4.2 2006-08-23" file="US07299318-20071120.XML" status="PRODUCTION" id="us-patent-grant" country="US" date-produced="20071106" date-publ="20071120">
<us-bibliographic-data-grant>
<publication-reference>
<document-id>
<country>US</country>
<doc-number>07299318</doc-number>
<kind>B2</kind>
<date>20071120</date>
</document-id>
</publication-reference>
<application-reference appl-type="utility">
<document-id>
<country>US</country>
<doc-number>10117020</doc-number>
<date>20020404</date>
</document-id>
</application-reference>
<us-application-series-code>10</us-application-series-code>
<us-term-of-grant>
<us-term-extension>457</us-term-extension>
</us-term-of-grant>
<classifications-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>G</section>
<class>06</class>
<subclass>F</subclass>
<main-group>12</main-group>
<subgroup>00</subgroup>
<symbol-position>F</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20071120</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>G</section>
<class>06</class>
<subclass>F</subclass>
<main-group>12</main-group>
<subgroup>08</subgroup>
<symbol-position>L</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20071120</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>G</section>
<class>06</class>
<subclass>F</subclass>
<main-group>12</main-group>
<subgroup>16</subgroup>
<symbol-position>L</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20071120</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
</classifications-ipcr>
<classification-national>
<country>US</country>
<main-classification>711118</main-classification>
<further-classification>711170</further-classification>
<further-classification>711171</further-classification>
<further-classification>711172</further-classification>
</classification-national>
<invention-title id="d0e53">Method for reducing cache conflict misses</invention-title>
<references-cited>
<citation>
<patcit num="00001">
<document-id>
<country>US</country>
<doc-number>5649143</doc-number>
<kind>A</kind>
<name>Parady</name>
<date>19970700</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>711220</main-classification></classification-national>
</citation>
<citation>
<patcit num="00002">
<document-id>
<country>US</country>
<doc-number>6006312</doc-number>
<kind>A</kind>
<name>Kohn et al.</name>
<date>19991200</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>711210</main-classification></classification-national>
</citation>
<citation>
<patcit num="00003">
<document-id>
<country>US</country>
<doc-number>2002/0118206</doc-number>
<kind>A1</kind>
<name>Knittel</name>
<date>20020800</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>345557</main-classification></classification-national>
</citation>
</references-cited>
<number-of-claims>19</number-of-claims>
<us-exemplary-claim>1</us-exemplary-claim>
<us-field-of-classification-search>
<classification-national>
<country>US</country>
<main-classification>711118</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>711170</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>711171</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>711172</main-classification>
</classification-national>
</us-field-of-classification-search>
<figures>
<number-of-drawing-sheets>9</number-of-drawing-sheets>
<number-of-figures>13</number-of-figures>
</figures>
<us-related-documents>
<us-provisional-application>
<document-id>
<country>US</country>
<doc-number>60343057</doc-number>
<kind>00</kind>
<date>20011220</date>
</document-id>
</us-provisional-application>
<related-publication>
<document-id>
<country>US</country>
<doc-number>20030120866</doc-number>
<kind>A1</kind>
<date>20030626</date>
</document-id>
</related-publication>
</us-related-documents>
<parties>
<applicants>
<applicant sequence="001" app-type="applicant-inventor" designation="us-only">
<addressbook>
<last-name>Stoutamire</last-name>
<first-name>David P.</first-name>
<address>
<city>San Juan Bautista</city>
<state>CA</state>
<country>US</country>
</address>
</addressbook>
<nationality>
<country>omitted</country>
</nationality>
<residence>
<country>US</country>
</residence>
</applicant>
</applicants>
<agents>
<agent sequence="01" rep-type="attorney">
<addressbook>
<orgname>Martine Penilla &amp; Gencarella LLP</orgname>
<address>
<country>unknown</country>
</address>
</addressbook>
</agent>
</agents>
</parties>
<assignees>
<assignee>
<addressbook>
<orgname>Sun Microsystems, Inc.</orgname>
<role>02</role>
<address>
<city>Santa Clara</city>
<state>CA</state>
<country>US</country>
</address>
</addressbook>
</assignee>
</assignees>
<examiners>
<primary-examiner>
<last-name>Shah</last-name>
<first-name>Sanjiv</first-name>
<department>2185</department>
</primary-examiner>
<assistant-examiner>
<last-name>Rojas</last-name>
<first-name>Midys</first-name>
</assistant-examiner>
</examiners>
</us-bibliographic-data-grant>
<abstract id="abstract">
<p id="p-0001" num="0000">An invention is provided for reducing cache conflict misses via specific placement of non-split functions and data objects in main memory based on cache size. A cache size of a computer cache memory is determined, and a first data block is placed within a main computer memory. The first data block includes a first sub-block that will be frequently referenced. In addition, the first sub-block ends at a first ending address. A second data block is then placed within the main computer memory. The second data block includes a second sub-block that will be frequently referenced, and is placed such that the second sub-block will be contiguous with the first sub-block in the computer cache memory during execution.</p>
</abstract>
<drawings id="DRAWINGS">
<figure id="Fig-EMI-D00000" num="00000">
<img id="EMI-D00000" he="81.36mm" wi="136.65mm" file="US07299318-20071120-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00001" num="00001">
<img id="EMI-D00001" he="134.20mm" wi="155.11mm" file="US07299318-20071120-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00002" num="00002">
<img id="EMI-D00002" he="217.93mm" wi="159.51mm" file="US07299318-20071120-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00003" num="00003">
<img id="EMI-D00003" he="171.62mm" wi="95.84mm" file="US07299318-20071120-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00004" num="00004">
<img id="EMI-D00004" he="214.38mm" wi="148.42mm" orientation="landscape" file="US07299318-20071120-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00005" num="00005">
<img id="EMI-D00005" he="215.05mm" wi="140.72mm" orientation="landscape" file="US07299318-20071120-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00006" num="00006">
<img id="EMI-D00006" he="215.73mm" wi="142.16mm" orientation="landscape" file="US07299318-20071120-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00007" num="00007">
<img id="EMI-D00007" he="217.85mm" wi="135.55mm" orientation="landscape" file="US07299318-20071120-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00008" num="00008">
<img id="EMI-D00008" he="275.59mm" wi="186.77mm" file="US07299318-20071120-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00009" num="00009">
<img id="EMI-D00009" he="255.02mm" wi="185.42mm" orientation="landscape" file="US07299318-20071120-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
</drawings>
<description id="description">
<?RELAPP description="Other Patent Relations" end="lead"?>
<heading id="h-0001" level="1">CROSS REFERENCE TO RELATED APPLICATIONS</heading>
<p id="p-0002" num="0001">This application claims the benefit of U.S. Provisional Patent Application having ser. No. 60/343,057, filed on Dec. 20, 2001, and entitled “Method for Reducing Cache Conflict Misses,” which is incorporated herein by reference.</p>
<?RELAPP description="Other Patent Relations" end="tail"?>
<?BRFSUM description="Brief Summary" end="lead"?>
<heading id="h-0002" level="1">BACKGROUND OF THE INVENTION</heading>
<p id="p-0003" num="0002">1. Field of the Invention</p>
<p id="p-0004" num="0003">This invention relates generally to computer memory, and more particularly to methods for reducing conflict misses within computer cache memory during program execution.</p>
<p id="p-0005" num="0004">2. Description of the Related Art</p>
<p id="p-0006" num="0005">Today, the speed with which a processor can access data often is critical to its performance. Unfortunately, computer architectures often rely on a mix of fast, less dense, memory and slower bulk memory. Many computer architectures have a multilevel memory architecture in which an attempt is made to find information in the fastest memory. If the information is not in that memory, a check is made at the next fastest memory. This process continues down through the memory hierarchy until the information sought is found. One critical component in such a memory hierarchy is a cache memory.</p>
<p id="p-0007" num="0006">Cache memory is a type of rapidly accessible memory that is used between a processing device and main memory. <figref idref="DRAWINGS">FIG. 1</figref> is a block diagram showing a conventional computer system <b>100</b>. The computer system <b>100</b> includes a main memory <b>102</b> in communication with a central processing unit (CPU) <b>104</b>. Located between the main memory <b>102</b> and the CPU <b>104</b>, is a cache memory <b>106</b>, which is usually constructed from higher speed memory devices such as static random access memory (SRAM). In operation, when a portion of data residing in the main memory <b>102</b> is accessed, a copy of the portion of data is placed into the cache memory <b>106</b> to increase the speed at which this data is accessed.</p>
<p id="p-0008" num="0007">Cache memories rely on the principle of locality to attempt to increase the likelihood that a processor will find the information it is looking for in the cache memory. To this end, cache memories typically store contiguous blocks of data. In addition, the cache memory stores a tag that is compared to an address to determine whether the information the processor is seeking is present in the cache memory. Finally, the cache memory may contain status or error correcting codes (ECC).</p>
<p id="p-0009" num="0008">The effectiveness of the cache memory <b>106</b> depends on the way a compiler or runtime system arranges the data structures and instructions in the main memory <b>102</b>. For example, cache memories <b>106</b> are ineffective when placement of data or instructions causes “conflict misses,” which can often be overcome by placing data at addresses that will be more favorable for the cache memory <b>106</b>. Thus, the location of the data within the main memory <b>104</b> generally is more important than what is done with the data. Thus, the placement of data within main memory <b>102</b> is important because of the interaction between the order of the data in main memory <b>102</b> and the order in which the computer hardware chooses to fetch the data into the cache memory <b>106</b>. As a result, methods have been conventionally used that cluster data together that is frequently accessed.</p>
<p id="p-0010" num="0009"><figref idref="DRAWINGS">FIG. 2</figref> is a block diagram showing a group of data <b>200</b>. The group of data <b>200</b> can represent computer functions, which are collections of basic building blocks of computer instructions, or computer objects, which are collections of fields of data. For purposes of illustration, <figref idref="DRAWINGS">FIG. 2</figref> will be described in terms of computer functions. The group of data <b>200</b> illustrated in <figref idref="DRAWINGS">FIG. 2</figref> includes three functions <b>201</b>, <b>202</b>, and <b>203</b>, each having a plurality of basic building blocks <b>206</b><i>a</i>-<i>i</i>. Since it is known that the computer hardware will fetch data that is located together, a number of methods have been used to arrange data more favorable for the cache memory, as discussed next with reference to <figref idref="DRAWINGS">FIGS. 3A-3C</figref>.</p>
<p id="p-0011" num="0010"><figref idref="DRAWINGS">FIG. 3A</figref> is a block diagram showing the group of data <b>200</b> having the functions rearranged in an attempt to improve cache performance. The group of data <b>200</b> shown in <figref idref="DRAWINGS">FIG. 3A</figref> has been rearranged such that function <b>202</b> and function <b>203</b> swap places. For example, functions <b>201</b> and <b>203</b> may be frequently accessed together. In this case, a compiler or runtime system may place functions <b>201</b> and <b>203</b> contiguously in main memory in an attempt to improve cache performance.</p>
<p id="p-0012" num="0011"><figref idref="DRAWINGS">FIG. 3B</figref> is a block diagram showing the group of data <b>200</b> having the basic blocks of the functions rearranged in an attempt to improve cache performance. The group of data <b>200</b> shown in <figref idref="DRAWINGS">FIG. 3A</figref> has been rearranged such that particular basic blocks of each function change places. For example, basic block A <b>206</b><i>a </i>and basic block B <b>206</b><i>b </i>of function <b>201</b> can change places. For example, basic block A <b>206</b><i>a </i>and basic block C <b>206</b><i>c </i>of function <b>201</b> may be frequently accessed together. In this case, a compiler or runtime system may place basic block A <b>206</b><i>a </i>and basic block C <b>206</b><i>c </i>contiguously in main memory in an attempt to improve cache performance.</p>
<p id="p-0013" num="0012"><figref idref="DRAWINGS">FIG. 3C</figref> is a block diagram showing the group of data <b>200</b> having the basic blocks of the functions split in an attempt to improve cache performance. By splitting the functions as shown in <figref idref="DRAWINGS">FIG. 3C</figref>, a compiler or runtime system actually changes the representation of the functions. In the example of <figref idref="DRAWINGS">FIG. 3C</figref>, a system determines which basic blocks are frequently accessed, also known as “hot,” and which basic blocks are infrequently accessed, also know as “cold.” For example, in <figref idref="DRAWINGS">FIG. 3C</figref> basic blocks A <b>206</b><i>a</i>, D <b>206</b><i>d</i>, and G <b>206</b><i>g </i>are “hot,” while basic blocks B <b>206</b><i>b</i>, C <b>206</b><i>c</i>, E <b>206</b><i>e</i>, F <b>206</b><i>f</i>, G <b>206</b><i>g</i>, and I <b>206</b><i>i </i>are “cold.”</p>
<p id="p-0014" num="0013">In the method of <figref idref="DRAWINGS">FIG. 3C</figref>, the compiler or runtime system splits the hot basic blocks from the cold basic blocks and connects them together using a pointer <b>302</b>. For example, the compiler or runtime system stores a pointer <b>302</b> with basic block A <b>206</b><i>a </i>that points to the area of memory storing basic blocks B <b>206</b><i>b </i>and C <b>206</b><i>c</i>. As a result, hot basic blocks A <b>206</b><i>a</i>, D <b>206</b><i>d</i>, and G <b>206</b><i>g </i>can be placed closer together in memory, while the cold basic blocks B <b>206</b><i>b</i>, C <b>206</b><i>c</i>, E <b>206</b><i>e</i>, F <b>206</b><i>f</i>, G <b>206</b><i>g</i>, and I <b>206</b><i>i </i>can be stored separately.</p>
<p id="p-0015" num="0014">Unfortunately, there are several drawbacks to splitting as shown in <figref idref="DRAWINGS">FIG. 3C</figref>. First, many instruction sets penalize branches between basic blocks that are far away from each other. In particular, branches between basic blocks normally are represented with a program counter offset of a fixed number of bits. However, the pointers used for splitting may require more bits to represent and require extra overhead. Second, for systems such as a Java Virtual Machine, there may be additional data associated with each function for debugging, pointer-locating, and inline caching. This data generally is more efficiently accessed when kept close to the code. In the case of data objects, splitting introduces additional overhead such as extra fields to link the hot and cold fields, which slows cold field lookups and requires increased memory management. In view of the foregoing, there is a need systems and methods that improve cache performance without introducing the extra overhead associated with splitting. The methods should allow data to be placed efficiently into cache memory and reduce conflict misses.</p>
<heading id="h-0003" level="1">SUMMARY OF THE INVENTION</heading>
<p id="p-0016" num="0015">Broadly speaking, the present invention fills these needs by providing a memory loading mechanism for reducing cache conflict misses via specific placement of non-split functions and data objects in main memory based on cache size. In one embodiment, a method is disclosed for reducing computer cache conflict misses. A cache size of a computer cache memory is determined, and a first data block is placed within a main computer memory. The first data block includes a first sub-block that will be frequently referenced. In addition, the first sub-block ends at a first ending address. Next, a second data block is placed within the main computer memory. The second data block includes a second sub-block that will be frequently referenced, and is placed such that the second sub-block will be contiguous with the first sub-block in the computer cache memory during execution. Generally, neither the first data block nor the second data block is split in the main computer memory.</p>
<p id="p-0017" num="0016">In loading the second data block, the second data block can be placed in the main memory such that the second sub-block is located at a main memory address that is a multiple of the cache size from the first ending address. The method can continue by loading a third data block, which includes a third sub-block that will be frequently referenced. The third data block can then be placed such that the third sub-block will be contiguous with the first sub-block and the second sub-block in the computer cache memory during execution. Similar to the second sub-block, the third data block can be placed such that the third sub-block is located at a main memory address that is a multiple of the cache size from an ending address of the second sub-block.</p>
<p id="p-0018" num="0017">In another embodiment, a computer program embodied on a computer readable medium is disclosed for reducing computer cache conflict misses. The computer program includes a code segment that places a first data block within a main computer memory, wherein the first data block includes a first sub-block that will be frequently referenced, and wherein the first sub-block ends at a first ending address. In addition, the computer program includes a code segment that places a second data block within the main computer memory. As above, the second data block includes a second sub-block that will be frequently referenced. The code segment places the second data block such that the second sub-block is located at a main memory address that is a multiple of a cache size of computer cache from the first ending address. Optionally, the computer program can include a code segment that determines which sub-blocks of a data block will be frequently accessed, and a code segment that determines the cache size. As above, generally neither the first data block nor the second data block is split in main memory.</p>
<p id="p-0019" num="0018">A further method for reducing computer cache conflict misses is disclosed in another embodiment of the present invention. A cache size of a computer cache memory is determined, and a first data block is placed within a main computer memory. As above, the first data block includes a first sub-block that will be frequently referenced, and ends at a first ending address. Next, a second data block is placed within the main computer memory, where the second data block includes a second sub-block that will be frequently referenced. To reduce cache conflict misses, the second data block is placed such that the second sub-block is located at a main memory address that is a multiple of the cache size from the first ending address. In one aspect, each data block can be a data object, and each sub-block can be a field. In this aspect, a third data object can be placed within the main computer memory, where the third data object includes a third field that will be frequently referenced. The third data object can be placed such that the third field is located at a main memory address that is a multiple of the cache size from the first ending address. In another aspect, each data block can be a function, and each sub-block can be a basic block. In this aspect, a third function can be placed within the main computer memory, where the third function includes a third basic block that will be frequently referenced. Similar to the data object, the third function can be placed such that the third basic block is located at an main memory address that is a multiple of the cache size from the first ending address. Other aspects and advantages of the invention will become apparent from the following detailed description, taken in conjunction with the accompanying drawings, illustrating by way of example the principles of the invention.</p>
<?BRFSUM description="Brief Summary" end="tail"?>
<?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?>
<description-of-drawings>
<heading id="h-0004" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading>
<p id="p-0020" num="0019">The invention, together with further advantages thereof, may best be understood by reference to the following description taken in conjunction with the accompanying drawings in which:</p>
<p id="p-0021" num="0020"><figref idref="DRAWINGS">FIG. 1</figref> is a block diagram showing a conventional computer system;</p>
<p id="p-0022" num="0021"><figref idref="DRAWINGS">FIG. 2</figref> is a block diagram showing a group of data;</p>
<p id="p-0023" num="0022"><figref idref="DRAWINGS">FIG. 3A</figref> is a block diagram showing the group of data having the functions rearranged in an attempt to improve cache performance;</p>
<p id="p-0024" num="0023"><figref idref="DRAWINGS">FIG. 3B</figref> is a block diagram showing the group of data having the basic blocks of the functions rearranged in an attempt to improve cache performance;</p>
<p id="p-0025" num="0024"><figref idref="DRAWINGS">FIG. 3C</figref> is a block diagram showing the group of data having the basic blocks of the functions split in an attempt to improve cache performance;</p>
<p id="p-0026" num="0025"><figref idref="DRAWINGS">FIG. 4A</figref> is a block diagram showing an exemplary data block having a plurality of sub-blocks;</p>
<p id="p-0027" num="0026">is <figref idref="DRAWINGS">FIG. 4B</figref> is a flow diagram showing an exemplary function representing the function stored in the data block of <figref idref="DRAWINGS">FIG. 4A</figref>;</p>
<p id="p-0028" num="0027"><figref idref="DRAWINGS">FIG. 4C</figref> is a diagram showing a computer memory system, in accordance with an embodiment of the present invention;</p>
<p id="p-0029" num="0028"><figref idref="DRAWINGS">FIG. 5A</figref> is a diagram showing a computer memory system, in accordance with an embodiment of the present invention;</p>
<p id="p-0030" num="0029"><figref idref="DRAWINGS">FIG. 5B</figref> is a diagram showing the computer memory system having a second data block stored with respect to cache line placement, in accordance with an embodiment of the present invention;</p>
<p id="p-0031" num="0030"><figref idref="DRAWINGS">FIG. 5C</figref> is a diagram showing the computer memory system having a third data block stored to allow contiguous storage of the hot sub-blocks in cache memory, in accordance with an embodiment of the present invention;</p>
<p id="p-0032" num="0031"><figref idref="DRAWINGS">FIG. 6</figref> is a flowchart showing a method for reducing cache conflict misses, in accordance with an embodiment of the present invention; and</p>
<p id="p-0033" num="0032"><figref idref="DRAWINGS">FIG. 7</figref> is a block diagram of an exemplary computer system for carrying out the processing according to the invention.</p>
</description-of-drawings>
<?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?>
<?DETDESC description="Detailed Description" end="lead"?>
<heading id="h-0005" level="1">DETAILED DESCRIPTION OF THE PREFERRED EMBODIMENTS</heading>
<p id="p-0034" num="0033">An invention is disclosed for reducing cache conflict misses via specific placement of non-split functions and data objects in main memory. In the following description, numerous specific details are set forth in order to provide a thorough understanding of the present invention. It will be apparent, however, to one skilled in the art that the present invention may be practiced without some or all of these specific details. In other instances, well known process steps have not been described in detail in order not to unnecessarily obscure the present invention.</p>
<p id="p-0035" num="0034"><figref idref="DRAWINGS">FIGS. 1-3C</figref> were described in terms of the prior art. <figref idref="DRAWINGS">FIG. 4A</figref> is a block diagram showing an exemplary data block <b>400</b> having a plurality of sub-blocks <b>402</b><i>a</i>-<b>402</b><i>c</i>. A computer program is comprised of a plurality of data blocks <b>400</b>, which can be either data objects or functions. A data block <b>400</b> that represents a data object includes a plurality of fields, represented by the sub-blocks <b>402</b><i>a</i>-<b>402</b><i>c </i>in <figref idref="DRAWINGS">FIG. 4A</figref>. Conversely, a data block <b>400</b> that represents a function comprises a plurality of basic blocks, represented by the sub-blocks <b>402</b><i>a</i>-<b>402</b><i>c</i>. Each basic block <b>402</b><i>a </i>comprises a set of computer instructions, as discussed next with reference to <figref idref="DRAWINGS">FIG. 4B</figref>.</p>
<p id="p-0036" num="0035"><figref idref="DRAWINGS">FIG. 4B</figref> is a flow diagram showing an exemplary function <b>450</b> representing the function stored in the data block <b>400</b> of <figref idref="DRAWINGS">FIG. 4A</figref>. As shown in <figref idref="DRAWINGS">FIG. 4B</figref>, the basic blocks <b>402</b><i>a</i>-<b>402</b><i>c </i>of a function interact with each other. For example, basic block A <b>402</b><i>a </i>can call either basic block B <b>402</b><i>b </i>or C <b>402</b><i>c </i>in <figref idref="DRAWINGS">FIG. 4B</figref>. Referring back to <figref idref="DRAWINGS">FIG. 4A</figref>, a data block <b>400</b> is used to describe either a function or a data object. Thus, in the following description, the term data block will be used as synonymous with data object and function. In particular, it should noted that the reference to a data block is intended to merely refer to any type of associated data that is stored in memory, such as a function or data object. Similarly, the term sub-block will be used as synonymous with field and basic block. In particular, it should noted that the reference to a sub-block is intended to merely refer to any type of data that is part of a data block, such as a field or basic block.</p>
<p id="p-0037" num="0036">Broadly speaking, embodiments of the present invention enhance cache performance by arranging data blocks <b>400</b> in main memory such that frequently accessed sub-blocks <b>402</b> are contiguously arranged in cache memory. As will be apparent to those skilled in the art, there exist a plurality of techniques to determine which sub-blocks are frequently accessed, or “hot,” and which sub-blocks are infrequently accessed, or “cold,” for example, the techniques of profiling, instrumentation, and compiler heuristics can be employed. When arranging data blocks <b>400</b>, embodiments of the present invention reduce cache conflict misses by clustering the hot sub-blocks of data blocks with respect to cache line placement instead of with respect to main memory addresses.</p>
<p id="p-0038" num="0037"><figref idref="DRAWINGS">FIG. 4C</figref> is a diagram showing a computer memory system <b>460</b>, in accordance with an alternate embodiment of the present invention. The computer memory system <b>460</b> includes a main memory <b>502</b> and a cache memory <b>504</b> of a particular cache size W<sub>510 </sub>bytes, for example 64K bytes. As previously mentioned, cache memories <b>504</b> rely on the principle of locality to increase the likelihood that a processor will find the information it is looking for in the cache memory <b>504</b>. Hence, when a portion of data residing in the main memory <b>502</b> is accessed, a copy of the portion of data is placed into the cache memory <b>504</b> to increase the speed at which this data can be accessed thereafter.</p>
<p id="p-0039" num="0038">Generally, each location in the main memory <b>502</b> maps to a fixed location in the cache memory <b>504</b>. For example, memory regions <b>465</b><i>a</i>, <b>465</b><i>b</i>, and <b>465</b><i>c </i>each represent regions of main memory <b>502</b> that occupy the same footprint as the cache memory <b>504</b>. Each memory regions <b>465</b><i>a</i>, <b>465</b><i>b</i>, and <b>465</b><i>c </i>is further divided into sub-regions <b>470</b>. Whenever data from one of the sub-regions <b>470</b> is copied to the cache memory <b>504</b>, the data is mapped to a corresponding cache sub-region <b>480</b> within the cache memory <b>504</b>.</p>
<p id="p-0040" num="0039">Hence, sub-regions <b>470</b> in the main memory <b>502</b> are typically forced to share cache sub-regions <b>480</b> within the cache memory <b>504</b>. For example, sub-block <b>402</b><i>a </i>is stored within the main memory <b>502</b> at a particular location <b>470</b><i>a</i>. When sub-block <b>402</b><i>a </i>is copied into the cache memory <b>504</b>, the sub-block <b>402</b><i>a </i>will occupy a sub-region <b>480</b><i>a </i>within the cache memory <b>504</b>. If another sub-block <b>402</b><i>b </i>is stored in the main memory <b>502</b> at an address offset from sub-block <b>402</b><i>a </i>equal to the cache size W<sub>510 </sub>bytes, sub-block <b>402</b><i>b </i>will occupy the same sub-region <b>480</b><i>a </i>in the cache memory <b>504</b> as occupied by sub-block <b>402</b><i>a</i>, thus overwriting sub-block <b>402</b><i>a </i>in cache memory <b>504</b>. Similarly, if a further sub-block <b>402</b><i>c </i>is stored in the main memory <b>502</b> at an address offset from sub-block <b>402</b><i>a </i>equal to a multiple of the cache size XW<sub>510</sub>, sub-block <b>402</b><i>c </i>will also occupy the same sub-region <b>480</b><i>a </i>in the cache memory <b>504</b> as occupied by sub-block <b>402</b><i>a </i>and <b>402</b><i>b</i>. As can be seen, <figref idref="DRAWINGS">FIG. 4C</figref> defines a structure that illustrates a problem with prior art depicted by the overwriting of sub-blocks <b>402</b><i>a </i>and <b>402</b><i>b </i>by sub-block <b>402</b><i>c</i>.</p>
<p id="p-0041" num="0040">Embodiments of the present invention utilize the above-described properties of cache memory to arrange the hot sub-blocks of data blocks contiguously within cache memory, as discussed next with reference to <figref idref="DRAWINGS">FIGS. 5A-5C</figref>. <figref idref="DRAWINGS">FIG. 5A</figref> is a diagram showing a computer memory system <b>500</b>, in accordance with an embodiment of the present invention. As above, the computer memory system <b>500</b> includes a main memory <b>502</b> and a cache memory <b>504</b> having a cache size of W<sub>510 </sub>bytes. In the example of <figref idref="DRAWINGS">FIG. 5A</figref>, a data block <b>400</b><i>a </i>having cold sub-blocks <b>402</b><i>a </i>and a hot sub-block <b>506</b><i>a </i>is stored in the main memory <b>502</b>. In particular, the hot sub-block <b>506</b><i>a </i>of the data block <b>400</b><i>a </i>is stored at an address <b>510</b><i>a </i>in the main memory, and will occupy address <b>520</b><i>a </i>in cache memory <b>504</b> when accessed and stored in the cache memory <b>504</b>.</p>
<p id="p-0042" num="0041">Since, the cold sub-blocks <b>402</b><i>a </i>are infrequently accessed, they will also be infrequently copied to the cache memory <b>504</b>. In contrast, the hot sub-block <b>506</b><i>a </i>is frequently accessed, and thus is desirable to have in cache memory <b>504</b> during execution. As a result, embodiments of the present invention store data blocks in main memory <b>502</b> based on the address of the hot sub-blocks in memory, rather than the address of the entire data block in memory. More particularly, embodiments of the present invention cluster the hot sub-blocks of data blocks with respect to cache line placement instead of with respect to main memory addresses.</p>
<p id="p-0043" num="0042">The embodiments of the present invention store data blocks in main memory <b>502</b> such that the hot sub-blocks will be contiguously stored in the cache memory <b>504</b> when accessed and stored in cache memory <b>504</b>. Thus, if a data block cannot be placed contiguously in main memory <b>502</b> to allow contiguous cache memory <b>504</b> storage of the hot sub-blocks, embodiments of the present invention offset the data block in main memory based on the cache size W<sub>510</sub>, as shown next in <figref idref="DRAWINGS">FIG. 5B</figref>.</p>
<p id="p-0044" num="0043"><figref idref="DRAWINGS">FIG. 5B</figref> is a diagram showing the computer memory system <b>500</b> having a second data block <b>400</b><i>b </i>stored with respect to cache line placement, in accordance with an embodiment of the present invention. In the example of <figref idref="DRAWINGS">FIG. 5B</figref>, the data block <b>400</b><i>b </i>having cold sub-blocks <b>402</b><i>b </i>and a hot sub-block <b>506</b><i>b </i>is to be stored in the main memory <b>502</b>. As can be seen, in the example of <figref idref="DRAWINGS">FIG. 5B</figref> the hot sub-block <b>506</b><i>b </i>of the data block <b>400</b><i>b </i>will not be stored contiguously in cache memory <b>504</b> if the data block <b>400</b><i>b </i>is stored next to the data block <b>400</b><i>a </i>in the main memory <b>502</b>.</p>
<p id="p-0045" num="0044">Hence, the embodiments of the present invention store the data block <b>400</b><i>b </i>such that the address <b>510</b><i>b </i>of the hot sub-block <b>506</b><i>b </i>is offset cache size W<sub>510 </sub>bytes from the ending address of the hot sub-block <b>506</b><i>a </i>of data block <b>400</b><i>a</i>. As a result, the hot sub-block <b>506</b><i>b </i>of data block <b>400</b><i>b </i>will occupy an address <b>520</b><i>b </i>within the cache memory <b>504</b> that is contiguous to the hot block <b>506</b><i>a</i>, which is stored at address <b>520</b><i>a</i>, when accessed and copied to cache memory <b>504</b>.</p>
<p id="p-0046" num="0045"><figref idref="DRAWINGS">FIG. 5C</figref> is a diagram showing the computer memory system <b>500</b> having a third data block <b>400</b><i>c </i>stored to allow contiguous storage of the hot sub-blocks in cache memory, in accordance with an embodiment of the present invention. In the example of <figref idref="DRAWINGS">FIG. 5C</figref>, the data block <b>400</b><i>c </i>having cold sub-blocks <b>402</b><i>c </i>and a hot sub-block <b>506</b><i>c </i>is to be stored in the main memory <b>502</b>. Here, the hot sub-block <b>506</b><i>c </i>of the data block <b>400</b><i>c </i>can be stored in the main memory <b>502</b> contiguous with the data block <b>400</b><i>a </i>while allowing contiguous storage of the hot sub-blocks in the cache memory <b>504</b>. Thus, data block <b>400</b><i>c </i>is stored in the main memory <b>502</b> contiguous with the data block <b>400</b><i>a</i>. The hot sub-block <b>506</b><i>c </i>is stored at an address <b>510</b><i>c </i>in the main memory <b>502</b>, resulting in storage at address <b>520</b><i>c</i>, which is contiguous with hot sub-block <b>506</b><i>b</i>, in the cache memory <b>504</b> when accessed and stored in the cache memory <b>504</b>. In this manner, the embodiments of the present invention allow storage of data blocks in main memory resulting in efficient cache usage while avoiding splitting of the data blocks and the resulting extra overhead caused by splitting.</p>
<p id="p-0047" num="0046"><figref idref="DRAWINGS">FIG. 6</figref> is a flowchart showing a method <b>600</b> for reducing cache conflict misses, in accordance with an embodiment of the present invention. In an initial operation <b>602</b>, preprocess operations are performed. Preprocess operations include program development, system provisioning, and other preprocess operations that will be apparent to those skilled in the art.</p>
<p id="p-0048" num="0047">In operation <b>604</b>, the hot sub-blocks of the data blocks comprising the program being loaded into memory are determined. As will be apparent to those skilled in the art, there exist a plurality of techniques to determine which sub-blocks are frequently accessed, or “hot,” and which sub-blocks are infrequently accessed, or “cold,” for example, profiling, instrumentation, and compiler heuristics.</p>
<p id="p-0049" num="0048">The cache size is determined in operation <b>606</b>. The cache size can be determined through manual system analysis, polling, or other techniques as will be apparent to those skilled in the art. As mentioned previously, the embodiments of the present invention utilize the cache size for placement of data blocks in main memory. In particular, embodiments of the present invention reduce cache conflict misses by clustering the hot sub-blocks of data blocks with respect to cache line placement instead of with respect to main memory addresses. The first data block is then placed in main memory, in operation <b>608</b>. The placement location of the first data block can be determined using any heuristic used in memory allocation software.</p>
<p id="p-0050" num="0049">A decision is then made as to whether additional data blocks remain to be allocated to memory, in operation <b>610</b>. If additional data blocks remain to be allocated to memory, the method <b>600</b> continues with operation <b>612</b>. Otherwise, the method <b>600</b> ends with post process operation <b>614</b>.</p>
<p id="p-0051" num="0050">In operation <b>612</b>, the next data block is placed such that the hot sub-block of the data block is offset a multiple of the cache size in bytes from the ending address of the last placed hot sub-block of the last placed data block. Embodiments of the present invention utilize the properties of cache memory to arrange the hot sub-blocks of data blocks contiguously within cache memory. The hot sub-blocks are frequently accessed, and thus are desirable to have in cache memory during execution. As a result, embodiments of the present invention store data blocks in main memory based on the address of the hot sub-blocks in memory, rather than the address of the entire data block in memory. More particularly, embodiments of the present invention cluster the hot sub-blocks of data blocks with respect to cache line placement instead of with respect to main memory addresses.</p>
<p id="p-0052" num="0051">The embodiments of the present invention store data blocks in main memory such that the hot sub-blocks will be contiguously stored in the cache memory when accessed and stored in cache memory. Thus, if a data block cannot be placed contiguously in main memory to allow contiguous cache memory storage of the hot sub-blocks, embodiments of the present invention offset the data block in main memory based on the cache size.</p>
<p id="p-0053" num="0052">More particularly, the embodiments of the present invention store the data blocks such that the address of a hot sub-block is offset cache size bytes from the ending address of the last placed hot sub-block of the last placed data block. As a result, the hot sub-block of a subsequent data block will occupy an address within the cache memory that is contiguous to the hot block of a previous data block when accessed and copied to cache memory.</p>
<p id="p-0054" num="0053">As previously mentioned, if the hot sub-block <b>506</b><i>c </i>of a data block can be stored in the main memory contiguous with another data block while allowing contiguous storage of the hot sub-blocks in the cache memory then it can be stored as such. In this manner, the embodiments of the present invention allow storage of data blocks in main memory resulting in efficient cache usage while avoiding splitting of the data blocks and the resulting extra overhead caused by splitting. Post process operations are performed in operation <b>614</b>. Post process operations include executing the computer program, further disk access, and other post process operations that will be apparent to those skilled in the art.</p>
<p id="p-0055" num="0054">Although a particular example of reducing conflict misses has been presented, it should be noted that embodiments of the present invention can reduce conflict misses using addition techniques. For example, in a system where objects are both allocated and deallocated, the deallocation of objects can leave a “hole” in the cache memory that can be usefully filled by a subsequent allocation using the embodiments of the present invention. In addition, memory often is organized into multiple levels, denoted for example, as L<b>1</b>,L<b>2</b>,L<b>3</b>, and Main. Each level typically includes its own size cache and often has different block sizes and conflict resolution polices. As such, although the embodiments of the present invention have been described in terms of a two level memory architecture, it should be noted that the embodiments of the present invention can be applied to any level of memory and generally to multiple levels at once. For example, a placement can be chosen that places the hot block in less conflicted blocks of both L<b>1</b> and L<b>2</b> simultaneously.</p>
<p id="p-0056" num="0055">Embodiments of the present invention may employ various computer-implemented operations involving data stored in computer systems to drive computer software, including application programs, operating system programs, peripheral device drivers, etc. These operations are those requiring physical manipulation of physical quantities. Usually, though not necessarily, these quantities take the form of electrical or magnetic signals capable of being stored, transferred, combined, compared, and otherwise manipulated. Further, the manipulations performed are often referred to in terms, such as producing, identifying, determining, or comparing.</p>
<p id="p-0057" num="0056">Any of the operations described herein that form part of the invention are useful machine operations. The invention also relates to a device or an apparatus for performing these operations. The apparatus may be specially constructed for the required purposes, or it may be a general purpose computer selectively activated or configured by a computer program stored in the computer. In particular, various general purpose machines may be used with computer programs written in accordance with the teachings herein, or it may be more convenient to construct a more specialized apparatus to perform the required operations. An exemplary structure for the invention is described below.</p>
<p id="p-0058" num="0057"><figref idref="DRAWINGS">FIG. 7</figref> is a block diagram of an exemplary computer system <b>700</b> for carrying out the processing according to the invention. The computer system <b>700</b> includes a digital computer <b>702</b>, a display screen (or monitor) <b>704</b>, a printer <b>706</b>, a floppy disk drive <b>708</b>, a hard disk drive <b>710</b>, a network interface <b>712</b>, and a keyboard <b>714</b>. The digital computer <b>702</b> includes a microprocessor <b>716</b>, a memory bus <b>718</b>, random access memory (RAM) <b>720</b>, read only memory (ROM) <b>722</b>, a peripheral bus <b>724</b>, and a keyboard controller (KBC) <b>726</b>. The digital computer <b>702</b> can be a personal computer (such as an IBM compatible personal computer, a Macintosh computer or Macintosh compatible computer), a workstation computer (such as a Sun Microsystems or Hewlett-Packard workstation), or some other type of computer.</p>
<p id="p-0059" num="0058">The microprocessor <b>716</b> is a general purpose digital processor which controls the operation of the computer system <b>700</b>. The microprocessor <b>716</b> can be a single-chip processor or can be implemented with multiple components. Using instructions retrieved from memory, the microprocessor <b>716</b> controls the reception and manipulation of input data and the output and display of data on output devices. According to the invention, a particular function of microprocessor <b>716</b> is to assist in loading of program data blocks into cache memory <b>716</b><i>a</i>. As discussed above, the cache memory <b>716</b><i>a </i>is a type of rapidly accessible memory that is used between the microprocessor <b>716</b> and the RAM <b>720</b>. Cache memories rely on the principle of locality to attempt to increase the likelihood that a processor will find the information it is looking for in the cache memory. To this end, cache memories store contiguous hot sub-blocks as described above.</p>
<p id="p-0060" num="0059">The memory bus <b>718</b> is used by the microprocessor <b>716</b> to access the RAM <b>720</b> and the ROM <b>722</b>. The RAM <b>720</b> is used by the microprocessor <b>716</b> as a general storage area and as scratch-pad memory, and can also be used to store input data and processed data. The ROM <b>722</b> can be used to store instructions or program code followed by the microprocessor <b>716</b> as well as other data.</p>
<p id="p-0061" num="0060">The peripheral bus <b>724</b> is used to access the input, output, and storage devices used by the digital computer <b>702</b>. In the described embodiment, these devices include the display screen <b>704</b>, the printer device <b>706</b>, the floppy disk drive <b>708</b>, the hard disk drive <b>710</b>, and the network interface <b>712</b>. The keyboard controller <b>726</b> is used to receive input from keyboard <b>714</b> and send decoded symbols for each pressed key to microprocessor <b>716</b> over bus <b>728</b>.</p>
<p id="p-0062" num="0061">The display screen <b>704</b> is an output device that displays images of data provided by the microprocessor <b>716</b> via the peripheral bus <b>724</b> or provided by other components in the computer system <b>700</b>. The printer device <b>706</b>, when operating as a printer, provides an image on a sheet of paper or a similar surface. Other output devices such as a plotter, typesetter, etc. can be used in place of, or in addition to, the printer device <b>706</b>.</p>
<p id="p-0063" num="0062">The floppy disk drive <b>708</b> and the hard disk drive <b>710</b> can be used to store various types of data. The floppy disk drive <b>708</b> facilitates transporting such data to other computer systems, and hard disk drive <b>710</b> permits fast access to large amounts of stored data.</p>
<p id="p-0064" num="0063">The microprocessor <b>716</b> together with an operating system operate to execute computer code and produce and use data. The computer code and data may reside on the RAM <b>720</b>, the ROM <b>722</b>, or the hard disk drive <b>710</b>. The computer code and data could also reside on a removable program medium and loaded or installed onto the computer system <b>700</b> when needed. Removable program media include, for example, CD-ROM, PC-CARD, floppy disk and magnetic tape.</p>
<p id="p-0065" num="0064">The network interface <b>712</b> is used to send and receive data over a network connected to other computer systems. An interface card or similar device and appropriate software implemented by the microprocessor <b>716</b> can be used to connect the computer system <b>700</b> to an existing network and transfer data according to standard protocols.</p>
<p id="p-0066" num="0065">The keyboard <b>714</b> is used by a user to input commands and other instructions to the computer system <b>700</b>. Other types of user input devices can also be used in conjunction with the present invention. For example, pointing devices such as a computer mouse, a track ball, a stylus, or a tablet can be used to manipulate a pointer on a screen of a general-purpose computer.</p>
<p id="p-0067" num="0066">The invention can also be embodied as computer readable code on a computer readable medium. The computer readable medium is any data storage device that can store data which can be thereafter be read by a computer system. Examples of the computer readable medium include read-only memory (ROM), random-access memory (RAM), CD-ROMs, magnetic tape, and optical data storage devices. The computer readable medium can also be distributed over a network that couples computer systems so that the computer readable code is stored and executed in a distributed fashion.</p>
<p id="p-0068" num="0067">Furthermore, the same or similar methods and apparatuses described above for programming a hardware device can also be used for performing other particular maintenance operations on the hardware device. For example, operations such as erasing a ROM, reading a ROM, or performing a checksum on a ROM can be performed. Although the foregoing invention has been described in some detail for purposes of clarity of understanding, it will be apparent that certain changes and modifications may be practiced within the scope of the appended claims. Accordingly, the present embodiments are to be considered as illustrative and not restrictive, and the invention is not to be limited to the details given herein, but may be modified within the scope and equivalents of the appended claims.</p>
<?DETDESC description="Detailed Description" end="tail"?>
</description>
<us-claim-statement>What is claimed is:</us-claim-statement>
<claims id="claims">
<claim id="CLM-00001" num="00001">
<claim-text>1. A method for reducing computer cache conflict misses, comprising the operations of:
<claim-text>determining a cache size of a computer cache memory;</claim-text>
<claim-text>placing a first data block within a main computer memory, the first data block occupying a contiguous portion of the main computer memory, wherein the first data block includes a first sub-block that will be frequently referenced, and wherein the first sub-block ends at a first ending address; and</claim-text>
<claim-text>placing a second data block within the main computer memory, the second data block including a second sub-block that will be frequently referenced, the second data block occupying a contiguous portion of the main computer memory, wherein the second data block is placed such that the second sub-block will be contiguous with the first sub-block in the computer cache memory during execution, and the second sub-block is located at a main memory address that is offset by at least one or a multiple of the cache size from the first ending address of the first sub-block to prevent cache conflict misses.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00002" num="00002">
<claim-text>2. A method as recited in <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising the operation of placing a third data block within the main computer memory, the third data block including a third sub-block that will be frequently referenced, wherein the third data block is placed such that the third sub-block will be contiguous with the first sub-block and the second sub-block in the computer cache memory during execution.</claim-text>
</claim>
<claim id="CLM-00003" num="00003">
<claim-text>3. A method as recited in <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein the third data block is placed such that the third sub-block is located at a main memory address that is offset by a multiple of the cache size from an ending address of the second sub-block to prevent cache conflict misses.</claim-text>
</claim>
<claim id="CLM-00004" num="00004">
<claim-text>4. A method as recited in <claim-ref idref="CLM-00003">claim 3</claim-ref>, wherein the first data block, the second data block, and the third data block each is a data object.</claim-text>
</claim>
<claim id="CLM-00005" num="00005">
<claim-text>5. A method as recited in <claim-ref idref="CLM-00003">claim 3</claim-ref>, wherein the first data block, the second data block, and the third data block each is a function.</claim-text>
</claim>
<claim id="CLM-00006" num="00006">
<claim-text>6. A computer program embodied on a computer readable medium for reducing computer cache conflict misses, comprising:
<claim-text>a code, segment that places a first data block within a main computer memory, the first data block occupying a first contiguous portion of the main computer memory, wherein the first data block includes a first sub-block that will be frequently referenced, and wherein the first sub-block ends at a first ending address; and</claim-text>
<claim-text>a code segment that places a second data block within the main computer memory, the second data block occupying a second contiguous portion of the main computer memory, the second data block including a second sub-block that will be frequently referenced, wherein the second data block is placed such that the second sub-block is located at a main memory address that is offset by at least one or a multiple of a cache size of the computer cache from the first ending address of the first sub-block to prevent cache conflict misses so that the second sub-block will be contiguous with the first sub-block in the computer cache during execution.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00007" num="00007">
<claim-text>7. A computer program as recited in <claim-ref idref="CLM-00006">claim 6</claim-ref>, further comprising a code segment that determines which sub-blocks of a data block will be frequently accessed.</claim-text>
</claim>
<claim id="CLM-00008" num="00008">
<claim-text>8. A computer program as recited in <claim-ref idref="CLM-00006">claim 6</claim-ref>, further comprising a code segment that determines the cache size.</claim-text>
</claim>
<claim id="CLM-00009" num="00009">
<claim-text>9. A computer program as recited in <claim-ref idref="CLM-00006">claim 6</claim-ref>, wherein the first data block is not split in main memory, and wherein the second data block is not split in main memory.</claim-text>
</claim>
<claim id="CLM-00010" num="00010">
<claim-text>10. A computer program as recited in <claim-ref idref="CLM-00006">claim 6</claim-ref>, further comprising a code segment that places a third data block within the main computer memory, the third data block including a third sub-block that will be frequently referenced, wherein the third data block is placed such that the third sub-block will be contiguous with the first sub-block and the second sub-block in the computer cache memory during execution.</claim-text>
</claim>
<claim id="CLM-00011" num="00011">
<claim-text>11. A computer program as recited in <claim-ref idref="CLM-00010">claim 10</claim-ref>, wherein the third data block is placed such that the third sub-block is located at a main memory address that is offset by a multiple of the cache size from an ending address of the second sub-block to prevent cache conflict misses.</claim-text>
</claim>
<claim id="CLM-00012" num="00012">
<claim-text>12. A computer program as recited in <claim-ref idref="CLM-00010">claim 10</claim-ref>, wherein the first data block, the second data block, and the third data block each is a data object.</claim-text>
</claim>
<claim id="CLM-00013" num="00013">
<claim-text>13. A computer program as recited in <claim-ref idref="CLM-00011">claim 11</claim-ref>, wherein the first data block, the second data block, and the third data block each is a function.</claim-text>
</claim>
<claim id="CLM-00014" num="00014">
<claim-text>14. A method for reducing computer cache conflict misses, comprising the operations of:
<claim-text>determining a cache size of a computer cache memory;</claim-text>
<claim-text>placing a first data block within a main computer memory, the first data block occupying a contiguous portion of the main computer memory, wherein the first data block includes a first sub-block that will be frequently referenced, and wherein the first sub-block ends at a first ending address; and</claim-text>
<claim-text>placing a second data block within the main computer memory, the second data block occupying a contiguous portion of the main computer memory, the second data block including a second sub-block that will be frequently referenced, wherein the second data block is placed such that the second sub-block is located at a main memory address that is offset by at least one or a multiple of the cache size from the first ending address of the first sub-block to prevent cache conflict misses so that the second sub-block will be contiguous with the first sub-block in the computer cache during execution.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00015" num="00015">
<claim-text>15. A method as recited in <claim-ref idref="CLM-00014">claim 14</claim-ref>, wherein each data block is a data object, and wherein each sub-block is a field.</claim-text>
</claim>
<claim id="CLM-00016" num="00016">
<claim-text>16. A method as recited in <claim-ref idref="CLM-00015">claim 15</claim-ref>, further comprising the operation of placing a third data object within the main computer memory, the third data object including a third field that will be frequently referenced, wherein the third data object is placed such that the third field is located at a main memory address that is offset by a multiple of the cache size from the ending address of the second sub-block to prevent cache conflict misses.</claim-text>
</claim>
<claim id="CLM-00017" num="00017">
<claim-text>17. A method as recited in <claim-ref idref="CLM-00014">claim 14</claim-ref>, wherein each data block is a function, and wherein each sub-block is a basic block.</claim-text>
</claim>
<claim id="CLM-00018" num="00018">
<claim-text>18. A method as recited in <claim-ref idref="CLM-00017">claim 17</claim-ref>, further comprising the operation of placing a third function within the main computer memory, the third function including a third basic block that will be frequently referenced, wherein the third function is placed such that the third basic block is located at a main memory address that is offset by a multiple of the cache size from the ending address of the second basic block to prevent cache conflict misses.</claim-text>
</claim>
<claim id="CLM-00019" num="00019">
<claim-text>19. A method for reducing computer cache conflict misses, comprising the operations of:
<claim-text>determining a cache size of a computer cache memory;</claim-text>
<claim-text>placing a first data block within a main computer memory, without splitting the first data block, wherein the first data block includes a first sub-block that will be frequently referenced, and wherein the first sub-block ends at a first ending address; and</claim-text>
<claim-text>placing a second data block within the main computer memory, without splitting the second data block, the second data block including a second sub-block that will be frequently referenced, wherein the second data block is placed such that the second sub-block will be contiguous with the first sub-block in the computer cache memory during execution, and the second sub-block is located at a main computer memory address that is offset by at least one or a multiple of the cache size from the first ending address of the first sub-block to prevent cache conflicting misses.</claim-text>
</claim-text>
</claim>
</claims>
</us-patent-grant>
