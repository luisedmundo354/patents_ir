<us-patent-grant lang="EN" dtd-version="v4.2 2006-08-23" file="US07298906-20071120.XML" status="PRODUCTION" id="us-patent-grant" country="US" date-produced="20071106" date-publ="20071120">
<us-bibliographic-data-grant>
<publication-reference>
<document-id>
<country>US</country>
<doc-number>07298906</doc-number>
<kind>B2</kind>
<date>20071120</date>
</document-id>
</publication-reference>
<application-reference appl-type="utility">
<document-id>
<country>US</country>
<doc-number>11068185</doc-number>
<date>20050228</date>
</document-id>
</application-reference>
<us-application-series-code>11</us-application-series-code>
<us-term-of-grant>
<us-term-extension>353</us-term-extension>
</us-term-of-grant>
<classifications-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>G</section>
<class>06</class>
<subclass>K</subclass>
<main-group>9</main-group>
<subgroup>46</subgroup>
<symbol-position>F</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20071120</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
</classifications-ipcr>
<classification-national>
<country>US</country>
<main-classification>382195</main-classification>
<further-classification>382173</further-classification>
</classification-national>
<invention-title id="d0e53">Hierarchical determination of feature relevancy for mixed data types</invention-title>
<references-cited>
<citation>
<patcit num="00001">
<document-id>
<country>US</country>
<doc-number>6865582</doc-number>
<kind>B2</kind>
<name>Obradovic et al.</name>
<date>20050300</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>7071041</main-classification></classification-national>
</citation>
<citation>
<patcit num="00002">
<document-id>
<country>US</country>
<doc-number>2004/0019574</doc-number>
<kind>A1</kind>
<name>Meng et al.</name>
<date>20040100</date>
</document-id>
</patcit>
<category>cited by other</category>
<classification-national><country>US</country><main-classification>706 15</main-classification></classification-national>
</citation>
<citation>
<patcit num="00003">
<document-id>
<country>WO</country>
<doc-number>WO 2005/008571</doc-number>
<date>20050100</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<nplcit num="00004">
<othercit>Ichino “General metrics for mixed features”, IEEE, pp. 494-497, 1988.</othercit>
</nplcit>
<category>cited by examiner</category>
</citation>
<citation>
<nplcit num="00005">
<othercit>Li, et al “Unsupervised learning with mixed numeric and nominal data”, IEEE, pp. 673-690, 2002.</othercit>
</nplcit>
<category>cited by examiner</category>
</citation>
<citation>
<nplcit num="00006">
<othercit>Biswas, et al “ITERATE: A conceptual clustering method for knowledge discovery in databases”, Vanderbilt University, pp. 1-29, 1995.</othercit>
</nplcit>
<category>cited by examiner</category>
</citation>
<citation>
<nplcit num="00007">
<othercit>Biswas, et al. “A conceptual clustering algorithm for data mining”, IEEE, pp. 100-111, May 1998.</othercit>
</nplcit>
<category>cited by examiner</category>
</citation>
<citation>
<nplcit num="00008">
<othercit>I. E. Frank; “Modern Nonlinear Regression Methods”: Chemometrics and Intelligent Laboratory Systems, vol. 27; pp. 1-19, 1995.</othercit>
</nplcit>
<category>cited by other</category>
</citation>
<citation>
<nplcit num="00009">
<othercit>Hwang, et al; “Hierarchical Discriminant Regression”; IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 22, No. 11, pp. 1277-1293; XP-002302472, Nov. 2000.</othercit>
</nplcit>
<category>cited by other</category>
</citation>
<citation>
<nplcit num="00010">
<othercit>Jain, et al; “Feature Selection: Evaluation, Application, and Small Sample Performance”; IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 19, No. 2; pp. 153-158; XP-002294891, Feb. 1997.</othercit>
</nplcit>
<category>cited by other</category>
</citation>
<citation>
<nplcit num="00011">
<othercit>E. Krusinska; “Two Step Semi-Optimal Branch and Bound Algorithm for Feature Selection in Mixed Variable Discrimination”; 1023 Pattern Recognition, vol. 22, No. 4, Head. Hill Hall, Oxford, GB; pp. 455-459; XP-000046518, 1989.</othercit>
</nplcit>
<category>cited by other</category>
</citation>
<citation>
<nplcit num="00012">
<othercit>P. M. Narandra et al.; “A Branch and Bound Algorithm for Feature Subset Selection”; IEEE Transactions on Computers, vol. C-26, No. 9; pp. 917-922; XP-0000647423, Sep. 1977.</othercit>
</nplcit>
<category>cited by other</category>
</citation>
<citation>
<nplcit num="00013">
<othercit>Communication pursuant to Article 96(2) EPC (Appln. No. 04 777 558.0-1224), Jun. 1, 2006.</othercit>
</nplcit>
<category>cited by other</category>
</citation>
<citation>
<nplcit num="00014">
<othercit>PCT Notification of Transmittal of the International Search Report and the Written Opinion of the International Searching Authority, or the Declaration, for International Application No. PCT/US2006/007158, 13 pages, Jul. 10, 2006.</othercit>
</nplcit>
<category>cited by other</category>
</citation>
<citation>
<nplcit num="00015">
<othercit>Hall et al.; “Benchmarking Attribute Selection Techniques for Discrete Class Data Mining”; Knowledge and Data Engineering, vol. 15, No. 6; pp. 1437-1447; XP-002387331, Nov. 6, 2003.</othercit>
</nplcit>
<category>cited by other</category>
</citation>
<citation>
<nplcit num="00016">
<othercit>P. Perner; “Improving the Accuracy of Decision Tree Induction by Feature Pre-Selection”; Applied Artificial Intelligence, vol. 15, No. 8; pp. 747-760; XP-002387332, 2001.</othercit>
</nplcit>
<category>cited by other</category>
</citation>
<citation>
<nplcit num="00017">
<othercit>R. L. DeMantaras; “A Distance-Based Attribute Selection Measure for Decision Tree Induction”; Machine Learning, vol. 6; pp. 81-92; XP-008065823, 1991.</othercit>
</nplcit>
<category>cited by other</category>
</citation>
</references-cited>
<number-of-claims>13</number-of-claims>
<us-exemplary-claim>1</us-exemplary-claim>
<us-field-of-classification-search>
<classification-national>
<country>US</country>
<main-classification>382173</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>382190</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>382195</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>382209</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>382225</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>382282</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>382288</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>707  4- 10</main-classification>
<additional-info>unstructured</additional-info>
</classification-national>
</us-field-of-classification-search>
<figures>
<number-of-drawing-sheets>3</number-of-drawing-sheets>
<number-of-figures>6</number-of-figures>
</figures>
<us-related-documents>
<continuation-in-part>
<relation>
<parent-doc>
<document-id>
<country>US</country>
<doc-number>10615885</doc-number>
<kind>00</kind>
<date>20030708</date>
</document-id>
<parent-status>PENDING</parent-status>
</parent-doc>
<child-doc>
<document-id>
<country>US</country>
<doc-number>11068185</doc-number>
</document-id>
</child-doc>
</relation>
</continuation-in-part>
<related-publication>
<document-id>
<country>US</country>
<doc-number>20050149518</doc-number>
<kind>A1</kind>
<date>20050707</date>
</document-id>
</related-publication>
</us-related-documents>
<parties>
<applicants>
<applicant sequence="001" app-type="applicant-inventor" designation="us-only">
<addressbook>
<last-name>Duan</last-name>
<first-name>Baofu</first-name>
<address>
<city>Cleveland Heights</city>
<state>OH</state>
<country>US</country>
</address>
</addressbook>
<nationality>
<country>omitted</country>
</nationality>
<residence>
<country>US</country>
</residence>
</applicant>
<applicant sequence="002" app-type="applicant-inventor" designation="us-only">
<addressbook>
<last-name>Meng</last-name>
<first-name>Zhuo</first-name>
<address>
<city>Broadview Heights</city>
<state>OH</state>
<country>US</country>
</address>
</addressbook>
<nationality>
<country>omitted</country>
</nationality>
<residence>
<country>US</country>
</residence>
</applicant>
<applicant sequence="003" app-type="applicant-inventor" designation="us-only">
<addressbook>
<last-name>Pao</last-name>
<first-name>Yoh-Han</first-name>
<address>
<city>Cleveland Heights</city>
<state>OH</state>
<country>US</country>
</address>
</addressbook>
<nationality>
<country>omitted</country>
</nationality>
<residence>
<country>US</country>
</residence>
</applicant>
</applicants>
<agents>
<agent sequence="01" rep-type="attorney">
<addressbook>
<orgname>Baker Botts L.L.P.</orgname>
<address>
<country>unknown</country>
</address>
</addressbook>
</agent>
</agents>
</parties>
<assignees>
<assignee>
<addressbook>
<orgname>Computer Associates Think, Inc.</orgname>
<role>02</role>
<address>
<city>Islandia</city>
<state>NY</state>
<country>US</country>
</address>
</addressbook>
</assignee>
</assignees>
<examiners>
<primary-examiner>
<last-name>Mariam</last-name>
<first-name>Daniel</first-name>
<department>2624</department>
</primary-examiner>
</examiners>
</us-bibliographic-data-grant>
<abstract id="abstract">
<p id="p-0001" num="0000">A method for feature selection based on hierarchical local-region analysis of feature characteristics in a data set of mixed data type is provided. A data space associated with a mixed-type data set is partitioned into a hierarchy of plural local regions. A relationship metric (for example, a similarity correlation metric) is used to evaluate for each local region a relationship measure between input features and a target. One or more relevant features is identified, by using the relationship measure for each local region.</p>
</abstract>
<drawings id="DRAWINGS">
<figure id="Fig-EMI-D00000" num="00000">
<img id="EMI-D00000" he="84.50mm" wi="109.14mm" file="US07298906-20071120-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00001" num="00001">
<img id="EMI-D00001" he="214.71mm" wi="157.31mm" file="US07298906-20071120-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00002" num="00002">
<img id="EMI-D00002" he="222.25mm" wi="150.88mm" file="US07298906-20071120-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00003" num="00003">
<img id="EMI-D00003" he="236.56mm" wi="156.13mm" file="US07298906-20071120-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
</drawings>
<description id="description">
<?RELAPP description="Other Patent Relations" end="lead"?>
<heading id="h-0001" level="1">CROSS-REFERENCE TO RELATED APPLICATIONS</heading>
<p id="p-0002" num="0001">This application is a continuation-in-part of U.S. application Ser. No. 10/615,885, filed Jul. 8, 2003 and entitled “HIERARCHICAL DETERMINATION OF FEATURE RELEVANCY”.</p>
<?RELAPP description="Other Patent Relations" end="tail"?>
<?BRFSUM description="Brief Summary" end="lead"?>
<heading id="h-0002" level="1">TECHNICAL FIELD</heading>
<p id="p-0003" num="0002">This application relates to pattern recognition and data mining. In particular, the application relates to determining feature relevancy in multivariate data analysis of a mixed-type data set.</p>
<heading id="h-0003" level="1">DESCRIPTION OF RELATED ART</heading>
<p id="p-0004" num="0003">Feature selection is of theoretical interest and practical importance in the practice of pattern recognition and data mining. Objects or data entities may be described in terms of many features. However, some features may be redundant or irrelevant for specific tasks, serving primarily as a source of confusion. In addition, for any one specific task, different subsets of features might be relevant in different regions of input data space. Accordingly feature selection is a matter of considerable interest and importance in multivariate data analysis.</p>
<p id="p-0005" num="0004">For example, in modeling a specific behavior of a given system, it is desirable to include only the parameters that contribute to that specific system behavior and not the ones that contribute to other behaviors of the system but are not much relevant to that specific behavior.</p>
<p id="p-0006" num="0005">Most of feature selection algorithms are either filter methods or wrapper methods. Filter methods use the feature selection as a preprocessing step and filter out irrelevant features before learning a model. Thus, the filter methods are independent of the learning algorithm. Wrapper methods embed the learning algorithm within the feature selection process and use the learning algorithm to evaluate the performance of the feature selection. Each approach has both advantages and disadvantages. A filter approach is usually more efficient and can be combined with any learning algorithm, while a wrapper approach can usually provide a better performance for the learning algorithm.</p>
<p id="p-0007" num="0006">In any event, conventional feature selection algorithms are typically adapted for processing of numerical features, but are not proficient for handling a data set of mixed data types. Improved methodologies for determining feature relevancy for mixed data types are needed.</p>
<heading id="h-0004" level="1">SUMMARY</heading>
<p id="p-0008" num="0007">Methods for feature selection based on hierarchical local-region analysis of feature characteristics in a data set of mixed data type are described herein. A method for feature selection based on hierarchical local-region analysis of feature characteristics in a data set of mixed data type, according to one exemplary embodiment, comprises partitioning a data space associated with a mixed-type data set into a hierarchy of pluralities of local regions, using a relationship metric to evaluate for each local region a relationship measure between input features and a target, and identifying one or more relevant features, by using the relationship measure for each local region. The relationship metric preferably is or includes a similarity correlation metric (discussed below).</p>
<p id="p-0009" num="0008">A similarity correlation metric can be used in methods for analyzing feature characteristics in a data set of mixed data type. According to an exemplary embodiment, a method for analyzing feature characteristics in a data set of mixed data type comprises mapping a data space associated with a mixed-type data set to a distance space, and determining a relationship measure between each input feature and the target by using a distance-based relationship measure. When a similarity correlation metric is used, the relationship measure is determined by using distance correlation.</p>
<p id="p-0010" num="0009">The similarity correlation metric is provided to handle features of different data types, and facilitates analysis of feature relevancy by computing the correlations between input features and a target.</p>
<?BRFSUM description="Brief Summary" end="tail"?>
<?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?>
<description-of-drawings>
<heading id="h-0005" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading>
<p id="p-0011" num="0010">The features of the present application can be more readily understood from the following detailed description with reference to the accompanying drawings wherein:</p>
<p id="p-0012" num="0011"><figref idref="DRAWINGS">FIG. 1A</figref> shows a flow chart of a method for feature selection based on hierarchical local-region analysis of feature characteristics in a data set of mixed data type, according to an exemplary embodiment of the present application;</p>
<p id="p-0013" num="0012"><figref idref="DRAWINGS">FIG. 1B</figref> shows a flow chart of a method for analyzing feature characteristics in a data set of mixed data type, according to an exemplary embodiment of the present application;</p>
<p id="p-0014" num="0013"><figref idref="DRAWINGS">FIG. 2</figref> shows a flow chart of another exemplary embodiment of a method for feature selection based on hierarchical local-region analysis of feature characteristics in a data set of mixed data type;</p>
<p id="p-0015" num="0014"><figref idref="DRAWINGS">FIG. 3</figref> shows a three-dimensional plot of an extended parity-2 problem;</p>
<p id="p-0016" num="0015"><figref idref="DRAWINGS">FIG. 4</figref> shows a table of feature relevancy values for an extended parity-2 problem, obtained by applying an exemplary embodiment of this application; and</p>
<p id="p-0017" num="0016"><figref idref="DRAWINGS">FIG. 5</figref> shows a table of feature relevancy values for the Monk's problem, obtained by applying an exemplary embodiment of this application.</p>
</description-of-drawings>
<?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?>
<?DETDESC description="Detailed Description" end="lead"?>
<heading id="h-0006" level="1">DETAILED DESCRIPTION</heading>
<p id="p-0018" num="0017">This application provides tools for determining feature relevancy of mixed data types in multivariate data analysis. The tools of this disclosure may be a computer program stored on a computer readable medium and/or transmitted via a computer network or other transmission medium.</p>
<p id="p-0019" num="0018">A similarity correlation metric is discussed exemplarily below, as a new relevancy evaluation metric for an enhancement of the Hierarchical Determination of Feature Relevancy (HDFR) methodology which is discussed in U.S. application Ser. No. 10/615,885, filed Jul. 8, 2003 and entitled “HIERARCHICAL DETERMINATION OF FEATURE RELEVANCY”, the entire contents of which are incorporated herein by reference. The enhanced HDFR methodology can be used to process features with mixed data types.</p>
<p id="p-0020" num="0019">The Hierarchical Determination of Feature Relevancy (HDFR) methodology can be employed for feature selection based on hierarchical local-region analysis of feature characteristics. Hierarchical k-means clustering can be applied to partition the entire data space into local regions, which may or may not be overlapping, and then for each local region evaluates each feature individually based on its correlation with the target. In each local region there might be a particular, corresponding subset of features that is relevant according to the target for the local region. The subsets do not need to be the same for different regions of input data space. In other words, a feature or subset of features might not show strong relevancy to a particular task over the entire range of data but might show strong relevancy over different delineated regions. Such a feature is still considered relevant and can be identified for use in the appropriate regions.</p>
<p id="p-0021" num="0020">A typical implementation of the HDFR methodology utilizes a linear correlation metric to evaluate the relevancy of a feature. The linear correlation metric computes the R-squared correlation between the feature and the target. However, the R-squared correlation computation is applied to numerical values. In order to use the linear correlation metric, non-numerical features (such as symbolic ones) are converted to numerical form by using conversion techniques such as, for example, cross tabulation. Cross tabulation converts a symbolic feature into many numerical features corresponding to the distinctive symbolic values. Such conversion often results in very large number of cross tabulated features with mostly 0 entries, which increases not only the computational cost but also the risks of discovering accidental relationships.</p>
<p id="p-0022" num="0021">On the other hand, some advances have been made in handling non-numerical features in machine learning techniques. For example, as disclosed in U.S. application Ser. No. 10/418,659, filed Apr. 18, 2003 and entitled “PROCESSING MIXED NUMERIC AND NON-NUMERIC DATA”, the entire contents of which are incorporated herein by reference, some similarity metrics such as set distance metric are designed to compute the distance of non-numerical values. Such similarity metrics can be used in learning methodologies such as hierarchical k-means clustering which can be utilized in the HDFR methodology. Therefore, the HDFR methodology can be adapted to have the capability of partitioning data space with non-numerical features. However, in order to evaluate relevancy of non-numerical features, a new relevancy evaluation metric other than the typical linear metric is desirable. A similarity correlation metric is discussed below as a new relevancy evaluation metric which adapts existing similarity metrics to compute the similarities of features and a target, and then computes the correlations between the similarities of input features and the similarities of the target.</p>
<p id="p-0023" num="0022">A method for feature selection based on hierarchical local-region analysis of feature characteristics in a data set of mixed data type is described below with reference to <figref idref="DRAWINGS">FIG. 1A</figref>. A data space associated with a mixed-type data set is partitioned into a hierarchy of plural local regions (step S<b>11</b>). A relationship metric (for example, a similarity correlation metric) is used to evaluate for each local region a relationship measure between input features and a target (step S<b>13</b>). One or more relevant features is identified, by using the relationship measure for each local region (step S<b>15</b>). The relationship metric preferably includes a similarity correlation metric.</p>
<p id="p-0024" num="0023">According to another embodiment, a method for analyzing feature characteristics in a data set of mixed data type (<figref idref="DRAWINGS">FIG. 1B</figref>) can be adapted to use the similarity correlation metric. A data space associated with a mixed-type data set is mapped to a distance space (step S<b>17</b>). A relationship measure between each input feature and the target is determined by using a distance-based relationship measure (step S<b>19</b>). When a similarity correlation metric is used, the relationship measure is determined by using distance correlation.</p>
<p id="p-0025" num="0024">In typical implementations of HDFR (heretofore), the linear correlation metric was used to evaluate the feature relevancy. The linear correlation metric computes the R-squared correlation between a feature and the target. Assume a numerical input feature is x and the numerical target is y. The R-squared value between x and y represents the proportion of the variance in y attributable to the variance in x. Given observed pairs, (x<sub>i</sub>, y<sub>i</sub>), i=1˜n, the mathematical formulation of R-square value is:</p>
<p id="p-0026" num="0025">
<maths id="MATH-US-00001" num="00001">
<math overflow="scroll">
<mtable>
  <mtr>
    <mtd>
      <mrow>
        <mi>RSQ</mi>
        <mo>=</mo>
        <mfrac>
          <msup>
            <mrow>
              <mo>(</mo>
              <mrow>
                <munder>
                  <mo>∑</mo>
                  <mi>i</mi>
                </munder>
                <mo>⁢</mo>
                <mrow>
                  <mrow>
                    <mo>(</mo>
                    <mrow>
                      <msub>
                        <mi>x</mi>
                        <mi>i</mi>
                      </msub>
                      <mo>-</mo>
                      <mover>
                        <mi>x</mi>
                        <mi>_</mi>
                      </mover>
                    </mrow>
                    <mo>)</mo>
                  </mrow>
                  <mo>⁢</mo>
                  <mrow>
                    <mo>(</mo>
                    <mrow>
                      <msub>
                        <mi>y</mi>
                        <mi>i</mi>
                      </msub>
                      <mo>-</mo>
                      <mover>
                        <mi>y</mi>
                        <mi>_</mi>
                      </mover>
                    </mrow>
                    <mo>)</mo>
                  </mrow>
                </mrow>
              </mrow>
              <mo>)</mo>
            </mrow>
            <mn>2</mn>
          </msup>
          <mrow>
            <munder>
              <mo>∑</mo>
              <mi>i</mi>
            </munder>
            <mo>⁢</mo>
            <mrow>
              <msup>
                <mrow>
                  <mo>(</mo>
                  <mrow>
                    <msub>
                      <mi>x</mi>
                      <mi>i</mi>
                    </msub>
                    <mo>-</mo>
                    <mover>
                      <mi>x</mi>
                      <mi>_</mi>
                    </mover>
                  </mrow>
                  <mo>)</mo>
                </mrow>
                <mn>2</mn>
              </msup>
              <mo>⁢</mo>
              <msup>
                <mrow>
                  <mo>(</mo>
                  <mrow>
                    <msub>
                      <mi>y</mi>
                      <mi>i</mi>
                    </msub>
                    <mo>-</mo>
                    <mover>
                      <mi>y</mi>
                      <mi>_</mi>
                    </mover>
                  </mrow>
                  <mo>)</mo>
                </mrow>
                <mn>2</mn>
              </msup>
            </mrow>
          </mrow>
        </mfrac>
      </mrow>
    </mtd>
    <mtd>
      <mrow>
        <mo>(</mo>
        <mn>1</mn>
        <mo>)</mo>
      </mrow>
    </mtd>
  </mtr>
</mtable>
</math>
</maths>
<br/>
where <o ostyle="single">x</o> and <o ostyle="single">y</o> are the mean of x<sub>i</sub>s and y<sub>i</sub>s
</p>
<p id="p-0027" num="0026">There is a limitation on the R-squared correlation computation that it can only apply to numerical values. This disclosure describes a new relevancy evaluation metric, the similarity correlation metric, to be used to evaluate relevancies of non-numerical features.</p>
<p id="p-0028" num="0027">In Ser. No. 10/418,659, some similarity metrics are discussed to handle non-numerical data types. One example is the set distance metric. The set distance metric can compute the distance between two non-numerical values. It also proposed some methods to compute the center of a set of non-numerical values.</p>
<p id="p-0029" num="0028">For a dimension covering symbolic data, the most likely distance metric is probably based on matching symbols. If a field of data points which corresponds to this dimension may be considered as a set, the following may be used as a distance metric between symbol sets A and B from two respective data points:</p>
<p id="p-0030" num="0029">
<maths id="MATH-US-00002" num="00002">
<math overflow="scroll">
<mtable>
  <mtr>
    <mtd>
      <mrow>
        <mi>d</mi>
        <mo>=</mo>
        <mfrac>
          <mrow>
            <mrow>
              <mo></mo>
              <mrow>
                <mi>A</mi>
                <mo>⋃</mo>
                <mi>B</mi>
              </mrow>
              <mo></mo>
            </mrow>
            <mo>-</mo>
            <mrow>
              <mo></mo>
              <mrow>
                <mi>A</mi>
                <mo>⋂</mo>
                <mi>B</mi>
              </mrow>
              <mo></mo>
            </mrow>
          </mrow>
          <mrow>
            <mo></mo>
            <mrow>
              <mi>A</mi>
              <mo>⋃</mo>
              <mi>B</mi>
            </mrow>
            <mo></mo>
          </mrow>
        </mfrac>
      </mrow>
    </mtd>
    <mtd>
      <mrow>
        <mo>(</mo>
        <mn>1</mn>
        <mo>)</mo>
      </mrow>
    </mtd>
  </mtr>
</mtable>
</math>
</maths>
</p>
<p id="p-0031" num="0030">Equation (1) represents a simple symbol match scaled to satisfy the mathematical requirements of a distance metric. It works well when the dimension is composed of fields that have simple nominal values (for example, a dimension “car color” formed by the interior and exterior colors of a car in which only a limited number of colors are available from the manufacturer).</p>
<p id="p-0032" num="0031">The above metric (Equation 1) may be generalized, if the value of a field cannot be considered as a simple set.</p>
<p id="p-0033" num="0032">One example is a free text field in the problem of information classification. Since there are repeated words and some words may carry a larger weight for classification purposes, weights for each unique symbol may be introduced.</p>
<p id="p-0034" num="0033">One method, which is compatible with Equation (1), using weights is proposed in Equation (2) as follows:</p>
<p id="p-0035" num="0034">
<maths id="MATH-US-00003" num="00003">
<math overflow="scroll">
<mtable>
  <mtr>
    <mtd>
      <mrow>
        <mi>d</mi>
        <mo>=</mo>
        <mfrac>
          <mrow>
            <mrow>
              <munderover>
                <mo>∑</mo>
                <mi>i</mi>
                <mi>A</mi>
              </munderover>
              <mo>⁢</mo>
              <mstyle>
                <mspace width="0.3em" height="0.3ex"/>
              </mstyle>
              <mo>⁢</mo>
              <msub>
                <mi>w</mi>
                <mi>Ai</mi>
              </msub>
            </mrow>
            <mo>+</mo>
            <mrow>
              <munderover>
                <mo>∑</mo>
                <mi>j</mi>
                <mi>B</mi>
              </munderover>
              <mo>⁢</mo>
              <mstyle>
                <mspace width="0.3em" height="0.3ex"/>
              </mstyle>
              <mo>⁢</mo>
              <msub>
                <mi>w</mi>
                <mi>Bj</mi>
              </msub>
            </mrow>
            <mo>-</mo>
            <mrow>
              <munderover>
                <mo>∑</mo>
                <mi>k</mi>
                <mrow>
                  <mi>A</mi>
                  <mo>⋂</mo>
                  <mi>B</mi>
                </mrow>
              </munderover>
              <mo>⁢</mo>
              <mstyle>
                <mspace width="0.3em" height="0.3ex"/>
              </mstyle>
              <mo>⁢</mo>
              <mrow>
                <mo>(</mo>
                <mrow>
                  <msub>
                    <mi>w</mi>
                    <mi>Ak</mi>
                  </msub>
                  <mo>+</mo>
                  <msub>
                    <mi>w</mi>
                    <mi>Bk</mi>
                  </msub>
                </mrow>
                <mo>)</mo>
              </mrow>
            </mrow>
          </mrow>
          <mrow>
            <mrow>
              <munderover>
                <mo>∑</mo>
                <mi>i</mi>
                <mi>A</mi>
              </munderover>
              <mo>⁢</mo>
              <mstyle>
                <mspace width="0.3em" height="0.3ex"/>
              </mstyle>
              <mo>⁢</mo>
              <msub>
                <mi>w</mi>
                <mi>Ai</mi>
              </msub>
            </mrow>
            <mo>+</mo>
            <mrow>
              <munderover>
                <mo>∑</mo>
                <mi>j</mi>
                <mi>B</mi>
              </munderover>
              <mo>⁢</mo>
              <mstyle>
                <mspace width="0.3em" height="0.3ex"/>
              </mstyle>
              <mo>⁢</mo>
              <msub>
                <mi>w</mi>
                <mi>Bj</mi>
              </msub>
            </mrow>
            <mo>-</mo>
            <mrow>
              <mfrac>
                <mn>1</mn>
                <mn>2</mn>
              </mfrac>
              <mo>⁢</mo>
              <mrow>
                <munderover>
                  <mo>∑</mo>
                  <mi>k</mi>
                  <mrow>
                    <mi>A</mi>
                    <mo>⋂</mo>
                    <mi>B</mi>
                  </mrow>
                </munderover>
                <mo>⁢</mo>
                <mstyle>
                  <mspace width="0.3em" height="0.3ex"/>
                </mstyle>
                <mo>⁢</mo>
                <mrow>
                  <mo>(</mo>
                  <mrow>
                    <msub>
                      <mi>w</mi>
                      <mi>Ak</mi>
                    </msub>
                    <mo>+</mo>
                    <msub>
                      <mi>w</mi>
                      <mi>Bk</mi>
                    </msub>
                  </mrow>
                  <mo>)</mo>
                </mrow>
              </mrow>
            </mrow>
          </mrow>
        </mfrac>
      </mrow>
    </mtd>
    <mtd>
      <mrow>
        <mo>(</mo>
        <mn>2</mn>
        <mo>)</mo>
      </mrow>
    </mtd>
  </mtr>
</mtable>
</math>
</maths>
<br/>
where w<sub>Ai </sub>(and w<sub>Ak</sub>) represents the weights associated with symbols Ai (and Ak) in symbol set A, w<sub>Bj </sub>(and w<sub>Bk</sub>) represents the weights associated with symbols Bj and Bk in symbol set B. When each of the weights is eciual to one, Equation (2) is reduced to the following:
</p>
<p id="p-0036" num="0035">
<maths id="MATH-US-00004" num="00004">
<math overflow="scroll">
<mtable>
  <mtr>
    <mtd>
      <mrow>
        <mi>d</mi>
        <mo>=</mo>
        <mfrac>
          <mrow>
            <mrow>
              <mo></mo>
              <mi>A</mi>
              <mo></mo>
            </mrow>
            <mo>+</mo>
            <mrow>
              <mo></mo>
              <mi>B</mi>
              <mo></mo>
            </mrow>
            <mo>-</mo>
            <mrow>
              <mn>2</mn>
              <mo>⁢</mo>
              <mrow>
                <mo></mo>
                <mrow>
                  <mi>A</mi>
                  <mo>⋂</mo>
                  <mi>B</mi>
                </mrow>
                <mo></mo>
              </mrow>
            </mrow>
          </mrow>
          <mrow>
            <mrow>
              <mo></mo>
              <mi>A</mi>
              <mo></mo>
            </mrow>
            <mo>+</mo>
            <mrow>
              <mo></mo>
              <mi>B</mi>
              <mo></mo>
            </mrow>
            <mo>-</mo>
            <mrow>
              <mo></mo>
              <mrow>
                <mi>A</mi>
                <mo>⋂</mo>
                <mi>B</mi>
              </mrow>
              <mo></mo>
            </mrow>
          </mrow>
        </mfrac>
      </mrow>
    </mtd>
    <mtd>
      <mrow>
        <mo>(</mo>
        <mn>3</mn>
        <mo>)</mo>
      </mrow>
    </mtd>
  </mtr>
</mtable>
</math>
</maths>
</p>
<p id="p-0037" num="0036">Equation (3) is equivalent to Equation (1) since the following is true:
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>|A∪B|=|A|+|B|−|A∩B|</i><?in-line-formulae description="In-line Formulae" end="tail"?>
</p>
<p id="p-0038" num="0037">More elaborate distance metric may also be used for text processing. For example, when searching a database of textual information, it may be desired to keep a sequence of cue words. In this case, a penalty may be introduced if the sequence is broken, even if all cue words are present. This may drastically reduce the number of hits which are less interesting or not relevant at all.</p>
<p id="p-0039" num="0038">The following steps may be easily expanded to work on other types of non-numerical data, if a reasonable distance metric can be defined.</p>
<p id="p-0040" num="0039">The similarity correlation metric takes advantages of these mixed data handling techniques. Assume both the input feature x and the target y have non-numerical values. The similarity correlation metric first uses the similarity metric to compute the center of the observed values. Next, the distances of individual observations to the center are computed by the similarity metric, to obtain distance pairs, (dist.x<sub>i</sub>, dist.y<sub>i</sub>), i=1˜n, from the raw data pairs (x<sub>i</sub>, y<sub>i</sub>). The similarity correlation metric measures the correlation between x and y using the distance correlation as in equation 2:</p>
<p id="p-0041" num="0040">
<maths id="MATH-US-00005" num="00005">
<math overflow="scroll">
<mtable>
  <mtr>
    <mtd>
      <mrow>
        <mi>SCR</mi>
        <mo>=</mo>
        <mfrac>
          <msup>
            <mrow>
              <mo>(</mo>
              <mrow>
                <munder>
                  <mo>∑</mo>
                  <mi>i</mi>
                </munder>
                <mo>⁢</mo>
                <mrow>
                  <mrow>
                    <mo>(</mo>
                    <mrow>
                      <mrow>
                        <mi>dist</mi>
                        <mo>.</mo>
                        <msub>
                          <mi>x</mi>
                          <mi>i</mi>
                        </msub>
                      </mrow>
                      <mo>-</mo>
                      <mover>
                        <mrow>
                          <mi>dist</mi>
                          <mo>.</mo>
                          <mi>x</mi>
                        </mrow>
                        <mi>_</mi>
                      </mover>
                    </mrow>
                    <mo>)</mo>
                  </mrow>
                  <mo>⁢</mo>
                  <mrow>
                    <mo>(</mo>
                    <mrow>
                      <mrow>
                        <mi>dist</mi>
                        <mo>.</mo>
                        <msub>
                          <mi>y</mi>
                          <mi>i</mi>
                        </msub>
                      </mrow>
                      <mo>-</mo>
                      <mover>
                        <mrow>
                          <mi>dist</mi>
                          <mo>.</mo>
                          <mi>y</mi>
                        </mrow>
                        <mi>_</mi>
                      </mover>
                    </mrow>
                    <mo>)</mo>
                  </mrow>
                </mrow>
              </mrow>
              <mo>)</mo>
            </mrow>
            <mn>2</mn>
          </msup>
          <mrow>
            <munder>
              <mo>∑</mo>
              <mi>i</mi>
            </munder>
            <mo>⁢</mo>
            <mrow>
              <msup>
                <mrow>
                  <mo>(</mo>
                  <mrow>
                    <mrow>
                      <mi>dist</mi>
                      <mo>.</mo>
                      <msub>
                        <mi>x</mi>
                        <mi>i</mi>
                      </msub>
                    </mrow>
                    <mo>-</mo>
                    <mover>
                      <mrow>
                        <mi>dist</mi>
                        <mo>.</mo>
                        <mi>x</mi>
                      </mrow>
                      <mi>_</mi>
                    </mover>
                  </mrow>
                  <mo>)</mo>
                </mrow>
                <mn>2</mn>
              </msup>
              <mo>⁢</mo>
              <msup>
                <mrow>
                  <mo>(</mo>
                  <mrow>
                    <mrow>
                      <mi>dist</mi>
                      <mo>.</mo>
                      <msub>
                        <mi>y</mi>
                        <mi>i</mi>
                      </msub>
                    </mrow>
                    <mo>-</mo>
                    <mover>
                      <mrow>
                        <mi>dist</mi>
                        <mo>.</mo>
                        <mi>y</mi>
                      </mrow>
                      <mi>_</mi>
                    </mover>
                  </mrow>
                  <mo>)</mo>
                </mrow>
                <mn>2</mn>
              </msup>
            </mrow>
          </mrow>
        </mfrac>
      </mrow>
    </mtd>
    <mtd>
      <mrow>
        <mo>(</mo>
        <mn>2</mn>
        <mo>)</mo>
      </mrow>
    </mtd>
  </mtr>
</mtable>
</math>
</maths>
</p>
<p id="p-0042" num="0041">where <o ostyle="single">dist.x</o> and <o ostyle="single">dist.y</o> are the mean of dist.x<sub>i</sub>s and dist.y<sub>i</sub>s</p>
<p id="p-0043" num="0042">Comparing equation 1 and 2, one can see that they are very similar. The similarity correlation metric is different from the linear correlation metric in that it actually computes the linear correlation in a mapped space, the distance space, rather than in the original raw data space.</p>
<p id="p-0044" num="0043">Mapping from the raw data space to the distance space allows the methodology to evaluate numerically the relevancies of non-numerical features. This is an advantage of the similarity correlation metric over the linear correlation metric. The similarity correlation metric also has an advantage over the linear correlation metric in discovering a nonlinear relationship in the numerical data space, which is demonstrated by the following simple example.</p>
<p id="p-0045" num="0044">Consider the simple nonlinear case y=x<sup>2</sup>, where x is from −∞ to +∞. A R-squared correlation score of 0 is obtained by using the linear correlation metric which results from the cancellation of negative correlation of x and y when x is negative, and the positive correlation of x and y when x is positive. The similarity correlation metric uses two steps to evaluate correlation between x and y. First it maps the original space to the distance space. Assume the mean of x is 0. The mapping results in function dist.y=(dist.x)<sup>2</sup>. The similarity correlation metric then computes the linear correlation between dist.x and dist.y. But due to the space mapping there is no more negative correlation between dist.x and dist.y. Therefore, a high similarity correlation score of 0.969 is obtained.</p>
<p id="p-0046" num="0045">The effect of space mapping is similar to space partitioning under the HDFR methodology. The HDFR methodology can partition (x, y) space along axis x=0 and then compute the correlation between x and y in two partitioned spaces. The space mapping is to fold the original space along axis x=0 and y=0 and compute the correlation in the folded space. Both techniques can be targeted to identify the nonlinear relations.</p>
<p id="p-0047" num="0046">This disclosure describes an enhancement to the HDFR methodology, utilizing the similarity correlation metric, to handle features with mixed data types. The enhanced HDFR methodology is designed to discover the feature relevancies of mixed data types. According to an exemplary embodiment of the enhanced HDFR methodology, a hierarchical k-means clustering is used to partition and transform data into groups of points in hyper-spherical local regions. The similarity correlation metric is used to evaluate the relevancies of input features of mixed data types. Relevancies of features over the local regions are summarized as the relevancies of input features.</p>
<p id="p-0048" num="0047">A flow chart of the enhanced HDFR methodology using a similarity correlation metric, according to an exemplary embodiment, is shown in <figref idref="DRAWINGS">FIG. 2</figref>. The mixed-type data space is partitioned initially into two regions (step S<b>21</b>). For each of the regions in the present level of the hierarchy, a similarity correlation metric is utilized to evaluate feature relevancies based on samples in the region (step S<b>22</b>). The feature relevancies in the level are used to identify relevant features which have significantly larger relevancies than the others (step S<b>23</b>). If no new relevant features can be identified for a certain number of levels (step S<b>24</b>, “NO”) or a specified maximum number of levels is reached (step S<b>25</b>, “YES”), the feature relevancies can be summarized at all of the levels and a list of relevant features and their relevancies provided (step S<b>27</b>). The local regions in the current level are split further for the next level (step S<b>26</b>), until no new relevant features can be identified for a specified or predetermined number of iterations or a specified maximum number of levels is reached.</p>
<p id="p-0049" num="0048">The performance of the enhanced HDFR methodology is discussed exemplarily through two examples below. One example is the extended parity-2 problem and the other is the three Monk's problems. The extended parity-2 problem is from the well-known parity-2 problem, but extended to use inputs and output of continuous values. Some random noise inputs are also added to see if the enhanced HDFR methodology can still work on numerical features. The three Monk's problems have been used as a standard for comparing many different learning methodologies. The problems have discrete input features and the first and the third problems can be used as feature selection tasks.</p>
<p id="p-0050" num="0049">The parity-2 problem is a well-known problem. In this problem, the output is the mod-2 sum of two binary input features. It is extended by using continuous inputs and output. A nonlinear equation is used to simulate the problem:
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>y=x</i><sub>1</sub><i>+x</i><sub>2</sub>−2*<i>x</i><sub>1</sub><i>*x</i><sub>2 </sub><?in-line-formulae description="In-line Formulae" end="tail"?>
<br/>
where x<sub>1</sub>, x<sub>2 </sub>and y ε [0, 1]
</p>
<p id="p-0051" num="0050">A 3-D plot of the above equation is shown in <figref idref="DRAWINGS">FIG. 3</figref>.</p>
<p id="p-0052" num="0051">For testing purposes, 8 random input features, x<sub>3 </sub>to x<sub>10</sub>, are added as noise and 500 samples are randomly generated. The task is to identify the relevant features, x<sub>1 </sub>and x<sub>2</sub>, from the noise features, x<sub>3 </sub>to x<sub>10. </sub></p>
<p id="p-0053" num="0052">Estimated feature relevancies obtained by applying the enhanced HDFR methodology to the data of the extended parity-2 problem are shown tabularly in <figref idref="DRAWINGS">FIG. 4</figref>. The results show that the relevant features, x<sub>1 </sub>and x<sub>2</sub>, have significantly larger relevancy values than other noise features. This demonstrates that the enhanced HDFR methodology using a similarity correlation metric can still identify the relevant numerical features.</p>
<p id="p-0054" num="0053">The Monk's problems have been used as a standard for comparing many different learning methodologies. There are six input features, x<sub>1</sub>, x<sub>2</sub>, . . . , x<sub>6</sub>, and one binary target y. Among the six input features, both feature x<sub>3 </sub>and x<sub>6 </sub>have only two discrete values and they are treated as binary numerical features. All of the other features have multiple discrete values and are treated as non-numerical features. The target y is binary and treated as numerical. The following is the definition of Monk's problems:
<ul id="ul0001" list-style="none">
    <li id="ul0001-0001" num="0000">
    <ul id="ul0002" list-style="none">
        <li id="ul0002-0001" num="0054">Monk-1: y=1 if (x<sub>1</sub>=x<sub>2</sub>) or (x<sub>5</sub>=1), otherwise y=0.</li>
    </ul>
    </li>
</ul>
</p>
<p id="p-0055" num="0055">Monk-2: y=1 if exactly two of {x<sub>1</sub>=1, x<sub>2</sub>=1, x<sub>3</sub>=1, x<sub>4</sub>=1, x<sub>5</sub>=1, x<sub>6</sub>=1}, otherwise y=0.</p>
<p id="p-0056" num="0056">Monk-3: y=1 if (x<sub>5</sub>=3 and x<sub>4</sub>=1) or (x<sub>5</sub>≠4 and x<sub>2</sub>≠3), otherwise y=0 (5% class noise added to the training set).</p>
<p id="p-0057" num="0057">From the definition one can check what features are relevant to the problems. For the first problem, one can see that only three features, x<sub>1</sub>, x<sub>2 </sub>and x<sub>5</sub>, are relevant to the problem. For the second problem, all six features are relevant to the problem. For the third problem, three features, x<sub>2</sub>, x<sub>4 </sub>and x<sub>5</sub>, are relevant to the problem. Therefore, the first and the third problem can be used as feature selection tasks. Exemplary feature relevancy values obtained by applying the enhanced HDFR methodology to the three Monk's problems is set forth in <figref idref="DRAWINGS">FIG. 5</figref>.</p>
<p id="p-0058" num="0058">As can be seen from the estimated feature relevancies shown tabularly in <figref idref="DRAWINGS">FIG. 5</figref>, the ranks of feature x<sub>1</sub>, x<sub>2 </sub>and x<sub>5 </sub>in the first problem are 2, 4 and 1 respectively. The ranks of feature x<sub>2</sub>, x<sub>4 </sub>and x<sub>5 </sub>in the third problem are 1, 5 and 2 respectively. Feature x<sub>4 </sub>in the third problem does not rank higher as might expect it to be. However, this is not surprising because the relevancy of feature x<sub>4 </sub>is actually very small from the problem definition. Without feature x<sub>4</sub>, only 12 out of 228 positive examples cannot be defined. Furthermore, the 5% class noise added to the dataset also diminishes the chance of methodologies to identify feature x<sub>4 </sub>as a relevant feature.</p>
<p id="p-0059" num="0059">This disclosure describes a new feature relevancy evaluation metric, the similarity correlation metric, which enhances the HDFR methodology, for handling features with mixed data types. Experiment results show that the enhanced HDFR methodology works well for features of both numerical and non-numerical data types.</p>
<p id="p-0060" num="0060">The enhanced HDFR methodology can be adapted for any of various applications. For example, it can be integrated in a context-sensitive search engine. As another example, it can be adapted to be applied in intelligent software for monitoring electronic communications (such as e-mails, instant messages, other Internet communications, etc.). In addition, the enhanced HDFR methodology can be integrated in a system for cataloging textual information (such as news stories, books, scientific papers, etc.). It can be used by on-line book vendors to automatically suggest alternative titles when a book sought by a customer is not available (or even if it is available). Numerous other applications of the enhanced HDFR methodology are possible.</p>
<p id="p-0061" num="0061">The above specific embodiments are illustrative, and many variations can be introduced on these embodiments without departing from the spirit of the disclosure or from the scope of the appended claims. For example, elements and/or features of different illustrative embodiments may be combined with each other and/or substituted for each other within the scope of this disclosure and appended claims.</p>
<p id="p-0062" num="0062">Additional variations may be apparent to one of ordinary skill in the art from reading the following applications, the entire contents of which are incorporated herein by reference.</p>
<p id="p-0063" num="0063">U.S. application Ser. No. 10/615,885, filed Jul. 8, 2003 and entitled “HIERARCHICAL DETERMINATION OF FEATURE RELEVANCY”; and</p>
<p id="p-0064" num="0064">U.S. application Ser. No. 10/418,659, filed Apr. 18, 2003 and entitled “PROCESSING MIXED NUMERIC AND NON-NUMERIC DATA”.</p>
<?DETDESC description="Detailed Description" end="tail"?>
</description>
<us-math idrefs="MATH-US-00001" nb-file="US07298906-20071120-M00001.NB">
<img id="EMI-M00001" he="12.70mm" wi="76.20mm" file="US07298906-20071120-M00001.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-math idrefs="MATH-US-00002" nb-file="US07298906-20071120-M00002.NB">
<img id="EMI-M00002" he="6.69mm" wi="76.20mm" file="US07298906-20071120-M00002.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-math idrefs="MATH-US-00003" nb-file="US07298906-20071120-M00003.NB">
<img id="EMI-M00003" he="16.93mm" wi="76.20mm" file="US07298906-20071120-M00003.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-math idrefs="MATH-US-00004" nb-file="US07298906-20071120-M00004.NB">
<img id="EMI-M00004" he="6.69mm" wi="76.20mm" file="US07298906-20071120-M00004.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-math idrefs="MATH-US-00005" nb-file="US07298906-20071120-M00005.NB">
<img id="EMI-M00005" he="14.14mm" wi="76.20mm" file="US07298906-20071120-M00005.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-claim-statement>What is claimed is:</us-claim-statement>
<claims id="claims">
<claim id="CLM-00001" num="00001">
<claim-text>1. A method for feature selection based on hierarchical local-region analysis of feature characteristics in a data set having at least one non-numeric feature, comprising:
<claim-text>partitioning a data space associated with the data set having at least one non-numeric feature into a hierarchy of pluralities of local regions;</claim-text>
<claim-text>evaluating a relationship measure for each local region using a metric based on relationship between input features and a target; and</claim-text>
<claim-text>identifying one or more relevant features by using the relationship measure for each local region.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00002" num="00002">
<claim-text>2. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the metric includes determining for each local region a center of data from said data set in the local region.</claim-text>
</claim>
<claim id="CLM-00003" num="00003">
<claim-text>3. The method of <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein the metric includes determining for each local region distances between respective data points in the local region and the center of the local region.</claim-text>
</claim>
<claim id="CLM-00004" num="00004">
<claim-text>4. The method of <claim-ref idref="CLM-00003">claim 3</claim-ref>, wherein the metric maps the data space associated with the data set to a distance space, and determines a correlation between each input feature and the target by using a distance correlation.</claim-text>
</claim>
<claim id="CLM-00005" num="00005">
<claim-text>5. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the metric includes a similarity correlation metric.</claim-text>
</claim>
<claim id="CLM-00006" num="00006">
<claim-text>6. A computer system, comprising:
<claim-text>a processor; and</claim-text>
<claim-text>a program storage device readable by the computer system, tangibly embodying a program of instructions executable by the processor to perform the method claimed in <claim-ref idref="CLM-00001">claim 1</claim-ref>.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00007" num="00007">
<claim-text>7. A program storage device readable by a machine, tangibly embodying a program of instructions executable by the machine to perform the method claimed in <claim-ref idref="CLM-00001">claim 1</claim-ref>.</claim-text>
</claim>
<claim id="CLM-00008" num="00008">
<claim-text>8. An apparatus for feature selection based on hierarchical local-region analysis of feature characteristics in a data set having at least one non-numeric feature, comprising:
<claim-text>partition means for participating a data space associated with the data set having at least one non-numeric feature into a hierarchy of pluralities of local regions;</claim-text>
<claim-text>evaluation means for evaluating a relationship measure for each local region using a metric based on relationship between input features and a target; and</claim-text>
<claim-text>relevant feature identification means for identifying one or more relevant features by using the relationship measure for each local region.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00009" num="00009">
<claim-text>9. The apparatus of <claim-ref idref="CLM-00008">claim 8</claim-ref>, wherein the metric includes a similarity correlation metric.</claim-text>
</claim>
<claim id="CLM-00010" num="00010">
<claim-text>10. A method for analyzing feature characteristics in a data set having at least one non-numeric feature, comprising:
<claim-text>partitioning a data space associated with the data set having at least one non-numeric feature into a hierarchy of pluralities of local regions;</claim-text>
<claim-text>mapping a data space associated with the data set having at least one non-numeric feature to a distance space; and</claim-text>
<claim-text>determining a relationship measure between each input feature and the target by using a distance-based relationship measure.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00011" num="00011">
<claim-text>11. The method of <claim-ref idref="CLM-00010">claim 10</claim-ref>, wherein the distance-based relationship measure is a distance correlation.</claim-text>
</claim>
<claim id="CLM-00012" num="00012">
<claim-text>12. A computer system, comprising:
<claim-text>a processor; and</claim-text>
<claim-text>a program storage device readable by the computer system, tangibly embodying a program of instructions executable by the processor to perform the method claimed in <claim-ref idref="CLM-00010">claim 10</claim-ref>.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00013" num="00013">
<claim-text>13. A program storage device readable by a machine, tangibly embodying a program of instructions executable by the machine to perform the method claimed in <claim-ref idref="CLM-00010">claim 10</claim-ref>.</claim-text>
</claim>
</claims>
</us-patent-grant>
