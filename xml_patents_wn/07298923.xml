<us-patent-grant lang="EN" dtd-version="v4.2 2006-08-23" file="US07298923-20071120.XML" status="PRODUCTION" id="us-patent-grant" country="US" date-produced="20071106" date-publ="20071120">
<us-bibliographic-data-grant>
<publication-reference>
<document-id>
<country>US</country>
<doc-number>07298923</doc-number>
<kind>B2</kind>
<date>20071120</date>
</document-id>
</publication-reference>
<application-reference appl-type="utility">
<document-id>
<country>US</country>
<doc-number>10974503</doc-number>
<date>20041027</date>
</document-id>
</application-reference>
<us-application-series-code>10</us-application-series-code>
<us-term-of-grant>
<us-term-extension>543</us-term-extension>
</us-term-of-grant>
<classifications-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>G</section>
<class>09</class>
<subclass>K</subclass>
<main-group>9</main-group>
<subgroup>32</subgroup>
<symbol-position>F</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20071120</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>G</section>
<class>09</class>
<subclass>G</subclass>
<main-group>5</main-group>
<subgroup>00</subgroup>
<symbol-position>L</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20071120</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>G</section>
<class>06</class>
<subclass>K</subclass>
<main-group>9</main-group>
<subgroup>36</subgroup>
<symbol-position>L</symbol-position>
<classification-value>N</classification-value>
<action-date><date>20071120</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
</classifications-ipcr>
<classification-national>
<country>US</country>
<main-classification>382294</main-classification>
<further-classification>345639</further-classification>
</classification-national>
<invention-title id="d0e53">Method and system for focus-adaptive reconstruction of spine images</invention-title>
<references-cited>
<citation>
<patcit num="00001">
<document-id>
<country>US</country>
<doc-number>5920657</doc-number>
<kind>A</kind>
<name>Bender et al.</name>
<date>19990700</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>382284</main-classification></classification-national>
</citation>
<citation>
<patcit num="00002">
<document-id>
<country>US</country>
<doc-number>6101238</doc-number>
<kind>A</kind>
<name>Murthy et al.</name>
<date>20000800</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<patcit num="00003">
<document-id>
<country>US</country>
<doc-number>6393163</doc-number>
<kind>B1</kind>
<name>Burt et al.</name>
<date>20020500</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>382294</main-classification></classification-national>
</citation>
<citation>
<patcit num="00004">
<document-id>
<country>US</country>
<doc-number>6757418</doc-number>
<kind>B2</kind>
<name>Wei et al.</name>
<date>20040600</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<patcit num="00005">
<document-id>
<country>US</country>
<doc-number>7088850</doc-number>
<kind>B2</kind>
<name>Wei et al.</name>
<date>20060800</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>382128</main-classification></classification-national>
</citation>
<citation>
<patcit num="00006">
<document-id>
<country>US</country>
<doc-number>7127090</doc-number>
<kind>B2</kind>
<name>Kreang-Arekul et al.</name>
<date>20061000</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>382128</main-classification></classification-national>
</citation>
</references-cited>
<number-of-claims>29</number-of-claims>
<us-exemplary-claim>1</us-exemplary-claim>
<us-field-of-classification-search>
<classification-national>
<country>US</country>
<main-classification>382294</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>382284</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>382286</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>382296</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>358540</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>358450</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>345629</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>345631</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>345634</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>345637</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>345639</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>345640</main-classification>
</classification-national>
</us-field-of-classification-search>
<figures>
<number-of-drawing-sheets>11</number-of-drawing-sheets>
<number-of-figures>11</number-of-figures>
</figures>
<us-related-documents>
<us-provisional-application>
<document-id>
<country>US</country>
<doc-number>60516793</doc-number>
<kind>00</kind>
<date>20031103</date>
</document-id>
</us-provisional-application>
<related-publication>
<document-id>
<country>US</country>
<doc-number>20050213850</doc-number>
<kind>A1</kind>
<date>20050929</date>
</document-id>
</related-publication>
</us-related-documents>
<parties>
<applicants>
<applicant sequence="001" app-type="applicant-inventor" designation="us-only">
<addressbook>
<last-name>Zhang</last-name>
<first-name>Bohong</first-name>
<address>
<city>Lexington</city>
<state>KY</state>
<country>US</country>
</address>
</addressbook>
<nationality>
<country>omitted</country>
</nationality>
<residence>
<country>US</country>
</residence>
</applicant>
<applicant sequence="002" app-type="applicant-inventor" designation="us-only">
<addressbook>
<last-name>Zhang</last-name>
<first-name>Li</first-name>
<address>
<city>Skillman</city>
<state>NJ</state>
<country>US</country>
</address>
</addressbook>
<nationality>
<country>omitted</country>
</nationality>
<residence>
<country>US</country>
</residence>
</applicant>
<applicant sequence="003" app-type="applicant-inventor" designation="us-only">
<addressbook>
<last-name>Fang</last-name>
<first-name>Ming</first-name>
<address>
<city>Princeton Jct.</city>
<state>NJ</state>
<country>US</country>
</address>
</addressbook>
<nationality>
<country>omitted</country>
</nationality>
<residence>
<country>US</country>
</residence>
</applicant>
</applicants>
<agents>
<agent sequence="01" rep-type="attorney">
<addressbook>
<last-name>Conover</last-name>
<first-name>Michele L.</first-name>
<address>
<country>unknown</country>
</address>
</addressbook>
</agent>
</agents>
</parties>
<assignees>
<assignee>
<addressbook>
<orgname>Siemens Medical Solutions USA, Inc.</orgname>
<role>02</role>
<address>
<city>Malvern</city>
<state>PA</state>
<country>US</country>
</address>
</addressbook>
</assignee>
</assignees>
<examiners>
<primary-examiner>
<last-name>Couso</last-name>
<first-name>Yon J.</first-name>
<department>2624</department>
</primary-examiner>
</examiners>
</us-bibliographic-data-grant>
<abstract id="abstract">
<p id="p-0001" num="0000">A method for aligning a pair of digital images includes providing a pair of digital images, wherein each said image comprises a plurality of intensities corresponding to a domain of points in a D-dimensional space, and the pair of images present adjacent views of a same object of interest. A weighting function is applied to each image of the pair of images, wherein the weighting function is centered on the object of interest, the weighting function has a maximum value on the object of interest, and the value of the weighting function decreases with increasing distance from the object of interest. The pair of images is aligned by correlated the weighted intensities on one image with those in the other image.</p>
</abstract>
<drawings id="DRAWINGS">
<figure id="Fig-EMI-D00000" num="00000">
<img id="EMI-D00000" he="148.76mm" wi="165.61mm" file="US07298923-20071120-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00001" num="00001">
<img id="EMI-D00001" he="71.97mm" wi="80.26mm" file="US07298923-20071120-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00002" num="00002">
<img id="EMI-D00002" he="194.65mm" wi="135.89mm" file="US07298923-20071120-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00003" num="00003">
<img id="EMI-D00003" he="194.06mm" wi="138.01mm" file="US07298923-20071120-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00004" num="00004">
<img id="EMI-D00004" he="147.91mm" wi="139.28mm" file="US07298923-20071120-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00005" num="00005">
<img id="EMI-D00005" he="152.65mm" wi="141.82mm" file="US07298923-20071120-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00006" num="00006">
<img id="EMI-D00006" he="188.30mm" wi="165.44mm" file="US07298923-20071120-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00007" num="00007">
<img id="EMI-D00007" he="183.98mm" wi="163.75mm" file="US07298923-20071120-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00008" num="00008">
<img id="EMI-D00008" he="229.02mm" wi="154.86mm" file="US07298923-20071120-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00009" num="00009">
<img id="EMI-D00009" he="230.63mm" wi="154.35mm" file="US07298923-20071120-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00010" num="00010">
<img id="EMI-D00010" he="169.33mm" wi="166.45mm" file="US07298923-20071120-D00010.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00011" num="00011">
<img id="EMI-D00011" he="190.08mm" wi="140.29mm" file="US07298923-20071120-D00011.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
</drawings>
<description id="description">
<?RELAPP description="Other Patent Relations" end="lead"?>
<heading id="h-0001" level="1">CROSS REFERENCE TO RELATED UNITED STATES APPLICATIONS</heading>
<p id="p-0002" num="0001">This application claims priority from “Focus-adaptive Method for MR Spine Composer”, U.S. Provisional Application No. 60/516,793 of Zhang, et al., filed Nov. 3, 2003, the contents of which are incorporated herein by reference.</p>
<?RELAPP description="Other Patent Relations" end="tail"?>
<?BRFSUM description="Brief Summary" end="lead"?>
<heading id="h-0002" level="1">TECHNICAL FIELD</heading>
<p id="p-0003" num="0002">This invention is directed to methods of constructing a composite image volume from several ordered constituent volumes of medical images.</p>
<heading id="h-0003" level="1">DISCUSSION OF THE RELATED ART</heading>
<p id="p-0004" num="0003">The diagnostically superior information available from data acquired from current imaging systems enables the detection of potential problems at earlier and more treatable stages. Given the vast quantity of detailed data acquirable from imaging systems, various algorithms must be developed to efficiently and accurately process image data. With the aid of computers, advances in image processing are generally performed on digital or digitized images.</p>
<p id="p-0005" num="0004">Digital images are created from an array of numerical values representing a property (such as a grey scale value or magnetic field strength) associable with an anatomical location points referenced by a particular array location. The set of anatomical location points comprises the domain of the image. In 2-D digital images, or slice sections, the discrete array locations are termed pixels. Three-dimensional digital images can be constructed from stacked slice sections through various construction techniques known in the art. The 3-D images are made up of discrete volume elements, also referred to as voxels, composed of pixels from the 2-D images. The pixel or voxel properties can be processed to ascertain various properties about the anatomy of a patient associated with such pixels or voxels.</p>
<p id="p-0006" num="0005">In many diagnostic imaging situations, the target object to be imaged is much larger than the field of view of the imaging device. Even when it is possible to increase the field of view to cover the whole target object, the resulting image has insufficient resolution or possible geometric distortions in the off-center areas. It may also not be possible to cover the entire target object in one image because of the geometry or topology of the target object. It is nevertheless useful to present the entire target object in a single image to the human for the purpose of diagnosis. Moreover, it is important to compose the image with certain precision requirement for quantitative measurement in many clinical applications. For example, many musculoskeletal disorders such as scoliosis require the examination of the spine as a whole so that its geometry can be seen or measured. Due to the size of spine, it is currently not possible to acquire a single MR image of the entire spine without lowering resolution, adding significant distortions, or deteriorating contrast. In present radiological practice, partial, overlapping constituent images are taken at several stations along the spine, starting from the back of the head down to the pelvis. The overlaps between the images can vary. Seam positions between individual volumes need to be accurate for the composite volume to look reasonable. This is very important in clinical diagnosis. A composite volume with poor quality could cause difficulties in physician's diagnostic decision-making process. The ability to view the entire object on a single image facilitates convenient and accurate diagnostic examination and measurement.</p>
<p id="p-0007" num="0006">The process of forming a compound image from overlapped, individual images is referred to herein as image composing. Prior systems for image composing include U.S. Pat. No. 6,101,238, titled “System for generating a compound x-ray image for diagnosis”, issued on Aug. 8, 2000, and U.S. Pat. No. 6,757,418, titled “Method and system for automatic computed radiography (CR) image composition by white band detection and consistency rechecking”, issued on Jun. 29, 2004. These patents disclose alignment of images using cross-correlation techniques and horizontal and vertical translations.</p>
<p id="p-0008" num="0007">Magnetic resonance composing refers to the automatic constructing of one composite 3D volume from several constituent ordered 3D volumes of MR images. When imaging the spine, the scans are performed so that there is an overlap region in each image. Thus, when considering a pair of images, one of an upper portion and one of a lower portion of a spine, the lower part of the upper region should overlap with the upper part of the lower region. The images are composed by aligning voxels of the upper overlap region with corresponding voxels of the lower overlap region. One seeks to determine a cut-line in the overlap region, where one image ends and the next image begins. The goodness of the alignment can be determined by correlating one overlap region with the other. By shifting the alignment parameters of each pairs of volumes in a 3-D search range (horizontal, vertical and depth) and computing the correlation of all the possible alignments within the search range for the overlap parts of the volumes, the highest correlation can be automatically found and the corresponding alignment parameters returned as the best match for the pair of volumes.</p>
<p id="p-0009" num="0008">In theory, this scheme should work well for ideally overlapped volumes. The best match always occurs exactly where the overlap parts of each pair of volumes completely match each other, which is corresponding to the highest correlation among all possible alignments. However in practice, when working with real data, this is not always true. If the original images are distorted in some way, it may not be possible to match all parts of the volumes simultaneously. In this case, the procedure can still return the highest correlation as ‘the best match’, which may not necessarily be the best match defined by a human.</p>
<heading id="h-0004" level="1">SUMMARY OF THE INVENTION</heading>
<p id="p-0010" num="0009">In one aspect of the invention, there is provided a method for aligning a pair of digital images. The method includes providing a pair of digital images, wherein each said image comprises a plurality of intensities corresponding to a domain of points in a D-dimensional space, and the pair of images present adjacent views of a same object of interest, applying a weighting function to each image of the pair of images, wherein the weighting function is centered on the object of interest, the weighting function has a maximum value on the object of interest, and the value of the weighting function decreases with increasing distance from the object of interest, and aligning the pair of images by correlated the weighted intensities on one image with those in the other image.</p>
<p id="p-0011" num="0010">In a further aspect of the invention, each image of the pair of images has an overlap region, and wherein the step of aligning is performed by correlating the weighted intensities in the overlap region of one image with the weighted intensities in the overlap region of the other image.</p>
<p id="p-0012" num="0011">In a further aspect of the invention, the two images are joined into a composed image at a cutline in the overlap regions of the two images.</p>
<p id="p-0013" num="0012">In a further aspect of the invention, the weighting function is applied to an image in the pair of images by multiplying the intensity of a point in the image by the value of the weighting function.</p>
<p id="p-0014" num="0013">In a further aspect of the invention, the object of interest in each image is the spine.</p>
<p id="p-0015" num="0014">In a further aspect of the invention, the object of interest in each image is an arm.</p>
<p id="p-0016" num="0015">In a further aspect of the invention, the object of interest in each image is a leg.</p>
<p id="p-0017" num="0016">In a further aspect of the invention, the images are magnetic resonance images.</p>
<p id="p-0018" num="0017">In a further aspect of the invention, the weighting function is a Gaussian of the form
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>P</i>(<i>x</i>)=exp(−<i>x</i><sup>2</sup>/2σ<sup>2</sup>)<?in-line-formulae description="In-line Formulae" end="tail"?>
<br/>
wherein the x-axis is along the image width direction, with the image center as x=0, where P(x)=1.0, and σ=width/6.
</p>
<p id="p-0019" num="0018">In a further aspect of the invention, the weighting function is of the form
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>P</i>(<i>x</i>)=σ<sup>2n</sup>/(σ<sup>2n</sup><i>+x</i><sup>2n</sup>),<?in-line-formulae description="In-line Formulae" end="tail"?>
<br/>
wherein the x-axis is along the image width direction, with the image center line defined as x=0, where P(x)=1.0, n can be any positive integer and σ=width/6.
</p>
<p id="p-0020" num="0019">In a further aspect of the invention, the weighting function is of the form</p>
<p id="p-0021" num="0020">
<maths id="MATH-US-00001" num="00001">
<math overflow="scroll">
<mrow>
  <mrow>
    <mrow>
      <mi>P</mi>
      <mo>⁡</mo>
      <mrow>
        <mo>(</mo>
        <mi>x</mi>
        <mo>)</mo>
      </mrow>
    </mrow>
    <mo>=</mo>
    <mrow>
      <mrow>
        <mo>-</mo>
        <msup>
          <mrow>
            <mo>(</mo>
            <mfrac>
              <mn>2</mn>
              <mi>w</mi>
            </mfrac>
            <mo>)</mo>
          </mrow>
          <mrow>
            <mn>2</mn>
            <mo>⁢</mo>
            <mi>n</mi>
          </mrow>
        </msup>
      </mrow>
      <mo>⁢</mo>
      <msup>
        <mrow>
          <mo>(</mo>
          <mrow>
            <msup>
              <mi>x</mi>
              <mn>2</mn>
            </msup>
            <mo>-</mo>
            <msup>
              <mrow>
                <mo>(</mo>
                <mfrac>
                  <mi>w</mi>
                  <mn>2</mn>
                </mfrac>
                <mo>)</mo>
              </mrow>
              <mn>2</mn>
            </msup>
          </mrow>
          <mo>)</mo>
        </mrow>
        <mi>n</mi>
      </msup>
    </mrow>
  </mrow>
  <mo>,</mo>
</mrow>
</math>
</maths>
<br/>
wherein the x-axis is along the image width direction, with the image center line defined as x=0, where P(x)=1.0, n is any positive integer and w is the image width.
</p>
<p id="p-0022" num="0021">In a further aspect of the invention, the weighting function is of the form
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>P</i>(<i>x</i>)={−2<i>x/w</i>+1:0<i>≦x≦w</i>/2;2<i>x</i>/+1:<i>−w</i>/2<i>≦x</i>&lt;0},<?in-line-formulae description="In-line Formulae" end="tail"?>
<br/>
wherein the x-axis is along the image width direction, with the image center line defined as x=0, where P(x)=1.0, and w is the image width.
</p>
<p id="p-0023" num="0022">In another aspect of the invention, there is provided a program storage device readable by a computer, tangibly embodying a program of instructions executable by the computer to perform the method steps for aligning a pair of digital images.</p>
<?BRFSUM description="Brief Summary" end="tail"?>
<?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?>
<description-of-drawings>
<heading id="h-0005" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading>
<p id="p-0024" num="0023"><figref idref="DRAWINGS">FIG. 1</figref> depicts an exemplary Gaussian weighing curve.</p>
<p id="p-0025" num="0024"><figref idref="DRAWINGS">FIG. 2</figref> depicts the result of a dataset with the original composing method.</p>
<p id="p-0026" num="0025"><figref idref="DRAWINGS">FIG. 3</figref> depicts the result using a focus-adaptive method on the same dataset as depicted in <figref idref="DRAWINGS">FIG. 2</figref>.</p>
<p id="p-0027" num="0026"><figref idref="DRAWINGS">FIG. 4</figref> depicts the result for the c-spine and t-spine, using the original composing method and the dataset of <figref idref="DRAWINGS">FIGS. 2</figref>. and <b>3</b>.</p>
<p id="p-0028" num="0027"><figref idref="DRAWINGS">FIG. 5</figref> depicts the result for the c-spine and t-spine, using a focus-adaptive composing method and the dataset of <figref idref="DRAWINGS">FIGS. 2</figref>. and <b>3</b>.</p>
<p id="p-0029" num="0028"><figref idref="DRAWINGS">FIG. 6</figref> depicts the result using another dataset composed with the original composing method.</p>
<p id="p-0030" num="0029"><figref idref="DRAWINGS">FIG. 7</figref> depicts the result using the dataset as in <figref idref="DRAWINGS">FIG. 6</figref>, but composed with a focus-adaptive method.</p>
<p id="p-0031" num="0030"><figref idref="DRAWINGS">FIG. 8</figref> illustrates the two-slice problem composing result with the original composing method.</p>
<p id="p-0032" num="0031"><figref idref="DRAWINGS">FIG. 9</figref> illustrates the two-slice problem composing result with a focus-adaptive composing method.</p>
<p id="p-0033" num="0032"><figref idref="DRAWINGS">FIG. 10</figref> depicts a flow chart of a preferred method of the invention.</p>
<p id="p-0034" num="0033"><figref idref="DRAWINGS">FIG. 11</figref> depicts an exemplary computer system for implementing a preferred embodiment of the invention.</p>
</description-of-drawings>
<?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?>
<?DETDESC description="Detailed Description" end="lead"?>
<heading id="h-0006" level="1">DETAILED DESCRIPTION OF THE PREFERRED EMBODIMENTS</heading>
<p id="p-0035" num="0034">Illustrative embodiments of the invention are described below. In the interest of clarity, not all features of an actual implementation are described in this specification. It will of course be appreciated that in the development of any such actual embodiment, numerous implementation-specific decisions must be made to achieve the developers' specific goals, such as compliance with system-related and business-related constraints, which will vary from one implementation to another. Moreover, it will be appreciated that such a development effort might be complex and time-consuming, but would nevertheless be a routine undertaking for those of ordinary skill in the art having the benefit of this disclosure.</p>
<p id="p-0036" num="0035">The methods and systems disclosed herein can be adapted to reconstructing a composite volume from several constituent ordered volumes of organs or anatomical regions including, without limitation, the spine, and limbs such as arms and legs. The software application and algorithm disclosed herein can employ 2-D and 3-D renderings and images of an organ or anatomical system. For illustrative purposes, a spinal system is described. However, it should be understood that the method can be applied to any of a variety of other applications, such as the arms and legs, as is known to those skilled in the art.</p>
<p id="p-0037" num="0036">In the reconstruction of spinal images, computing the correlation of all the possible alignments for the overlap regions of the volumes sometimes provides poor alignment results, as the spine is not always aligned by the automatic alignment parameters computed by the composing algorithm. It has been found that poor alignment can be caused by inconsistency of the volume pair overlaps.</p>
<p id="p-0038" num="0037">The 3-D correlation method works well with ideal images, where overlapping parts of each pair of volumes are consistent. Here, the correlation method provides perfect alignment results for ideal input images. However for inconsistent images, there is no perfect match. One possible cause for the inconsistency is the process of distortion correction. Since MR machines can sometimes cause image distortions, the individual original MR image volumes need to be distortion-corrected before being sent as the inputs for the composing process. However, the distortion corrected images might not be consistent with real data. If input images are warped to some degree, it is not possible to find a perfect match between image pairs even by manual adjustment. For example, in the case of a sagittal dataset, there is frequently a bright stripe along the patient's back due to the surface coil used in the data acquisition. If the sagittal image happens to be distorted, it might be possible to match either the spine area, which is usually near the center of the image, or the stripe area, which is at the edge of the image, but not both simultaneously. Correlation computation, by its nature, favors matching large homogeneous areas over other areas. Therefore, the best alignment computed automatically by a pure 3-D correlation algorithm may tend to favor positions where the patient's back match. In this case, the highest correlation of a back image match is misleading, because the composed image is misaligned for the spinal area, which is the image portion of interest.</p>
<p id="p-0039" num="0038">The methods presented herein, referred to as focus adapted methods, mimic the natural process of human manual adjustments for alignment parameters. When performing manual adjustments of spinal images, people tend to focus more on the part of the image where the spine is located, and not on the peripheral regions of the images, since the spinal area is more important for clinical diagnosis. In a focus adaptive method, which places more weight on the spinal area, the alignment of the spine automatically computed by the correlation process can be improved. Assuming that the spinal area is in the center of the image, pixels on the center line are given an image a weight of 1.0, and pixel intensities are gradually suppressed toward the left and right sides of the image. Note that the weight is applied only while computing alignment parameters, thus the image itself is not modified. Once the alignment parameters are obtained, the original images are composed according to these parameters.</p>
<p id="p-0040" num="0039">Referring now to the flowchart depicted in <figref idref="DRAWINGS">FIG. 10</figref>, the weight function is applied at steps <b>1001</b>, <b>1002</b> to each image of a pair of images to be aligned. The application of the weighting function can be performed by multiplying the intensity at each point in each image by an appropriate value of the weighting function. In one embodiment, the weighing curve chosen is a Gaussian distribution, depicted in <figref idref="DRAWINGS">FIG. 1</figref>. An exemplary Gaussian distribution P(x) can be defined as P(x)=exp(−x<sup>2</sup>/2σ<sup>2</sup>). The X-axis can be taken to be along the image width direction, with the image center line defined as x=0, where P(x)=1.0, representing no suppression on the center line. To determine the shape of the Gaussian curve, i.e. how sharp the suppression should be, one needs to choose a value for the standard deviation, σ. In this embodiment, the standard deviation is chosen to be +3σ at the right edge, and −3σ at the left edge. Therefore, the total image width is 6σ, i.e. σ=width/6. This value of σ is for example only, and other values of σ are possible and within the scope of the invention. The weight can be computed at every location along the image width direction and it can be applied to the image pixel values before computing the correlation value. After application of the weighting function to each of the two images, the images can be aligned as before by computing the correlation value at step <b>1003</b> for the weighted image values in the overlap regions of each image. The aligned images can then be joined at step <b>1004</b> at a cutline in the overlap regions. This method of applying a weight function to the images before correlating the overlap regions can be performed <b>1005</b> for each successive pair of images to be joined into the final composed image. By using this focus-adaptive method in spine images, more reasonable composing results can be obtained when the volume pairs are not completely consistent.</p>
<p id="p-0041" num="0040">Note that other weighting distributions can be used for the focus adaptive method. In another embodiment, the weight function is of the form P(x)=σ<sup>2n</sup>/(σ<sup>2n</sup>+x<sup>2n</sup>), where n can be any positive integer and σ can be chosen as before. In another embodiment, the weight function is of the form</p>
<p id="p-0042" num="0041">
<maths id="MATH-US-00002" num="00002">
<math overflow="scroll">
<mrow>
  <mrow>
    <mrow>
      <mi>P</mi>
      <mo>⁡</mo>
      <mrow>
        <mo>(</mo>
        <mi>x</mi>
        <mo>)</mo>
      </mrow>
    </mrow>
    <mo>=</mo>
    <mrow>
      <mrow>
        <mo>-</mo>
        <msup>
          <mrow>
            <mo>(</mo>
            <mfrac>
              <mn>2</mn>
              <mi>w</mi>
            </mfrac>
            <mo>)</mo>
          </mrow>
          <mrow>
            <mn>2</mn>
            <mo>⁢</mo>
            <mi>n</mi>
          </mrow>
        </msup>
      </mrow>
      <mo>⁢</mo>
      <msup>
        <mrow>
          <mo>(</mo>
          <mrow>
            <msup>
              <mi>x</mi>
              <mn>2</mn>
            </msup>
            <mo>-</mo>
            <msup>
              <mrow>
                <mo>(</mo>
                <mfrac>
                  <mi>w</mi>
                  <mn>2</mn>
                </mfrac>
                <mo>)</mo>
              </mrow>
              <mn>2</mn>
            </msup>
          </mrow>
          <mo>)</mo>
        </mrow>
        <mi>n</mi>
      </msup>
    </mrow>
  </mrow>
  <mo>,</mo>
</mrow>
</math>
</maths>
<br/>
where again n is any positive integer and w is the image width. In another embodiment, the weight function is of the form P(x)={−2x/w+1:0≦x≦w/2 /2x/w+1:−w/2≦x&lt;0}, with w being the image width. These weight functions are exemplary, and any weight function that has a maximum value at a line of reflection in the image, and whose value decreases monotonically with increasing distance from the line of reflection, is within the scope of the invention. Although the example functions presented here are symmetric with respect to the reflection axis, this is not an absolute requirement, as long as the weighting decreases with increasing distance from the reflection axis.
</p>
<p id="p-0043" num="0042">Test results show the performance improvement by using the focus-adaptive methods of the invention. For those datasets that had satisfactory performance with the original composing algorithm, using focus-adaptive method will not affect the original alignment results.</p>
<p id="p-0044" num="0043">A first example uses a dataset in which the images were distortion-corrected, but where the distortion correction parameters were not correctly chosen. This caused the images to be strongly distorted. This extreme case is used to show how the focus-adaptive method can successfully overcome image distortion and still produce reasonable and robust alignment results. <figref idref="DRAWINGS">FIG. 2</figref> shows the alignment result with the original method. In clear-cut mode, the composing results are poor, with the spine not at all aligned. <figref idref="DRAWINGS">FIG. 3</figref> depicts the result obtained on the same dataset by applying the focus-adaptive methods disclosed herein. As can be seen, the composed image in <figref idref="DRAWINGS">FIG. 3</figref> is of much better quality.</p>
<p id="p-0045" num="0044">The differences between the original method and the focus-adaptive method can be further explored by comparing <figref idref="DRAWINGS">FIGS. 4 and 5</figref>. These images represent results for the cervical spine (c-spine) and the thoracic spine (t-spine) in the previous dataset. The image depicted in <figref idref="DRAWINGS">FIG. 4</figref> was obtained using the original method. It can be seen from the cut-line position, that although the spine is poorly aligned, the bright area at the patient's back is well aligned. Due to the inconsistency caused by image distortion, it is not possible to match both the spine and the bright stripe area at the back. As discussed above, the correlation computation favors matching the large homogeneous areas, and thus this alignment has the highest correlation among all possible alignment combinations in this case. The image depicted in <figref idref="DRAWINGS">FIG. 5</figref> shows the results obtained using the focus-adaptive method. Because the focus-adaptive method suppressed the bright areas at the edge of the image by putting less weight on it and focused more on the center area where spine is located, the composed result shows a good alignment along the spine, but not at the edge of the image. If the original input images are strongly distorted, the result in <figref idref="DRAWINGS">FIG. 5</figref> is preferred over that in <figref idref="DRAWINGS">FIG. 4</figref>, because the spine is of more significance to the diagnosis.</p>
<p id="p-0046" num="0045"><figref idref="DRAWINGS">FIGS. 6 and 7</figref> present results using another data set. The original method provides the result of a vertical shift as being 378 pixels (<figref idref="DRAWINGS">FIG. 6</figref>). This alignment needs a manual adjustment of 369 pixels, for a difference between automatic and manual adjustments of 9 pixels. However, by applying the focus-adaptive method, the ideal alignment result is obtained, as shown in <figref idref="DRAWINGS">FIG. 7</figref>.</p>
<p id="p-0047" num="0046">Another example uses a test dataset with only two slices in each volume. The most reasonable composing result obtained by manual adjustment should not include incomplete slices, i.e. the optimal depth shift should be 0. However, the original method gives the result shown in <figref idref="DRAWINGS">FIG. 8</figref>, which has incomplete slices, with a depth shift of −1 pixels between the c-spine and the t-spine. The focus adaptive method solved the incomplete slice problem by focusing on the spine area, as shown in <figref idref="DRAWINGS">FIG. 9</figref>. Note that the depth shift is 0 pixels between the c-spine and the t-spine in <figref idref="DRAWINGS">FIG. 9</figref>.</p>
<p id="p-0048" num="0047">It is to be understood that the present invention can be implemented in various forms of hardware, software, firmware, special purpose processes, or a combination thereof. In one embodiment, the present invention can be implemented in software as an application program tangible embodied on a computer readable program storage device. The application program can be uploaded to, and executed by, a machine comprising any suitable architecture.</p>
<p id="p-0049" num="0048">Referring now to <figref idref="DRAWINGS">FIG. 11</figref>, according to an embodiment of the present invention, a computer system <b>1101</b> for implementing the present invention can comprise, inter alia, a central processing unit (CPU) <b>1102</b>, a memory <b>1103</b> and an input/output (I/O) interface <b>1104</b>. The computer system <b>1101</b> is generally coupled through the I/O interface <b>1104</b> to a display <b>1105</b> and various input devices <b>1106</b> such as a mouse and a keyboard. The support circuits can include circuits such as cache, power supplies, clock circuits, and a communication bus. The memory <b>1103</b> can include random access memory (RAM), read only memory (ROM), disk drive, tape drive, etc., or a combinations thereof. The present invention can be implemented as a routine <b>1107</b> that is stored in memory <b>1103</b> and executed by the CPU <b>1102</b> to process the signal from the signal source <b>1108</b>. As such, the computer system <b>1101</b> is a general purpose computer system that becomes a specific purpose computer system when executing the routine <b>1107</b> of the present invention.</p>
<p id="p-0050" num="0049">The computer system <b>1101</b> also includes an operating system and micro instruction code. The various processes and functions described herein can either be part of the micro instruction code or part of the application program (or combination thereof) which is executed via the operating system. In addition, various other peripheral devices can be connected to the computer platform such as an additional data storage device and a printing device.</p>
<p id="p-0051" num="0050">It is to be further understood that, because some of the constituent system components and method steps depicted in the accompanying figures can be implemented in software, the actual connections between the systems components (or the process steps) may differ depending upon the manner in which the present invention is programmed. Given the teachings of the present invention provided herein, one of ordinary skill in the related art will be able to contemplate these and similar implementations or configurations of the present invention.</p>
<p id="p-0052" num="0051">While the invention is susceptible to various modifications and alternative forms, specific embodiments thereof have been shown by way of example in the drawings and are herein described in detail. It should be understood, however, that the description herein of specific embodiments is not intended to limit the invention to the particular forms disclosed, but on the contrary, the intention is to cover all modifications, equivalents, and alternatives falling within the spirit and scope of the invention as defined by the appended claims.</p>
<p id="p-0053" num="0052">The particular embodiments disclosed above are illustrative only, as the invention may be modified and practiced in different but equivalent manners apparent to those skilled in the art having the benefit of the teachings herein. Furthermore, no limitations are intended to the details of construction or design herein shown, other than as described in the claims below. It is therefore evident that the particular embodiments disclosed above may be altered or modified and all such variations are considered within the scope and spirit of the invention. Accordingly, the protection sought herein is as set forth in the claims below.</p>
<?DETDESC description="Detailed Description" end="tail"?>
</description>
<us-math idrefs="MATH-US-00001" nb-file="US07298923-20071120-M00001.NB">
<img id="EMI-M00001" he="6.69mm" wi="76.20mm" file="US07298923-20071120-M00001.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-math idrefs="MATH-US-00002" nb-file="US07298923-20071120-M00002.NB">
<img id="EMI-M00002" he="6.69mm" wi="76.20mm" file="US07298923-20071120-M00002.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-math idrefs="MATH-US-00003" nb-file="US07298923-20071120-M00003.NB">
<img id="EMI-M00003" he="6.69mm" wi="76.20mm" file="US07298923-20071120-M00003.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-math idrefs="MATH-US-00004" nb-file="US07298923-20071120-M00004.NB">
<img id="EMI-M00004" he="6.69mm" wi="76.20mm" file="US07298923-20071120-M00004.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-claim-statement>What is claimed is:</us-claim-statement>
<claims id="claims">
<claim id="CLM-00001" num="00001">
<claim-text>1. A method for aligning a pair of digital images, said method comprising the steps of:
<claim-text>providing a pair of digital images, wherein each said image comprises a plurality of intensities corresponding to a domain of points in a D-dimensional space, and the pair of images present adjacent views of a same object of interest;</claim-text>
<claim-text>applying a weighting function to each image of the pair of images, wherein the weighting function is centered on the object of interest, the weighting function has a maximum value on the object of interest, and the value of the weighting function decreases with increasing distance from the object of interest; and</claim-text>
<claim-text>aligning the pair of images by correlated the weighted intensities on one image with those in the other image.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00002" num="00002">
<claim-text>2. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein each image of the pair of images has an overlap region, and wherein the step of aligning is performed by correlating the weighted intensities in the overlap region of one image with the weighted intensities in the overlap region of the other image.</claim-text>
</claim>
<claim id="CLM-00003" num="00003">
<claim-text>3. The method of <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein the two images are joined into a composed image at a cutline in the overlap regions of the two images.</claim-text>
</claim>
<claim id="CLM-00004" num="00004">
<claim-text>4. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the weighting function is applied to an image in the pair of images by multiplying the intensity of a point in the image by the value of the weighting function.</claim-text>
</claim>
<claim id="CLM-00005" num="00005">
<claim-text>5. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the object of interest in each image is the spine.</claim-text>
</claim>
<claim id="CLM-00006" num="00006">
<claim-text>6. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the object of interest in each image is an arm.</claim-text>
</claim>
<claim id="CLM-00007" num="00007">
<claim-text>7. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the object of interest in each image is a leg.</claim-text>
</claim>
<claim id="CLM-00008" num="00008">
<claim-text>8. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the images are magnetic resonance images.</claim-text>
</claim>
<claim id="CLM-00009" num="00009">
<claim-text>9. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the weighting function is a Gaussian of the form
<claim-text>
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>P</i>(<i>x</i>)=exp(−<sup>x </sup><sup><sup2>2</sup2></sup>/2σ<sup>2</sup>)<?in-line-formulae description="In-line Formulae" end="tail"?>
</claim-text>
</claim-text>
<claim-text>wherein the x-axis is along the image width direction, with the image center line defined as x=0, where P(x)=1.0, and σ=width/6.</claim-text>
</claim>
<claim id="CLM-00010" num="00010">
<claim-text>10. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the weighting function is of the form
<claim-text>
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>P</i>(<i>x</i>)=σ<sup>2n</sup>/(σ<sup>2n</sup><i>+x</i><sup>2n</sup>),<?in-line-formulae description="In-line Formulae" end="tail"?>
</claim-text>
</claim-text>
<claim-text>wherein the x-axis is along the image width direction, with the image center line defined as x=0, where P(x)=1.0, n can be any positive integer and σ=width/6.</claim-text>
</claim>
<claim id="CLM-00011" num="00011">
<claim-text>11. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the weighting function is of the form</claim-text>
<claim-text>
<maths id="MATH-US-00003" num="00003">
<math overflow="scroll">
<mrow>
  <mrow>
    <mrow>
      <mi>P</mi>
      <mo>⁡</mo>
      <mrow>
        <mo>(</mo>
        <mi>x</mi>
        <mo>)</mo>
      </mrow>
    </mrow>
    <mo>=</mo>
    <mrow>
      <mrow>
        <mo>-</mo>
        <msup>
          <mrow>
            <mo>(</mo>
            <mfrac>
              <mn>2</mn>
              <mi>w</mi>
            </mfrac>
            <mo>)</mo>
          </mrow>
          <mrow>
            <mn>2</mn>
            <mo>⁢</mo>
            <mi>n</mi>
          </mrow>
        </msup>
      </mrow>
      <mo>⁢</mo>
      <msup>
        <mrow>
          <mo>(</mo>
          <mrow>
            <msup>
              <mi>x</mi>
              <mn>2</mn>
            </msup>
            <mo>-</mo>
            <msup>
              <mrow>
                <mo>(</mo>
                <mfrac>
                  <mi>w</mi>
                  <mn>2</mn>
                </mfrac>
                <mo>)</mo>
              </mrow>
              <mn>2</mn>
            </msup>
          </mrow>
          <mo>)</mo>
        </mrow>
        <mi>n</mi>
      </msup>
    </mrow>
  </mrow>
  <mo>,</mo>
</mrow>
</math>
</maths>
</claim-text>
<claim-text>wherein the x-axis is along the image width direction, with the image center line defined as x=0, where P(x)=1.0, n is any positive integer and w is the image width.</claim-text>
</claim>
<claim id="CLM-00012" num="00012">
<claim-text>12. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the weighting function is of the form
<claim-text>
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>P</i>(<i>x</i>)={−2<i>x/w</i>+1:0<i>≦x≦w/</i>2; 2<i>x/w</i>+1<i>:−w/</i>2<i>≦x</i>&lt;0},<?in-line-formulae description="In-line Formulae" end="tail"?>
</claim-text>
</claim-text>
<claim-text>wherein the x-axis is along the image width direction, with the image center line defined as x=0, where P(x)=1.0, and w is the image width.</claim-text>
</claim>
<claim id="CLM-00013" num="00013">
<claim-text>13. A program storage device readable by a computer, tangibly embodying a program of instructions executable by the computer to perform the method steps for aligning a pair of digital images, said method comprising the steps of:
<claim-text>providing a pair of digital images, wherein each said image comprises a plurality of intensities corresponding to a domain of points in a D-dimensional space, and the pair of images present adjacent views of a same object of interest;</claim-text>
<claim-text>applying a weighting function to each image of the pair of images, wherein the weighting function is centered on the object of interest, the weighting function has a maximum value on the object of interest, and the value of the weighting function decreases with increasing distance from the object of interest; and</claim-text>
<claim-text>aligning the pair of images by correlated the weighted intensities on one image with those in the other image.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00014" num="00014">
<claim-text>14. The computer readable program storage device of <claim-ref idref="CLM-00013">claim 13</claim-ref>, wherein each image of the pair of images has an overlap region, and wherein the step of aligning is performed by correlating the weighted intensities in the overlap region of one image with the weighted intensities in the overlap region of the other image.</claim-text>
</claim>
<claim id="CLM-00015" num="00015">
<claim-text>15. The computer readable program storage device of <claim-ref idref="CLM-00014">claim 14</claim-ref>, wherein the two images are joined into a composed image at a cutline in the overlap regions of the two images.</claim-text>
</claim>
<claim id="CLM-00016" num="00016">
<claim-text>16. The computer readable program storage device of <claim-ref idref="CLM-00013">claim 13</claim-ref>, wherein the weighting function is applied to an image in the pair of images by multiplying the intensity of a point in the image by the value of the weighting function.</claim-text>
</claim>
<claim id="CLM-00017" num="00017">
<claim-text>17. The computer readable program storage device of <claim-ref idref="CLM-00013">claim 13</claim-ref>, wherein the object of interest in each image is the spine.</claim-text>
</claim>
<claim id="CLM-00018" num="00018">
<claim-text>18. The computer readable program storage device of <claim-ref idref="CLM-00013">claim 13</claim-ref>, wherein the object of interest in each image is an arm.</claim-text>
</claim>
<claim id="CLM-00019" num="00019">
<claim-text>19. The computer readable program storage device of <claim-ref idref="CLM-00013">claim 13</claim-ref>, wherein the object of interest in each image is a leg.</claim-text>
</claim>
<claim id="CLM-00020" num="00020">
<claim-text>20. The computer readable program storage device of <claim-ref idref="CLM-00013">claim 13</claim-ref>, wherein the images are magnetic resonance images.</claim-text>
</claim>
<claim id="CLM-00021" num="00021">
<claim-text>21. The computer readable program storage device of <claim-ref idref="CLM-00013">claim 13</claim-ref>, wherein the weighting function is a Gaussian of the form
<claim-text>
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>P</i>(<i>x</i>)=exp(−<i>x</i><sup>2</sup>/2σ<sup>2</sup>)<?in-line-formulae description="In-line Formulae" end="tail"?>
</claim-text>
</claim-text>
<claim-text>wherein the x-axis is along the image width direction, with the image center line defined as x=0, where P(x)=1.0, and σ=width/6.</claim-text>
</claim>
<claim id="CLM-00022" num="00022">
<claim-text>22. The computer readable program storage device of <claim-ref idref="CLM-00013">claim 13</claim-ref>, wherein the weighting function is of the form
<claim-text>
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>P</i>(<i>x</i>)=σ<sup>2n</sup>/(σ<sup>2n</sup><i>+x</i><sup>2n</sup>),<?in-line-formulae description="In-line Formulae" end="tail"?>
</claim-text>
</claim-text>
<claim-text>wherein the x-axis is along the image width direction, with the image center line defined as x=0, where P(x)=1.0, n can be any positive integer and σ=width/6.</claim-text>
</claim>
<claim id="CLM-00023" num="00023">
<claim-text>23. The computer readable program storage device of <claim-ref idref="CLM-00013">claim 13</claim-ref>, wherein the weighting function is of the form</claim-text>
<claim-text>
<maths id="MATH-US-00004" num="00004">
<math overflow="scroll">
<mrow>
  <mrow>
    <mrow>
      <mi>P</mi>
      <mo>⁡</mo>
      <mrow>
        <mo>(</mo>
        <mi>x</mi>
        <mo>)</mo>
      </mrow>
    </mrow>
    <mo>=</mo>
    <mrow>
      <mrow>
        <mo>-</mo>
        <msup>
          <mrow>
            <mo>(</mo>
            <mfrac>
              <mn>2</mn>
              <mi>w</mi>
            </mfrac>
            <mo>)</mo>
          </mrow>
          <mrow>
            <mn>2</mn>
            <mo>⁢</mo>
            <mi>n</mi>
          </mrow>
        </msup>
      </mrow>
      <mo>⁢</mo>
      <msup>
        <mrow>
          <mo>(</mo>
          <mrow>
            <msup>
              <mi>x</mi>
              <mn>2</mn>
            </msup>
            <mo>-</mo>
            <msup>
              <mrow>
                <mo>(</mo>
                <mfrac>
                  <mi>w</mi>
                  <mn>2</mn>
                </mfrac>
                <mo>)</mo>
              </mrow>
              <mn>2</mn>
            </msup>
          </mrow>
          <mo>)</mo>
        </mrow>
        <mi>n</mi>
      </msup>
    </mrow>
  </mrow>
  <mo>,</mo>
</mrow>
</math>
</maths>
</claim-text>
<claim-text>wherein the x-axis is along the image width direction, with the image center line defined as x=0, where P(x)=1.0, n is any positive integer and w is the image width.</claim-text>
</claim>
<claim id="CLM-00024" num="00024">
<claim-text>24. The computer readable program storage device of <claim-ref idref="CLM-00013">claim 13</claim-ref>, wherein the weighting function is of the form
<claim-text>
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>P</i>(<i>x</i>)={−2<i>x/w</i>+1:0<i>≦x≦w/</i>2; 2<i>x/w</i>+1<i>:−w/</i>2<i>≦x</i>&lt;0},<?in-line-formulae description="In-line Formulae" end="tail"?>
</claim-text>
</claim-text>
<claim-text>wherein the x-axis is along the image width direction, with the image center line defined as x=0, where P(x)=1.0, and w is the image width.</claim-text>
</claim>
<claim id="CLM-00025" num="00025">
<claim-text>25. A method for aligning a pair of digital images, said method comprising the steps of:
<claim-text>providing a pair of digital images, wherein each said image comprises a plurality of intensities corresponding to a domain of points in a D-dimensional space, the pair of images present adjacent views of a portion of a same object of interest, and each image of the pair of images has an overlap region;</claim-text>
<claim-text>multiplying the intensity of a point in each image of the pair of images by a value of a weighting function, wherein the weighting function is a Gaussian of the form
<claim-text>
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>P</i>(<i>x</i>)=exp(−<i>x</i><sup>2</sup>/2σ<sup>2</sup>)<?in-line-formulae description="In-line Formulae" end="tail"?>
</claim-text>
</claim-text>
</claim-text>
<claim-text>wherein the x-axis is along the image width direction, with the image center line defined as x=0, where P(x)=1.0, and σ=width/6;
<claim-text>aligning the pair of images by correlating the weighted intensities in the overlap region of one image with the weighted intensities in the overlap region of the other image; and</claim-text>
<claim-text>joining the two images into a composed image at a cutline in the overlap regions of the two images.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00026" num="00026">
<claim-text>26. The method of <claim-ref idref="CLM-00025">claim 25</claim-ref>, wherein the object of interest in each image is the spine.</claim-text>
</claim>
<claim id="CLM-00027" num="00027">
<claim-text>27. The method of <claim-ref idref="CLM-00025">claim 25</claim-ref>, wherein the object of interest in each image is an arm.</claim-text>
</claim>
<claim id="CLM-00028" num="00028">
<claim-text>28. The method of <claim-ref idref="CLM-00025">claim 25</claim-ref>, wherein the object of interest in each image is a leg.</claim-text>
</claim>
<claim id="CLM-00029" num="00029">
<claim-text>29. The method of <claim-ref idref="CLM-00025">claim 25</claim-ref>, wherein the images are magnetic resonance images.</claim-text>
</claim>
</claims>
</us-patent-grant>
