<us-patent-grant lang="EN" dtd-version="v4.2 2006-08-23" file="US07299290-20071120.XML" status="PRODUCTION" id="us-patent-grant" country="US" date-produced="20071106" date-publ="20071120">
<us-bibliographic-data-grant>
<publication-reference>
<document-id>
<country>US</country>
<doc-number>07299290</doc-number>
<kind>B2</kind>
<date>20071120</date>
</document-id>
</publication-reference>
<application-reference appl-type="utility">
<document-id>
<country>US</country>
<doc-number>09815824</doc-number>
<date>20010322</date>
</document-id>
</application-reference>
<us-application-series-code>09</us-application-series-code>
<us-term-of-grant>
<us-term-extension>973</us-term-extension>
</us-term-of-grant>
<classifications-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>G</section>
<class>06</class>
<subclass>F</subclass>
<main-group>15</main-group>
<subgroup>16</subgroup>
<symbol-position>F</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20071120</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
</classifications-ipcr>
<classification-national>
<country>US</country>
<main-classification>709231</main-classification>
<further-classification>709232</further-classification>
</classification-national>
<invention-title id="d0e53">Method and system for providing multimedia information on demand over wide area networks</invention-title>
<references-cited>
<citation>
<patcit num="00001">
<document-id>
<country>US</country>
<doc-number>5611049</doc-number>
<kind>A</kind>
<name>Pitts</name>
<date>19970300</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<patcit num="00002">
<document-id>
<country>US</country>
<doc-number>5657468</doc-number>
<kind>A</kind>
<name>Stallmo et al.</name>
<date>19970800</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<patcit num="00003">
<document-id>
<country>US</country>
<doc-number>5758085</doc-number>
<kind>A</kind>
<name>Kouoheris et al.</name>
<date>19980500</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>709231</main-classification></classification-national>
</citation>
<citation>
<patcit num="00004">
<document-id>
<country>US</country>
<doc-number>5832309</doc-number>
<kind>A</kind>
<name>Noe et al.</name>
<date>19981100</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>710 61</main-classification></classification-national>
</citation>
<citation>
<patcit num="00005">
<document-id>
<country>US</country>
<doc-number>5862312</doc-number>
<kind>A</kind>
<name>Mann et al.</name>
<date>19990100</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<patcit num="00006">
<document-id>
<country>US</country>
<doc-number>5875456</doc-number>
<kind>A</kind>
<name>Stallmo et al.</name>
<date>19990200</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<patcit num="00007">
<document-id>
<country>US</country>
<doc-number>6052759</doc-number>
<kind>A</kind>
<name>Stallmo et al.</name>
<date>20000400</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<patcit num="00008">
<document-id>
<country>US</country>
<doc-number>6085234</doc-number>
<kind>A</kind>
<name>Pitts et al.</name>
<date>20000700</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<patcit num="00009">
<document-id>
<country>US</country>
<doc-number>6112206</doc-number>
<kind>A</kind>
<name>Morris et al.</name>
<date>20000800</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<patcit num="00010">
<document-id>
<country>US</country>
<doc-number>6138247</doc-number>
<kind>A</kind>
<name>McKay et al.</name>
<date>20001000</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<patcit num="00011">
<document-id>
<country>US</country>
<doc-number>6148414</doc-number>
<kind>A</kind>
<name>Brown et al.</name>
<date>20001100</date>
</document-id>
</patcit>
<category>cited by other</category>
<classification-national><country>US</country><main-classification>714  9</main-classification></classification-national>
</citation>
<citation>
<patcit num="00012">
<document-id>
<country>US</country>
<doc-number>6151297</doc-number>
<kind>A</kind>
<name>Congdon et al.</name>
<date>20001100</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>370216</main-classification></classification-national>
</citation>
<citation>
<patcit num="00013">
<document-id>
<country>US</country>
<doc-number>6192411</doc-number>
<kind>B1</kind>
<name>Chan et al.</name>
<date>20010200</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>709232</main-classification></classification-national>
</citation>
<citation>
<patcit num="00014">
<document-id>
<country>US</country>
<doc-number>6216173</doc-number>
<kind>B1</kind>
<name>Jones et al.</name>
<date>20010400</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<patcit num="00015">
<document-id>
<country>US</country>
<doc-number>6243829</doc-number>
<kind>B1</kind>
<name>Chan</name>
<date>20010600</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<patcit num="00016">
<document-id>
<country>US</country>
<doc-number>6304895</doc-number>
<kind>B1</kind>
<name>Schneider et al.</name>
<date>20011000</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>709203</main-classification></classification-national>
</citation>
<citation>
<patcit num="00017">
<document-id>
<country>US</country>
<doc-number>6341315</doc-number>
<kind>B1</kind>
<name>Arroyo et al.</name>
<date>20020100</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>709230</main-classification></classification-national>
</citation>
<citation>
<patcit num="00018">
<document-id>
<country>US</country>
<doc-number>6389432</doc-number>
<kind>B1</kind>
<name>Pothapragada et al.</name>
<date>20020500</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>707205</main-classification></classification-national>
</citation>
<citation>
<patcit num="00019">
<document-id>
<country>US</country>
<doc-number>6400730</doc-number>
<kind>B1</kind>
<name>Latif et al.</name>
<date>20020600</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>370466</main-classification></classification-national>
</citation>
<citation>
<patcit num="00020">
<document-id>
<country>US</country>
<doc-number>6401126</doc-number>
<kind>B1</kind>
<name>Douceur et al.</name>
<date>20020600</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>709231</main-classification></classification-national>
</citation>
<citation>
<patcit num="00021">
<document-id>
<country>US</country>
<doc-number>6405256</doc-number>
<kind>B1</kind>
<name>Lin et al.</name>
<date>20020600</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>709231</main-classification></classification-national>
</citation>
<citation>
<patcit num="00022">
<document-id>
<country>US</country>
<doc-number>6463508</doc-number>
<kind>B1</kind>
<name>Wolf et al.</name>
<date>20021000</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>711133</main-classification></classification-national>
</citation>
<citation>
<patcit num="00023">
<document-id>
<country>US</country>
<doc-number>6567853</doc-number>
<kind>B2</kind>
<name>Shomler</name>
<date>20030500</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>709229</main-classification></classification-national>
</citation>
<citation>
<patcit num="00024">
<document-id>
<country>US</country>
<doc-number>6574795</doc-number>
<kind>B1</kind>
<name>Carr</name>
<date>20030600</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>725 71</main-classification></classification-national>
</citation>
<citation>
<patcit num="00025">
<document-id>
<country>US</country>
<doc-number>2001/0049740</doc-number>
<kind>A1</kind>
<name>Karpoff</name>
<date>20011200</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<patcit num="00026">
<document-id>
<country>WO</country>
<doc-number>WO 02/08899</doc-number>
<kind>A2</kind>
<date>20020100</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<patcit num="00027">
<document-id>
<country>WO</country>
<doc-number>WO 02/13033</doc-number>
<kind>A1</kind>
<date>20020200</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<nplcit num="00028">
<othercit>“Storage Virtualization”, Storage Virtualization Brief, <i>VERITAS Software Corporation</i>, pp. 1-19 (Apr. 2001).</othercit>
</nplcit>
<category>cited by other</category>
</citation>
<citation>
<nplcit num="00029">
<othercit>“VERITAS SANPoint Foundation Sutie HA Sharing Data in Storage Area Networks,” <i>VERITAS Sofware Corporation</i>, pp. 1-8 (Jan. 2001).</othercit>
</nplcit>
<category>cited by other</category>
</citation>
<citation>
<nplcit num="00030">
<othercit>Sosinsky, B., “The Business Value of Virtual Volume Management”, <i>VERITAS Software Corporation</i>, pp. 1-12 (Oct. 2001).</othercit>
</nplcit>
<category>cited by other</category>
</citation>
<citation>
<nplcit num="00031">
<othercit>Winett, B., Silicon Storage Applicances <i>Implementation Advantage, DataDirect Networks, Inc</i>., pp. 1-13 (Jun. 2001, Rev. Nov. 30, 2001).</othercit>
</nplcit>
<category>cited by other</category>
</citation>
<citation>
<nplcit num="00032">
<othercit>Woolery, R., “DataDirect Netorks'Silicon Storage Appliance Enabling Storage Networks,” <i>DataDirect Netorsk,Inc</i>., pp. 1-10, no date.</othercit>
</nplcit>
<category>cited by other</category>
</citation>
</references-cited>
<number-of-claims>12</number-of-claims>
<us-exemplary-claim>1</us-exemplary-claim>
<us-field-of-classification-search>
<classification-national>
<country>US</country>
<main-classification>709231</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>709224</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>709203</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>709205</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>709232</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>709211</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>709208</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>370466</main-classification>
</classification-national>
</us-field-of-classification-search>
<figures>
<number-of-drawing-sheets>18</number-of-drawing-sheets>
<number-of-figures>24</number-of-figures>
</figures>
<us-related-documents>
<us-provisional-application>
<document-id>
<country>US</country>
<doc-number>60191237</doc-number>
<kind>00</kind>
<date>20000322</date>
</document-id>
</us-provisional-application>
<related-publication>
<document-id>
<country>US</country>
<doc-number>20010049740</doc-number>
<kind>A1</kind>
<date>20011206</date>
</document-id>
</related-publication>
</us-related-documents>
<parties>
<applicants>
<applicant sequence="001" app-type="applicant-inventor" designation="us-only">
<addressbook>
<last-name>Karpoff</last-name>
<first-name>Wayne T.</first-name>
<address>
<city>Sherwood Park</city>
<country>CA</country>
</address>
</addressbook>
<nationality>
<country>CA</country>
</nationality>
<residence>
<country>CA</country>
</residence>
</applicant>
</applicants>
<agents>
<agent sequence="01" rep-type="attorney">
<addressbook>
<orgname>Townsend and Townsend and Crew LLP</orgname>
<address>
<country>unknown</country>
</address>
</addressbook>
</agent>
<agent sequence="02" rep-type="attorney">
<addressbook>
<last-name>Gray</last-name>
<first-name>Gerald T.</first-name>
<address>
<country>unknown</country>
</address>
</addressbook>
</agent>
</agents>
</parties>
<assignees>
<assignee>
<addressbook>
<orgname>YottaYotta, Inc.</orgname>
<role>02</role>
<address>
<city>Kirkland</city>
<state>WA</state>
<country>US</country>
</address>
</addressbook>
</assignee>
</assignees>
<examiners>
<primary-examiner>
<last-name>Vaughn</last-name>
<first-name>William</first-name>
<department>2144</department>
</primary-examiner>
<assistant-examiner>
<last-name>Maniwang</last-name>
<first-name>Joseph</first-name>
</assistant-examiner>
</examiners>
</us-bibliographic-data-grant>
<abstract id="abstract">
<p id="p-0001" num="0000">Systems and methods for delivering streaming data content to a client device over a data communication network in response to a request for the data content from the client device. The client request is received by a server or a controller device that is typically located on a network switch device. If received by a server, the server sends a request to the controller device to control the transfer of the requested data to the client. The controller device includes the processing capability required for retrieving the streaming data and delivering the streaming data directly to the client device without involving the server system. In some cases, the controller device mirrors the data request to another controller device to handle the data processing and delivery functions. In other cases, the controller device coordinates the delivery of the requested data using one or more other similar controller devices in a pipelined fashion.</p>
</abstract>
<drawings id="DRAWINGS">
<figure id="Fig-EMI-D00000" num="00000">
<img id="EMI-D00000" he="252.73mm" wi="166.37mm" file="US07299290-20071120-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00001" num="00001">
<img id="EMI-D00001" he="234.61mm" wi="159.94mm" file="US07299290-20071120-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00002" num="00002">
<img id="EMI-D00002" he="259.93mm" wi="174.33mm" orientation="landscape" file="US07299290-20071120-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00003" num="00003">
<img id="EMI-D00003" he="204.05mm" wi="139.28mm" orientation="landscape" file="US07299290-20071120-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00004" num="00004">
<img id="EMI-D00004" he="102.62mm" wi="157.65mm" file="US07299290-20071120-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00005" num="00005">
<img id="EMI-D00005" he="177.46mm" wi="171.20mm" file="US07299290-20071120-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00006" num="00006">
<img id="EMI-D00006" he="209.97mm" wi="178.39mm" file="US07299290-20071120-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00007" num="00007">
<img id="EMI-D00007" he="174.58mm" wi="172.30mm" file="US07299290-20071120-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00008" num="00008">
<img id="EMI-D00008" he="98.47mm" wi="140.97mm" file="US07299290-20071120-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00009" num="00009">
<img id="EMI-D00009" he="248.33mm" wi="166.71mm" file="US07299290-20071120-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00010" num="00010">
<img id="EMI-D00010" he="260.18mm" wi="168.49mm" file="US07299290-20071120-D00010.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00011" num="00011">
<img id="EMI-D00011" he="241.05mm" wi="171.96mm" file="US07299290-20071120-D00011.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00012" num="00012">
<img id="EMI-D00012" he="145.80mm" wi="153.75mm" file="US07299290-20071120-D00012.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00013" num="00013">
<img id="EMI-D00013" he="213.11mm" wi="140.89mm" orientation="landscape" file="US07299290-20071120-D00013.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00014" num="00014">
<img id="EMI-D00014" he="183.13mm" wi="153.67mm" file="US07299290-20071120-D00014.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00015" num="00015">
<img id="EMI-D00015" he="253.15mm" wi="168.23mm" file="US07299290-20071120-D00015.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00016" num="00016">
<img id="EMI-D00016" he="237.15mm" wi="162.14mm" file="US07299290-20071120-D00016.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00017" num="00017">
<img id="EMI-D00017" he="226.06mm" wi="165.18mm" file="US07299290-20071120-D00017.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00018" num="00018">
<img id="EMI-D00018" he="213.78mm" wi="160.95mm" file="US07299290-20071120-D00018.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
</drawings>
<description id="description">
<?RELAPP description="Other Patent Relations" end="lead"?>
<heading id="h-0001" level="1">CROSS-REFERENCES TO RELATED APPLICATIONS</heading>
<p id="p-0002" num="0001">This application is related to, and claims priority from, U.S. Provisional Patent Application Ser. No. 60/191,237, filed Mar. 22, 2000, entitled “STORAGE ROUTING AND EXTENDABLE SCREENING SERVER SYSTEMS AND METHODS FOR IMPLEMENTING THE SAME,” the disclosure of which is hereby incorporated by reference in its entirety.</p>
<?RELAPP description="Other Patent Relations" end="tail"?>
<?BRFSUM description="Brief Summary" end="lead"?>
<heading id="h-0002" level="1">BACKGROUND OF THE INVENTION</heading>
<p id="p-0003" num="0002">The present invention relates to Storage Area Networks (SANs). In particular, the present invention relates to methods and systems for providing multimedia data, such as video data, to a client making a request to a data delivery system over a communication network such as a Wide Area Network (WAN).</p>
<p id="p-0004" num="0003">The Communication Network used to deliver multimedia and video to an end-user (client) typically includes the following three main components: a back-end network comprised of a server system, an end-user system, and a front-end network for connecting a plurality of end-users (clients) to the server system.</p>
<p id="p-0005" num="0004">The front-end network of a Communication Network is typically comprised of a Wide Area Network (WAN), Local Area Network (LAN), or a Broadcast Area Network (BAN).</p>
<p id="p-0006" num="0005">Recent developments in both the telephone and cable television services are capitalizing on recent advances in technology in the Art. For example, the increasing level of integration in Very-Large-Scale-Integration (VLSI) technology has facilitated the reduction in cost of motion video compression/decompression hardware and enabled technology such as Asymmetric Digital Subscriber Loop (ADSL).</p>
<p id="p-0007" num="0006">Similarly, the advances in fiber optic transmission technology and its declining cost have enabled upgrades in front-end network systems such as cable TV network trunk and feeder systems. Traditionally, these systems have increased the bandwidth of the network sufficiently to provide each subscriber his own dedicated channel to the head-end for receiving compressed digital video. Direct broadcast satellite technology and other emerging wireless communication technology also provide dedicated multimedia and video channels between a large number of end-users and the server systems.</p>
<p id="p-0008" num="0007">Personal computers and set top boxes for the end-user are also emerging which enable networked multimedia applications. Each of these is taking advantage of the low cost video compression/decompression hardware and advances in microprocessor technology.</p>
<p id="p-0009" num="0008">While the end-user (client) system and the front-end network system infrastructure is evolving rapidly to meet the requirement of interactive multimedia services, current server systems continue to be expensive and impractical for delivering these services because of the limited capacity of the server system. Current server systems are unable to process the large number of streams that are required by streaming multimedia and video services.</p>
<p id="p-0010" num="0009">The current choices of servers are typically off-the-shelf mainframes or workstation technology based parallel computing systems. The hardware and software in both cases is optimized for computation intensive applications and for supporting multiple concurrent users (time-sharing) with very limited emphasis on moving data to and from the network interface and the Input/Output (I/O) device. A typical example of an input/output device, in accordance with the present invention, is a storage subsystem.</p>
<p id="p-0011" num="0010">For example, the bandwidth from the memory to cache in an RS/6000 is 400 Mbytes/sec, while the bandwidth from or to the I/O or network device is only 80 Mbytes/sec. The floating-point support adds to the cost of the system without providing any benefit to the delivery of multimedia video and audio data.</p>
<p id="p-0012" num="0011">The above factors have forced the price and performance of general purpose computing systems to be much higher than server systems optimized for delivery of multimedia data.</p>
<p id="p-0013" num="0012">Typically, the acknowledged public activity in addressing the above mentioned limitations have been minimal. One methodology has been in the implementation of an optimization in the placement of data on an array of disks. This architecture is used to maximize the disk throughput in the video server application. A second methodology has been in the implementation of the policy of optimization of buffering of data retrieved from disk to maximize its reuse in the video server application. Another methodology would see the implementation of the optimization of the file systems for accompanying multimedia data.</p>
<p id="p-0014" num="0013">However, the above mentioned improvements may typically only improve the overall performance of current video server systems by a factor of two or four times, whereas the current need in the Industry requires improvements in the range of 100 to 1000 times current technology to make the interactive streaming video services economically feasible.</p>
<p id="p-0015" num="0014">Notwithstanding the foregoing, another key to multimedia audio and video streaming is the concept of Quality of Service.</p>
<p id="p-0016" num="0015">“Quality of Service” (QoS) generally refers to a technique for managing computer system resources such as bandwidth by specifying user visible parameters such as message delivery time. Policy rules are used to describe the operation of network elements to make these guarantees. Relevant standards for QoS in the IETF (Internet Engineering Task Force) are the RSVP (Resource Reservation Protocol) and COPS (Common Open Policy Service) protocols. RSVP allows for the reservation of bandwidth in advance, while COPS allows routers and switches to obtain policy rules from a server.</p>
<p id="p-0017" num="0016">A major requirement in providing Quality of Service is the ability to deliver video frame data at a guaranteed uniform rate. Failure to maintain Quality of Service may typically result in an image that is jerky or distorted.</p>
<p id="p-0018" num="0017">Traditional server system architectures have not been equipped with the functionality necessary for the implementation of providing Quality of Service on a large scale (more than one dedicated server for each client on the network). With an increasing load on the server systems to provide streaming multimedia applications, an increased volume of user (end-clients), and the above mentioned deficiencies in current server system technology, a need exists to provide a server system architecture which will be able to address this need.</p>
<p id="p-0019" num="0018">U.S. Pat. No. 5,758,085 (hereinafter, “085′ Patent”) assigned to the Industrial Business Machine (IBM) Corporation addresses the above-named problems by providing a plurality of intelligent switches in a Storage Area Network (SAN) with the server system. When the end-user (client) makes a request to receive video and multimedia data, a request is sent to the host processor which in turn sends a request to a plurality of intelligent switches on the SAN. The intelligent switches include a cache for storing the requested data. The data is relayed directly from these switches to the end-user (client) requesting the multimedia data.
<ul id="ul0001" list-style="none">
    <li id="ul0001-0001" num="0000">
    <ul id="ul0002" list-style="none">
        <li id="ul0002-0001" num="0019">However, the IBM system described above provides for the storage of data onto switches, it does not allow the individual switches to cooperate together as a distributed architecture in order to pool bandwidth together to supply the backbone network. Current technology allows only for a 1-2 Gigabyte data stream coming out of a single peripheral device such as an array of disks, wherein the network backbone may accommodate a 10 Gigabyte or higher data stream. Also, in the '085 Patent, the individual switches are not able to work together to distribute a delivery request over multiple switches for load balancing and streaming of the requested data.</li>
    </ul>
    </li>
</ul>
</p>
<p id="p-0020" num="0020">Accordingly, it is desirable to provide systems and methods that allow for efficient delivery of multi-media and other data content to clients and which overcome problems inherent in existing systems.</p>
<heading id="h-0003" level="1">SUMMARY OF THE INVENTION</heading>
<p id="p-0021" num="0021">The present invention provides systems and methods for providing video, multimedia and other continuous media content to a client over a network.</p>
<p id="p-0022" num="0022">The present invention provides systems and methods for delivering streaming data content to a client device over a data communication network in response to a request for the data content from the client device. The client request is received by a server or a controller device that is typically located on a network switch device. If received by a server, the server sends a request to the controller device to control the transfer of the requested data to the client. The controller device includes the processing capability required for retrieving the streaming data and delivering the streaming data directly to the client device without involving the server system. In some cases, the controller device mirrors the data request to another controller device to handle the data processing and delivery functions. In other cases, the controller device coordinates the delivery of the requested data using one or more other similar controller devices in a pipelined fashion.</p>
<p id="p-0023" num="0023">As used herein, the terms “Storage Area Network,” and “Network Attached Storage” are defined as set forth in the publication titled Building Storage Area Networks<sup>1 </sup>by Marc Farley, the contents of which are herein incorporated by reference for all purposes.</p>
<p id="p-0024" num="0024">“Storage Area Network” (SAN) refers typically to a Network which connects one or more servers together. SANs are commonly thought of as fiber channel storage networks transmitting Input/Output (I/O) traffic using serial Small Computer Systems Interface (SCSI) I/O protocol called Fiber Channel Protocol (FCP).</p>
<p id="p-0025" num="0025">SANs generally uses Fiber channel technology to connect the elements of the SAN together, such as between the server system and the physical disks. Generally, data is transferred on the block level, rather than as actually files. SANs typically are connected directly to the storage device on the network rather than through an I/O bus or channel on the server.</p>
<p id="p-0026" num="0026">“Network Attached Storage” (NAS) refers typically to a storage system which connects directly from a server. NAS are commonly understood to be turnkey file servers with their own file systems. The Network associated with NAS generally uses Ethernet technology to connect the elements of the NAS together, such as between the server system and the NAS storage element. Generally, data is transferred on the file level, rather than the disk block level. NAS typically is connected to the storage device through an I/O bus or channel on the server, rather than direct attached storage.</p>
<p id="p-0027" num="0027">As used herein, the terms “Wide Area Network,” “Local Area Network,” and “Broadcast Area Network” are defined as set forth in the Dictionary of Storage and Storage Networking Terminology<sup>2 </sup>produced by SNIA (Storage Networking Industry Association), the contents of which are herein incorporated by reference for all purposes.</p>
<p id="p-0028" num="0028">“Wide Area Network” (WAN) generally refers to a communication network that is geographically dispersed and that includes telecommunication links. A commonly used WAN is the public telephone network. The telephone network today provides access to electronically stored data in various media. These media include multimedia, video, audio and textual information.</p>
<p id="p-0029" num="0029">“Local Area Network” (LAN) refers generally to a communication infrastructure designed to use dedicated wiring over a limited distance (typically a diameter of less than five kilometers) to connect a large number of intercommunicating nodes. A Commonly used LAN is the Ethernet.</p>
<p id="p-0030" num="0030">“Broadcast Area Network” (BAN) refers generally to a communication infrastructure designed for the transmission of data over the broadcast/cable television system. A commonly used BAN is the cable connection provided in the home for watching multimedia and video programming.</p>
<p id="p-0031" num="0031">“Metropolitan Area Network” (MAN) generally refers to a network that interconnects users with computer resources in a geographic area or region larger than that covered by even a large local area network (LAN) but smaller than the area covered by a wide area network (WAN). The term is applied to the interconnection of networks in a city into a single larger network (which may then also offer efficient connection to a wide area network). It is also used to mean the interconnection of several local area networks by bridging them with a backbone.</p>
<p id="p-0032" num="0032">Collectively, the term “Front End Network” (FEN) will be used to describe the various communication infrastructures, as set forth above: WAN, LAN, BAN, MAN, and SAN. The term “FEN” may also be comprised of any combination, or sub combination of these various communication infrastructures.</p>
<p id="p-0033" num="0033">It should be apparent to one skilled in the art, that the scope of the invention is intended on included any other communication network used in the communication of digital data over a network interconnect, according to the embodiments of the present invention.</p>
<p id="p-0034" num="0034">The present invention provides a number of advantages over traditional Host Bus Adapter (HBA) implementations. Such advantages include:
<ul id="ul0003" list-style="none">
    <li id="ul0003-0001" num="0000">
    <ul id="ul0004" list-style="none">
        <li id="ul0004-0001" num="0035">Latency between individual blades is significantly reduced;</li>
        <li id="ul0004-0002" num="0036">The bandwidth between individual blades is improved;</li>
        <li id="ul0004-0003" num="0037">Access to the fabric, depending upon the nature of the switching fabric implementation, provide multicast communication to other blades, even though the external network may not support multicast or may not support multicast in an efficient manner;</li>
    </ul>
    </li>
</ul>
</p>
<p id="p-0035" num="0038">The above provides advantages to most any storage control logic that involves multiple servers or disk connections. Moving the individual functional components of the controller device onto the switching fabric has some specific benefits, including:
<ul id="ul0005" list-style="none">
    <li id="ul0005-0001" num="0000">
    <ul id="ul0006" list-style="none">
        <li id="ul0006-0001" num="0039">Moving the cache management logic onto the controller device means the synchronization communication between individual controller devices would enjoy reduced latency.;</li>
        <li id="ul0006-0002" num="0040">Moving the cache onto the controller device improves the efficiency of cache pooling—the concept of getting data from remote controller devices without the requirement of going to disk because of reduced latency and, potentially, increased bandwidth. In applications such as video streaming, there is the opportunity for an improved Quality of Service (Q of S);</li>
        <li id="ul0006-0003" num="0041">Moving the RAID engine onto the controller device increases the overall throughput by allowing the additional RAID I/O streams (e.g. two streams involved with mirroring across redundancy groups) to be sent in parallel with other communications. In an HBA implementation, all streams must share a single channel on the external network (e.g. Fiberchannel). There is a balance here. Placing the cache on the blades (within the switching fabric) may increase the latency of copying from cache into the server. However, as the block speed of the physical network increases, this generally becomes less of an issue; and</li>
    </ul>
    </li>
</ul>
</p>
<p id="p-0036" num="0042">Moving the administrative functionality onto the controller device generally reduces the time required for administrative functions such as replication and backups.</p>
<p id="p-0037" num="0043">According to an aspect of the present invention, a method is provided for delivering streaming data content to a client device over a data communication network in response to a request for the data content from the client device. The method typically includes receiving, by a server, a request from a first client device over the data communication network, the request identifying streaming data content stored on a storage system, identifying a first controller device associated with the storage system on which the data content is stored, and transmitting a data request message from the server to the first controller device, the data request message identifying the first client device and the data content requested by the first client device. The method also typically includes retrieving, by the first controller device, the streaming data content from the storage system, and transferring the retrieved data content directly to the first client device over the data communication network from the first controller device through a communication port for communicably coupling the first controller device to the data communication network.</p>
<p id="p-0038" num="0044">According to another aspect of the present invention, a method is provided for delivering streaming data content to a client device over a data communication network in response to a request for the data content from the client device. The method typically includes receiving, by a first controller device, a request from a first client device over the data communication network, the request identifying streaming data content stored on a storage system, identifying a second controller device associated with the storage system on which the data content is stored, and transmitting a data request message from the first controller device to the second controller device, the data request message identifying the first client device and the data content requested by the first client device. The method also typically includes retrieving, by the second controller device, the streaming data content from the storage system, and transferring the retrieved data content directly to the first client device over the data communication network from the second controller device through a communication port for communicably coupling the second controller device to the data communication network.</p>
<p id="p-0039" num="0045">According to yet another aspect of the present invention, a method is provided for delivering streaming data content to a client device over a data communication network in response to a request for the data content from the client device. The method typically includes receiving, by a server, a request from a first client device over a first data communication network, the request identifying streaming data content stored on a storage system, transmitting a data request message from the server to a first controller device, the data request message identifying the first client device and the data content requested by the first client device, identifying a second controller device associated with the storage system on which the data content is stored, and transmitting a second data request message to the second controller device, the second data request message identifying the first client device and the data content requested by the first client device. The method also typically includes retrieving, by the second controller device, the streaming data content from the storage system, and transferring the retrieved data content directly to the first client device from the second controller device.</p>
<p id="p-0040" num="0046">According to a further aspect of the present invention, a method is provided for delivering streaming data content to a client device from two or more controller devices over a data communication network in response to a request for the data content from the client device, wherein the data content includes two or more blocks of data stored on a storage system. The method typically includes receiving, by a server, a request from a first client device over the data communication network, the request identifying streaming data content stored on a storage system, transmitting a data request message from the server to a first controller device associated with the storage system, the data request message identifying the first client device and the data content requested by the first client device, and retrieving a first block of the data content from the storage system by the first controller device. the method also typically includes sending a second data request message from the first controller device to a second controller device associated with the storage system, the second data request message identifying the first client device and a second block of the data content, retrieving the second block of the data content from the storage system by the second controller device, transferring the first block of data directly to the first client device from the first controller device, sending a synchronization message from the first controller device to the second controller device, and in response to the synchronization message, transferring the second block of data directly to the first client device from the second controller device.</p>
<p id="p-0041" num="0047">According to yet a further aspect of the present invention, a method is provided for delivering streaming data content to a client device over a data communication network in response to a request for the data content from the client device. The method typically includes receiving, by a server, a request from a first client device over the data communication network, the request identifying streaming data content stored on a storage system, and transmitting a data request message over the data communication network from the server to a first controller device, wherein the data request message identifies the first client device and the data content requested by the first client device, and wherein the first controller is coupled to the storage system over a storage area network (SAN). The method also typically includes retrieving, by the first controller device, the streaming data content from the storage system over the SAN, and transferring the retrieved data content directly to the first client device over the data communication network from the first controller device.</p>
<p id="p-0042" num="0048">According to still a further aspect of the present invention, a method is provided for delivering streaming data content to a client device over a data communication network in response to a request for the data content from the client device. The method typically includes receiving, by a first controller device, a request sent by a first client device to a server over the data communication network, the request identifying streaming data content stored on a storage system, wherein the first controller device and the server are coupled by the data communication network, processing the request by the first controller device, and controlling, by the first controller device, the delivery of the requested streaming data directly to the first client device over the data communication network by one of the first controller device and a second controller device. Typically, the processing by the first controller device and the delivery of the data content is performed without involvement by the server to which the request was originally intended.</p>
<p id="p-0043" num="0049">Reference to the remaining portions of the specification, including the drawings and claims, will realize other features and advantages of the present invention. Further features and advantages of the present invention, as well as the structure and operation of various embodiments of the present invention, are described in detail below with respect to the accompanying drawings. In the drawings, like reference numbers indicate identical or functionally similar elements.</p>
<?BRFSUM description="Brief Summary" end="tail"?>
<?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?>
<description-of-drawings>
<heading id="h-0004" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading>
<p id="p-0044" num="0050">The foregoing summary of the invention, as well as the following detailed description of preferred embodiments, is better understood when read in conjunction with the accompanying drawings, which are included by way of example, and not intended on being limiting by way of the claimed invention.</p>
<p id="p-0045" num="0051"><figref idref="DRAWINGS">FIGS. 1A-D</figref> illustrates a comparison of the various SCSI architectures for both Ultra SCSI and Wide Ultra SCSI for SCSI-1, SCSI-2 and SCSI-3;</p>
<p id="p-0046" num="0052"><figref idref="DRAWINGS">FIG.2</figref> illustrates the scope of the SCSI-3 architecture, according to an embodiment of a messaging protocol of the present invention;</p>
<p id="p-0047" num="0053"><figref idref="DRAWINGS">FIG. 3A</figref> is a block diagram of a typical Fiber Channel Protocol Stack, according to an embodiment of a messaging protocol of the present invention;</p>
<p id="p-0048" num="0054"><figref idref="DRAWINGS">FIG. 3B</figref>. is a comparison of the various Fiber Channel Protocols, according to an embodiment of a messaging protocol of the present invention;</p>
<p id="p-0049" num="0055"><figref idref="DRAWINGS">FIG. 4A</figref> illustrates a typical Infiniband Architecture, according to an embodiment of a messaging protocol of the present invention;</p>
<p id="p-0050" num="0056"><figref idref="DRAWINGS">FIG. 4B</figref> illustrates a typical Infiniband Fabric Architecture on the network including the subnetworks architecture;</p>
<p id="p-0051" num="0057"><figref idref="DRAWINGS">FIG. 5</figref> illustrates a typical Ethernet Protocol Stack, according to an embodiment of a messaging protocol of the present invention;</p>
<p id="p-0052" num="0058"><figref idref="DRAWINGS">FIG. 6</figref> shows the Ethernet Packet, according to an embodiment of a messaging protocol illustrating a typical data format for an audio and video payload;</p>
<p id="p-0053" num="0059"><figref idref="DRAWINGS">FIG. 7</figref> shows an exemplary configuration of a data communication network, which includes an individual switch with at least one controller device according to one embodiment of the present invention;</p>
<p id="p-0054" num="0060"><figref idref="DRAWINGS">FIG. 8</figref> shows an exemplary configuration of <figref idref="DRAWINGS">FIG. 7</figref>, wherein an individual switch includes a plurality of controller devices;</p>
<p id="p-0055" num="0061"><figref idref="DRAWINGS">FIG. 9</figref> shows an exemplary configuration of a data communication network, which includes a plurality of individual switches, each of which includes an individual controller device;</p>
<p id="p-0056" num="0062"><figref idref="DRAWINGS">FIG. 10</figref> shows an exemplary configuration of <figref idref="DRAWINGS">FIG. 9</figref>, wherein each individual switch includes a plurality of controller devices;</p>
<p id="p-0057" num="0063"><figref idref="DRAWINGS">FIG. 11</figref> shows an array of controller devices communicating with a pair of fiber channel switches for feeding a high-speed network channel;</p>
<p id="p-0058" num="0064"><figref idref="DRAWINGS">FIG. 12A</figref> shows an exemplary view of the controller device according to the present invention;</p>
<p id="p-0059" num="0065"><figref idref="DRAWINGS">FIG. 12B</figref> shows an exemplary configuration of the controller device according to a switched based fabric configuration according to the present invention;</p>
<p id="p-0060" num="0066"><figref idref="DRAWINGS">FIG. 13</figref> shows an exemplary configuration of the controller device according to a carrier class configuration according to the present invention;</p>
<p id="p-0061" num="0067"><figref idref="DRAWINGS">FIG. 14</figref> shows an exemplary configuration of the controller device according to a Host Based Adapter (HBA) configuration according to the present invention;</p>
<p id="p-0062" num="0068"><figref idref="DRAWINGS">FIG. 15</figref> shows an exemplary configuration of an array of controller devices used in the streaming of data blocks, wherein at least one of the controller devices receives request, such as an HTTP request, from a server and distributes the load of the request across multiple controller devices according to an embodiment of the present invention;</p>
<p id="p-0063" num="0069"><figref idref="DRAWINGS">FIG. 16</figref> illustrates a block diagram describing communications that occur between the various nodes on the network for streaming data to the client, in accordance with the present invention;</p>
<p id="p-0064" num="0070"><figref idref="DRAWINGS">FIG. 17A</figref> illustrates an embodiment of the invention, according to step <b>304</b> of <figref idref="DRAWINGS">FIG. 16</figref> in which the server <b>12</b> communicates with a controller device <b>100</b>′ which is located on another SAN <b>141</b>′ through a mitigating controller card <b>100</b> located on SAN <b>141</b> over a BEN <b>15</b>; and</p>
<p id="p-0065" num="0071"><figref idref="DRAWINGS">FIG. 17B</figref> illustrates an embodiment of the invention, according to step <b>304</b> of <figref idref="DRAWINGS">FIG. 16</figref>. in which the request message is sent directly to a controller device <b>100</b> on SAN <b>141</b> which communicates with a controller device <b>100</b>′ which is located on SAN <b>141</b>′ over a BEN <b>15</b>.</p>
</description-of-drawings>
<?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?>
<?DETDESC description="Detailed Description" end="lead"?>
<heading id="h-0005" level="1">DESCRIPTION OF THE SPECIFIC EMBODIMENTS</heading>
<p id="p-0066" num="0072">In the following description, although the use of a packet-switched network which transports only fixed size cells is described, the following can easily be adapted for use in networks which transport variable size packets or in circuit switched network.</p>
<p id="p-0067" num="0073">The present invention includes two components, the first component includes the messaging scheme (hereinafter, “Messaging Protocols”) for communicating the video data or other data content from the controller card to the client with the intermittent interaction by the host or server through the switch, and the second component is the hardware and software functionality provided in the controller devices, switches and servers for effecting th messaging schemes (hereinafter, “Network Architecture”).</p>
<p id="p-0068" num="0074">(1) Messaging Protocols</p>
<p id="p-0069" num="0075">The messaging schemes of the present invention use a number of different messaging protocols used in the transmission of data across the network. The term “messaging protocol” is defined to include the protocol, or combination of protocols, used in the communication of a request packet sent from the client to the server, or the client to the controller device, and the control message or portion thereof between the server and the controller device, and the data packets from the controller device to the client (hereinafter, “nodes”).</p>
<p id="p-0070" num="0076">Typical examples of messaging protocols used in accordance with the present invention for sending messages between the various nodes on the network include: Small Computer System Interface (SCSI), Fiberchannel (FC),), Infiniband (IB), Gigabit Ethernet (GE), Ten Gigabit Ethernet (10GE), and Sonet. Although, it should be apparent to one skilled in the art that the invention is not restricted to only the protocols listed above, any messaging protocol, or combination of messaging protocols which allows for the communication of messages between the various nodes of the network are intended to be within the scope of the invention.</p>
<p id="p-0071" num="0077">Small Systems Computer Interface (SCSI) Protocols:</p>
<p id="p-0072" num="0078">Originally, an American National Standard Institute (ANSI) standard, SCSI protocols were introduced as an open standard for networked storage services that enabled an n-tier approach to the scale of storage capacity.</p>
<p id="p-0073" num="0079">Referring to <figref idref="DRAWINGS">FIGS. 1A-1D</figref>, a comparison of the various SCSI architectures is listed for both Ultra SCSI and Wide Ultra SCSI for SCSI-1, SCSI-2 and SCSI-3.</p>
<p id="p-0074" num="0080">In particular, a 320 Mbytes per second SCSI bus standard has been submitted to ANSI under SCSI Parallel Interface-4 (SPI-4) Features, defined by SPI-4. It is anticipated that products supporting this specification will be shipping by mid-2001.<sup>1 </sup></p>
<p id="p-0075" num="0081">To overcome the distance sacrifice that faster versions of the SCSI parallel bus interface were subject to, SCSI-3 set out to add new functionality in the form of longer distance separation between devices, serialized transport mechanisms, and network-capable software protocols. In order to accommodate all of these features and maintain backward compatibility, SCSI-3 expanded to become a “family” of standards, categorized as either a logical or a physical interface.</p>
<p id="p-0076" num="0082">Referring to <figref idref="DRAWINGS">FIG. 2</figref>, the scope of the SCSI-3 architecture is illustrated. As shown in <figref idref="DRAWINGS">FIG. 2</figref>, three physical interfaces are defined by the SCSI Architecture Model (SAM) of SCSI-3 that enable serialized transport of SCSI channel protocol traffic: 1394 (also known as Fire Wire), Serial Storage Architecture (SSA), and Fiber Channel. Fiber Channel has emerged as an open standard that virtually all providers of storage solutions are implementing to enable a serial transport for SCSI.</p>
<p id="p-0077" num="0083">Fiber Channel Protocols:</p>
<p id="p-0078" num="0084">Fiber Channel is unique among broadly adopted open standard transports in that it accommodates data traffic that includes both channel and networking protocols.</p>
<p id="p-0079" num="0085">Referring to <figref idref="DRAWINGS">FIG. 3A</figref>, a typical Fiber Channel Protocol Stack is shown. Fiber Channel provides a high-speed physical layer and a low latency data link mechanism that is well suited for the demands of storage I/O applications. Furthermore, it is specifically designated to transparently support upper-layer transport protocols such as the SCSI command protocol. It also offers improved performance versus the SCSI physical and data link standards.</p>
<p id="p-0080" num="0086">Referring to <figref idref="DRAWINGS">FIG. 3B</figref>, a comparison is given of the various Fiber Channel Protocols. Fiber Channel enables link distances of up to 10 kilometers between any two servers, storage, or network infrastructure devices. This compares to a maximum radius of 25 meters for all devices when using SCSI bus technology. Fiber Channel supports a range of link media and transceiver types depending on the distance requirement of individual links. Link media types range from twin-axial copper to multi-mode fiber optic and single-mode fiber optic cable. Transceiver types range from electrical for copper media to short wavelength lasers and long wavelength lasers for fiber optic cable.</p>
<p id="p-0081" num="0087">Infiniband Protocols:</p>
<p id="p-0082" num="0088">The goal of Infiniband is to replace PCI's contentious shared-bus topology with a switched fabric architecture. This seemingly simple design offers increased system performance, enhanced reliability, greater availability and independent scalability of fabric elements. In fact, by replacing the shared-bus architecture with Infiniband Technology, servers have the flexibility to remove I/O from the server chassis, creating greater server density. Furthermore, the removal of the I/O from the server chassis allows for a more flexible and scalable data center as independent fabric nodes may be added based on individual need. Performance is increased by the switched fabric nature of the infrastructure so that applications don't contend for bandwidth as in a shared-bus environment.</p>
<p id="p-0083" num="0089">Referring to <figref idref="DRAWINGS">FIG. 4A</figref>, a typical InfiniBand Server Architecture is shown. The InfiniBand architecture defines an architecture that enables remote DMA (RDMA) or channel oriented communication, making it an ideal solution for server clustering. The adapters that attach nodes to an InfiniBand fabric execute this mapping via a hardware-oriented link protocol that offloads transport functionality from the host, resulting in minimal CPU utilization. Infiniband link protocol defines two layers of communication for each data transaction. The core fabric element of data transfer is the packet, a routable unit of transfer that can support a wide range of Maximum Transmission Units (MTUs). These packets are typically multiplexed into a logical format for transport using messages, each mapped to one of 16 virtual lanes on a link that provides for flow control over a serial transport. The fabric may support links including a varying number of virtual lanes, although mapping algorithms are defined in the specification that allow for interoperability between unlike links.</p>
<p id="p-0084" num="0090">In executing transactions, a working list of messages is compiled in memory tables and scheduled for delivery, and information is transferred between any two nodes in the form of full-duplexed exchanges known as queue pairs. As an added management, Infiniband specifies a means by which to define reliable Quality of Service metrics for each queued transaction based upon the application for which the data transfer is associated.</p>
<p id="p-0085" num="0091">Referring to <figref idref="DRAWINGS">FIG. 4B</figref>, a typical Infiniband Fabric Architecture is shown. Infiniband design parameters provide for the creation of highly scalable “sub networks”, groups of nodes that lie within a 300-meter diameter. These subnetworks (“subnets”) are based upon a fabric-centric configuration, and Infiniband provides a transport by which interprocessor storage I/O, or network I/O traffic can be interconnected through a single I/O controller. Like Fiber Channel, Infiniband establishes channels between any two points in the subnet, utilizing a high-throughput, highly redundant, mesh-fabric switching architecture. Nodes attached to the fabric can be assembled into logical subsets or partitions in order to group hosts or devices with like attributes, much like zoning capabilities of Fiber Channel fabrics. For example, a Storage Service Provide (SSP) may choose to zone a particular subnet of clients requiring streaming video into one zone, and another subnet of clients into another zone.</p>
<p id="p-0086" num="0092">One of the nodes in a subnet, typically a fabric switch, serves the function of subnet manager that configures all attached devices and constantly pings connected nodes to ensure their readiness to send or receive datagrams. Between subnets, inter-networking of Infiniband traffic can be routed using the addressing scheme provided by IP version 6 (Ipv6). Although message delivery transpires at the transport layer, routing is done at the network layer through the use of global routing with packets, which allows for identifying and controlling IPC and I/O processes. Should a TCP/IP-oriented data stream from the Internet reach the router of an InfiniBand data center, delivery of the datagram to the appropriate node with the Infiniband fabric could be expedited by the presence of a TCP/IP offload engine in the firewall or the router.</p>
<p id="p-0087" num="0093">Referring back to <figref idref="DRAWINGS">FIG. 4A</figref>, a typical InfiniBand Architecture <b>20</b> includes one or more Central Processing Units (CPUs) <b>30</b>, a Memory Controller <b>28</b>, a Host Interconnect <b>29</b>, a Host Channel Adapter (HCA) <b>22</b>, a Target Channel Adapter (TCA) <b>24</b>, and one or more Switches <b>26</b>.</p>
<p id="p-0088" num="0094">The HCA <b>22</b> is typically an adapter installed within the host, server or controller device of the fabric. The HCA <b>22</b> typically connects the memory controller <b>28</b> in the host, server or controller device to the network. HCA (<b>22</b>) supports all of the functions that are necessary to provide varying degrees of service for data delivery. Additionally, the HCA <b>22</b> provides security features that protect the memory regions on both ends of a link. Further, the HCA <b>22</b> provides a link protocol engine that implements the protocol of an Infiniband link in hardware. The link protocol engine is typically being developed as functional blocks to be integrated into system chips such as CPUs <b>30</b>, and as stand alone devices to assist in the migration from legacy systems and target side applications.</p>
<p id="p-0089" num="0095">Typically, the HCA <b>22</b> is connected to a plurality of CPUs <b>30</b> across the Host Interconnect <b>29</b>. At least one memory controller <b>28</b> provides a connection to a memory module <b>25</b>.</p>
<p id="p-0090" num="0096">The TCA <b>24</b> is an adapter that attaches to the end nodes of the fabric. TCAs <b>24</b> typically only implement what is required to minimally support both the fabric and any capability that is specific to the device in which it is embedded. For example, if the TCA <b>24</b> is embedded into a storage array, Fiber Channel processes that support internal disk drives are typically buried in the TCA <b>24</b> and bridged to Infiniband. Like the HCA <b>22</b>, a link protocol engine is required on the target end, as is a work queue engine having both memory and channel functionality.</p>
<p id="p-0091" num="0097">The functionality of the switches <b>26</b> in an Infiniband fabric revolves around routing only packets to the nodes within a subnet. As a result, the cost of these devices is not as inhibiting due to the reduced complexity. Typically, these switches <b>26</b> include a forwarding table that establishes where incoming packets are to be routed based upon the level of service for the traffic. They also serve to maintain partitions within the fabric to segment data paths.</p>
<p id="p-0092" num="0098">Ethernet Protocols:</p>
<p id="p-0093" num="0099">Referring to <figref idref="DRAWINGS">FIG. 5</figref>, a typical Ethernet Protocol Stack <b>50</b> is shown. Ethernet defines a Layer <b>2</b> addressing scheme, working at both the physical and data link layers of the Open Systems Interconnection (OSI) standard. The Ethernet Protocol Stack <b>50</b>) typically includes an Application Layer <b>52</b>, a Transport Layer <b>54</b>, a Network Layer <b>56</b>, a Link Layer <b>58</b> and a Physical Layer <b>60</b>. The Application Layer <b>52</b> includes the Hypertext Transfer Protocol (HTTP) according to one embodiment of the invention.</p>
<p id="p-0094" num="0100">Furthermore, Ethernet works in conjunction with TCP/IP which resides at Layers <b>3</b> and higher to execute both a local and wide-area networking implementation. Just as TCP/IP breaks information down into packets for transmission, data has to be segmented in the LAN to ensure equivalent bandwidth availability to all host on the network.</p>
<p id="p-0095" num="0101">Towards this end, each computer attached to the Ethernet LAN, which includes both the clients <b>10</b>) and server <b>12</b> (e.g., with reference to <figref idref="DRAWINGS">FIG. 7</figref>, transmits information in a way that accommodates a predefined packet size called a Maximum Transmission Unit (MTU), the value of which is 1,518 bytes for Ethernet. These packets are addressed at the LAN level using an Ethernet header that contains a unique MAC address. Every computer that attaches to an Ethernet network has a unique 48-bit MAC address, distinct from the 32-bit IP address, to determine which node on the LAN is the correct destination for the packet.</p>
<p id="p-0096" num="0102">Referring to <figref idref="DRAWINGS">FIG. 6</figref>, a typical ethernet packet <b>70</b> is shown. The packet <b>70</b> typically includes the following components: an ethernet header component <b>72</b>, an IP header component <b>74</b>, a transport header component <b>76</b>, and a payload or data component <b>78</b>. The payload component <b>78</b> also typically includes a trailer portion <b>79</b>. Each of the header components <b>72</b>-<b>78</b> of the Ethernet packet <b>70</b> corresponds to a layer of the Ethernet Protocol Stack <b>50</b>. For example, the Link Layer <b>58</b> includes the Ethernet header component <b>72</b> which is typically a 48-bit MAC address. The Network Layer (<b>56</b>) includes the IP header component <b>74</b> which is typically a 32-bit IP address. The IP address provides the source and destination of the particular messaging packet as is concerned with routing the packet between the various nodes on the network. The Transport layer <b>54</b> includes the Transport header component <b>76</b> or Transmission Control Protocol (TCP) which is involved in the construction of the data packets. The typical size of the Transport header component <b>76</b> is 30-40 bits. The Application Layer <b>52</b> includes the payload or data <b>78</b> which is sent down the wire between the various nodes on the network. The size of the packet varies depending upon the type and content of the data. Finally, the Physical Layer <b>60</b> includes the physical hardware used to send the Ethernet packets between the various nodes of the network. This includes the various devices and the actual physical wires used to transmit the message.</p>
<p id="p-0097" num="0103">The payload component <b>78</b> of the ethernet packet <b>70</b> can include audio, video, text, or any other type of binary data. However, according to streaming audio and video data in accordance to at least one embodiment of the invention the payload component <b>78</b> includes audio and video data formats. Any type of audio or video format that is supported by the ethernet protocol is within the scope of the invention.</p>
<p id="p-0098" num="0104">Referring back to <figref idref="DRAWINGS">FIG. 6</figref>, a typical data format for an audio and video payload is shown. In one embodiment, MPEG-II is used for transporting data payload <b>78</b>, carrying one video and one audio channel as a video content. Each second of video data is compressed into 4 Megabits of digital data. The payload <b>78</b> is comprised of sequential 4 Megabit packets <b>82</b>. Each packet <b>82</b> is preferably streamed over the network <b>14</b> (e.g., <figref idref="DRAWINGS">FIG. 7</figref>) from a controller device <b>100</b> to a client <b>10</b> initiating the request for audio and video data, the details of which will be described hereinafter.</p>
<p id="p-0099" num="0105">Referring to <figref idref="DRAWINGS">FIG. 7</figref>, a data communication network <b>1</b> is shown, in accordance with the present invention. The network <b>1</b> includes a storage controller device (hereinafter, “controller device”) <b>100</b>, which typically provides control over storage functions and network access. In one embodiment, the network <b>1</b> includes one or more clients <b>10</b>, at least one server (<b>12</b>), a network <b>14</b> connecting the clients <b>10</b> to the server <b>12</b>. A typical network <b>14</b> includes one of a Local Area Network (LAN), a Wide Area Network (WAN), a Metropolitan Area Network (MAN), and a Storage Area Network (SAN) (hereinafter, collectively called “FEN”) or any other network that provides for communication between the client and the server.</p>
<p id="p-0100" num="0106">Network <b>1</b> also includes one or more substorage devices <b>16</b>, which are typically, an array of disk or tape drives. The substorage devices <b>16</b> are connected to the controller device <b>100</b> over a network, which is typically a Storage Area Network (SAN). The controller device <b>100</b> is preferably included in a switch <b>18</b> used in communicating between the various nodes on the network <b>1</b>. It should be appreciated that the controller device <b>100</b> may be implemented in a router, bridge or other network device.</p>
<p id="p-0101" num="0107">Referring to <figref idref="DRAWINGS">FIG. 8</figref>, which shows an alternate embodiment, a plurality of controller devices <b>100</b> are included in a switch device <b>18</b>, with each controller device <b>100</b> connected to the others over an interconnect medium such as a Host Bus Adapter (HBA) interface, which is typically a Peripheral Computer Interface (PCI).</p>
<p id="p-0102" num="0108">Referring to <figref idref="DRAWINGS">FIG. 9</figref>, which shows another alternate embodiment, a plurality of switches <b>18</b> are provided, each switch <b>18</b> having at least one controller card <b>100</b> communicating with the controller devices <b>100</b> on the other switches <b>18</b>.</p>
<p id="p-0103" num="0109">Referring to <figref idref="DRAWINGS">FIG. 10</figref>, which shows another alternate embodiment, each one of the switches <b>18</b> includes a plurality of controller devices <b>100</b>. Each of the controller devices <b>100</b> on a switch <b>18</b> is able to communicate over the switch fabric <b>109</b> with each and every other controller card <b>100</b> on the other switches <b>18</b>. Alternatively, each of the controller devices <b>100</b> are able to communicate with each and every other controller device <b>100</b> over network <b>141</b>.</p>
<p id="p-0104" num="0110">In one embodiment, the storage management and administration functionality of the server <b>12</b> is integrated on the controller device <b>100</b>, such that a client <b>10</b> may communicate directly with the controller device <b>100</b> through the FEN <b>14</b> without involving the server <b>12</b>.</p>
<p id="p-0105" num="0111">In this embodiment, referring to <figref idref="DRAWINGS">FIG. 12B</figref>, the controller device <b>100</b> includes a Central Processing Unit (CPU) <b>102</b>, a cache memory module <b>104</b>) used as a data cache and for supporting processing and a system interconnect interface <b>103</b> for connecting the CPU <b>102</b>, cache memory <b>104</b> and communication ports <b>109</b>. The controller device <b>100</b> typically includes at least one communication port <b>109</b> used to communicate with the external network, other controller devices or other peripherals. For example, the communication port <b>109</b> may be comprised of a SAN port <b>111</b> for communication with physical disks <b>110</b>, a WAN port <b>107</b> used to connect with the wide area network (WAN), a server port <b>108</b> used to connect with servers, and at least one additional communication port <b>109</b> used for communication between a plurality of controller devices <b>100</b>. The system interface <b>103</b> may typically be comprised of a Field Programmable Gate Array (FPGA) or Application Specific Integrated Circuit (ASIC). An optional computation engine <b>112</b> may be included on the system interface <b>103</b> which is used to accelerate operations such as RAID check sum calculations, encryption, and compression, and assorted data routing and support chips.</p>
<p id="p-0106" num="0112">As mentioned previously, more than one of the above communication ports <b>106</b>-<b>109</b> may be combined into a single communication port supporting many different protocols. For example, the communication port <b>107</b> used to connect the controller device to the server, and the communication port <b>108</b> used to connect the controller device <b>100</b> to a disk drive peripheral <b>110</b>, and the communication port <b>109</b> used to connect the controller device <b>100</b> to another controller device <b>100</b> may all be a Fiber Channel (FC) communication ports. It should be appreciated to one skilled in the art that the term “communication port” is intended to be construed broadly to include any combination and sub combination of different communication ports for allowing the communication of messaging protocols over the network.</p>
<p id="p-0107" num="0113">Also, it should be appreciated that many modifications and configuration changes are intended to be within the scope of the invention for designing the controller devices <b>100</b> in accordance with the present invention. As disclosed in <figref idref="DRAWINGS">FIG. 12B</figref>, the hardware configuration of the controller device <b>100</b> is implemented in a controller card (e.g., “blade”). The actual hardware configuration may be modified depending upon the optimization of the application(s) required by the client upon the network configuration. U.S. Pat. No. 6,148,414, which is hereby incorporated by reference in its entirety for all purposes, provides useful controller card and device configurations for embodiments of the present invention.</p>
<p id="p-0108" num="0114">In one embodiment, shown in <figref idref="DRAWINGS">FIG. 11</figref>, two Fiber Channel gateway switches <b>120</b> are used to communicate with a plurality disks <b>16</b> via dual redundant Fiber Channel switches <b>18</b>. In one embodiment, the interconnect medium <b>124</b> between the disks <b>16</b> and each switch <b>120</b> and the interconnect medium <b>126</b> between each switch <b>120</b> and the controller device <b>100</b> is typically 1-2 Gigabit Fiber Channel interconnect. The interconnect medium <b>129</b> between the controller devices <b>100</b> and the high-speed network <b>130</b> is typically a 10 Gibabit or higher network interconnect.</p>
<p id="p-0109" num="0115">Each controller device <b>100</b> communicates with every other communication device <b>100</b> over the interconnect medium <b>126</b>. The distributed nature of the controller device architecture allows the aggregate of the array of controller devices <b>100</b>, each of which has a 1-2 Gigabit FC stream into each controller device on the SAN side, to provide a 10 Gigabit or higher stream coming out of the array of controller devices <b>100</b> on the FEN side.</p>
<p id="p-0110" num="0116">Many form factors for the implementation of the controller devices <b>100</b> on the network are possible. For example, in one embodiment, one form factor (with reference to <figref idref="DRAWINGS">FIG. 12</figref>), the “Switch Blade” <b>120</b> implementation, includes multiple controllers <b>100</b> integrated directly into a high-speed switch <b>18</b>. In <figref idref="DRAWINGS">FIG. 7</figref>, the interface between the controller device <b>100</b> and the switch <b>18</b> includes a bus <b>122</b>) using the appropriate messaging protocol. Examples of appropriate messaging protocols include PCI, Infiniband, Utopia, etc. One advantage of the Switch Blade <b>120</b> implementation results from direct access to the internal cross communication fabric <b>145</b> in a high-speed switch. This is ideal for communicating between the controller cards <b>100</b> as it typically provides extremely low latency and high bandwidth. It is also a convenient interface for driving high-speed networks in the manner described below. In the Switch Blade <b>120</b> implementation, communication to the server <b>12</b> is either though the switch <b>18</b> using protocols such as SCSI over IP (iSCSI) or via a Fiber Channel Network.</p>
<p id="p-0111" num="0117">In another embodiment, a second form factor (e.g., <figref idref="DRAWINGS">FIG. 13</figref>), the Carrier Class Implementation (CCI) <b>130</b>, includes multiple controller devices, each residing physically in a rack or chassis <b>132</b> that is independent of the network switches <b>18</b>. A communication port <b>109</b> on each controller device provides for communication to a standard high-speed network, such as 10 Gigabit Ethernet, OC192 Sonet, OC768 Sonet, Fiber Channel or Infiniband which is connected to the Wide Area Network (WAN) <b>14</b>. One advantage of this implementation is that no cooperative development is required with switch manufacturers.</p>
<p id="p-0112" num="0118">Referring to <figref idref="DRAWINGS">FIG. 13</figref>, the CCI implementation includes a plurality of controller cards. The controller cards may be connected to different networks depending upon the communication port <b>109</b> provided on the controller card. For example, the controller device may include a WAN communication port. The term “WAN Blade” is used to define a controller device <b>134</b> that includes at least one communication port <b>109</b> for connection to the WAN <b>14</b>. Similarly, the term “LAN Blade” is used to define a controller device <b>136</b> that includes at least one communication port <b>109</b> for connection to the LAN <b>121</b>. The LAN Blade <b>136</b> is typically connected to a LAN switch for communication with a server <b>12</b>. Furthermore, a controller device which is used for connection to the SAN, is defined as a “SAN Blade” <b>138</b>. The SAN Blade <b>138</b> is typically connected to a Fiber Channel (FC) Switch <b>123</b> for communication with one or more storage devices <b>16</b> such as an array of disks, for example. Both, the WAN Blade <b>134</b> and the LAN Blade <b>136</b>) may be connected by an external server <b>12</b>′ for communication between each other.</p>
<p id="p-0113" num="0119">It should be apparent to one skilled in the art, that the controller devices of the present invention may include a communication port <b>109</b> for connection to other networks, including any proprietary and non-proprietary networks. Also, it is intended to be within the scope of the invention that the controller devices can include more than one communication port <b>109</b>. Furthermore, a single controller device <b>100</b> may include different communication ports <b>109</b> for communication to different networks. For example, a single controller device may include a first communication port for communication to a LAN, and a second communication port for communication to a WAN.</p>
<p id="p-0114" num="0120">In another embodiment, yet another form factor, as shown in <figref idref="DRAWINGS">FIG. 14</figref>, the Host Based Adapter (HBA) implementation <b>140</b>, one or more controller devices <b>100</b> reside inside the host or server <b>12</b> and are interconnected via the host input/output bus <b>169</b>, typically a PCI. The interface to the FEN is via ports <b>109</b> coupled to the I/O buss <b>169</b> via a controller card <b>100</b>.</p>
<p id="p-0115" num="0121">At step <b>300</b>, a request is sent from the client <b>10</b> to a controller device <b>100</b>, or to server <b>12</b> communicating with the client <b>10</b>, over the FEN <b>14</b>. The messaging protocol <b>301</b> is typically an ethernet packet <b>70</b>, such as an HTTP request. At step <b>302</b>, the request is received by the controller device <b>100</b>, or the server <b>12</b>, and processed. At step <b>304</b>, a notification message <b>303</b> is sent from the server <b>12</b>, or from controller device <b>100</b>, to the appropriate controller device <b>100</b>′ on the SAN <b>141</b> (e.g., <figref idref="DRAWINGS">FIG. 14</figref>) that is responsible for streaming the required data to the client <b>10</b>. The controller device <b>100</b> or server <b>12</b> typically includes an HTTP look-up engine for locating which controller device <b>100</b>′ has the required data.</p>
<p id="p-0116" num="0122">Models for driving fast data streams include a host controlled model and a controller controlled model. In the Host Controlled model (e.g., <figref idref="DRAWINGS">FIG. 7</figref>), a request for data typically originates from a client <b>10</b> and is communicated via the FEN and is directed to a server <b>12</b>. The server <b>12</b> recognizes the request to be a large block of data. Rather than simply reading the data into the server <b>12</b> and then transmitting the requested data to the client <b>10</b>, the server <b>12</b> sends a Streaming Data Request (SDR) to the controller device <b>100</b>. The SDR includes the target address of the client <b>10</b>, the messaging protocol (such as HTTP, FTP or RTSP), any optional delivery protocol parameters, the virtual volume identifier, the filename of the request, the file offset of the request, and the length of the request. The SDR may optionally also include the delivery encryption method, the delivery encryption key, the delivery compression method, and delivery compression parameters. The data may also be encrypted on the disk subsystem, therefore the SDR may also include the point of presence (POP) encryption key. The SDR is preferably encrypted in untrusted environments and is delivered from the host to a Streaming Manager (SM) running on the controller device <b>100</b> via a Remote Procedure Call (RPC) mechanism. If the Streaming Request requires a delivery bandwidth that exceeds the speeds by which data can be extracted from the disk, then the Streaming Manager decomposes the request into a list of disk blocks that must be delivered. These disk blocks are stored on multiple physical disks in a rotating order. This approach is called striping and is commonly used in disk subsystems. Each of the physical disks <b>110</b> has independent fiber channel links into the fiber channel network. This allows multiple controller devices <b>100</b> to read independent blocks of data in parallel, thereby producing a high aggregate input rate from disk to the collection of controllers. Therefore, the streaming manager sends a number of other controller devices <b>100</b> an ordered list of blocks that that controller device <b>100</b> have responsibility for in the streaming operation.</p>
<p id="p-0117" num="0123">Referring to <figref idref="DRAWINGS">FIG. 15</figref>, for example, each controller device initially reads their initial blocks of data into memory located on the individual controllers. The request to read the data is received from a server <b>12</b>. Typically, the request may be made from an HTTP Server using an RPC request to the first controller device <b>150</b>. If decryption from POP, encryption for delivery or compression is required, this is done by each individual controller on the blocks it has read producing a delivery ready block. The first controller <b>150</b> then sends its block <b>160</b> at the high-speed rates. As soon as the first controller device <b>150</b> has finished, a short synchronization message <b>170</b> is sent from the first controller <b>150</b> to the second controller <b>151</b> to start the delivery of the second data block <b>161</b>. This process is repeated for the array of remaining controller devices <b>151</b>-<b>153</b> participating in the streaming process. While the second controller <b>151</b> and subsequent controllers <b>152</b> and <b>153</b> are delivering their data block <b>162</b> and <b>163</b> to the FEN, the first controller device <b>151</b> is reading and applying encryption and decompression operations on the next block in its assigned sequence so that by the time the last controller device, e.g., device <b>153</b>) in the controller array has delivered its data block to the FEN, the first controller <b>151</b> has read, at low speed, the next packet in its list and is ready to deliver the packet at high speed to the FEN. This rotating sequence of writes overlapped with pipelined reads is repeated until all blocks in the SDR are delivered. For example, the first controller device <b>151</b> and the second controller device <b>152</b> begin streaming the data block <b>5</b> <b>164</b> and data block <b>6</b> <b>165</b> while the third controller devices <b>152</b> and the fourth controller device <b>153</b> apply encryption to the subsequent data blocks. Data blocks can also be cached in the memory of the controller devices to allow deliver of subsequent requests from other clients without reading from disk, thus increasing the amount of concurrent streaming requests deliverable at any given time.</p>
<p id="p-0118" num="0124">Referring to <figref idref="DRAWINGS">FIG. 15</figref>, it should be apparent to one skilled in the art that the configuration may include any number, N, of controller devices as well as any configuration of these controller devices within one or more switches for providing the desired I/O stream of data blocks depending upon the client application. As discussed previously, it should also be apparent that many different messaging protocols may be used in the streaming of the data blocks.</p>
<p id="p-0119" num="0125">Note that the above streaming operation can also be reversed whereby the client writes a large block of data. This requires a single server to be the recipient of the data and to forward it to an array of controller devices for writing to disk. However, given the large size of each cache <b>104</b> of <figref idref="DRAWINGS">FIG. 12B</figref> on the controller device in one embodiment (e.g., up to 8 gigabytes per controller device), it is simpler to buffer an incoming write in cache and write it at a more leisurely pace after the fact. The write operation may, however, be mirrored to other controllers to ensure there is no data loss due to controller failure, in which case the process of writing the data to disk can be distributed across multiple controllers. U.S. Pat. No. 6,148,414, which was previously incorporated by reference, provides useful techniques for mirroring to other controllers.</p>
<p id="p-0120" num="0126">In a Controller Controlled Model (CCM), each of the controller devices executes a request engine, such as Hyper Text Transfer Protocol (HTTP), File Transfer Protocol (FTP), or Real Time Streaming Protocol (RTSP), directly on the controller device. In one embodiment, all requests come from the client directly to a controller device. All such requests are handled by the controller device without server intervention except perhaps requests requiring client authentication (e.g. a password) or requests requiring user executable code (e.g. a cgi/bin request). User executable code generally cannot be allowed to execute on the controller device because it violates the security model by allowing the potential of unauthorized access to the underlying subsystem (hacking). A daemon runs on the server that cooperates with the controller-based request engine via an encrypted socket. Therefore, these types of requests are typically executed on the server and the results returned to the request engine running on the controller device. Large streaming requests are delivered in the FENusing an array of device controllers in the same way as in the Host Controlled model described above.</p>
<p id="p-0121" num="0127">The communication conduit between server and the controller device described above assumes the Switch Blade implementation. In the Carrier Class implementation, the communication occurs via a control messages delivered via fiber channel packets. In the HBA implementation, a control message to the HBA device driver causes a message to be delivered over the PCI buss. These implementations deliver the same effect as with the Switch Blade implementation.</p>
<p id="p-0122" num="0128">It should be appreciated that the above implementations also allow streaming to slower networks by deploying only a single controller device per stream, and that requests from multiple clients can be handled simultaneously by a pool of device controllers.</p>
<p id="p-0123" num="0129">Referring to <figref idref="DRAWINGS">FIG. 16</figref>, here is shown a block diagram illustrating a process for streaming data to the client, in accordance with an embodiment of the present invention.</p>
<p id="p-0124" num="0130">At step <b>300</b>, a request is sent from the client <b>10</b> to a controller device <b>100</b>, or to server <b>12</b> communicating with the client <b>10</b>, over the FEN <b>14</b>. The messaging protocol <b>301</b> is typically an ethernet packet <b>70</b>, such as an HTTP request. At step <b>302</b>, the request is received by the controller device <b>100</b>, or the server <b>12</b>, and processed. At step <b>304</b>, a notification message <b>303</b> is sent from the server <b>12</b>, or from controller device <b>100</b>, to the appropriate controller device <b>100</b>′ on the SAN <b>141</b> that is responsible for streaming the required data to the client <b>10</b>. The controller device <b>100</b> or server <b>12</b> typically includes an HTTP look-up engine for locating which controller device <b>100</b>′ has the required data.</p>
<p id="p-0125" num="0131">Referring to <figref idref="DRAWINGS">FIG. 17A</figref>, according to an embodiment for the invention, at step <b>304</b>, the controller device <b>100</b>, or the server <b>12</b>, communicates with the controller device <b>100</b>′ which is located on another SAN <b>141</b>′. The server <b>12</b> typically forwards the notification message <b>303</b> over a Back-End Network (BEN) <b>15</b> which is connected to the SAN <b>141</b>′. The BEN <b>15</b> may include a FEN <b>14</b>, but is distinguishable by the original FEN <b>14</b> in that the BEN <b>15</b> may be geographically separated from the FEN <b>14</b>. For example the FEN <b>14</b> may be located in the continent of North America wherein the BEN <b>15</b> may be in Europe. The communication interconnect between the two networks may include an optical fiber, for example, but may be comprised of other mediums.</p>
<p id="p-0126" num="0132">Referring to <figref idref="DRAWINGS">FIG. 17B</figref>, in one embodiment of the invention, at step <b>304</b>, a receiving controller device <b>100</b> located on a SAN <b>141</b> communicates with a controller device <b>100</b>′ which is located on another SAN <b>141</b>′. The controller device <b>100</b> forwards the notification message <b>303</b> over a Back-End Network (BEN) <b>15</b> which is connected to the SAN <b>141</b>′. The BEN <b>15</b> may include a FEN <b>14</b>, but is distinguishable from the original FEN <b>14</b> in that the BEN <b>15</b> may be geographically separated from the FEN <b>14</b>. For example the FEN <b>14</b> may be located in the continent of North America wherein the BEN <b>15</b> may be in Europe. The communication interconnect between the two network may include an optical fiber, for example, but may be comprised of other mediums.</p>
<p id="p-0127" num="0133">At step <b>306</b>, the request sent to the server (<b>12</b> or controller device <b>100</b> is answered and sent back to the client <b>10</b> requesting the data <b>82</b>. The message <b>307</b> is typically an ethernet packet <b>70</b>, such as an HTTP request. Step <b>306</b> may occur concurrently with step <b>304</b>, or may be controlled to occur after the appropriate controller device <b>100</b>′ is notified by signal message <b>305</b>.</p>
<p id="p-0128" num="0134">At step <b>308</b>, the client <b>10</b> sends a request to the controller device <b>100</b>′. The message <b>307</b> is typically an ethernet packet <b>70</b>, such as an HTTP request. The message <b>307</b> is similar to message <b>301</b> of step <b>300</b>, except with the address of the appropriate controller device <b>100</b>′ designated during step <b>304</b>. At step <b>310</b>, the controller device <b>100</b>′ determines whether the requested data <b>82</b> is present in cache <b>104</b>. At step <b>314</b>, if the requested data <b>82</b> is not present in cache <b>104</b> then a data <b>82</b> request message <b>313</b> for the data <b>82</b> is sent to the appropriate substorage device such as an array of disks <b>16</b>. Otherwise, at step <b>312</b> the requested data <b>82</b> is streamed back to the requesting client by the controller device <b>100</b>′.</p>
<p id="p-0129" num="0135">As discussed previously, the control device <b>100</b> or server <b>12</b> sends an RPC request to the controller device <b>100</b>′ during step <b>304</b>. The request is typically an RPC request, as disclosed previously. The controller device <b>100</b>′ may be received by a first controller device <b>100</b>′ which forwards the request to other controller device <b>100</b>′. An array of controller devices <b>100</b> and <b>100</b>′ may be used to complete the streaming operation.</p>
<p id="p-0130" num="0136">While the invention has been described by way of example and in terms of the specific embodiments, it is to be understood that the invention is not limited to the disclosed embodiments. To the contrary, it is intended to cover various modifications and similar arrangements as would be apparent to those skilled in the art. For example, although the controller devices of the present invention are typically implemented in switches as described, it should be apparent that the controller devices may be implemented in routers/bridges, et cetera. Additionally, it should be apparent to one skilled in the art that the present invention is useful for handling compressed or uncompressed video information and other continuous media information like streaming audio, for example. Different audio or video information could be compressed at different rates. For example, music may be compressed at a higher bit rate (lower compression ratio) than voice conversation. A continuous media stream could also consist of several streams of different media types multiplexed together. Therefore, the scope of the appended claims should be accorded the broadest interpretation so as to encompass all such modifications and similar arrangements.</p>
<?DETDESC description="Detailed Description" end="tail"?>
</description>
<us-claim-statement>What is claimed is:</us-claim-statement>
<claims id="claims">
<claim id="CLM-00001" num="00001">
<claim-text>1. A method of delivering streaming data content to a client device from two or more controller devices over a data communication network in response to a request for the data content from the client device, wherein the data content includes two or more blocks of data stored on a storage system, the method comprising:
<claim-text>receiving, by a server, a request from a first client device over the data communication network, the request identifying streaming data content stored on a storage system;</claim-text>
<claim-text>transmitting a data request message from the server to a first controller device associated with the storage system, the data request message identifying the first client device and the data content requested by the first client device;</claim-text>
<claim-text>retrieving a first block of the data content from the storage system by the first controller device;</claim-text>
<claim-text>sending a second data request message from the first controller device to a second controller device associated with the storage system, the second data request message identifying the first client device and a second block of the data content;</claim-text>
<claim-text>retrieving the second block of the data content from the storage system by the second controller device;</claim-text>
<claim-text>transferring the first block of data to the first client device from the first controller device;</claim-text>
<claim-text>sending a synchronization message from the first controller device to the second controller device; and</claim-text>
<claim-text>in response to the synchronization message, transferring the second block of data to the first client device from the second controller device over a communication path that does not include the first controller device, wherein the first and second controller devices transfer the first and second data blocks over the data communication network at a faster rate than the rate at which the first and second data blocks are retrieved from the storage system.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00002" num="00002">
<claim-text>2. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the first and second controller devices are located in geographically remote locations relative to each other.</claim-text>
</claim>
<claim id="CLM-00003" num="00003">
<claim-text>3. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the first and second controller devices communicate over a second data communication network different from the data communication network.</claim-text>
</claim>
<claim id="CLM-00004" num="00004">
<claim-text>4. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the server communicates with the first controller device over the data communication network.</claim-text>
</claim>
<claim id="CLM-00005" num="00005">
<claim-text>5. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the second controller communicates with the first client device over the data communication network.</claim-text>
</claim>
<claim id="CLM-00006" num="00006">
<claim-text>6. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the second controller device communicates with the storage system over a storage area network.</claim-text>
</claim>
<claim id="CLM-00007" num="00007">
<claim-text>7. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the steps of retrieving the data blocks, each include reading the data block from the storage system and applying one of an encryption and a decompression algorithm to the read data block.</claim-text>
</claim>
<claim id="CLM-00008" num="00008">
<claim-text>8. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the first and second controller devices are communicably coupled over a bus.</claim-text>
</claim>
<claim id="CLM-00009" num="00009">
<claim-text>9. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the first and second controller devices are communicably coupled over a storage area network.</claim-text>
</claim>
<claim id="CLM-00010" num="00010">
<claim-text>10. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the first and second controller devices are communicably coupled to the storage system over a storage area network.</claim-text>
</claim>
<claim id="CLM-00011" num="00011">
<claim-text>11. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the first controller device communicates with the storage system over a storage area network.</claim-text>
</claim>
<claim id="CLM-00012" num="00012">
<claim-text>12. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the first controller device is located in a network switch device coupled to the data communication network.</claim-text>
</claim>
</claims>
</us-patent-grant>
