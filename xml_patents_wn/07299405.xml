<us-patent-grant lang="EN" dtd-version="v4.2 2006-08-23" file="US07299405-20071120.XML" status="PRODUCTION" id="us-patent-grant" country="US" date-produced="20071106" date-publ="20071120">
<us-bibliographic-data-grant>
<publication-reference>
<document-id>
<country>US</country>
<doc-number>07299405</doc-number>
<kind>B1</kind>
<date>20071120</date>
</document-id>
</publication-reference>
<application-reference appl-type="utility">
<document-id>
<country>US</country>
<doc-number>09521252</doc-number>
<date>20000308</date>
</document-id>
</application-reference>
<us-application-series-code>09</us-application-series-code>
<classifications-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>G</section>
<class>06</class>
<subclass>F</subclass>
<main-group>17</main-group>
<subgroup>00</subgroup>
<symbol-position>F</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20071120</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>G</section>
<class>06</class>
<subclass>F</subclass>
<main-group>15</main-group>
<subgroup>00</subgroup>
<symbol-position>L</symbol-position>
<classification-value>N</classification-value>
<action-date><date>20071120</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
</classifications-ipcr>
<classification-national>
<country>US</country>
<main-classification>7155001</main-classification>
<further-classification>709204</further-classification>
<further-classification>704500</further-classification>
</classification-national>
<invention-title id="d0e43">Method and system for information management to facilitate the exchange of ideas during a collaborative effort</invention-title>
<references-cited>
<citation>
<patcit num="00001">
<document-id>
<country>US</country>
<doc-number>4578718</doc-number>
<kind>A</kind>
<name>Parker et al.</name>
<date>19860300</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<patcit num="00002">
<document-id>
<country>US</country>
<doc-number>4686698</doc-number>
<kind>A</kind>
<name>Tompkins et al.</name>
<date>19870800</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<patcit num="00003">
<document-id>
<country>US</country>
<doc-number>4787063</doc-number>
<kind>A</kind>
<name>Muguet</name>
<date>19881100</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<patcit num="00004">
<document-id>
<country>US</country>
<doc-number>4905094</doc-number>
<kind>A</kind>
<name>Pocock et al.</name>
<date>19900200</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<patcit num="00005">
<document-id>
<country>US</country>
<doc-number>4963995</doc-number>
<kind>A</kind>
<name>Lang</name>
<date>19901000</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<patcit num="00006">
<document-id>
<country>US</country>
<doc-number>5091931</doc-number>
<kind>A</kind>
<name>Milewski</name>
<date>19920200</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<patcit num="00007">
<document-id>
<country>US</country>
<doc-number>5164839</doc-number>
<kind>A</kind>
<name>Lang</name>
<date>19921100</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<patcit num="00008">
<document-id>
<country>US</country>
<doc-number>5206929</doc-number>
<kind>A</kind>
<name>Langford et al.</name>
<date>19930400</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<patcit num="00009">
<document-id>
<country>US</country>
<doc-number>5265205</doc-number>
<kind>A</kind>
<name>Schroder</name>
<date>19931100</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<patcit num="00010">
<document-id>
<country>US</country>
<doc-number>5321396</doc-number>
<kind>A</kind>
<name>Lamming et al.</name>
<date>19940600</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<patcit num="00011">
<document-id>
<country>US</country>
<doc-number>5475741</doc-number>
<kind>A</kind>
<name>Davis et al.</name>
<date>19951200</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<patcit num="00012">
<document-id>
<country>US</country>
<doc-number>5491511</doc-number>
<kind>A</kind>
<name>Odle</name>
<date>19960200</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<patcit num="00013">
<document-id>
<country>US</country>
<doc-number>5502774</doc-number>
<kind>A</kind>
<name>Bellegarda et al.</name>
<date>19960300</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<patcit num="00014">
<document-id>
<country>US</country>
<doc-number>5530235</doc-number>
<kind>A</kind>
<name>Stefik et al.</name>
<date>19960600</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<patcit num="00015">
<document-id>
<country>US</country>
<doc-number>5535063</doc-number>
<kind>A</kind>
<name>Lamming</name>
<date>19960700</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<patcit num="00016">
<document-id>
<country>US</country>
<doc-number>5537141</doc-number>
<kind>A</kind>
<name>Harper et al.</name>
<date>19960700</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<patcit num="00017">
<document-id>
<country>US</country>
<doc-number>5539665</doc-number>
<kind>A</kind>
<name>Lamming et al.</name>
<date>19960700</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<patcit num="00018">
<document-id>
<country>US</country>
<doc-number>5596581</doc-number>
<kind>A</kind>
<name>Saeijs et al.</name>
<date>19970100</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<patcit num="00019">
<document-id>
<country>US</country>
<doc-number>5610841</doc-number>
<kind>A</kind>
<name>Tanaka et al.</name>
<date>19970300</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<patcit num="00020">
<document-id>
<country>US</country>
<doc-number>5673016</doc-number>
<kind>A</kind>
<name>Lutes</name>
<date>19970900</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<patcit num="00021">
<document-id>
<country>US</country>
<doc-number>5686957</doc-number>
<kind>A</kind>
<name>Baker</name>
<date>19971100</date>
</document-id>
</patcit>
<category>cited by other</category>
<classification-national><country>US</country><main-classification>348 36</main-classification></classification-national>
</citation>
<citation>
<patcit num="00022">
<document-id>
<country>US</country>
<doc-number>5706290</doc-number>
<kind>A</kind>
<name>Shaw et al.</name>
<date>19980100</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<patcit num="00023">
<document-id>
<country>US</country>
<doc-number>5717869</doc-number>
<kind>A</kind>
<name>Moran et al.</name>
<date>19980200</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<patcit num="00024">
<document-id>
<country>US</country>
<doc-number>5717879</doc-number>
<kind>A</kind>
<name>Moran et al.</name>
<date>19980200</date>
</document-id>
</patcit>
<category>cited by other</category>
<classification-national><country>US</country><main-classification>345716</main-classification></classification-national>
</citation>
<citation>
<patcit num="00025">
<document-id>
<country>US</country>
<doc-number>5721878</doc-number>
<kind>A</kind>
<name>Ottesen et al.</name>
<date>19980200</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<patcit num="00026">
<document-id>
<country>US</country>
<doc-number>5729931</doc-number>
<kind>A</kind>
<name>Wade</name>
<date>19980300</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<patcit num="00027">
<document-id>
<country>US</country>
<doc-number>5734719</doc-number>
<kind>A</kind>
<name>Tsevdos et al.</name>
<date>19980300</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<patcit num="00028">
<document-id>
<country>US</country>
<doc-number>5751281</doc-number>
<kind>A</kind>
<name>Hoddie et al.</name>
<date>19980500</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<patcit num="00029">
<document-id>
<country>US</country>
<doc-number>5760767</doc-number>
<kind>A</kind>
<name>Shore et al.</name>
<date>19980600</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>345723</main-classification></classification-national>
</citation>
<citation>
<patcit num="00030">
<document-id>
<country>US</country>
<doc-number>5764789</doc-number>
<kind>A</kind>
<name>Pare, Jr. et al.</name>
<date>19980600</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<patcit num="00031">
<document-id>
<country>US</country>
<doc-number>5767897</doc-number>
<kind>A</kind>
<name>Howell</name>
<date>19980600</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<patcit num="00032">
<document-id>
<country>US</country>
<doc-number>5793365</doc-number>
<kind>A</kind>
<name>Tang et al.</name>
<date>19980800</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>345758</main-classification></classification-national>
</citation>
<citation>
<patcit num="00033">
<document-id>
<country>US</country>
<doc-number>5799150</doc-number>
<kind>A</kind>
<name>Hamilton et al.</name>
<date>19980800</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<patcit num="00034">
<document-id>
<country>US</country>
<doc-number>5802294</doc-number>
<kind>A</kind>
<name>Ludwig et al.</name>
<date>19980900</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>709204</main-classification></classification-national>
</citation>
<citation>
<patcit num="00035">
<document-id>
<country>US</country>
<doc-number>5845261</doc-number>
<kind>A</kind>
<name>McAlbian et al.</name>
<date>19981200</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<patcit num="00036">
<document-id>
<country>US</country>
<doc-number>5854831</doc-number>
<kind>A</kind>
<name>Parsadayan et al.</name>
<date>19981200</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<patcit num="00037">
<document-id>
<country>US</country>
<doc-number>5862292</doc-number>
<kind>A</kind>
<name>Kubota et al.</name>
<date>19990100</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<patcit num="00038">
<document-id>
<country>US</country>
<doc-number>5946654</doc-number>
<kind>A</kind>
<name>Newman et al.</name>
<date>19990800</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<patcit num="00039">
<document-id>
<country>US</country>
<doc-number>5978477</doc-number>
<kind>A</kind>
<name>Hull et al.</name>
<date>19991100</date>
</document-id>
</patcit>
<category>cited by other</category>
<classification-national><country>US</country><main-classification>358403</main-classification></classification-national>
</citation>
<citation>
<patcit num="00040">
<document-id>
<country>US</country>
<doc-number>5986655</doc-number>
<kind>A</kind>
<name>Chiu et al.</name>
<date>19991100</date>
</document-id>
</patcit>
<category>cited by other</category>
<classification-national><country>US</country><main-classification>345839</main-classification></classification-national>
</citation>
<citation>
<patcit num="00041">
<document-id>
<country>US</country>
<doc-number>5987454</doc-number>
<kind>A</kind>
<name>Hobbs</name>
<date>19991100</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<patcit num="00042">
<document-id>
<country>US</country>
<doc-number>5990934</doc-number>
<kind>A</kind>
<name>Nalwa</name>
<date>19991100</date>
</document-id>
</patcit>
<category>cited by other</category>
<classification-national><country>US</country><main-classification>348 36</main-classification></classification-national>
</citation>
<citation>
<patcit num="00043">
<document-id>
<country>US</country>
<doc-number>5991429</doc-number>
<kind>A</kind>
<name>Coffin et al.</name>
<date>19991100</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<patcit num="00044">
<document-id>
<country>US</country>
<doc-number>5999173</doc-number>
<kind>A</kind>
<name>Ubillos</name>
<date>19991200</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<patcit num="00045">
<document-id>
<country>US</country>
<doc-number>6008807</doc-number>
<kind>A</kind>
<name>Bretschneider et al.</name>
<date>19991200</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<patcit num="00046">
<document-id>
<country>US</country>
<doc-number>6020883</doc-number>
<kind>A</kind>
<name>Herz et al.</name>
<date>20000200</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<patcit num="00047">
<document-id>
<country>US</country>
<doc-number>6084582</doc-number>
<kind>A</kind>
<name>Qureshi et al.</name>
<date>20000700</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<patcit num="00048">
<document-id>
<country>US</country>
<doc-number>6154601</doc-number>
<kind>A</kind>
<name>Yaegashi et al.</name>
<date>20001100</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<patcit num="00049">
<document-id>
<country>US</country>
<doc-number>6154771</doc-number>
<kind>A</kind>
<name>Rangan et al.</name>
<date>20001100</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<patcit num="00050">
<document-id>
<country>US</country>
<doc-number>6189783</doc-number>
<kind>B1</kind>
<name>Motomiya et al.</name>
<date>20010200</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<patcit num="00051">
<document-id>
<country>US</country>
<doc-number>6209000</doc-number>
<kind>B1</kind>
<name>Klein et al.</name>
<date>20010300</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<patcit num="00052">
<document-id>
<country>US</country>
<doc-number>6249281</doc-number>
<kind>B1</kind>
<name>Chen et al.</name>
<date>20010600</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<patcit num="00053">
<document-id>
<country>US</country>
<doc-number>6249765</doc-number>
<kind>B1</kind>
<name>Adler et al.</name>
<date>20010600</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>704500</main-classification></classification-national>
</citation>
<citation>
<patcit num="00054">
<document-id>
<country>US</country>
<doc-number>6332147</doc-number>
<kind>B1</kind>
<name>Moran et al.</name>
<date>20011200</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>7155001</main-classification></classification-national>
</citation>
<citation>
<patcit num="00055">
<document-id>
<country>US</country>
<doc-number>6334109</doc-number>
<kind>B1</kind>
<name>Kanevsky</name>
<date>20011200</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<patcit num="00056">
<document-id>
<country>US</country>
<doc-number>6349297</doc-number>
<kind>B1</kind>
<name>Shaw et al.</name>
<date>20020200</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<patcit num="00057">
<document-id>
<country>US</country>
<doc-number>6369835</doc-number>
<kind>B1</kind>
<name>Lin</name>
<date>20020400</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<patcit num="00058">
<document-id>
<country>US</country>
<doc-number>6396500</doc-number>
<kind>B1</kind>
<name>Qureshi et al.</name>
<date>20020500</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<patcit num="00059">
<document-id>
<country>US</country>
<doc-number>6405203</doc-number>
<kind>B1</kind>
<name>Collart</name>
<date>20020600</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<patcit num="00060">
<document-id>
<country>US</country>
<doc-number>6469711</doc-number>
<kind>B2</kind>
<name>Foreman et al.</name>
<date>20021000</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<patcit num="00061">
<document-id>
<country>US</country>
<doc-number>6490601</doc-number>
<kind>B1</kind>
<name>Markus et al.</name>
<date>20021200</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<patcit num="00062">
<document-id>
<country>US</country>
<doc-number>6510553</doc-number>
<kind>B1</kind>
<name>Hazra</name>
<date>20030100</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<patcit num="00063">
<document-id>
<country>US</country>
<doc-number>6646655</doc-number>
<kind>B1</kind>
<name>Brandt et al.</name>
<date>20031100</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<patcit num="00064">
<document-id>
<country>US</country>
<doc-number>6728753</doc-number>
<kind>B1</kind>
<name>Parasnis et al.</name>
<date>20040400</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<patcit num="00065">
<document-id>
<country>US</country>
<doc-number>6779024</doc-number>
<kind>B2</kind>
<name>DeLaHuerga</name>
<date>20040800</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<patcit num="00066">
<document-id>
<country>US</country>
<doc-number>6789228</doc-number>
<kind>B1</kind>
<name>Merrill et al.</name>
<date>20040900</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<patcit num="00067">
<document-id>
<country>US</country>
<doc-number>6816858</doc-number>
<kind>B1</kind>
<name>Coden et al.</name>
<date>20041100</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<patcit num="00068">
<document-id>
<country>US</country>
<doc-number>2002/0097885</doc-number>
<kind>A1</kind>
<name>Birchfield et al.</name>
<date>20020700</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<patcit num="00069">
<document-id>
<country>JP</country>
<doc-number>403129990</doc-number>
<date>19910600</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<patcit num="00070">
<document-id>
<country>JP</country>
<doc-number>10-246041</doc-number>
<date>19980900</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<patcit num="00071">
<document-id>
<country>WO</country>
<doc-number>WO 02/013522</doc-number>
<kind>A2</kind>
<date>20020200</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<patcit num="00072">
<document-id>
<country>WO</country>
<doc-number>WO 02/058432</doc-number>
<kind>A2</kind>
<date>20020700</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<nplcit num="00073">
<othercit>Rist, Thomas et al., Adding animated presentation agents to the interface, ACM International Conference on Intelligent User Interfaces, 1997, pp. 79-86.</othercit>
</nplcit>
<category>cited by examiner</category>
</citation>
<citation>
<nplcit num="00074">
<othercit>Product Description for Meeting Companion by Quindi Corporation, downloaded from &lt;http://quindi.com/product.htm&gt;on Jan. 24, 2005, copyright 2004.</othercit>
</nplcit>
<category>cited by other</category>
</citation>
<citation>
<nplcit num="00075">
<othercit>Addlesee, M.D.; Jones, A.H.; Livesey, F.; and Samaria, F.S., “The ORL Active Floor,” IEEE Personal Communications, vol. 4, No. 5, Oct. 1997, pp. 35-41. ftp://ftp.uk.research.att.com:/pub/docs/att/tr.97.11.pdf.</othercit>
</nplcit>
<category>cited by other</category>
</citation>
<citation>
<nplcit num="00076">
<othercit>Cunado, D.; Nash, J.M.; Nixon, M.S.; and Carter, J.N., “Gait Extraction and Description by Evidencing Gathering,” Proceedings of the Second International Conference on Audio and Video-based Person Identification, Washington, D.C., Mar. 22-23, 1999, pp. 43-48.</othercit>
</nplcit>
<category>cited by other</category>
</citation>
<citation>
<nplcit num="00077">
<othercit>Eldridge, M.; Lamming, M.; and Flynn, M., “Does A Video Diary Help Recall?”, Technical Report EPC-1991-124, People and Computers VII, eds. Monk et al., 1992, pp. 257-269.</othercit>
</nplcit>
<category>cited by other</category>
</citation>
<citation>
<nplcit num="00078">
<othercit>Foote, J. et al. “An Intelligent Media Browser Using Automatic Multimodal Analysis,” ACM Multimedia, 1998, pp. 375-380.</othercit>
</nplcit>
<category>cited by other</category>
</citation>
<citation>
<nplcit num="00079">
<othercit>Girgensohn, A, and Boreczky, J.S. “Time-Constrained Keyframe Selection Technique,” Multimedia Tools, 11(3): 347-358, 2000.</othercit>
</nplcit>
<category>cited by other</category>
</citation>
<citation>
<nplcit num="00080">
<othercit>Konneker, L., “Automating Receptionists,” Proceedings of the 1986 IEEE International Conference on Systems, Man, and Cybernetics, Atlanta, GA, Oct. 14-17, 1986, pp. 1592-1596.</othercit>
</nplcit>
<category>cited by other</category>
</citation>
<citation>
<nplcit num="00081">
<othercit>Lamming, M.G.; and Newman, W.N., “Activity-based Information Retrieval: Technology in Support of Personal Memory,” in F.H. Vogt (ed.), Personal Computers and Intelligent Systems. Proceedings of Information Processing 92, vol. III, Elsevier Science Publishers, 1992, pp. 68-81.</othercit>
</nplcit>
<category>cited by other</category>
</citation>
<citation>
<nplcit num="00082">
<othercit>Lovstrand, L., “Being Selectively Aware with the Khronika System,” Proceedings of the Second European Conference on Computer-Supported Cooperative Work, Kluwer Academic Publishers, 1991, pp. 265-277.</othercit>
</nplcit>
<category>cited by other</category>
</citation>
<citation>
<nplcit num="00083">
<othercit>Newman, W.M.; Eldridge; and Lamming, M.G., “PEPSYS: Generating Autobiographies by Automatic Tracking,” Proceedings of the Second European Conference on Computer-Supported Cooperative Work; Sep. 25-27, 1991, Amsterdam, The Netherlands, pp. 175-188.</othercit>
</nplcit>
<category>cited by other</category>
</citation>
<citation>
<nplcit num="00084">
<othercit>Plamondon, R.; and Lorette, G., “Automatic Signature Verification and Writer Identification—The State of the Art,” Pattern Recognition, vol. 22, No. 2, 1989, pp. 107-131.</othercit>
</nplcit>
<category>cited by other</category>
</citation>
<citation>
<nplcit num="00085">
<othercit>Rangan, P.V. “Software Implementation of VCRs on Personal Computing Systems,” IEEE, 1992, pp. 635-640.</othercit>
</nplcit>
<category>cited by other</category>
</citation>
<citation>
<nplcit num="00086">
<othercit>Rangan, P.V. et al., “A Window-Based Editor for Digital Video and Audio,” IEEE 1992 pp. 640-648.</othercit>
</nplcit>
<category>cited by other</category>
</citation>
<citation>
<nplcit num="00087">
<othercit>Rosenschein, S., “New Techniques for Knowledge Capture,” from TTI/Vanguard Conference: Knowledge Management Comes of Age, pp. 1-3, Sep. 23-24, 2003.</othercit>
</nplcit>
<category>cited by other</category>
</citation>
<citation>
<nplcit num="00088">
<othercit>Seiko Instruments, Inc., “Smart Lobby: The Electronic Sign-In Book That Tracks Visitors and Prints Badges, User Guide for Windows,” Manual Part No. 22-93000-00, copyright 1997.</othercit>
</nplcit>
<category>cited by other</category>
</citation>
<citation>
<nplcit num="00089">
<othercit>Sony Music Corporation, “E-Guide Unmanned Reception System,” Japan Industrial Journal, May 20, 1996, p. 6, (http://salmon.crc.ricoh.com:8001/hull/1999/8/11207/11207.html).</othercit>
</nplcit>
<category>cited by other</category>
</citation>
<citation>
<nplcit num="00090">
<othercit>Viredaz, M.A., “The Ilsy Pocket Computer Version 1.5: User's Manual,” Technical Note TN-54, Compaq Western Research Laboratory, Jul. 1998., pp. 1-37.</othercit>
</nplcit>
<category>cited by other</category>
</citation>
<citation>
<nplcit num="00091">
<othercit>Want, R.; Hopper, A.; Falcao, V.; and Gibbons, J.J., “The Active Badge Location System,” ACM TOIS, Transactions on Information Systems, vol. 10, No. 1, Jan. 1992, pp. 91-102.</othercit>
</nplcit>
<category>cited by other</category>
</citation>
</references-cited>
<number-of-claims>47</number-of-claims>
<us-exemplary-claim>1</us-exemplary-claim>
<us-field-of-classification-search>
<classification-national>
<country>US</country>
<main-classification>7155001</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>709204</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>345723</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>345758</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>704500</main-classification>
</classification-national>
</us-field-of-classification-search>
<figures>
<number-of-drawing-sheets>8</number-of-drawing-sheets>
<number-of-figures>9</number-of-figures>
</figures>
<parties>
<applicants>
<applicant sequence="001" app-type="applicant-inventor" designation="us-only">
<addressbook>
<last-name>Lee</last-name>
<first-name>Dar-Shyang</first-name>
<address>
<city>Fremont</city>
<state>CA</state>
<country>US</country>
</address>
</addressbook>
<nationality>
<country>omitted</country>
</nationality>
<residence>
<country>US</country>
</residence>
</applicant>
<applicant sequence="002" app-type="applicant-inventor" designation="us-only">
<addressbook>
<last-name>Hull</last-name>
<first-name>Jonathan J.</first-name>
<address>
<city>San Carlos</city>
<state>CA</state>
<country>US</country>
</address>
</addressbook>
<nationality>
<country>omitted</country>
</nationality>
<residence>
<country>US</country>
</residence>
</applicant>
<applicant sequence="003" app-type="applicant-inventor" designation="us-only">
<addressbook>
<last-name>Graham</last-name>
<first-name>Jamey</first-name>
<address>
<city>Menlo Park</city>
<state>CA</state>
<country>US</country>
</address>
</addressbook>
<nationality>
<country>omitted</country>
</nationality>
<residence>
<country>US</country>
</residence>
</applicant>
<applicant sequence="004" app-type="applicant-inventor" designation="us-only">
<addressbook>
<last-name>Gage</last-name>
<first-name>Pamela</first-name>
<address>
<city>Redwood City</city>
<state>CA</state>
<country>US</country>
</address>
</addressbook>
<nationality>
<country>omitted</country>
</nationality>
<residence>
<country>US</country>
</residence>
</applicant>
</applicants>
<agents>
<agent sequence="01" rep-type="attorney">
<addressbook>
<orgname>Townsend &amp; Townsend &amp; Crew LLP</orgname>
<address>
<country>unknown</country>
</address>
</addressbook>
</agent>
</agents>
</parties>
<assignees>
<assignee>
<addressbook>
<orgname>Ricoh Company, Ltd.</orgname>
<role>03</role>
<address>
<city>Tokyo</city>
<country>JP</country>
</address>
</addressbook>
</assignee>
</assignees>
<examiners>
<primary-examiner>
<last-name>Bashore</last-name>
<first-name>William</first-name>
<department>2176</department>
</primary-examiner>
</examiners>
</us-bibliographic-data-grant>
<abstract id="abstract">
<p id="p-0001" num="0000">A unique combination of components is disclosed to facilitate information management during a collaborative effort. Interaction during a meeting is supported by the use of devices in a manner that allows attendees to quickly retrieve and display documents on-command or automatically. The information includes previously recorded meetings among other things. The disclosed method and system, thus, facilitates the exchange of information during meetings.</p>
</abstract>
<drawings id="DRAWINGS">
<figure id="Fig-EMI-D00000" num="00000">
<img id="EMI-D00000" he="108.54mm" wi="213.19mm" file="US07299405-20071120-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00001" num="00001">
<img id="EMI-D00001" he="193.55mm" wi="156.21mm" orientation="landscape" file="US07299405-20071120-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00002" num="00002">
<img id="EMI-D00002" he="229.79mm" wi="129.71mm" orientation="landscape" file="US07299405-20071120-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00003" num="00003">
<img id="EMI-D00003" he="215.05mm" wi="149.18mm" orientation="landscape" file="US07299405-20071120-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00004" num="00004">
<img id="EMI-D00004" he="194.23mm" wi="153.84mm" orientation="landscape" file="US07299405-20071120-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00005" num="00005">
<img id="EMI-D00005" he="213.61mm" wi="121.58mm" orientation="landscape" file="US07299405-20071120-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00006" num="00006">
<img id="EMI-D00006" he="247.99mm" wi="160.27mm" file="US07299405-20071120-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00007" num="00007">
<img id="EMI-D00007" he="213.61mm" wi="163.32mm" orientation="landscape" file="US07299405-20071120-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00008" num="00008">
<img id="EMI-D00008" he="203.62mm" wi="108.88mm" orientation="landscape" file="US07299405-20071120-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
</drawings>
<description id="description">
<?BRFSUM description="Brief Summary" end="lead"?>
<heading id="h-0001" level="1">BACKGROUND OF THE INVENTION</heading>
<p id="p-0002" num="0001">The present invention relates generally to information management and more particularly to multi-media-based, real-time, interactive information management for supporting collaborative interaction among a group of participants.</p>
<p id="p-0003" num="0002">In any project involving a group of people, cooperative and coordinated interaction typically is key to the success or failure of the undertaking. The project begins with a series of meetings to identify the desired goals, and to begin understanding the tasks needed to achieve the goal. Numerous meetings subsequently follow to further identify how to achieve the desired goals. In an engineering setting, for example, this typically involves teams of engineers holding several meetings to understand the technical hurdles they face, and to develop and design the components which constitute the end product. In a marketing situation, product managers and sales persons convene frequently to define the product line or services, to identify potential markets and target customers, to develop advertising strategies and product roll-out scenarios, and so on.</p>
<p id="p-0004" num="0003">As a project continues along, various groups of the project team will have periodic meetings to address and resolve issues and problems which inevitably arise during the course of any significant endeavor. Meetings will take place to assess the progress of the project, to assess the scope of the effort in view of changing external conditions, to re-order priorities and so on.</p>
<p id="p-0005" num="0004">Sometimes, the meetings are difficult to convene. For example, engineers from various offices in geographically distant locations might need to participate. Certain members may not be conveniently available; e.g. they are on the road at a customer site.</p>
<p id="p-0006" num="0005">The creation and dissemination of information is often inefficiently managed. Conventionally, attendees in a meeting record the events of the meeting by taking notes, whether handwritten, or entered in a portable computer such as a laptop computer, or entered into a personal data accessory (PDA), or by video taping the meeting. Notes of the meeting events are recorded from the point of view of the observer. Consequently information that is omitted and retained will be a function of the experiences and understandings (or misunderstandings) of the observer. In addition, note taking activity tends to distract from the discussion taking place, and so it is possible that certain points of interest might be missed.</p>
<p id="p-0007" num="0006">Such inefficiencies of information management are exacerbated when in so-called “brainstorming sessions,” when ideas are presented at a quick pace and many trains of thought are produced. Participants generally stop taking notes at that point, being frustrated by the flurry of information being presented. The situation is further aggravated for those attendees who are at remote locations, being unable to fully participate in the session.</p>
<p id="p-0008" num="0007">Documentation may not always be readily available. For example, a topic may arise which was unexpected and for which the relevant materials are not at hand. This creates inefficiency because another meeting must then be scheduled with the risk that the needed participants may have conflicting schedules. At other times, the meeting may need to be stalled until needed material is obtained.</p>
<p id="p-0009" num="0008">Notes and ideas developed from previous meetings are not always available. The shear volume of information accumulated over the course of many meetings may require the expenditure of much human effort to sort through the material and to render it in a readily accessible form. A problem, of course, is that one cannot know a priori what information will be relevant in subsequent meetings. Consequently, a fully cross-referenced index is desirable, but tedious and typically not made.</p>
<p id="p-0010" num="0009">Although computers have tremendously improved document management and workflow process in many business settings, there has been very poor computer support for the creative and intellectual activities which take place in meetings and brainstorming sessions in the work environment. Means for capturing these events is important not only for preserving corporate knowledge, but also for facilitating the dissemination of ideas and the assimilation of information. With the availability of ever-increasing processor speed, communication bandwidth, and storage capacity, overcoming the technical impediments are no longer an issue. Instead, the effective coordination and utilization of vast amounts of information generated when individuals collaborate toward a common goal become the challenge.</p>
<p id="p-0011" num="0010">A need therefore exists for a method and system to provide information support services during a meeting. It is desirable to provide a method and system which can effectively capture the events of the meeting. There is a need to provide access and retrieval of information that can facilitate the progress of the meeting. It is further desirable to provide these capabilities with minimal human intervention so as not to distract the participants in the meeting.</p>
<heading id="h-0002" level="1">SUMMARY OF THE INVENTION</heading>
<p id="p-0012" num="0011">An information support system in accordance with the invention facilitates the management information during a meeting. Information retrieval and other management functions are provided during an ongoing meeting. The meeting participants are alleviated of the distracting tasks of manually accessing the information, having to delay topics of discussion while certain documents must be manually retrieved, and so on.</p>
<p id="p-0013" num="0012">In accordance with the invention, a method and system for managing information during a meeting includes recording the activities of the participants in a meeting during the meeting. A participant directive is identified by analyzing the recorded meeting data. The participant directive represents a desired action on certain information. In response to the participant directive, the desired action is acted upon. The participant directive is determined by an analysis of the textual content of the recorded meeting data. The directive is either an explicit command issued by a participant or is implicitly determined based on the context of the meeting.</p>
<p id="p-0014" num="0013">Data interaction devices are provided for participants to interact with the data and to provide commands and other input to the information support system. In one embodiment, attendee identification capability is provided. Information access and presentation is based on the access permissions associated with the identified attendees. In another embodiment, attendee location tracking is provided. Information access and presentation is based on the location of the attendees, in addition to the associated access permissions.</p>
<?BRFSUM description="Brief Summary" end="tail"?>
<?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?>
<description-of-drawings>
<heading id="h-0003" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading>
<p id="p-0015" num="0014"><figref idref="DRAWINGS">FIG. 1</figref> is a simplified block diagram of an embodiment of the present invention.</p>
<p id="p-0016" num="0015"><figref idref="DRAWINGS">FIG. 2</figref> shows an exemplary list of various capture devices and some of the data manipulation and control elements of <figref idref="DRAWINGS">FIG. 1</figref>.</p>
<p id="p-0017" num="0016"><figref idref="DRAWINGS">FIG. 3</figref>. shows an exemplary list of data interaction devices of the present invention.</p>
<p id="p-0018" num="0017"><figref idref="DRAWINGS">FIG. 4</figref> illustrates an embodiment of a typical arrangement of some of the elements comprising the present invention.</p>
<p id="p-0019" num="0018"><figref idref="DRAWINGS">FIG. 5</figref> illustrates the processes of the present invention.</p>
<p id="p-0020" num="0019"><figref idref="DRAWINGS">FIG. 6</figref>. is a high level flow of the text retrieval process of the present invention.</p>
<p id="p-0021" num="0020"><figref idref="DRAWINGS">FIG. 7</figref> is a high level flow of the data retrieval process of the present invention.</p>
<p id="p-0022" num="0021"><figref idref="DRAWINGS">FIG. 8</figref> illustrates a user interface for accessing data captured during a meeting.</p>
<p id="p-0023" num="0022"><figref idref="DRAWINGS">FIG. 9</figref> is schematic representation of a networked configuration of the present invention.</p>
</description-of-drawings>
<?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?>
<?DETDESC description="Detailed Description" end="lead"?>
<heading id="h-0004" level="1">DESCRIPTION OF THE SPECIFIC EMBODIMENTS</heading>
<p id="p-0024" num="0023"><figref idref="DRAWINGS">FIG. 1</figref> is a simplified block diagram of an embodiment of the information support system <b>100</b> in accordance with the present invention. The information support system includes a database component <b>160</b>. A data capture portion <b>110</b> in data communication with database component <b>160</b> feeds data captured during a meeting to the database. A data processing portion <b>120</b> processes data contained in the database component <b>160</b>. A data interaction component <b>130</b> comprises data interaction devices by which members in a meeting exchange information with database component <b>160</b>. A data retrieval portion <b>140</b> provides a second data path between data interaction component <b>130</b> and database component <b>160</b>. A communication portion <b>150</b> provides access to various communication networks <b>172</b>-<b>176</b>.</p>
<p id="p-0025" num="0024">Data capture portion <b>110</b> comprises various devices for recording the events and information produced during a meeting. Video recordings are produced and stored in database component <b>160</b>. Video recordings include videos of the participants in the meetings, presentations made during the meeting, written information produced by the attendees, and so on. Audio recordings are also made and stored in the database component. Data capture portion <b>110</b> further includes document capture.</p>
<p id="p-0026" num="0025">Data processing portion <b>120</b> comprises computer programs configured to analyze data contained in database component <b>160</b>. More specifically, there are computer programs for extracting information contained in the video and audio recordings. The information includes text and images which are also stored in the database component. There are computer programs which allow a user to access selected segments of the various video and audio recordings contained in the database component.</p>
<p id="p-0027" num="0026">Data interaction component <b>130</b> comprises various input/output devices for use by the members of a meeting. The devices allow access to various information contained in database component <b>160</b>. Members are able to view the information, both textual and graphical. Selective distribution of information is provided. There is the ability to create new information by assimilating pieces of existing information. Members in the meeting can even modify certain pieces of information, depending on privilege levels, the type of information, and so on.</p>
<p id="p-0028" num="0027">Data retrieval portion <b>140</b> comprises various computer programs configured to retrieve information from database component <b>160</b>. There are computer programs to search for and present relevant documents based on information captured by data capture component <b>110</b>. The data retrieval portion is in data communication with the interaction devices of data interaction component <b>130</b> to obtain cues from the attendees of the meeting to determine when and what information to retrieve. Retrieved documents are then displayed via data interaction component <b>130</b>.</p>
<p id="p-0029" num="0028">Communication portion <b>150</b> provides channels of communication to various communication networks. A public switched network <b>172</b> provides access via conventional telephone lines to remote modems, fax machines, and the like. In one embodiment of the invention, public switched network <b>172</b> includes a private branch exchange (“PBX”) in front of it. A global communication network <b>174</b>, such as the Internet, provides access to various information sources, such as the world wide web (“web”), news groups, and so on. Typically, public switched network <b>172</b> and global communication network <b>174</b> share the same physical channels. However, the logical view shown in <figref idref="DRAWINGS">FIG. 1</figref> is presented to simplify the discussion. Communication portion <b>150</b> also can be provisioned with access to a local communication network <b>176</b>, such as an intranet, local area network (“LAN”), wide area network (“WAN”), and so on.</p>
<p id="p-0030" num="0029">Communication portion <b>150</b> comprises the hardware and software conventionally used to access the various communication networks. For example, access over public switched network <b>172</b> is typically accomplished with a modem. Access to global communication network <b>174</b> can be by way of a modem, a digital subscriber line (“DSL”), an integrated services digital network (“ISDN”) connection, cable modem, and the like. Local communication network access is typically via an Ethernet connection.</p>
<p id="p-0031" num="0030">Database component <b>160</b> is a back-end database which stores documents and multi-media data produced during the meeting. The database includes records from previous meetings. Preferably, database component <b>160</b> comprises a data server system. This permits remote access to the database over a communication network. This also permits documents and information contained on the web and other such information sources to be continuously available. Database component <b>160</b> comprises conventional hardware and software to provide the functions of a database. In one embodiment, the database is implemented as a relational database using conventional structured query language (“SQL”) techniques for accessing data. The data is stored on a disk system configured as a redundant array of individual disks (“RAID”) to provide high-speed access to the stored data with backup capability. In another embodiment of the invention, the data repository is the IM<sup>3 </sup>and/or the eCabinet™ products manufactured and sold by the assignee of the present invention.</p>
<p id="p-0032" num="0031">Referring to <figref idref="DRAWINGS">FIG. 2</figref>, various data capture devices and related controls in accordance with the invention are exemplified in block diagram format. A panoramic camera component <b>210</b> provides a video recording of the participants of the meeting. A whiteboard capture portion <b>212</b> includes hardware and software to capture the writings made upon the whiteboard and to store the captured information to database component <b>160</b>. An attendee identification system <b>214</b> identifies participants as they enter and leave a meeting. A presentation projector component <b>216</b> allows participants to present slides during a meeting. An audio recording component <b>218</b> provides an audio record of the meeting. A document capture component <b>220</b> provides data capture capability of documents used during the meeting.</p>
<p id="p-0033" num="0032"><figref idref="DRAWINGS">FIG. 2</figref> also shows the data processing portion <b>120</b> of the information system <b>100</b> of <figref idref="DRAWINGS">FIG. 1</figref>. The data processing portion comprises an information extraction portion <b>240</b> which includes computer programs to analyze the data contained in database component <b>160</b>. A session control module <b>250</b> provides selectively controlled access to the capture devices and to the captured data. The session control module comprises a capture control portion <b>252</b> and a retrieval interface portion <b>254</b>.</p>
<p id="p-0034" num="0033">Panoramic camera component <b>210</b> comprises a panoramic video capture device. A company called Cyclo Vision Technologies, Inc. sells an attachment comprising a parabolic mirror, relay lens, and PC-based software to produce panospheric views. The attachment is adapted for a variety of commercially available digital cameras, including the following models: the Agfa ePhoto 1680 from Agfa; the Nikon CoolPix 950, 900, &amp; 700 models from Nikon, Inc.; the Kodak DC 265, &amp; 260 models from the Eastman Kodak Company; the Olympus C-2000 from Olympus America, Inc.; and the Epson PhotoPC 750Z from Epson America, Inc. An adapted camera produces a warped image due to the parabolic mirror, but which nonetheless contains a 360° image. The software re-samples the digital image to produce a de-warped image. The image can then be panned or zoomed by the software. Panoramic camera component <b>210</b> is in data communication with database component <b>160</b> to which the de-warped image is delivered for storage. Preferably, the warped image is also stored in database component so that additional processing on the raw data can be made as needed.</p>
<p id="p-0035" num="0034">Another company called Be Here Corporation manufactures a similar system. Other panoramic systems produce an integrated image by stitching together a series of sequential images. U.S. Pat. No. 5,990,934 describes such a technique and is herein fully incorporated by reference for all purposes. In an embodiment of the invention, two or more such cameras in the meeting room are provided to generate the video information necessary to produce 3-dimensional images.</p>
<p id="p-0036" num="0035">Whiteboard capture portion <b>212</b> typically comprises one or more conventionally known electronic whiteboards configured to capture notes and diagrams that are written thereupon during a meeting. Preferably, in accordance with the present invention, color information is captured along with the written information. Each whiteboard device is coupled to database component <b>160</b> to store the captured information. In one embodiment, a whiteboard drawing reaches a stage that needs to be recorded, a meeting participant pushes a button, and the device uploads the captured information to the database. In another embodiment, data is continuously captured and delivered to database component <b>160</b>.</p>
<p id="p-0037" num="0036">In yet another alternative embodiment, existing non-electronic whiteboards is retrofitted with x-y location devices. The x-y location device tracks the movements of the pen which represent the written matter, sketches, notes, and so on. The x-y information is then stored to the database. In still yet another alternative, a video camera is provided to record the writings.</p>
<p id="p-0038" num="0037">Attendee identification system <b>214</b> includes a badge worn by the participants of the meeting. In one embodiment, a visitor is given such a badge upon arriving. Preferably, the badge contains a wireless identification device to permit unobtrusive identification of the participants. For example, the identification badge might conform to the wireless standard known as Bluetooth. This standard uses an unregulated part of the spectrum between 2.4000 GHz-2.4835 GHz, known as the industrial-scientific-medical (“ISM”) band. The Bluetooth standard defines a standard operating range of 10 meters, but in practice can be extendable beyond that range. In addition, Bluetooth does not require line-of-sight communication. This has the advantage being able to receive signals from the badges worn by the participants without artificially restricting their movement.</p>
<p id="p-0039" num="0038">Presentation projector component <b>216</b> comprises, in one embodiment, a conventionally known slide projector for presenting prepared material such as slides and the like. The projector includes an image capture capability, having optics which direct and project a presented image upon a charge coupled device (“CCD”) array, in addition to a projection screen. On-board logic reads out the CCD array and delivers it to database component <b>160</b>. Alternatively, the on-board logic includes a serial or parallel port configured for use with a personal computer (“PC”). The PC reads out the data which then uploads it to the database.</p>
<p id="p-0040" num="0039">In another embodiment, as an alternative or addition to a projection unit, a PC or more typically, a notebook (laptop) computer, is provided with presentation software. For example, a commonly available presentation package is PowerPoint, sold by Microsoft Corporation. The software enables the production of a slides and their presentation. The computer is typically provisioned with a video adapter which produces a National Television System Committee (“NTSC”) or video graphics adapter (“VGA”) compliant signal. The signal is fed into the video input of a television or computer monitor for presentation in the meeting. The output is tapped, digitized, and uploaded to the database component. In yet another embodiment, presentation projector component <b>216</b> is some combination of standard projectors and portable notebook computers.</p>
<p id="p-0041" num="0040">Audio recording component <b>218</b> typically comprises multiple microphones. In one embodiment, the microphones are located about the meeting or conference room. Source localization and selective amplification can be achieved by proper placement of the microphones. A minimum of two microphones can indicate the direction from which sound emanates. In another embodiment, three microphones are provided to produce a recording which contains elevation information in addition to direction. Preferably, any one of a number of known echo cancellation techniques is additionally provided to improve the quality of the recorded audio. The audio component includes a digitizing portion to convert the analog signal to a form that can be stored in database component <b>160</b>.</p>
<p id="p-0042" num="0041">Document capture component <b>220</b> comprises various systems for converting hardcopy documents into electronic form for storage in database component <b>160</b>. Conventionally, this is directly achieved by the use of scanners, including flatbed scanners and handheld models. A more transparent collection method is to integrate the capture capability into devices such as printers, copiers, fax machines, and so forth. U.S. Pat. No. 5,978,477 assigned to the assignee of the present invention discloses precisely such a technique and is fully incorporated herein by reference for all purposes.</p>
<p id="p-0043" num="0042">Information extraction portion <b>240</b> comprises computer programs configured to extract text and images from the captured data. Referring to <figref idref="DRAWINGS">FIG. 6</figref> for a moment, it can be seen that information extraction portion <b>240</b> comprises two primary data flow segments. Video recordings are analyzed for text and images. In step <b>610</b>, video images are read out of database component <b>160</b>. This includes video and image recordings made by panoramic camera <b>210</b>, whiteboard capture portion <b>220</b>, and presentation projector component <b>216</b>. Data from panoramic camera <b>210</b> is analyzed for images contained in the recording. For example, a vendor might bring some samples of a product. The samples can be recorded, and later identified and stored for future recall based on the identification. In step <b>612</b>, image analysis of the video identifies the samples and stores them in the database for reference. Similarly, textual information picked up in data from the whiteboard and the presentation projector components, and to a lesser extent in the video recording, is analyzed using known optical character recognition techniques. In step <b>614</b>, the text and images are cross-referenced with the video and stored in the database. By providing the cross-referencing back to the original video, it is then possible to call up the portion of video for a given image or given segments of text. In step <b>630</b>, the text that is extracted is indexed and stored into the database. In addition to individual words, groups of words such as phrases and sentences can be indexed to enhance context searching discussed below in connection with <figref idref="DRAWINGS">FIG. 7</figref>.</p>
<p id="p-0044" num="0043">Audio recordings are treated in a similar manner. In step <b>620</b>, segments of audio tracks are retrieved. In step <b>622</b>, known speech recognition techniques are applied to extract text from the audio segments. The identified text is cross-indexed with the audio tracks in step <b>624</b>. This allows relevant portions of the audio to be called up for a given segment of text. In step <b>630</b>, the text identified in the audio tracks is indexed and stored into the database. In addition, to single words, phrases and sentences can be indexed to enhance context searching discussed below in connection with <figref idref="DRAWINGS">FIG. 7</figref>.</p>
<p id="p-0045" num="0044">Session control module <b>250</b> comprises computer programs which mediate access to the capture devices and to the captured data. The data capture devices can thus be controlled from a single station in the conference room, or remotely controlled in situations where some of the participants are at remote locations. This permits real-time broadcasting or offline playback, and so on. The session control module mediates multiple users and coordinates (and limits) their access to the capture devices and captured data. The selective control and access functions are provided depending on the user.</p>
<p id="p-0046" num="0045">Capture control portion <b>252</b> comprises the various interfacing components which access the control functions of the various capture devices. In accordance with the present invention, capture control portion <b>252</b> provide basic functions such as starting and stopping the data capture function of one or more data capture devices. It is worth noting that some data capture devices may not be equipped for remote control access by capture control portion <b>252</b>, in which case manual operation of those devices is required. However, some devices can be remotely controlled by wireless techniques or wired connections, for example an Ethernet connection. Such devices might have additional remote operation control capability accessible by the capture control portion. For example, hue and color balance controls in a video camera might be remotely available to capture control portion <b>252</b>. Depending on session control module <b>250</b>, all, some, or none of the control functions are available for any one given participant attempting to access those functions.</p>
<p id="p-0047" num="0046">Session controller module <b>250</b> can establish levels of access by a login process. For example, visitors might not be given any access or very limited access. A system administrator might be given complete access.</p>
<p id="p-0048" num="0047">Retrieval interface <b>254</b> provides access to the captured data to review (playback) and/or edit the material. Retrieval interface <b>254</b> preferably is based on graphical user interface design principles, since the nature of the captured data for the most part comprises video and still images.</p>
<p id="p-0049" num="0048">Referring to <figref idref="DRAWINGS">FIG. 3</figref>, various data interaction devices are exemplified. The data interaction devices are another source of information which feeds into database component <b>160</b>. In addition, the data interaction devices provide the function of data presentation to the users. Each data interaction component includes related “drivers,” which are computer programs configured to provide a common interface between the specific hardware and internal software or firmware constituting a particular data interaction device and the external computing environment. As can be seen in <figref idref="DRAWINGS">FIG. 3</figref>, the computing environment that is external to the data interaction devices comprises database component <b>160</b>, data retrieval portion <b>140</b>, and communication portion <b>150</b>. Communication portion <b>150</b> provides access to the various communication networks shown in <figref idref="DRAWINGS">FIG. 1</figref> and generally represented in <figref idref="DRAWINGS">FIG. 3</figref> by communication bubble <b>170</b>.</p>
<p id="p-0050" num="0049">Desktop devices <b>302</b> and associated drivers <b>303</b> provide access conventionally used by users of PCs. Desktop devices <b>302</b> include a display, a keyboard, and a mouse or a stylus. Data is input via the keyboard and displayed on the monitor. Monitors include flat-panel displays, larger units for large-scale presentations, and so on. The mouse typically provides the functions of: action selection; data selection; and action initiation. Desktop devices also include handheld devices and laptop computers.</p>
<p id="p-0051" num="0050">Personal data accessories <b>304</b> and associated drivers <b>305</b> provide a portable and convenient store for personalized data such as schedules, address books, and so on. These devices typically have interfacing software to synchronize the stored information with a computer. In accordance with the invention, personal data accessories are interfaced with database component <b>160</b>. This allows a meeting participant to upload her schedule and other pertinent information to be shared by others. An example of a personal data accessory is the Palm V, manufactured and sold by 3COM Corporation.</p>
<p id="p-0052" num="0051">Audio component <b>306</b> and associated drivers <b>307</b> provide a speech command interface for accessing information contained in database component <b>160</b>. Speech recognition systems are known. In one embodiment, documents are retrieved simply by speaking the action and the name of the document. Similarly, internet access and “surfing” on the net by visiting plural web sites are achieved over the speech command interface. In another embodiment, audio component <b>306</b> includes a speech synthesis portion to provide audio “readout” of documents to the participants in the meeting. Speech synthesis would be especially useful for the visually impaired. The synthesized speech is broadcast over a loudspeaker system or over headphones.</p>
<p id="p-0053" num="0052">Video component <b>308</b> and associated driver <b>309</b> include personal display systems such as displays adapted to be worn like eyeglasses. Virtual reality goggles <b>315</b> are 3-dimensional imaging tools which can be used to provide stereographic imagining. A virtual reality engine <b>314</b> generates the virtual images. Virtual reality markup language (“VRML”) can be incorporated to define the 3-dimensional elements and their interactions.</p>
<p id="p-0054" num="0053">Data gloves <b>310</b> and associated drivers can be used to enhance the virtual reality experience. Combined with the virtual reality goggles, a user is given the ability to manipulate virtual objects. Meeting participants equipped with virtual reality goggles and data gloves can cooperate in a virtual reality environment. Data gloves having tactile and force feedback features can be used to provide the user with additional information.</p>
<p id="p-0055" num="0054">A graphics tablet <b>312</b> and associated driver <b>313</b> provides for situations in which freehand sketches need to be captured. Conventional mouse input devices are not appropriate for handwritten material. Signature capture would be facilitated by the use of a graphics tablet where signed documents are needed.</p>
<p id="p-0056" num="0055">Refer now to <figref idref="DRAWINGS">FIG. 4</figref> for an illustration of a typical meeting room configuration in accordance with the invention. It is noted that the figure is purely exemplary and is not intended to limit the scope of the invention. There is a conference table <b>402</b> having plural chairs <b>404</b> arranged about it. A panoramic camera <b>410</b> captures a panoramic view of the events taking place in the meeting. As mentioned above, additional cameras (not shown) can be provided to generate 3-dimensional images.</p>
<p id="p-0057" num="0056">Arranged atop conference table <b>402</b> are various data interaction devices. Computer displays and keyboards <b>414</b> are provided. Personal data accessories <b>416</b> are also shown. A pair of speakerphones <b>418</b> allow for teleconferencing. A printer <b>422</b> is shown.</p>
<p id="p-0058" num="0057">Various data capture devices include whiteboards <b>424</b> for sketching ideas and so forth. Microphones <b>412</b> are arranged about the room to capture the audio. Speakers (not shown) may also be provided for audio playback. Audio headphones (not shown) can be provided for unobtrusive audio. A projector <b>420</b> is provided for slide presentation.</p>
<p id="p-0059" num="0058">The data capture and data interaction devices can be connected using conventional wired techniques. As mentioned above, serial connections, parallel cabling, Ethernet wiring and other wired standards are contemplated. Preferably a wireless approach is used whenever available, in order to simplify the setup and to avoid excessive cabling within the conference room.</p>
<p id="p-0060" num="0059">Two wireless standards are commonly available. The Infrared Data Association (IrDA) specification specifies an infrared wireless communication standard. Personal data accessories typically use this medium for communicating data between units. Keyboards are available with an infrared interface. Certain operating limitations, however, make IrDA devices less than ideal. IrDA provides a range of up to a meter or so and has a working range of about 30°. Increasing the operating range involves increasing the power output of the infrared signal, thus presenting certain hazardous conditions to the operators.</p>
<p id="p-0061" num="0060">As mentioned above, the Bluetooth wireless standard is preferable, especially since line-of-sight is not required between two communicating devices. Various devices are shown equipped with a Bluetooth interface; such as personal data accessories <b>416</b>. It is understood that other devices can be similarly equipped. A Bluetooth transmitter/receiver provides a data path between the devices and database component <b>160</b>. In fact, data over the communication network <b>170</b> can be provided through this data path.</p>
<p id="p-0062" num="0061">The data rate for Bluetooth is 1 Mbps (megabits per second), as compared to IrDA which is 4 Mbps. However, for most circumstances a 1 Mbps data rate is sufficient. Where higher data rates are needed, such as in the case of video transmissions, another wireless standard known as IEEE 802.11b can be used. IEEE 802.11b provides higher data rates, up to 11 Mbps. Moreover, the standard defines operating distances of 30 meters. The choice of wireless standards, of course, will depend on commercial availability, cost factors and the like, for any given conference room configuration.</p>
<p id="p-0063" num="0062">Referring now to <figref idref="DRAWINGS">FIG. 5</figref>, it is shown that various processes associated with the foregoing data capture devices are initiated and execute continuously for the duration of a meeting. It is noted that for silent moments during the meeting data is not generated, though the process may be executing. A record video process <b>510</b> continuously collects data from the one or more cameras comprising panoramic camera component <b>210</b>. Likewise, a record audio process <b>520</b> continuously collects data from the one or more microphones comprising the audio capture component <b>218</b>. A whiteboard monitor process <b>540</b> monitors the activity occurring at each whiteboard. In one embodiment, whiteboard data capture occurs continuously as the user writes. In an alternative embodiment, data capture occurs on-demand, upon receiving a user's directive such a vocal command or the push of a button. Projector monitoring process <b>550</b> monitors the presentation of slides and captures the slide data each time a new slide is presented.</p>
<p id="p-0064" num="0063">A process <b>530</b> for receiving personal characteristics of the attendees in a meeting includes a continuously executing process <b>532</b> for identifying each attendee. This process includes receiving data from the identification badges worn by everyone. In one embodiment, a visiting attendee's presence is logged <b>534</b> upon entering the premises. This can take place, for example, at a receptionist's desk when the badge is initially issued. In another embodiment, a frequent visitor may have a permanently issued badge. A sensor at each entrance detects the visitor's ingress and egress and logs the activity to the database. In a co-pending application U.S. patent application Ser. No. 09/714,785, filed Nov. 15, 2000, entitled “A NETWORKED PERIPHERAL FOR VISITOR GREETING, IDENTIFICATION, BIOGRAPHICAL LOOKUP AND TRACKING” and owned by the assignee of the present invention, such an automated identification system is disclosed whereby a visitor's proximity to a visitor kiosk enables the system to detect and log her presence. In another embodiment, a biometric data logging function <b>536</b> is provided in addition to attendance logging. For example, identification can be based on chromatic information and facial features of an attendee's image. Identity verification can be made at a receptionist's desk or at a visitor kiosk when a visiting attendee arrives.</p>
<p id="p-0065" num="0064">In yet another embodiment, biometric data logging process <b>536</b> includes location tracking capability. Multiple audio inputs are provided to allow for triangulating the location of each user. Alternatively, multiple Bluetooth receivers provide the triangulation data by noting the time differential between signals received from a user as she moves about the room. Knowledge of an attendee's location permits the information support system to route displayed information proximate her location. For example, a capture device (e.g., microphone sensitivity) is adjusted as a user moves about the room. As another example, if an attendee moves from one meeting room to another meeting room, any relevant information being provided to her is automatically relocated. Used in conjunction with the other identification aspects, certain sensitive data can be selectively displayed depending on who is in a particular room and what their access privileges are. More generally, the actions permitted can be made dependent on the location of an attendee and the other members in the vicinity of the attendee.</p>
<p id="p-0066" num="0065">Referring to <figref idref="DRAWINGS">FIGS. 5-7</figref>, data provided by data capture portion <b>110</b> and data interaction component <b>130</b> contain various cues which are discovered by the monitoring processes outlined in <figref idref="DRAWINGS">FIG. 5</figref>. These cues are fed to a data retrieval daemon <b>710</b>. Similarly, text extracted by the processes shown in <figref idref="DRAWINGS">FIG. 6</figref> feed into data retrieval daemon <b>710</b>. The data retrieval daemon comprises various continuously executing programs which provide information services to support a meeting in response to the various cues detected during the meeting.</p>
<p id="p-0067" num="0066">There are cues which are explicit directives from the participants in the meeting. For example, data manipulation services <b>720</b> include document management activities such as file editing, file renaming, and file creation and deletion. On-demand internet access <b>722</b> is provided. Various messaging functions <b>724</b> are available to the members of the meeting, including electronic mail, voice over internet capability, message posting on the internet and in newsgroups, and the like. Communication functions <b>726</b> allow members to establish conference calls, connect to remote members, and so on. Information retrieval services <b>728</b> permit meeting members to access documents, meeting schedules, previously captured meetings, and so on. Typical cues include commands received from the voice command interface and commands entered from a keyboard or written on a whiteboard. The virtual reality generator can provide a virtual input interface through which user commands can be issued. Typical explicit commands include actions such as accessing a particular web page, composing email and specifying a list of recipients, and so on.</p>
<p id="p-0068" num="0067">In accordance with the invention, data retrieval daemon <b>710</b> processes implicit cues to provide information services in a context-driven manner. Implicit cues are inferred from the context of the meeting and do not result from a participant issuing a command. Thus, an explicit command might be “COMPUTER, display April shipping orders.” An implicit information retrieval cue might be issued indirectly when a speaker mentions the name of a document, but does not call out to actually retrieve it. The data retrieval daemon, continually monitoring the video and audio recordings and using information provided by the processes outlined in <figref idref="DRAWINGS">FIGS. 5 and 6</figref>, makes such determinations. For example, data retrieval daemon <b>710</b> knows that a certain speaker is a visitor. By accessing the visitor's meeting schedule, the data retrieval daemon can access the web or some other information source for traffic conditions or transportation schedules and provide the information to the visitor at an appropriate time.</p>
<p id="p-0069" num="0068">As another example, data retrieval daemon <b>710</b> can be programmed to act on the detection of certain keywords or phrases. In response, the web can be accessed and a search made to pull up whitepapers, companies, or other such information for the participants of the meeting.</p>
<p id="p-0070" num="0069">In addition to the services discussed above, context-driven searches <b>730</b>, <b>732</b> proceed in the background. The information, whether stored in database component <b>160</b> or obtained over the web, is accumulated and ranked for relevance. As additional context is received, a further refinement of the search criteria is made to update candidate information. At the appropriate time, the information can be presented.</p>
<p id="p-0071" num="0070">There are several ways in which meeting content can be used to retrieve relevant documents for real time feedback. For example, the audio transcription of a meeting within a time window, either from the beginning of the meeting to the current time, or within sliding time frame, can be used as query input. A single input can be formed collectively from all participants or separate tracks can be allocated to each speaker. Given a window of such text input, many conventional information retrieval techniques can be applied.</p>
<p id="p-0072" num="0071">Typical preprocessing techniques including stop word removal, stemming, morphological analysis, are applied first. Then, content words are identified based on the word frequency difference between the input text and English norm. Documents in the database are scored by how many content words they contained, possibly weighted by other factors such as how many other documents also contain a given content word. Finally, documents are ranked according to their scores on a given set of query words. The result may contain a fixed number of top ranked documents or a variable number of documents based on a threshold on the score. Of course, spoken input is very unstructured and noisy due to speech recognition errors. Techniques are know which can compensate for significant levels of errors.</p>
<p id="p-0073" num="0072">It is possible to provide a set of predetermined topics. The transcribed text in a time window can be classified to one of these topics using known document classification techniques. Typically, a single profile of word frequencies is computed for all documents belonging to the same category. To classify a new document, the word frequency profile of the new document is compared to the profile of each category. The document is classified to the category whose profile is most similar, and the profile for that category is then recomputed to reflect the inclusion of the new document. If none of the category profiles is of sufficient similarity, as determined by some threshold criteria, no topic is returned. It is reasonable to assume that documents within a category bear some relevancy to the meeting and would be retrieved for possible use.</p>
<p id="p-0074" num="0073">The set of topics may be manually defined, such as folders in a hierarchical document management system, or automatically inferred by a document categorization system. Document categorization works in similar ways to document classification. However, instead of trying to find a closest matching topic, document categorization analyzes the distribution of document profiles to find natural groupings using clustering algorithms.</p>
<p id="p-0075" num="0074">Several different embodiments of the foregoing are possible. For example, a singly content window can be created for the entire meeting or on a per speaker basis. Similarly, query retrieval can be applied to the entire web, a corporate database, or to an individual's document collection on the eCabinet™. Feedback can be configured such that all users receive the same set of documents, or each receive a different set of documents tailored to his preference, or a combination of the two.</p>
<p id="p-0076" num="0075">Information retrieval from a recorded presentation basicallly follows the same process after applying oprical character recognition (“OCR”). It is worth pointing out some important characteristic about presentation recordings. Slides are usually much more structured than conversation, often having topics and subtopics that are clearly denoted. The appearance of characters on the slides (boldface, color, etc.) generally indicates the importance of such concepts. In addition, OCR performance is typically better than speech recognition and so the output is not as noisy. Slides usually contain short phrases, providing very high information content. Furthermore, the timing on slides provides an explicit window for its context. Based on these characteristics, one would expect more precise retrieval performance from presentations.</p>
<p id="p-0077" num="0076">Known online handwriting recognition software can be integrated with whiteboard capture devices to provide an additional query input. Since writings on whiteboard may be spotty and contain drawings or other non-character markings, a filter can be used to recognize only a limited number of words. Special keywords such as “print” or “search” may be used to guide the interpretation of subsequent strokes and trigger appropriate actions.</p>
<p id="p-0078" num="0077">To further improve the performance of speech recognition, OCR results (from a slide presentation, for example) and relevant documents can provide a contextual backdrop for the process. It is well known that speech processing relies heavily on contextual information, such as grammars and dictionaries. Therefore, any information about the context of a meeting can help improve the performance of speech transcription. Because of the highly structured, compact representation of slides, they provide a good source for a domain specific lexicon which can be extracted to assist interpreting meeting content. For example, we often see special terms or acronyms presented in slides. Such information can provide an additional lexicon to a speech recognition engine.</p>
<p id="p-0079" num="0078">Documents related to meeting attendees can also provide a hint about meeting content. Once the meeting attendees are identified, documents recently read by one or more participants are very likely to be relevant to the discussion.</p>
<p id="p-0080" num="0079">Referring to <figref idref="DRAWINGS">FIG. 8</figref>, an embodiment of retrieval interface <b>254</b> is shown. <figref idref="DRAWINGS">FIG. 8</figref> shows in simplified detail an embodiment of a graphical user interface in accordance with the invention. The graphical interface comprises an information area <b>810</b>. This area displays information such as the time, date and a title associated with the meeting for which the data was captured. Other information can be associated in addition to or instead of the information shown in the embodiment shown in <figref idref="DRAWINGS">FIG. 8</figref>. This area can be made read-only, though area <b>810</b> may be selectively write-enabled for authorized users to edit variable information such as the meeting title.</p>
<p id="p-0081" num="0080">A textual search area <b>820</b> provides text searches of the captured data. A search string can be specified via a text box <b>822</b>. Check boxes for speech <b>824</b> and for slides <b>826</b> specify where the search is to take place. In one embodiment, the check boxes have a mutually exclusive behavior, namely, only one or the other box can be checked, to limit the search to one or the other of the captured data. In another embodiment, both check boxes <b>824</b>, <b>826</b> can be checked so that the search is done in both of the captured data sources.</p>
<p id="p-0082" num="0081">A speaker area <b>830</b> is provided so that playback of the captured data can be selected based on the selected speaker. This area comprises a series of thumbnail images <b>832</b> of each speaker, along with identifying information such as their names. Since there might be a large number of attendees in a given meeting and since the interface has limited area, only so many speakers at a time can be displayed. In the embodiment shown in <figref idref="DRAWINGS">FIG. 8</figref>, three thumbnail images are provided. A slider control <b>834</b> allows the user to display other members in the meeting, three at a time by sliding the control in the left and right direction. This action is can be achieved by moving a pointing device such as a mouse, for example, to the slider button and dragging the mouse while “clicking” a mouse button. As the slider is manipulated in this way, the thumbnail images are updated with images of other members. The order of appearance can be alphabetical by name, by rank in the company, by age, and so on.</p>
<p id="p-0083" num="0082">A playback control area <b>840</b> provides graphical metaphors for conventional playback buttons similar to those found on a compact disc player. There is a play button <b>842</b> and a stop button <b>841</b>. A fast forward button <b>843</b> plays back the captured data at an accelerated speed. A fast forward-to-end button <b>844</b> has a context dependent action. In the case where the captured data is a completed recording, the fast forward-to-end button simply takes the user to the end of the recorded data. In the case where the meeting is still in progress and data capture is still occurring, this button simply takes the user to real time viewing, allowing her to view the data as it is being captured. A rewind button <b>845</b> plays the captured data in the reverse direction in an accelerated manner. A rewind-to-the-beginning button <b>846</b> takes the user to the beginning of the recording session.</p>
<p id="p-0084" num="0083">The fast forward and rewind speeds can be varied. In one embodiment, additional controls (not shown) are provided which specify these speeds; e.g. 2×, 4×, and so on. In an alternative embodiment, simply clicking one of the buttons <b>843</b>, <b>845</b> multiple times can effectuate an increase in the fast forward or rewind speed. The speed increases can cycle from 1× speed up to a maximum speed and then drop back to 1× speed.</p>
<p id="p-0085" num="0084">A time selection area <b>850</b> allows a user to enter a time indication to go directly to a specific time in a recorded (or presently recording) meeting. The time specification can be a relative time; i.e., so many minutes into a meeting. Alternatively, the time can be specified by the particular time of the day; i.e., playback of the meeting from 3:30 PM.</p>
<p id="p-0086" num="0085">In accordance with the invention, retrieval interface <b>254</b> allows playback of one data stream, for example video data, in synchrony with playback of another data stream, for example projection data. A master data selector <b>862</b> and a slave data selector <b>864</b>, and a master time scale area <b>870</b> and a slave time scale area <b>874</b> are provided to facilitate this capability. The master and slave data selectors <b>862</b>, <b>864</b> allow the user to select which captured data will be the “master” and which captured data will be the “slave”. The captured data include video recordings, audio recordings, slide presentations, and whiteboard data. Data selectors <b>862</b>, <b>864</b> are dropdown menus which list the captured data available. <figref idref="DRAWINGS">FIG. 8</figref> illustrates slave data selector <b>864</b> with its dropdown menu exposed, showing the list and showing that ‘slides captured data’ is selected.</p>
<p id="p-0087" num="0086">Controls <b>840</b>, in particular fast forward and rewind buttons <b>843</b> and <b>845</b> respectively, can be used to view the data stream in an event-driven manner. For example, in the case of projection data an event might be the presentation of a new slide. During the display of a whiteboard recording, an event might be the completion of a writing action so that only the final written expression is displayed. In the case of a speech segment, the event might be the silent period of a pause during the discourse in the meeting so that segments of speech can be more easily skipped over.</p>
<p id="p-0088" num="0087">Time scales <b>870</b>, <b>872</b> indicate the play time during playback. Time indicators <b>871</b>, <b>873</b> move back and forth as the data is forwarded and rewound. A synchronization box <b>874</b> provides synchronized playback between the master and slave data when the box is checked. Time indicators <b>871</b>, <b>873</b> can be manipulated by the user as another means by which to control the playback of the captured data, simply by dragging an indicator back and forth. In synchronized mode, dragging either of the time indicators causes playback of both the master and the slave data to advance or reverse in synchrony. If synchronization box <b>874</b> is un-checked, then dragging one indicator will not affect the playback action of the other.</p>
<p id="p-0089" num="0088">A master window <b>882</b> and a slave window <b>884</b> display the captured data being played back. A volume control <b>886</b> is provided for audio playback.</p>
<p id="p-0090" num="0089">When playback is stopped, the still images in the windows <b>882</b>, <b>884</b> can be “cut” and “pasted” into other documents. Still images can be stored in any of a number of known formats such as JPEG, GIF, bitmap, and so on.</p>
<p id="p-0091" num="0090">A video clip composition area <b>890</b> provides controls for specifying clips of video which can be saved. A video clip of interest is selected by shuttling through the video to specify the desired segment. The “compose” button concatenates the selected segments to produce the video composition. The “clear” button serves as a reset to so that a new video composition can be produced. A time indicator informs the user as to the length of the video clip. The video clips can be saved in any of the known video formats; e.g., MPEG, AV1.</p>
<p id="p-0092" num="0091"><figref idref="DRAWINGS">FIG. 9</figref> shows an embodiment of a networked arrangement of the information support system in accordance with the present invention. Here, three instances of support system <b>100</b> are provided in three locations around the world, namely, a California site <b>902</b>, a site in New York <b>904</b>, and a site in Tokyo <b>906</b>. Each site shares the same database component <b>160</b>. In this embodiment, database component <b>160</b> is shown contained in communication network <b>170</b> to emphasize that the sites share a commonly accessible database component. Any of a number of known data sharing techniques can be used to implement this shared component. In practice, there may be multiple databases using synchronization procedures such as volume shadowing to ensure that each site logically perceives database component <b>160</b> as a single database entity. Communication network <b>170</b> allows an intranet local to each site to be accessible by the other sites. Appropriate firewall schemes and security measures can be implemented to ensure the secured access to the individual intranets.</p>
<p id="p-0093" num="0092">A particularly useful meeting paradigm is possible with the architecture of <figref idref="DRAWINGS">FIG. 9</figref>, namely, the 24 hour meeting. The meeting can begin in New York. Participants might generate documents, notes, and so on which are stored in the database. Later in the day, the California site comes on line. Since all aspects of the New York meeting have been recorded and are readily accessible, the California meeting members can quickly come up to speed on the highlights of the meeting and join in on the discussion. As the day comes to end in New York, the meeting can continue in California. As the California site begins to wrap up its efforts, the Tokyo site can join in to continue the effort. The Tokyo members would have complete access to all the notes, whiteboard sketches, presentations and even the comments and contributions of the attendees of the New York and California meetings. The cooperating of the information support systems <b>902</b>-<b>906</b> ensure that needed documents and other materials are prepared and ready so that as each site begins its work, a seamless transition of the meeting from one site to the next site.</p>
<p id="p-0094" num="0093">The figures sometimes show overlapping functionality. For example, data interaction devices are sources of for initiating actions by the inventive information support system; however, analysis of the information from the data capture devices can also result in context-driven initiation of actions. Hence, allocations of functionality among the figures are provided for the purpose of simplifying the explanation of the various aspects of the invention and should not be construed as limiting how the functions are provided. In general, there are no bright line distinctions or intimations as to where the actual functionality is located. Persons of ordinary skill will understand that such specific details are determined by design constraints, the hardware and software that is available at the time, performance considerations, cost considerations, and so forth.</p>
<?DETDESC description="Detailed Description" end="tail"?>
</description>
<us-claim-statement>What is claimed is:</us-claim-statement>
<claims id="claims">
<claim id="CLM-00001" num="00001">
<claim-text>1. A computer executable method for managing information during a meeting comprising steps of:
<claim-text>recording activities among participants during said meeting to produce recorded meeting data;</claim-text>
<claim-text>analyzing, while said meeting is ongoing, said recorded meeting data absent direct human intervention to identify textual content contained in said recorded meeting data, said textual content indicative of a participant directive representing an action on said information to be performed during said meeting; and</claim-text>
<claim-text>in response to identifying said participant directive in said recorded meeting data, performing said action represented by said participant directive on said information while said meeting is ongoing,</claim-text>
<claim-text>thereby facilitating the management of information during said meeting.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00002" num="00002">
<claim-text>2. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref> wherein said recorded meeting data includes video recordings and audio recordings.</claim-text>
</claim>
<claim id="CLM-00003" num="00003">
<claim-text>3. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref> wherein said action includes an activity selected from the group consisting of: document management activities, document editing activities, messaging functions, establishing communication with a new meeting participant, and manipulation of said recorded meeting data.</claim-text>
</claim>
<claim id="CLM-00004" num="00004">
<claim-text>4. The method of <claim-ref idref="CLM-00003">claim 3</claim-ref> wherein said document management activity includes accessing said recorded meeting data during said meeting.</claim-text>
</claim>
<claim id="CLM-00005" num="00005">
<claim-text>5. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref> wherein said activities include verbal communication, written communication, presentation of prepared material using a projection system.</claim-text>
</claim>
<claim id="CLM-00006" num="00006">
<claim-text>6. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref> further including ascertaining identities of said participants and selectively effectuating said participant directive based on said identities.</claim-text>
</claim>
<claim id="CLM-00007" num="00007">
<claim-text>7. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref> further including tracking locations of said participants and selectively effectuating said participant directive based on said locations.</claim-text>
</claim>
<claim id="CLM-00008" num="00008">
<claim-text>8. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref> further including locating said participants in different geographic locations.</claim-text>
</claim>
<claim id="CLM-00009" num="00009">
<claim-text>9. A computer executable method for providing information services during a meeting involving two or more participants comprising steps of:
<claim-text>producing a continuous video recording of at least one of said participants for the duration of said meeting;</claim-text>
<claim-text>producing a continuous audio recording of at least one of said participants for the duration of said meeting;</claim-text>
<claim-text>storing said video and said audio recordings in a data store;</claim-text>
<claim-text>analyzing, while said meeting is ongoing, either or both of said video recording or said audio recording absent direct human intervention to identify textual content contained therein to detect a participant directive in said textual content representing an action to be performed during said meeting; and</claim-text>
<claim-text>providing one or more information-related services based on said participant directive,</claim-text>
<claim-text>thereby providing information-related services based on gestures and vocal utterances made by said participants during said meeting.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00010" num="00010">
<claim-text>10. The method of <claim-ref idref="CLM-00009">claim 9</claim-ref> wherein said detecting a participant directive includes receiving participant input from an input device.</claim-text>
</claim>
<claim id="CLM-00011" num="00011">
<claim-text>11. The method apparatus system of <claim-ref idref="CLM-00009">claim 9</claim-ref> wherein said detecting a participant directive is based on the context of the meeting.</claim-text>
</claim>
<claim id="CLM-00012" num="00012">
<claim-text>12. The method of <claim-ref idref="CLM-00009">claim 9</claim-ref> wherein said detecting a participant directive includes detecting an information retrieval cue, said method further including:
<claim-text>producing certain information based on said information retrieval cue, including accessing said data store to retrieve one or more segments of said video and said audio recordings and accessing one or more databases to retrieve information contained therein; and presenting said certain information to one or more of said participants.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00013" num="00013">
<claim-text>13. The method of <claim-ref idref="CLM-00012">claim 12</claim-ref> wherein said detecting an information retrieval cue includes receiving participant input from an input device.</claim-text>
</claim>
<claim id="CLM-00014" num="00014">
<claim-text>14. The method of <claim-ref idref="CLM-00012">claim 12</claim-ref> wherein said presenting certain information is a step of presenting said certain information to less than all of said participants.</claim-text>
</claim>
<claim id="CLM-00015" num="00015">
<claim-text>15. The method of <claim-ref idref="CLM-00012">claim 12</claim-ref> further including tracking locations of said participants, wherein said presenting said certain information is a step of selectively presenting said certain information based on said locations.</claim-text>
</claim>
<claim id="CLM-00016" num="00016">
<claim-text>16. The method of <claim-ref idref="CLM-00009">claim 9</claim-ref> wherein said information services include accessing information, editing information, assimilating information to produce new information, establishing communication with a new participant, transmitting and receiving messages, accessing a global information network, and accessing a local network.</claim-text>
</claim>
<claim id="CLM-00017" num="00017">
<claim-text>17. The method of <claim-ref idref="CLM-00016">claim 16</claim-ref> wherein said messages include electronic mail.</claim-text>
</claim>
<claim id="CLM-00018" num="00018">
<claim-text>18. The method of <claim-ref idref="CLM-00009">claim 9</claim-ref> wherein provision of said information services depends on permissions associated with said participants.</claim-text>
</claim>
<claim id="CLM-00019" num="00019">
<claim-text>19. The method of <claim-ref idref="CLM-00018">claim 18</claim-ref> further including identifying said participants to determine associated permissions.</claim-text>
</claim>
<claim id="CLM-00020" num="00020">
<claim-text>20. The method of <claim-ref idref="CLM-00009">claim 9</claim-ref> further including locating said participants in geographically distinct locations.</claim-text>
</claim>
<claim id="CLM-00021" num="00021">
<claim-text>21. A system for providing information services during an interaction between two or more participants, comprising:
<claim-text>a video capture component configured to produce a continuous video recording of at least one of said participants;</claim-text>
<claim-text>an audio capture component configured to produce a continuous audio recording of at least one of said participants;</claim-text>
<claim-text>a data storage component containing information, said data storage component in communication with said video and audio capture components and configured to store said video and audio recordings;</claim-text>
<claim-text>a detection component in communication with said video and audio capture components and configured to analyze either or both of said video recordings or said audio recording to identify a participant directive in textual content representing an action to be performed during said interaction; and</claim-text>
<claim-text>a service provision component in communication with said data storage component and configured to provide an information service based on said participant directive,</claim-text>
<claim-text>thereby providing information services during said interaction based on gestures and vocal utterances made by said participants.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00022" num="00022">
<claim-text>22. The system of <claim-ref idref="CLM-00021">claim 21</claim-ref> wherein said participant directive is an information retrieval cue, and wherein said service provision component is further configured to retrieve certain information from said data storage component in response to said information retrieval cue and to provide said certain information to said display component.</claim-text>
</claim>
<claim id="CLM-00023" num="00023">
<claim-text>23. The system of <claim-ref idref="CLM-00022">claim 22</claim-ref> wherein said certain information includes said continuous video recording and said continuous audio recording.</claim-text>
</claim>
<claim id="CLM-00024" num="00024">
<claim-text>24. The system of <claim-ref idref="CLM-00021">claim 21</claim-ref> further including a user input portion configured to receive user-provided input data, said detection component further configured to identify a participant directive from said user-provided input data.</claim-text>
</claim>
<claim id="CLM-00025" num="00025">
<claim-text>25. The system of <claim-ref idref="CLM-00021">claim 21</claim-ref> further including a participant identification component, said service provision component being further configured to selectively provide said information service based on identities of one or more of said participants.</claim-text>
</claim>
<claim id="CLM-00026" num="00026">
<claim-text>26. The system of <claim-ref idref="CLM-00025">claim 25</claim-ref> wherein said identities have associated permissions, said service provision component being further configured to selectively provide said information service based on said permissions.</claim-text>
</claim>
<claim id="CLM-00027" num="00027">
<claim-text>27. The system of <claim-ref idref="CLM-00025">claim 25</claim-ref> wherein said participant identification component is configured to provide participant locations, said service provision component being further configured to selectively provide said information service based on said participant locations.</claim-text>
</claim>
<claim id="CLM-00028" num="00028">
<claim-text>28. The system of <claim-ref idref="CLM-00021">claim 21</claim-ref> wherein said information service includes providing access functions over a communication network.</claim-text>
</claim>
<claim id="CLM-00029" num="00029">
<claim-text>29. The system of <claim-ref idref="CLM-00028">claim 28</claim-ref> wherein said information service includes data retrieval from a global communication network.</claim-text>
</claim>
<claim id="CLM-00030" num="00030">
<claim-text>30. The system of <claim-ref idref="CLM-00028">claim 28</claim-ref> wherein said information service includes data retrieval from a plurality of web sites.</claim-text>
</claim>
<claim id="CLM-00031" num="00031">
<claim-text>31. A computer executable method of utilizing plural information sources to enhance information management during a meeting between two or more attendees, comprising:
<claim-text>producing and storing a continuous audio-visual recording of one or more of said attendees, said audio-visual recording comprising a video data component and an audio data component;</claim-text>
<claim-text>extracting and storing textual information from said audio and video data components;</claim-text>
<claim-text>extracting and storing image information from said video data component;</claim-text>
<claim-text>analyzing said textual information or said image information to detect attendee action cues from said audio data component or said video data component;</claim-text>
<claim-text>accessing certain information from said information sources based on said attendee action cues; and</claim-text>
<claim-text>presenting said certain information,</claim-text>
<claim-text>thereby providing information services during said meeting on the basis of the actions of said attendees.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00032" num="00032">
<claim-text>32. The method of <claim-ref idref="CLM-00031">claim 31</claim-ref> wherein said accessing certain information includes searching through said audio-visual recording.</claim-text>
</claim>
<claim id="CLM-00033" num="00033">
<claim-text>33. The method of <claim-ref idref="CLM-00031">claim 31</claim-ref> wherein said detecting attendee action cues includes receiving explicit commands from an attendee.</claim-text>
</claim>
<claim id="CLM-00034" num="00034">
<claim-text>34. The method of <claim-ref idref="CLM-00031">claim 31</claim-ref> further including tracking locations of said attendees, said presenting certain information being based on said locations.</claim-text>
</claim>
<claim id="CLM-00035" num="00035">
<claim-text>35. The method of <claim-ref idref="CLM-00031">claim 31</claim-ref> further including editing information in said information sources based on said attendee action cues; and producing new information by assimilating portions of information in said information sources based on said attendee action cues.</claim-text>
</claim>
<claim id="CLM-00036" num="00036">
<claim-text>36. The method of <claim-ref idref="CLM-00031">claim 31</claim-ref> further includes identifying said attendees from said audio and video data components to produce attendee identifiers.</claim-text>
</claim>
<claim id="CLM-00037" num="00037">
<claim-text>37. The method of <claim-ref idref="CLM-00036">claim 36</claim-ref> further including manipulating information in said information sources based on said attendee action cues and said attendee identifiers.</claim-text>
</claim>
<claim id="CLM-00038" num="00038">
<claim-text>38. The method of <claim-ref idref="CLM-00037">claim 37</claim-ref> wherein said manipulating information includes editing said information.</claim-text>
</claim>
<claim id="CLM-00039" num="00039">
<claim-text>39. The method of <claim-ref idref="CLM-00031">claim 31</claim-ref> further including locating said attendees in different parts of the world.</claim-text>
</claim>
<claim id="CLM-00040" num="00040">
<claim-text>40. A system to facilitate the management of information during a meeting between two or more attendees, comprising:
<claim-text>an information storage portion configured to receive and store information and to access and provide information;</claim-text>
<claim-text>an audio-visual capture portion in communication with said information storage portion and configured to produce real-time video recordings and real-time audio recordings of one or more of said attendees;</claim-text>
<claim-text>a text classification portion in communication with said information storage portion and configured to produce and store textual information extracted from said audio and video recordings;</claim-text>
<claim-text>an image classification portion in communication with said information storage portion and configured to produce and store image information in said video recordings;</claim-text>
<claim-text>a cue detection portion configured to detect attendee action cues from said textual information during said meeting;</claim-text>
<claim-text>an information retrieval portion in communication with said information storage portion and configured to access information therefrom based on said attendee action cues;</claim-text>
<claim-text>an information manipulation portion in communication with said information storage portion and configured to manipulate information stored therein based on said attendee action cues; and</claim-text>
<claim-text>an information presentation portion operatively coupled to said information retrieval portion and said information manipulation portion and configured to display retrieved and manipulated information,</claim-text>
<claim-text>wherein information including said textual information and said image information can be retrieved and manipulated by gestures and voice input of said attendees during said meeting.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00041" num="00041">
<claim-text>41. The system of <claim-ref idref="CLM-00040">claim 40</claim-ref> further including an attendee identification portion coupled to said information storage portion and configured to identify said attendees from said audio and video recordings and to provide attendee identifiers.</claim-text>
</claim>
<claim id="CLM-00042" num="00042">
<claim-text>42. The system of <claim-ref idref="CLM-00041">claim 41</claim-ref> wherein said information presentation portion is further configured to selectively display said retrieved and manipulated information to attendees depending on said attendee identifiers.</claim-text>
</claim>
<claim id="CLM-00043" num="00043">
<claim-text>43. The system of <claim-ref idref="CLM-00040">claim 40</claim-ref> wherein said attendee action cues include directives to access said video and audio recordings of said meeting.</claim-text>
</claim>
<claim id="CLM-00044" num="00044">
<claim-text>44. The system of <claim-ref idref="CLM-00040">claim 40</claim-ref> wherein said cue detection portion is further configured to detect said attendee action cues on the basis of the context of said meeting.</claim-text>
</claim>
<claim id="CLM-00045" num="00045">
<claim-text>45. The system of <claim-ref idref="CLM-00040">claim 40</claim-ref> further including an attendee tracking portion configured to track movements and locations of said attendees.</claim-text>
</claim>
<claim id="CLM-00046" num="00046">
<claim-text>46. The system of <claim-ref idref="CLM-00045">claim 45</claim-ref> wherein said information presentation portion is further configured to selectively display said retrieved and manipulated information to attendees depending on said attendee identifiers, said attendee cues, and said locations of attendees.</claim-text>
</claim>
<claim id="CLM-00047" num="00047">
<claim-text>47. The system of <claim-ref idref="CLM-00045">claim 45</claim-ref> wherein said attendee identification portion includes creating new identifiers for unknown attendees.</claim-text>
</claim>
</claims>
</us-patent-grant>
