<us-patent-grant lang="EN" dtd-version="v4.2 2006-08-23" file="US07297929-20071120.XML" status="PRODUCTION" id="us-patent-grant" country="US" date-produced="20071106" date-publ="20071120">
<us-bibliographic-data-grant>
<publication-reference>
<document-id>
<country>US</country>
<doc-number>07297929</doc-number>
<kind>B2</kind>
<date>20071120</date>
</document-id>
</publication-reference>
<application-reference appl-type="utility">
<document-id>
<country>US</country>
<doc-number>11375008</doc-number>
<date>20060315</date>
</document-id>
</application-reference>
<us-application-series-code>11</us-application-series-code>
<us-term-of-grant>
<us-term-extension>20</us-term-extension>
</us-term-of-grant>
<classifications-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>H</section>
<class>01</class>
<subclass>J</subclass>
<main-group>40</main-group>
<subgroup>14</subgroup>
<symbol-position>F</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20071120</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
</classifications-ipcr>
<classification-national>
<country>US</country>
<main-classification>250221</main-classification>
<further-classification>250214 AL</further-classification>
<further-classification>250214 R</further-classification>
</classification-national>
<invention-title id="d0e53">Light sensor and processor incorporating historical readings</invention-title>
<references-cited>
<citation>
<patcit num="00001">
<document-id>
<country>US</country>
<doc-number>4329581</doc-number>
<kind>A</kind>
<name>Helfrich, Jr. et al.</name>
<date>19820500</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<patcit num="00002">
<document-id>
<country>US</country>
<doc-number>5329295</doc-number>
<kind>A</kind>
<name>Medin et al.</name>
<date>19940700</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<patcit num="00003">
<document-id>
<country>US</country>
<doc-number>5359577</doc-number>
<kind>A</kind>
<name>Hoshino et al.</name>
<date>19941000</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<patcit num="00004">
<document-id>
<country>US</country>
<doc-number>5491330</doc-number>
<kind>A</kind>
<name>Sato et al.</name>
<date>19960200</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<patcit num="00005">
<document-id>
<country>US</country>
<doc-number>5701058</doc-number>
<kind>A</kind>
<name>Roth</name>
<date>19971200</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<patcit num="00006">
<document-id>
<country>US</country>
<doc-number>5910653</doc-number>
<kind>A</kind>
<name>Campo</name>
<date>19990600</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<patcit num="00007">
<document-id>
<country>US</country>
<doc-number>5932861</doc-number>
<kind>A</kind>
<name>Iwaguchi et al.</name>
<date>19990800</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<patcit num="00008">
<document-id>
<country>US</country>
<doc-number>D435473</doc-number>
<kind>S</kind>
<name>Eckel et al.</name>
<date>20001200</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<patcit num="00009">
<document-id>
<country>US</country>
<doc-number>6396040</doc-number>
<kind>B1</kind>
<name>Hill</name>
<date>20020500</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<patcit num="00010">
<document-id>
<country>US</country>
<doc-number>6507286</doc-number>
<kind>B2</kind>
<name>Weindorf et al.</name>
<date>20030100</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<patcit num="00011">
<document-id>
<country>US</country>
<doc-number>6687515</doc-number>
<kind>B1</kind>
<name>Kosaka</name>
<date>20040200</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<patcit num="00012">
<document-id>
<country>US</country>
<doc-number>6806485</doc-number>
<kind>B2</kind>
<name>Jackson, Jr.</name>
<date>20041000</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<patcit num="00013">
<document-id>
<country>US</country>
<doc-number>2002/0084404</doc-number>
<kind>A1</kind>
<name>Jackson</name>
<date>20020700</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<patcit num="00014">
<document-id>
<country>US</country>
<doc-number>2002/0101166</doc-number>
<kind>A1</kind>
<name>Weindorf et al.</name>
<date>20020800</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<patcit num="00015">
<document-id>
<country>US</country>
<doc-number>2006/0132635</doc-number>
<kind>A1</kind>
<name>Land</name>
<date>20060600</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>348311</main-classification></classification-national>
</citation>
</references-cited>
<number-of-claims>21</number-of-claims>
<us-exemplary-claim>1</us-exemplary-claim>
<us-field-of-classification-search>
<classification-national>
<country>US</country>
<main-classification>250221</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>250214 AL</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>250214 R</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>250214 P</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>250214 D</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>2502141</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>2502081</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>34053926</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>340541</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>340555</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>340556</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>340557</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>356619</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>356620</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>356219</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>356221</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>356222</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>396 98</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>396108</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>396323</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>396335</main-classification>
</classification-national>
</us-field-of-classification-search>
<figures>
<number-of-drawing-sheets>8</number-of-drawing-sheets>
<number-of-figures>9</number-of-figures>
</figures>
<us-related-documents>
<related-publication>
<document-id>
<country>US</country>
<doc-number>20070215794</doc-number>
<kind>A1</kind>
<date>20070920</date>
</document-id>
</related-publication>
</us-related-documents>
<parties>
<applicants>
<applicant sequence="001" app-type="applicant-inventor" designation="us-only">
<addressbook>
<last-name>Cernasov</last-name>
<first-name>Andrei</first-name>
<address>
<city>Ringwood</city>
<state>NJ</state>
<country>US</country>
</address>
</addressbook>
<nationality>
<country>omitted</country>
</nationality>
<residence>
<country>US</country>
</residence>
</applicant>
<applicant sequence="002" app-type="applicant-inventor" designation="us-only">
<addressbook>
<last-name>De La Vega</last-name>
<first-name>Fernando R.</first-name>
<address>
<city>Ridgefield Park</city>
<state>NJ</state>
<country>US</country>
</address>
</addressbook>
<nationality>
<country>omitted</country>
</nationality>
<residence>
<country>US</country>
</residence>
</applicant>
<applicant sequence="003" app-type="applicant-inventor" designation="us-only">
<addressbook>
<last-name>Porawski</last-name>
<first-name>Donald J.</first-name>
<address>
<city>Cedar Grove</city>
<state>NJ</state>
<country>US</country>
</address>
</addressbook>
<nationality>
<country>omitted</country>
</nationality>
<residence>
<country>US</country>
</residence>
</applicant>
</applicants>
<agents>
<agent sequence="01" rep-type="attorney">
<addressbook>
<orgname>Ingrassia Fisher &amp; Lorenz</orgname>
<address>
<country>unknown</country>
</address>
</addressbook>
</agent>
</agents>
</parties>
<assignees>
<assignee>
<addressbook>
<orgname>Honeywell International, Inc.</orgname>
<role>02</role>
<address>
<city>Morristown</city>
<state>NJ</state>
<country>US</country>
</address>
</addressbook>
</assignee>
</assignees>
<examiners>
<primary-examiner>
<last-name>Le</last-name>
<first-name>Que T</first-name>
<department>2878</department>
</primary-examiner>
</examiners>
</us-bibliographic-data-grant>
<abstract id="abstract">
<p id="p-0001" num="0000">A sensor includes a light detector for dividing light in an environment into optical zones and detecting light from the optical zones. The sensor also includes a processor coupled to the light detector; and a memory coupled to the processor for storing the detected light as historic light readings.</p>
</abstract>
<drawings id="DRAWINGS">
<figure id="Fig-EMI-D00000" num="00000">
<img id="EMI-D00000" he="122.85mm" wi="156.63mm" file="US07297929-20071120-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00001" num="00001">
<img id="EMI-D00001" he="112.78mm" wi="96.18mm" file="US07297929-20071120-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00002" num="00002">
<img id="EMI-D00002" he="187.71mm" wi="132.84mm" orientation="landscape" file="US07297929-20071120-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00003" num="00003">
<img id="EMI-D00003" he="189.91mm" wi="127.85mm" orientation="landscape" file="US07297929-20071120-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00004" num="00004">
<img id="EMI-D00004" he="143.26mm" wi="165.61mm" file="US07297929-20071120-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00005" num="00005">
<img id="EMI-D00005" he="164.25mm" wi="168.99mm" file="US07297929-20071120-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00006" num="00006">
<img id="EMI-D00006" he="144.02mm" wi="170.69mm" file="US07297929-20071120-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00007" num="00007">
<img id="EMI-D00007" he="164.25mm" wi="157.48mm" file="US07297929-20071120-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00008" num="00008">
<img id="EMI-D00008" he="150.37mm" wi="101.60mm" file="US07297929-20071120-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
</drawings>
<description id="description">
<?BRFSUM description="Brief Summary" end="lead"?>
<heading id="h-0001" level="1">FIELD</heading>
<p id="p-0002" num="0001">Embodiments generally relate to methods and apparatus for detecting light.</p>
<heading id="h-0002" level="1">BACKGROUND</heading>
<p id="p-0003" num="0002">Typically, conventional ambient light sensors consist of a reversed biased photodiode, an amplifier, and an analog to digital converter (ADC). <figref idref="DRAWINGS">FIG. 1</figref> is a schematic diagram illustrating a conventional ambient light sensor <b>100</b>. Ambient light sensor <b>100</b> includes a reversed biased photodiode <b>102</b> for detecting light <b>104</b> that strikes ambient light sensor <b>100</b>. Photodiode <b>102</b> is coupled to a voltage source <b>106</b> and a resistor <b>108</b>. Resistor <b>106</b> is coupled to ground.</p>
<p id="p-0004" num="0003">Ambient light sensor <b>100</b> also includes an amplifier <b>110</b> coupled to photodiode <b>102</b>. Amplifier <b>110</b> amplifies the signal from photodiode <b>102</b>. Ambient light sensor <b>100</b> also includes an analog to digital converter (ADC) <b>112</b> coupled to amplifier <b>108</b>. ADC <b>112</b> converts the analog signal from amplifier into a digital signal for further processing. The spectral response of the photodiodes, such as photodiode <b>102</b>, may be tuned, to the extent possible by the photodiode, to mimic the spectral response of the human eye.</p>
<p id="p-0005" num="0004">Ambient light sensors, such as sensor <b>100</b>, may be incorporated into electronic devices such as televisions, computer monitors, and cell phones. In these devices, ambient light sensors provide environmental illumination information to backlight controllers or front light controllers of displays in the electronic devices. The information provided by the ambient light sensors assists in determining the brightness of the display in these devices. For example, if the ambient light sensor detects that the environmental illumination conditions are bright (e.g. a cell phone outside on a sunny day), the brightness of the display may be increased so that images on the displays may be viewable in the bright environmental conditions.</p>
<p id="p-0006" num="0005">These ambient light sensors, however, as designed to assume only homogeneous and static environmental illumination conditions. That is, these sensors assume that light from all angles is the same and the light does not change quickly over time. Because the ambient light sensors assume such illumination conditions, these sensors do not detect light well in settings where the intensity and direction of light may change rapidly, for example, within seconds or less. As such, these ambient light sensors are not suitable for devices such as avionics displays where lighting conditions may change rapidly. For example, fighter jets make rapid movements during flight, especially during combat. As such, the lighting conditions inside the cockpit of jet change rapidly as the jet maneuvers.</p>
<p id="p-0007" num="0006">Additionally, these ambient light sensors are not designed to account for certain conditions of the human eye, such as behavior of the eye when exposed to flashes of light, the dependence of eye sensitivity to illumination history, the response of the eye to extremely patterned light, and the adaptation of the eye to contrast. As such, these ambient light sensors are not suitable where these conditions may be a factor.</p>
<heading id="h-0003" level="1">SUMMARY</heading>
<p id="p-0008" num="0007">An embodiment of the invention concerns a sensor. The sensor includes a light detector for dividing light in an environment into optical zones and detecting light from the optical zones. The sensor also includes a processor coupled to the light detector. The processor is configured to determine characteristics of the optical zones based on the detected light. The sensor also includes a memory coupled to the processor for storing the detected light as historic light readings.</p>
<p id="p-0009" num="0008">Another embodiment of the invention concerns a method for detecting light. The method includes detecting light from an environment by dividing the light into optical zone and storing the detected light as historic light readings for the optical zones. The method also includes determining a luminosity of a light source based on the historic light readings.</p>
<p id="p-0010" num="0009">Yet another embodiment of invention concerns a sensor. The sensor includes means for dividing an ambient environment into optical zones and means for detecting light from the optical zones positioned adjacent to the dividing means. The sensor also includes means for processing the detected light coupled to the detecting means and means for storing the detected light as historic light readings coupled to the processing means.</p>
<p id="p-0011" num="0010">Additional embodiments will be set forth in part in the description which follows, and in part will be obvious from the description, or may be learned by practice of the invention. The embodiments will be realized and attained by means of the elements and combinations particularly pointed out in the appended claims.</p>
<p id="p-0012" num="0011">It is to be understood that both the foregoing general description and the following detailed description are exemplary and explanatory only and are not restrictive of the invention, as claimed.</p>
<p id="p-0013" num="0012">The accompanying drawings, which are incorporated in and constitute a part of this specification, illustrate several embodiments and together with the description, serve to explain the principles of the invention.</p>
<?BRFSUM description="Brief Summary" end="tail"?>
<?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?>
<description-of-drawings>
<heading id="h-0004" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading>
<p id="p-0014" num="0013"><figref idref="DRAWINGS">FIG. 1</figref> is a diagram illustrating a conventional ambient light sensor.</p>
<p id="p-0015" num="0014"><figref idref="DRAWINGS">FIG. 2</figref> is a diagram illustrating a sensor consistent with embodiments of the invention.</p>
<p id="p-0016" num="0015"><figref idref="DRAWINGS">FIGS. 3A and 3B</figref> are diagrams illustrating arrangements of a multi-element sensor consistent with embodiments of the invention.</p>
<p id="p-0017" num="0016"><figref idref="DRAWINGS">FIGS. 4</figref>, <b>5</b>, and <b>6</b> are diagrams illustrating an optical arrangement for a multi-element sensor consistent with embodiments of the invention.</p>
<p id="p-0018" num="0017"><figref idref="DRAWINGS">FIG. 7</figref> is a diagram illustrating a multi-element sensor consistent with embodiments of the invention.</p>
<p id="p-0019" num="0018"><figref idref="DRAWINGS">FIG. 8</figref> is a diagram illustrating a method of sensing light consistent with embodiments of the invention.</p>
</description-of-drawings>
<?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?>
<?DETDESC description="Detailed Description" end="lead"?>
<heading id="h-0005" level="1">DETAILED DESCRIPTION</heading>
<p id="p-0020" num="0019">Embodiments of the invention are directed to a light sensor and a light sensing method. Accordingly to embodiments, the light sensor includes a multi-element light detector for detecting light in multiple optical zones in the environment. The multi-element light detector allows the light sensor to detect light in the optical zones and determine the characteristics of the environmental illumination conditions for each optical zone.</p>
<p id="p-0021" num="0020">Specifically, the multi-element light detector divides the environment into optical zones so that illumination conditions for each zone may be considered instead of considering the environment as a whole. By dividing the environment into optical zones, the light sensor may detect the environmental illumination conditions for the different optical zones. As such, the light sensor may determine the characteristics of the environmental illumination conditions for each optical zone.</p>
<p id="p-0022" num="0021">For example, the light sensor may determine the spatial characteristics of the environmental illumination conditions. The light sensor may determine the spatial distribution of light, spatial differentials of the environmental illumination conditions, and the spatial density of the environmental illumination conditions.</p>
<p id="p-0023" num="0022">Further, the light sensor stores the detected light in memory as historic data for the different optical zones. The light sensor may use the historic data to determine temporal characteristics of the environmental illumination conditions. For example, the light sensor may determine the rate of change in the illumination levels. Also, the historic data may be used to determine the spatial characteristics of the environmental illumination conditions over time.</p>
<p id="p-0024" num="0023">Further, the light sensor may use the detected environmental illumination conditions to determine the behavior of the human eye to these conditions. The light sensor may apply the environmental illumination conditions to models of human eye behavior. For example, the light sensor may use such models as Ricco's law, Bloch's law, Weber's law, Broca-Sulzer effect, Ferry-Porter Law, and Brucke-Bartley Effect. Also, the light sensor may use viewer specific metrics such as eye mobility, tolerances to visual excitations, and time constants.</p>
<p id="p-0025" num="0024">The light sensor may be used with electronic devices, which include displays, such as televisions, cell phones, cameras, avionic devices, and computer displays. The light sensor, when coupled to these electronic devices, uses the detected light from the optical zones to determine the brightness for the displays. The light sensor assists in determining the driving signal of the display by applying the historic data, the modeled eye behavior, spatial conditions of the environment, or temporal conditions of the environment to determine the display brightness best suited for the environmental conditions.</p>
<p id="p-0026" num="0025">By detecting light in optical zones instead of the whole environment, the light sensor may detect the environmental illumination conditions for non-homogenous illumination conditions. Further, by storing the detected illumination conditions over time, the light sensor may detect illumination conditions that change rapidly. This allows the light sensor to detect light well in settings where the intensity and direction of light may change rapidly. Further, by determining human eye behavior in response to the illumination conditions, the light sensor may assist electronic devices to display images tailored to the human user and environment.</p>
<p id="p-0027" num="0026">Reference will now be made in detail to embodiments of the invention, examples of which are illustrated in the accompanying drawings. Wherever possible, the same reference numbers will be used throughout the drawings to refer to the same or like parts.</p>
<p id="p-0028" num="0027"><figref idref="DRAWINGS">FIG. 2</figref> is a diagram illustrating a light sensor <b>200</b> consistent with embodiments. One skilled in the art will realize that light sensor <b>200</b> illustrated in <figref idref="DRAWINGS">FIG. 2</figref> is a generalized diagram and that other components may be added or existing components may be removed or modified.</p>
<p id="p-0029" num="0028">Light sensor <b>200</b> may be used in various types of electronic devices such as televisions, camera, computer displays, avionics controls, and cell phones. Light sensor <b>200</b> may be used to detect the environmental illumination conditions in which the electronic devices are located. One skilled in the art will realize that light sensor <b>200</b> may be used with any type of electronic device.</p>
<p id="p-0030" num="0029">Light sensor <b>200</b> may be used in these electronic devices to determine the lighting conditions of a display of the electronic devices. For example, as illustrated in <figref idref="DRAWINGS">FIG. 2</figref>, light sensor <b>200</b> may be coupled to display driver <b>212</b>. Light sensor <b>200</b> may provide display driver <b>212</b> with information regarding the environmental illumination conditions. Display driver <b>212</b> may use the information to power the display at appropriate levels for the environmental illumination conditions.</p>
<p id="p-0031" num="0030">Additionally, light sensor <b>200</b> may be independently housed. As such, light sensor may be coupled to these electronic devices though a typical input/output connection.</p>
<p id="p-0032" num="0031">As shown in <figref idref="DRAWINGS">FIG. 2</figref>, light sensor <b>200</b> includes a multi-element sensor <b>202</b> for detecting light that strikes light sensor <b>200</b>. Multi-element sensor <b>202</b> includes multiple light detector elements for detecting light at wide angles and in multiple optical zones.</p>
<p id="p-0033" num="0032">Multi-element sensor <b>202</b> may include any number of light detector elements for detecting light at wide angles and in multiple optical zones. The multiple light detector elements are divided into optical zones. Each light detector element would detect light from different optical zones in the environment, which would represent an illumination zone. The light detector elements would detect the illumination information for the environment for optical zone. Examples of illumination information may be brightness, color, direction, or combination thereof.</p>
<p id="p-0034" num="0033">The multiple light detector elements of multi-element sensor <b>202</b> may be arranged in any pattern to cover the wide angles and multiple optical zones. For example, the light detector elements may be arranged in an array of elements in order to detect light from different angles and for different optical zones.</p>
<p id="p-0035" num="0034">The light detector elements may be any hardware, software, or combination thereof for detecting light. For example, light detector elements may be semiconductor light detectors such as photodiodes or reverse-biased photodiodes. One skilled in the art will realize that light detector elements may be any type of device capable of detecting light.</p>
<p id="p-0036" num="0035">Additionally, multi-element sensor <b>202</b> may include optics to focus the light on the multiple light detector elements. Within each illumination zone, the optics may be non-imaging. For example, multi-element sensor <b>202</b> may include a lens for focusing light from different angles onto the different light detector elements.</p>
<p id="p-0037" num="0036"><figref idref="DRAWINGS">FIGS. 3A</figref>, <b>3</b>B, <b>4</b>, <b>5</b>, <b>6</b>, and <b>7</b> (described below) are diagrams illustrating several exemplary multi-element sensors that may be used as multi-element sensor <b>202</b>. One skilled in the art will realize that multi-element sensors illustrated in <figref idref="DRAWINGS">FIGS. 3A</figref>, <b>3</b>B, <b>4</b>, <b>5</b>, <b>6</b>, and <b>7</b> are generalized diagrams and that other components may be added or existing components may be removed or modified.</p>
<p id="p-0038" num="0037"><figref idref="DRAWINGS">FIGS. 3A and 3B</figref> illustrates two examples of the arrangement of the multiple detector elements in a multi-element sensor <b>202</b>. <figref idref="DRAWINGS">FIG. 3A</figref> illustrates an arrangement <b>302</b> in which the multiple detector elements <b>304</b> may be arranged in a circular pattern. As shown in <figref idref="DRAWINGS">FIG. 3A</figref>, detector elements <b>304</b> are arranged in a circular pattern with eight detector elements arranged around one detector element. The different detector elements <b>304</b> detect light from different optical zones in the environment.</p>
<p id="p-0039" num="0038"><figref idref="DRAWINGS">FIG. 3B</figref> illustrates another arrangement <b>310</b> in which multiple elements may be arranged in a square array. As shown in <figref idref="DRAWINGS">FIG. 3B</figref>, light detector elements <b>304</b> are arranged in a four by four square array. The different light detector elements <b>304</b> detect light from different optical zones in the environment.</p>
<p id="p-0040" num="0039">In both arrangement <b>302</b> and <b>310</b>, each light detector element may be composed of a single or multiple light detectors. For example, each light detector element may be composed of a single or multiple semiconductor light detectors such as photodiodes or reverse-biased photodiodes and associated hardware.</p>
<p id="p-0041" num="0040">In order for the light detector elements <b>304</b> to detect light from the different optical zones of the environment, light from the environment must be focused onto multi-element sensor <b>202</b> from different angles including wide angles. <figref idref="DRAWINGS">FIGS. 4</figref>, <b>5</b>, and <b>6</b> are diagrams illustrating different optical arrangements for focusing light onto multi-element sensor <b>202</b> including light detector elements <b>302</b>. <figref idref="DRAWINGS">FIGS. 4</figref>, <b>5</b>, and <b>6</b> are illustrated using arrangement <b>302</b>. Additionally, the optical arrangements may be used with arrangement <b>310</b> illustrated in <figref idref="DRAWINGS">FIG. 3B</figref></p>
<p id="p-0042" num="0041">As illustrated in <figref idref="DRAWINGS">FIG. 4</figref>, a wide angle refractive lens <b>402</b> is placed in front of multi-element sensor <b>202</b>. Wide angle refractive lens <b>402</b> focuses the light from different angles in the environment including wide angles. Wide angle refractive lens <b>402</b> focuses the light from the environment on light detector elements <b>304</b> of multi-element sensor <b>202</b>.</p>
<p id="p-0043" num="0042"><figref idref="DRAWINGS">FIG. 5</figref> is a diagram illustrating another optical arrangement for focusing light onto multi-element sensor <b>202</b>. As illustrated in <figref idref="DRAWINGS">FIG. 5</figref>, a prismatic reflector <b>502</b> and a lens <b>504</b> focus light from the environment onto multi-element sensor <b>202</b>. Multi-element sensor <b>202</b> is positioned in front of prismatic reflector <b>502</b> and lens <b>504</b>. Prismatic reflector <b>502</b> reflects light from the environment onto lens <b>504</b>. Lens <b>504</b> focuses the reflected light from prismatic reflector <b>502</b> onto multi-element sensor <b>202</b>.</p>
<p id="p-0044" num="0043"><figref idref="DRAWINGS">FIG. 6</figref> is a diagram illustrating another optical arrangement for focusing light onto multi-element sensor <b>202</b>. As illustrated in <figref idref="DRAWINGS">FIG. 6</figref>, a parabolic reflector <b>602</b> and a lens <b>604</b> focus light from the environment onto multi-element sensor <b>202</b>. Multi-element sensor <b>202</b> is positioned in front of parabolic reflector <b>602</b> and lens <b>604</b>. Parabolic reflector <b>602</b> reflects light from the environment onto lens <b>604</b>. Lens <b>604</b> focuses the reflected light from parabolic reflector <b>602</b> onto multi-element sensor <b>202</b>.</p>
<p id="p-0045" num="0044"><figref idref="DRAWINGS">FIG. 7</figref> illustrates a multi-element sensor <b>202</b> with discrete detector elements <b>702</b>. As illustrated in <figref idref="DRAWINGS">FIG. 7</figref>, each discrete detector element <b>702</b> is directed at a different angle. As such, each discrete detector element <b>702</b> detects light from a different optical zone. <figref idref="DRAWINGS">FIG. 7</figref> illustrates discrete detector elements <b>702</b> arranged at different angles in one plane. Discrete detector elements <b>702</b> may be arranged at different angles in multiple planes.</p>
<p id="p-0046" num="0045">Each detector element <b>702</b> may be composed of narrow angle optics and light detector elements. Each detector element <b>702</b> may be composed of a single or multiple light detector elements. For example, each element may be composed of a single or multiple semiconductor light detectors such as photodiodes or reverse-biased photodiodes.</p>
<p id="p-0047" num="0046">Returning to <figref idref="DRAWINGS">FIG. 2</figref>, light sensor <b>200</b> also includes a signal conditioner <b>204</b>. Signal conditioner <b>204</b> may be any type of circuitry to modify the signal from multi-element light sensor <b>202</b> as required by light sensor <b>200</b>. For example, signal conditioner <b>204</b> may be an amplifier for amplifying the signal from multi-element sensor <b>202</b>. Signal conditioner <b>204</b> may also be a noise filter for removing noise from the signal from multi-element sensor <b>202</b>. One skilled in the art will realize that the above examples of signal conditioner <b>204</b> are exemplary and signal conditioner <b>204</b> may be any type of hardware, software, or combination thereof to modify the signal from multi-element light sensor <b>202</b> as required by light sensor <b>200</b>. Alternatively, light sensor <b>200</b> may not require a signal conditioner <b>204</b>.</p>
<p id="p-0048" num="0047">Light sensor <b>200</b> also includes an analog to digital converter (ADC) <b>206</b> coupled to the signal conditioner. If a signal conditioner <b>204</b> is not included in light sensor <b>200</b>, ADC <b>206</b> is coupled directly to multi-element light sensor <b>202</b>. ADC <b>206</b> may be any type of hardware, software, or combination thereof to convert signals from analog to digital.</p>
<p id="p-0049" num="0048">Light sensor <b>200</b> also includes a processor <b>208</b> coupled to ADC <b>206</b>. Processor <b>206</b> is coupled to a memory <b>210</b>. Processor <b>206</b> may be any type of hardware, software, or combination thereof to receive signals from multi-element sensor <b>202</b> and perform processing on the signal. For example, processor <b>206</b> may be a digital signal processor (DSP). Memory <b>210</b> may be any type of memory capable of storing information. For example, memory <b>210</b> may be semiconductor media, such as DRAM, SRAM, EPROM, EEPROM, or memory stick.</p>
<p id="p-0050" num="0049">Processor <b>208</b> in combination with memory <b>210</b> performs processing such as analyzing, manipulating, and storing signals from multi-element sensor <b>202</b>. Additionally, processor <b>208</b> in combination with memory <b>210</b> may store the signals from sensor <b>202</b> as zone illumination information over a period of time.</p>
<p id="p-0051" num="0050">Processor <b>208</b> would receive the signal from the different light detector elements in multi-element sensor <b>202</b> and store the signals from the respective light detectors as illumination information for that optical zone. Processor <b>208</b> in combination with memory <b>210</b> may store the optical zone illumination information over a period of time. For example, the optical zone illumination information may be stored for a period of time ranging from a few second to hours.</p>
<p id="p-0052" num="0051">As described above, light sensor <b>200</b> may be used to determine the luminosity of a display in an electronic device. To determine the proper display luminosity, processor <b>208</b> may determine the characteristics of the environmental illumination using the optical zone illumination information. Processor <b>208</b> in combination with memory <b>210</b> may determine the spatial and temporal characteristics of the detected light. For example, processor <b>208</b> may determine spatial and temporal distributions of the detected light, spatial differentials of the detected light, the rate of change of the illumination levels in the optical zones, and temporal and spatial densities of the detected light.</p>
<p id="p-0053" num="0052">In addition, processor <b>208</b> may model the behavior of the eye-brain system when exposed to the illumination conditions detected by multi-element light sensor <b>202</b>. Processor <b>208</b> may use standard eye behavior models such as Ricco's law, Bloch's law, Weber's law, Broca-Sulzer effect, Ferry-Porter Law, and Brucke-Bartley Effect as described below. Also, viewer specific metrics such as eye mobility, tolerances to visual excitations, and time constants may be used.</p>
<p id="p-0054" num="0053">Processor <b>208</b> and memory <b>210</b> may include the necessary hardware, software, or combination thereof to determine the characteristics of the environmental illumination as described above. For example, processor <b>208</b> or memory <b>210</b> may contain the necessary logic to perform the analyzing, manipulating, and storing of signals from multi-element sensor <b>202</b> to determine the characteristics of the environmental illumination.</p>
<p id="p-0055" num="0054">Once determined, processor <b>208</b> may provide display driver <b>212</b> with information such as the characteristics of the environmental illumination. Display driver <b>212</b> may use the information to power the display at appropriate levels for the environmental illumination conditions. Also, processor <b>208</b> may generate a display driver signal based on the characteristics of the environmental illumination.</p>
<p id="p-0056" num="0055"><figref idref="DRAWINGS">FIG. 8</figref> illustrates a method <b>800</b> for using light sensor <b>200</b> for detecting light consistent with embodiments. Light sensor <b>200</b> performs method <b>800</b> in combination with a device such as a television, cell phone, avionics device, camera, or computer display. Light sensor <b>200</b> performs method <b>800</b> to detect the environmental illumination conditions in which the electronic devices are located.</p>
<p id="p-0057" num="0056">Method <b>800</b> begins with light sensor <b>200</b> detecting light from an environment by dividing the light into optical zones (stage <b>802</b>). By dividing the environment into optical zones, light sensor <b>200</b> may detect the environmental illumination conditions for the different optical zones. Light sensor may divide the light into optical zones using multi-element light detector <b>202</b> such the example of multi-element light detector <b>202</b> illustrated in <figref idref="DRAWINGS">FIGS. 3A</figref>, <b>3</b>B, <b>4</b>, <b>5</b>, <b>6</b>, and <b>7</b>.</p>
<p id="p-0058" num="0057">Next, light sensor <b>200</b> stores the detected light as historic light readings for the optical zones (stage <b>804</b>). Specifically, the light detected by multi-element light detector <b>202</b> is transferred to processor <b>208</b>. Processor <b>208</b> transfers the detected light signal into memory <b>210</b>.</p>
<p id="p-0059" num="0058">Then, light sensor <b>200</b> determines a luminosity of a light source based on the historic light readings (stage <b>805</b>). For example, processor <b>208</b> may determine characteristics of the environmental illumination based on the detected light. Then, processor <b>208</b> sends a signal to display driver <b>212</b> so that display driver <b>212</b> may power the display at the appropriate levels for the environmental illumination conditions.</p>
<p id="p-0060" num="0059">Light sensor <b>200</b> may determine various characteristics of the environmental illumination conditions. Light sensor <b>200</b> may determine the spatial characteristics of the environmental illumination conditions. Light sensor <b>200</b> may determine the spatial characteristics by comparing the light detected from the multiple light detector elements, which represent the optical zones. For example, the light sensor may determine the spatial distribution of light, spatial differentials of the environmental illumination conditions, and the spatial density of the environmental illumination conditions.</p>
<p id="p-0061" num="0060">Further, light sensor <b>200</b> may use the historic data to determine temporal characteristics of the environmental illumination conditions. Light sensor <b>200</b> may determine the temporal characteristics by comparing the light detected from the multiple light detector elements, which represent the optical zones, over time. For example, light sensor <b>200</b> may determine the rate of change in the illumination levels of each optical zone. Also, the historic data may be used to determine the spatial characteristics of the environmental illumination conditions over time.</p>
<p id="p-0062" num="0061">Further, light sensor <b>200</b> may use the environmental illumination conditions to determine the behavior of the human eye to these conditions. Light sensor <b>200</b> may apply the environmental illumination conditions, the temporal characteristics, and the spatial characteristics to a model of human eye behavior. Light sensor <b>200</b> may use such models as Ricco's law, Bloch's law, Weber's law, Broca-Sulzer effect, Ferry-Porter Law, and Brucke-Bartley Effect as described below. Each of these models may be used to determine the effect of a condition of the environment illumination effect on the human eye.</p>
<p id="p-0063" num="0062">To accommodate both sensitivity and resolution, the human eye developed two overlapping (duplex) sensory networks: one for high sensitivity and the other for increased resolution. The high sensitivity network consists of a mesh of rods (photoreceptor cells in the retina) and ganglion cells (neurons located in the retina of the eye attached to photoreceptor cells) with large receptive fields. The vision provided by this network is known as scotopic vision. Scotopic vision dominates the way we see at night.</p>
<p id="p-0064" num="0063">The second network has a much higher percentage of cones (cells in the eye retina of the eye which function in relative bright light and detect color) than rods. The cones are associated with ganglion cells with very small receptive fields. Vision using this network is called photopic vision. Photopic vision provides most of our visual input under bright light conditions.</p>
<p id="p-0065" num="0064">The human eye contains over 130 million photoreceptors. The photoreceptors are roughly divided between 3 million L (red) cones, 3 million M (green) cones, and only 1 million S (blue) cones, with the balance consisting of achromatic rods. At the center of the macula (center of the retina), we find mostly cones—about 150,000 of them packed into an area of one square millimeter.</p>
<p id="p-0066" num="0065">Since cones occupy most of the macula, photopic visual acuity is associated mostly with the cones. The low density of S cones also indicates that our eyes have a much lower resolution for blue color detail. Still, the brain integrates the S cone signals into a continuous blue image which explains why gaps do not appear when observing a blue object.</p>
<p id="p-0067" num="0066">Ricco's law concerns the relationship between size of an object and the threshold illumination level needed to perceive the object. As it gets darker, humans have more and more difficulty reading small print and distinguishing small object. For a quantitative evaluation of Ricco's law, objects are standardized to a set of small white disks of different areas. For a fixed period of time, the threshold intensity, I, is inversely proportional to the area, A, of the disk. The relationship between the threshold intensity and area is known as Ricco's Law of Spatial Summations:
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>I*A</i>=constant<?in-line-formulae description="In-line Formulae" end="tail"?>
</p>
<p id="p-0068" num="0067">The retinal response is an integral or summation of the photon flux over small retinal areas. In relative darkness, the threshold response disk area spans a 0.5 degree angle, while under good lighting condition the angle is found to be 0.06 degrees. Past a certain disk size (known as the area of complete summation), the threshold light intensity remains constant, which may indicate a dynamic pixilation of the retina. Once a certain object detail size is reached, that size becomes a pixel equivalent for the rest of the visual processing chain.</p>
<p id="p-0069" num="0068">There is considerable evidence that each area of complete summation is processed by single ganglion cell. The area of complete summation quantifies the two-dimensional resolution of the eye. In good lighting conditions, the eye's one-dimensional (line to line) threshold response may be up to three times better (approximately 0.02 degrees) than its two-dimensional performance. This may possibly be due to signal processing inference rather than eye optics.</p>
<p id="p-0070" num="0069">According to Ricco's law, high convergence areas of the retina (large receptive fields) correspond to high sensitivity. In other words, the larger the receptive filed, the lower the intensity threshold. In low convergence areas, the size of individual receptive fields is significantly smaller. Because these fields are closely packed, the resolution of the eye is much higher, but the intensity threshold increases accordingly.</p>
<p id="p-0071" num="0070">Bloch's law describes behavior when disk sizes and intensity level are kept constant, but the duration of each observation is varied. The time, T, it takes to perceive an object of a given (small) size is in inverse proportion to the luminosity, I, on that object. The relationship between I and T is given by Bloch's law:
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>I*T</i>=constant<?in-line-formulae description="In-line Formulae" end="tail"?>
</p>
<p id="p-0072" num="0071">When the duration is increased past a critical value (called the duration of complete summation), the threshold intensity becomes constant. For rods, Bloch's law levels off at about 100 milliseconds. For cones, Bloch's law levels off after 50 milliseconds.</p>
<p id="p-0073" num="0072">The formal similarity between Ricco's law and Bloch's law suggests that threshold intensity level may be determined by the total energy of the photons contributed to a ganglion cell, regardless if it is obtained by summation over time or integration over local retinal area.</p>
<p id="p-0074" num="0073">The receptive fields associated with ganglion cells are not all the same, but the most common type is the center-surround receptive field. The center-surround receptive field's distinguishing feature is the presence of two distinct concentric subfields with opposite stimuli responses: a center circle and a doughnut. Subfields may be center-on/doughnut-off and center-off/doughnut-on fields. The center-on/doughnut-off field responds stronger when the center is illuminated more than the periphery. The center-off/doughnut-on responds stronger when the periphery is illuminated more.</p>
<p id="p-0075" num="0074">This arrangement makes the associated ganglion cell indifferent to uniform illumination but sensitive to high illumination gradients. High illumination gradient may be found in high-contrast images or around the edges of objects. Weber's law describes what happens when an illumination gradient ΔI is superimposed on top of a uniform illumination I<sub>o</sub>. Weber's law states that a minimally noticeable change in light intensity is a constant proportion of the background intensity.
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>ΔI/I</i><sub>o</sub>=constant<?in-line-formulae description="In-line Formulae" end="tail"?>
</p>
<p id="p-0076" num="0075">An object illuminated by a local source shows more detail when placed against a dark background than when placed against a well-lit background. This phenomenon is known as lightness constancy. However, when both the object and the background are illuminated by the same source the degree of detail evidenced by the object is the same regardless of scene illumination (for the same background). In this case, both ΔI and I<sub>o </sub>are the result of surface reflectance. As such, the object's and background's ratio remains the same for all illumination levels. This effect is known as lightness contrast. A related parameter is maximum linear eye resolution which, under photopic conditions, is found to be about 0.02 degrees.</p>
<p id="p-0077" num="0076">An interesting side effect of Bloch's law is the Broca-Sulzer effect. The Broca-Sulzer occurs when the eye is exposed to bright flashes of light comparable in length to the Bloch critical duration (50 to 100 milliseconds). Under these conditions, a normalized decay follows a transient apparent amplification of the intensity of the light. A secondary gain mechanism appears during the temporal summation period that subsides when the summation is complete. This effect mimics a transient response of an under-damped feedback system.</p>
<p id="p-0078" num="0077">The critical flicker frequency (CFF) is a frequency value of an alternating illumination source above which the light appears to be continuous. Many factors influence our perception of flicker. One factor is the size of the light source. A large light source as a TV monitor may not exhibit flicker when viewed directly. The flicker may be quite pronounced if viewed at an angle. This occurrence suggests a higher CFF for the peripheral retina (rod-dominated) than for the macula (cone-dominated).</p>
<p id="p-0079" num="0078">CFF is also higher for larger objects than for smaller ones. Humans may also be trained to tolerate flicker. For example, Europe uses 50 Hz Pal monitors. Although most Europeans do not see the flicker, many visitors who grew up watching 60 Hz NTSC monitors are quite aware of flicker in the 50 Hz monitors.</p>
<p id="p-0080" num="0079">According to the Ferry-Porter law, the CFF is a linear function of the logarithm of the source luminance, I:
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>CFF=A </i>log(<i>I</i>)+<i>B </i><?in-line-formulae description="In-line Formulae" end="tail"?>
</p>
<p id="p-0081" num="0080">where A and B are constants.</p>
<p id="p-0082" num="0081">The Ferry-Porter law indicates that the higher the light intensity is, the higher the CFF. In other words, for a given stimulus frequency, the lower the intensity, the lower the CFF. The flickering of a Pal monitor, for example, becomes less evident if the luminosity is lowered or if the monitor is moved into a well-lit area (unless the area's lights are also powered by a 50 Hz frequency power source).</p>
<p id="p-0083" num="0082">Above CFF, there is not a distinction between the perceived luminosity of a continuous source and that of an intermittent source of equivalent emissive power. The Talbot Plateau law sets forth this effect.</p>
<p id="p-0084" num="0083">The perceived brightness of a source operating below CFF is significantly higher then the brightness of an equivalent continuous source (Brucke-Bartly effect). Peak perceived brightness is reached when the “on” time of the flicker approaches the Bloch critical duration. This explains the high effectiveness of colored flashing warning signs operating in the 15 to 20 Hz range. The Brucke-Bartly effect is a steady state variation of the Broca-Sulzer effect discussed earlier.</p>
<p id="p-0085" num="0084">However, this maximum sensitivity peak holds only for photopic (cone-dominated) vision. Then, the CFF is close to a familiar 60 Hz and the flicker contrast threshold is around 1 percent. For scotopic vision, the CFF drops to 15 Hz and the contrast threshold increase to 20 percent.</p>
<p id="p-0086" num="0085">To account for all these effects a general function for the compensated output of a display can be written as:
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>O</i>(<i>t</i>)=<i>f[[I</i><sub>i</sub>(<i>t</i>),[<i>I</i><sub>i</sub>(<i>t</i>)−<i>I</i><sub>j</sub>(<i>t</i>)],[<i>I</i><sub>i</sub>(<i>t</i>)−<i>I</i><sub>i</sub>(<i>t</i><sub>k</sub>)])]<?in-line-formulae description="In-line Formulae" end="tail"?>
</p>
<p id="p-0087" num="0086">Where O(t) is the display output at time t, I<sub>i</sub>(t) is the ambient light intensity seen by element i at time t, I<sub>j</sub>(t) is the ambient light intensity seen by element j at time t, and I<sub>i</sub>(t<sub>k</sub>) is the ambient light intensity seen by element i at a past lime t<sub>k</sub>. To a linear approximation this expression can be reduced to:
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>O</i>(<i>t</i>)=Σ<sub>i</sub>α<sub>i</sub><i>I</i><sub>i</sub>(<i>t</i>)+Σ<sub>i</sub>Σ<sub>j</sub>β<sub>i,j</sub><i>[I</i><sub>i</sub>(<i>t</i>)−<i>I</i><sub>j</sub>(<i>t</i>)]+Σ<sub>i</sub>Σ<sub>k</sub>γ<sub>i,k</sub><i>[I</i><sub>i</sub>(<i>t</i>)−<i>I</i><sub>i</sub>(<i>t</i><sub>k</sub>)])]<?in-line-formulae description="In-line Formulae" end="tail"?>
</p>
<p id="p-0088" num="0087">Where the summations occur across elements (i,j) and across time (k), and the coefficients α<sub>i, j</sub>β<sub>i,j</sub>, and γ<sub>i,k </sub>are empirical in nature and depend on the characteristics of the user. The above approximation concerns the environmental lighting condition of ambient light intensity. One skilled in the art will realize that the above approximation may be used with other environmental lighting conditions. Further, one skilled in the art will realize that the above linear approximation is exemplary and that higher order non-linear approximations or other approximations or other models may also be used.</p>
<p id="p-0089" num="0088">The execution of the above model may be performed by any hardware, software, or combination thereof, such as a microprocessor, a field programmable gate array (FPGA), or by other hardware, firmware, or software methods.</p>
<p id="p-0090" num="0089">For example, processor <b>208</b> in combination memory may determine the eye's responses to the environmental illumination conditions as a linear combination such as the above approximation. Processor <b>208</b> would apply the illumination information from multi-element sensor <b>202</b> to the models mentioned above to determine the eyes response to such conditions. The linear combination of this information would be used to determine the driving signal of a display device. Processor <b>208</b> may use various conditions such as contrast and intensity to determine the eyes response.</p>
<p id="p-0091" num="0090">Also, light sensor <b>200</b> may use viewer specific metrics such as eye mobility, tolerances to visual excitations, and time constants. These metrics may be standard for a typical user or these metrics may be programmed into light sensor for a particular user.</p>
<p id="p-0092" num="0091">Other embodiments will be apparent to those skilled in the art from consideration of the specification and practice of the invention disclosed herein. It is intended that the specification and examples be considered as exemplary only, with a true scope and spirit of the invention being indicated by the following claims.</p>
<?DETDESC description="Detailed Description" end="tail"?>
</description>
<us-claim-statement>What is claimed is:</us-claim-statement>
<claims id="claims">
<claim id="CLM-00001" num="00001">
<claim-text>1. A sensor comprising:
<claim-text>a light detector for dividing light in an environment into optical zones and detecting light from the optical zones;</claim-text>
<claim-text>a processor coupled to the light detector, wherein the processor is configured to determine spatial and temporal characteristics of the detected light, including a rate of change of an illumination level in the optical zones over time; and</claim-text>
<claim-text>a memory coupled to the processor for storing the spatial and temporal characteristics of the detected light and the rate of change of the illumination level in the optical zones over time as historic light readings.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00002" num="00002">
<claim-text>2. The detector of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the light sensor comprises:
<claim-text>a wide angle optic for capturing the light at wide angles in the environment; and</claim-text>
<claim-text>a multi-element light detector positioned adjacent to the wide angle optic for detecting the light in the optical zones.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00003" num="00003">
<claim-text>3. The detector of <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein the wide angle optic comprises:
<claim-text>a wide angle refractive optic for capturing the light at wide angles in the environment and for focusing the light on the multi-element light detector.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00004" num="00004">
<claim-text>4. The detector of <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein the wide angle optic comprises:
<claim-text>a prismatic reflector for capturing the light at wide angles in the environment; and</claim-text>
<claim-text>a lens positioned adjacent to the prismatic reflector for focusing the on the multi-element light detector.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00005" num="00005">
<claim-text>5. The detector of <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein the wide angle optic comprises:
<claim-text>a parabolic reflector for capturing the light at wide angles in the environment; and</claim-text>
<claim-text>a lens positioned adjacent to the parabolic reflector for focusing the light on the multi-element light detector.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00006" num="00006">
<claim-text>6. The detector of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the light detector comprises:
<claim-text>narrow angle optics arranged to capture the light at wide angles in the environment; and</claim-text>
<claim-text>narrow angle light detector positioned adjacent to the narrow angle optics for detecting the light in the optical zones.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00007" num="00007">
<claim-text>7. The detector of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the processor comprises:
<claim-text>logic for retrieving the historic light readings from the memory; and</claim-text>
<claim-text>logic for determining a luminosity of a light source based on a model of eye behavior and on the historic ambient light readings.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00008" num="00008">
<claim-text>8. The detector of <claim-ref idref="CLM-00007">claim 7</claim-ref>, wherein the processor further comprises:
<claim-text>logic for producing a display light source control signal based on the determined luminosity.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00009" num="00009">
<claim-text>9. The detector of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the processor comprises:
<claim-text>logic for determining spatial characteristics of the light in the optical zones; and</claim-text>
<claim-text>logic for producing a display light source control signal based on the determined spatial characteristics.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00010" num="00010">
<claim-text>10. The detector of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the processor comprises:
<claim-text>logic for determining temporal characteristics of the light in the optical zones; and</claim-text>
<claim-text>logic for producing a display light source control signal based on the determined temporal characteristics.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00011" num="00011">
<claim-text>11. A method for detecting light, comprising:
<claim-text>detecting light from an environment by dividing the light into optical zones;</claim-text>
<claim-text>determining spatial and temporal characteristics of the detected light, including a rate of change of an illumination level in the optical zones over time;</claim-text>
<claim-text>storing the spatial and temporal characteristics of the detected light and the rate of change of the illumination level in the optical zones over time as historic light readings for the optical zones; and</claim-text>
<claim-text>determining a luminosity of a light source based on the historic light readings.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00012" num="00012">
<claim-text>12. The method of <claim-ref idref="CLM-00011">claim 11</claim-ref>, further comprising:
<claim-text>producing a display light source control signal based on the determined luminosity.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00013" num="00013">
<claim-text>13. The method of <claim-ref idref="CLM-00011">claim 11</claim-ref>, wherein determining the luminosity of the light source, comprises:
<claim-text>determining the luminosity of the light source by applying the historic light readings to an eye behavior model.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00014" num="00014">
<claim-text>14. A sensor comprising:
<claim-text>means for dividing an ambient environment into optical zones;</claim-text>
<claim-text>means for detecting light from the optical zones positioned adjacent to the dividing means;</claim-text>
<claim-text>means for processing the detected light coupled to the detecting means;</claim-text>
<claim-text>means for storing the detected light as historic light readings coupled to the processing means;</claim-text>
<claim-text>means for retrieving the historic light readings from the storing means; and</claim-text>
<claim-text>means for determining a luminosity of a light source based on a model of eye behavior and on the historic light readings.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00015" num="00015">
<claim-text>15. The sensor of <claim-ref idref="CLM-00014">claim 14</claim-ref>, wherein the processing means further comprises:
<claim-text>means for determining spatial characteristics of the detected light; and</claim-text>
<claim-text>means for determining a luminosity of a light source based on the spatial characteristics.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00016" num="00016">
<claim-text>16. The sensor of <claim-ref idref="CLM-00014">claim 14</claim-ref>, wherein the processing means further comprises:
<claim-text>means for determining temporal characteristics of the detected light; and</claim-text>
<claim-text>means for determining a luminosity of a light source based on the temporal characteristics.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00017" num="00017">
<claim-text>17. The detector of <claim-ref idref="CLM-00007">claim 7</claim-ref>, wherein the model of eye behavior includes Ricco's law, Bloch's law, Weber's law, Broca-Sulzer effect, Ferry-Porter Law, and Brucke-Bartley Effect.</claim-text>
</claim>
<claim id="CLM-00018" num="00018">
<claim-text>18. The method of <claim-ref idref="CLM-00013">claim 13</claim-ref>, wherein the eye behavior model includes Ricco's law, Bloch's law, Weber's law, Broca-Sulzer effect, Ferry-Porter Law, and Brucke-Bartley Effect.</claim-text>
</claim>
<claim id="CLM-00019" num="00019">
<claim-text>19. The sensor of <claim-ref idref="CLM-00014">claim 14</claim-ref>, wherein the model of eye behavior includes Ricco's law, Bloch's law, Weber's law, Broca-Sulzer effect, Ferry-Porter Law, and Brucke-Bartley Effect.</claim-text>
</claim>
<claim id="CLM-00020" num="00020">
<claim-text>20. The detector of <claim-ref idref="CLM-00016">claim 16</claim-ref>, wherein the luminosity of the light source is used to determine a driving signal of a display device.</claim-text>
</claim>
<claim id="CLM-00021" num="00021">
<claim-text>21. The sensor of <claim-ref idref="CLM-00014">claim 14</claim-ref>, wherein the luminosity of the light source is used to determine a driving signal of a display device.</claim-text>
</claim>
</claims>
</us-patent-grant>
