<us-patent-grant lang="EN" dtd-version="v4.2 2006-08-23" file="US07298376-20071120.XML" status="PRODUCTION" id="us-patent-grant" country="US" date-produced="20071106" date-publ="20071120">
<us-bibliographic-data-grant>
<publication-reference>
<document-id>
<country>US</country>
<doc-number>07298376</doc-number>
<kind>B2</kind>
<date>20071120</date>
</document-id>
</publication-reference>
<application-reference appl-type="utility">
<document-id>
<country>US</country>
<doc-number>10628781</doc-number>
<date>20030728</date>
</document-id>
</application-reference>
<us-application-series-code>10</us-application-series-code>
<us-term-of-grant>
<us-term-extension>370</us-term-extension>
</us-term-of-grant>
<classifications-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>G</section>
<class>09</class>
<subclass>G</subclass>
<main-group>5</main-group>
<subgroup>00</subgroup>
<symbol-position>F</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20071120</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
</classifications-ipcr>
<classification-national>
<country>US</country>
<main-classification>345584</main-classification>
<further-classification>345424</further-classification>
<further-classification>345426</further-classification>
<further-classification>345428</further-classification>
<further-classification>345440</further-classification>
<further-classification>345612</further-classification>
<further-classification>345644</further-classification>
<further-classification>382108</further-classification>
<further-classification>382109</further-classification>
<further-classification>382195</further-classification>
<further-classification>382206</further-classification>
</classification-national>
<invention-title id="d0e53">System and method for real-time co-rendering of multiple attributes</invention-title>
<references-cited>
<citation>
<patcit num="00001">
<document-id>
<country>US</country>
<doc-number>4467461</doc-number>
<kind>A</kind>
<name>Rice</name>
<date>19840800</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<patcit num="00002">
<document-id>
<country>US</country>
<doc-number>5949424</doc-number>
<kind>A</kind>
<name>Cabral et al.</name>
<date>19990900</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>345426</main-classification></classification-national>
</citation>
<citation>
<patcit num="00003">
<document-id>
<country>US</country>
<doc-number>6396495</doc-number>
<kind>B1</kind>
<name>Parghl et al.</name>
<date>20020500</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<patcit num="00004">
<document-id>
<country>US</country>
<doc-number>6690820</doc-number>
<kind>B2</kind>
<name>Lees et al.</name>
<date>20040200</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>382154</main-classification></classification-national>
</citation>
<citation>
<patcit num="00005">
<document-id>
<country>US</country>
<doc-number>6940507</doc-number>
<kind>B2</kind>
<name>Repin et al.</name>
<date>20050900</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>345424</main-classification></classification-national>
</citation>
<citation>
<patcit num="00006">
<document-id>
<country>US</country>
<doc-number>7006085</doc-number>
<kind>B1</kind>
<name>Acosta et al.</name>
<date>20060200</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>345419</main-classification></classification-national>
</citation>
<citation>
<patcit num="00007">
<document-id>
<country>US</country>
<doc-number>7102647</doc-number>
<kind>B2</kind>
<name>Sloan et al.</name>
<date>20060900</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>345584</main-classification></classification-national>
</citation>
<citation>
<patcit num="00008">
<document-id>
<country>US</country>
<doc-number>2002/0172401</doc-number>
<kind>A1</kind>
<name>Lees et al.</name>
<date>20021100</date>
</document-id>
</patcit>
<category>cited by other</category>
<classification-national><country>US</country><main-classification>382109</main-classification></classification-national>
</citation>
<citation>
<patcit num="00009">
<document-id>
<country>US</country>
<doc-number>2004/0081353</doc-number>
<kind>A1</kind>
<name>Lees et al.</name>
<date>20040400</date>
</document-id>
</patcit>
<category>cited by other</category>
<classification-national><country>US</country><main-classification>382154</main-classification></classification-national>
</citation>
<citation>
<nplcit num="00010">
<othercit>Mark J. Kilgard, “A Practical and Robust Bump-mapping Technique for Today's GPUs,” Jul. 5, 2000, pp. 1-39, NVIDIA Corporation, Santa Clara, CA.</othercit>
</nplcit>
<category>cited by other</category>
</citation>
<citation>
<nplcit num="00011">
<othercit>Jack Lees, “Constructing Faults from Seed Picks by Voxel Tracking,” Mar. 1999, pp. 338, 340, <i>The Leading Edge</i>.</othercit>
</nplcit>
<category>cited by other</category>
</citation>
</references-cited>
<number-of-claims>46</number-of-claims>
<us-exemplary-claim>1</us-exemplary-claim>
<us-field-of-classification-search>
<classification-national>
<country>US</country>
<main-classification>345584</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>345612</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>345644</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>382109</main-classification>
</classification-national>
</us-field-of-classification-search>
<figures>
<number-of-drawing-sheets>5</number-of-drawing-sheets>
<number-of-figures>8</number-of-figures>
</figures>
<us-related-documents>
<related-publication>
<document-id>
<country>US</country>
<doc-number>20050237334</doc-number>
<kind>A1</kind>
<date>20051027</date>
</document-id>
</related-publication>
</us-related-documents>
<parties>
<applicants>
<applicant sequence="001" app-type="applicant-inventor" designation="us-only">
<addressbook>
<last-name>Chuter</last-name>
<first-name>Christopher John</first-name>
<address>
<city>Austin</city>
<state>TX</state>
<country>US</country>
</address>
</addressbook>
<nationality>
<country>omitted</country>
</nationality>
<residence>
<country>US</country>
</residence>
</applicant>
</applicants>
<agents>
<agent sequence="01" rep-type="attorney">
<addressbook>
<orgname>Crain Caton &amp; James</orgname>
<address>
<country>unknown</country>
</address>
</addressbook>
</agent>
</agents>
</parties>
<assignees>
<assignee>
<addressbook>
<orgname>Landmark Graphics Corporation</orgname>
<role>02</role>
<address>
<city>Houston</city>
<state>TX</state>
<country>US</country>
</address>
</addressbook>
</assignee>
</assignees>
<examiners>
<primary-examiner>
<last-name>Tung</last-name>
<first-name>Kee M.</first-name>
<department>2628</department>
</primary-examiner>
<assistant-examiner>
<last-name>Caschera</last-name>
<first-name>Antonio A</first-name>
</assistant-examiner>
</examiners>
</us-bibliographic-data-grant>
<abstract id="abstract">
<p id="p-0001" num="0000">An apparatus and method for enhancing the combined image of multiple attributes without compromising the image of either attribute. The combined image of the multiple attributes is enhanced for analyzing a predetermined property revealed by the attributes. The combined image can be interactively manipulated to display each attribute relative to an imaginary light source or highlighted using a specular component. The method and apparatus are best described as particularly useful for analytical, diagnostic and interpretive purposes.</p>
</abstract>
<drawings id="DRAWINGS">
<figure id="Fig-EMI-D00000" num="00000">
<img id="EMI-D00000" he="178.22mm" wi="261.87mm" file="US07298376-20071120-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00001" num="00001">
<img id="EMI-D00001" he="114.72mm" wi="126.83mm" file="US07298376-20071120-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00002" num="00002">
<img id="EMI-D00002" he="263.06mm" wi="180.00mm" orientation="landscape" file="US07298376-20071120-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00003" num="00003">
<img id="EMI-D00003" he="337.99mm" wi="240.62mm" file="US07298376-20071120-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00004" num="00004">
<img id="EMI-D00004" he="323.51mm" wi="224.03mm" file="US07298376-20071120-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00005" num="00005">
<img id="EMI-D00005" he="327.41mm" wi="246.97mm" file="US07298376-20071120-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
</drawings>
<description id="description">
<?BRFSUM description="Brief Summary" end="lead"?>
<heading id="h-0001" level="1">CROSS-REFERENCE TO RELATED APPLICATIONS</heading>
<p id="p-0002" num="0001">Not Applicable.</p>
<heading id="h-0002" level="1">STATEMENT REGARDING FEDERALLY SPONSORED RESEARCH OR DEVELOPMENT</heading>
<p id="p-0003" num="0002">Not Applicable.</p>
<heading id="h-0003" level="1">BACKGROUND OF THE INVENTION</heading>
<p id="p-0004" num="0003">1. Field of the Invention</p>
<p id="p-0005" num="0004">The present invention relates to a visualization technique for co-rendering multiple attributes in real time, thus forming a combined image of the attributes. The combined image is visually intuitive in that it distinguishes certain features of an object that are substantially indistinguishable in their natural environment.</p>
<p id="p-0006" num="0005">2. Related Art</p>
<p id="p-0007" num="0006">In the applied sciences, various fields of study require the analysis of two-dimensional (2-D) or three-dimensional (3-D) volume data sets wherein each data set may have multiple attributes representing different physical properties. An attribute, sometimes referred to as a data value, represents a particular physical property of an object within a defined 2-D or 3-D space. A data value may, for instance, be an 8-byte data word which includes 256 possible values. The location of an attribute is represented by (x, y, data value) or (x, y, z, data value). If the attribute represents pressure at a particular location, then the attribute location may be expressed as (x, y, z, pressure).</p>
<p id="p-0008" num="0007">In the medical field, a computerized axial topography (CAT) scanner or magnetic resonance imaging (MRI) device is used to produce a picture or diagnostic image of some specific area of a person's body, typically representing the coordinate and a determined attribute. Normally, each attribute within a predetermined location must be imaged separate and apart from another attribute. For example, one attribute representing temperature at a predetermined location is typically imaged separate from another attribute representing pressure at the same location. Thus, the diagnosis of a particular condition based upon these attributes is limited by the ability to display a single attribute at a predetermined location.</p>
<p id="p-0009" num="0008">In the field of earth sciences, seismic sounding is used for exploring the subterranean geology of an earth formation. An underground explosion excites seismic waves, similar to low-frequency sound waves that travel below the surface of the earth and are detected by seismographs. The seismographs record the time of arrival of seismic waves, both direct and reflected waves. Knowing the time and place of the explosion the time of travel of the waves through the interior can be calculated and used to measure the velocity of the waves in the interior. A similar technique can be used for offshore oil and gas exploration. In offshore exploration, a ship tows a sound source and underwater hydrophones. Low frequency, (e.g., 50 Hz) sound waves are generated by, for example, a pneumatic device that works like a balloon burst. The sounds bounce off rock layers below the sea floor and are picked up by the hydrophones. In either application, subsurface sedimentary structures that trap oil, such as faults and domes are mapped by the reflective waves.</p>
<p id="p-0010" num="0009">The data is collected and processed to produce 3-D volume data sets. A 3-D volume data set is made up of “voxels” or volume elements having x, y, z coordinates. Each voxel represents a numeric data value (attribute) associated with some measured or calculated physical property at a particular location. Examples of geological data values include amplitude, phase, frequency, and semblance. Different data values are stored in different 3-D volume data sets, wherein each 3-D volume data set represents a different data value. In order to analyze certain geological structures referred to as “events” information from different 3-D volume data sets must be separately imaged in order to analyze the event.</p>
<p id="p-0011" num="0010">Certain techniques have been developed in this field for imaging multiple 3-D volume data sets in a single display, however, not without considerable limitations. One example includes the technique published in <i>The Leading Edge </i>called “Constructing Faults from Seed Picks by Voxel Tracking” by Jack Lees. This technique combines two 3-D volume data sets in a single display, thereby restricting each original 256-value attribute to 128 values of the full 256-value range. The resolution of the display is, therefore, significantly reduced, thereby limiting the ability to distinguish certain events or features from the rest of the data. Another conventional method combines the display of two 3-D volume data sets, containing two different attributes, by making some data values more transparent than others. This technique becomes untenable when more than two attributes are combined.</p>
<p id="p-0012" num="0011">Another technique used to combine two different 3-D volume data sets in the same image is illustrated in U.S. patent application Ser. No. 09/936,780, assigned to Magic Earth, Inc. and incorporated herein by reference. This application describes a technique for combining a first 3-D volume data set representing a first attribute and a second 3-D volume data set representing a second attribute in a single enhanced 3-D volume data set by comparing each of the first and second attribute data values with a preselected data value range or criteria. For each data value where the criteria are met, a first selected data value is inserted at a position corresponding with the respective data value in the enhanced 3-D volume data set. For each data value where the criteria are not met, a second selected data value is inserted at a position corresponding with the respective data value in the enhanced 3-D volume data set. The first selected data value may be related to the first attribute and the second selected data value may be related to the second attribute. The resulting image is an enhanced 3-D volume data set comprising a combination or hybrid of the original first 3-D volume data set and the second 3-D volume data set. As a result, the extra processing step needed to generate the enhanced 3-D volume data set causes interpretation delays and performance slow downs. Furthermore, this pre-processing technique is compromised by a “lossy” effect which compromises data from one seismic attribute in order to image another seismic attribute. Consequently, there is a significant loss of data visualization.</p>
<p id="p-0013" num="0012">In non-scientific applications, techniques have been developed to define surface details (texture) on inanimate objects through lighting and/or shading techniques. For example, in the video or computer graphics field, one technique commonly used is texture mapping. Texture typically refers to bumps, wrinkles, grooves or other irregularities on surfaces. Textured surfaces are recognized by the way light interacts with the surface irregularities. In effect, these irregularities are part of the complete geometric form of the object although they are relatively small compared to the size and form of the object. Conventional texture mapping techniques have been known to lack the necessary surface detail to accomplish what is conventionally meant by texture. In other words, conventional texture mapping techniques provide objects with a colorful yet flat appearance. To this end, texture mapping was expanded to overcome this problem with what is now commonly referred to as bump mapping.</p>
<p id="p-0014" num="0013">Bump mapping is explained in an article written by Mark Kilgard called “A Practical and Robust Bump Mapping Technique for Today's GPU's” (hereinafter Kilgard) which is incorporated herein by reference. In this article, bump mapping is described as “a texture-based rendering approach for simulating lighting effects caused by pattern irregularities on otherwise smooth surfaces.” Kilgard, p. 1. According to Kilgard, “bump mapping simulates a surface's irregular lighting appearance without the complexity and expense of modeling the patterns as true geometric perturbations to the surface.” Kilgard, p. 1. Nevertheless, the computations required for original bump mapping techniques proposed by James Blinn in 1978 are considerably more expensive than those required for conventional hardware texture mapping. Kilgard at p. 2.</p>
<p id="p-0015" num="0014">In view of the many attempts that have been made over the last two decades to reformulate bump mapping into a form suitable for hardware implementation, Kilgard proposes a new bump mapping technique. In short, Kilgard divides bump mapping into two steps. First, a perturbed surface normal is computed. Then, a lighting computation is performed using the perturbed surface normal. These two steps must be performed at each and every visible fragment of a bump-mapped surface. Kilgard.</p>
<p id="p-0016" num="0015">Although Kilgard's new technique may be suitable for simulating surface irregularities (texture) representative of true geometric perturbations, it does not address the use of similar lighting effects to distinguish certain features of an object that are substantially indistinguishable in their natural environment.</p>
<heading id="h-0004" level="1">SUMMARY OF THE INVENTION</heading>
<p id="p-0017" num="0016">The present invention therefore, provides system and method for enhancing the combined image of multiple attributes representing 2-D or 3-D objects. In one embodiment, a first attribute is selected from a source of available attributes and represents one property of the object. A second attribute is selected from the same source of attributes and represents another property of the object. Additional attributes may be selected, depending on the available source of attributes.</p>
<p id="p-0018" num="0017">A normal map is created using voxels from either the first attribute or the second attribute. The normal map is derived from the data values representing the first or second attribute, hereinafter the underlying attribute, and is used to construct lighting effects that provide an illusion of height, depth and geometry on a planar surface.</p>
<p id="p-0019" num="0018">In order to obtain a more accurate lighting effect, a vertex program is applied to the vertices that constrain the planar surface of the underlying attribute and the vertices that constrain the corresponding planar surface of the normal map. As a result, a new coordinate space is created thus, forming a matrix that is commonly referred to as tangent space that is later used by the register combiners</p>
<p id="p-0020" num="0019">The register combiners, or texture shaders, are used to calculate ambient and diffuse lighting effects (illumination) for the normal map, after the vertex program is applied, and the other first or second attribute which are combined to form an enhanced image representing the first and second attributes. In this manner, the combined image of the co-rendered attributes is displayed thus, revealing certain features of the object represented by the attributes that are substantially indistinguishable in their natural environment.</p>
<p id="p-0021" num="0020">In another embodiment, select features of the object are interactively highlighted by altering lighting coefficients representing the specular and/or diffuse component of an imaginary light source. In this manner, the register combiners are reapplied to alter the ambient and diffuse lighting effects and highlight certain features of the object as the combined image is displayed.</p>
<p id="p-0022" num="0021">In another embodiment, the light source is interactively repositioned or the combined image is interactively rotated to reveal select features of the object represented by the attributes. As the image is rotated, or the light source repositioned, certain voxels representing the first attribute become darkly shaded or shadowed, while others representing the second attribute become visible and vice versa. This embodiment is therefore, useful for enhancing images of select features of an object which, in their natural environment, are indistinguishable from the rest of the object. In this manner, the vertex program and register combiners are reapplied and the image is displayed.</p>
<p id="p-0023" num="0022">In another embodiment, the per-pixel lighting height is interactively controlled. The per-pixel lighting height is often referred to as the height of the bumps or depth of the indentions defined by the lighting effect produced on a per pixel basis. As the per-pixel lighting height is altered, the normal map is recalculated, the vertex program and register combiners are reapplied, and the image is displayed.</p>
<p id="p-0024" num="0023">In yet another embodiment, one or more different attributes are selected to image other select features of the object in the manner thus described. Thus, in this embodiment, the foregoing steps and techniques are reapplied as a new combined image is displayed.</p>
<p id="p-0025" num="0024">In yet another embodiment, the combined image is interactively controlled (moved/resized) to display select features of the object at different locations. In this manner, the attributes are resampled, the normal map is recalculated, the vertex program and register combiners are reapplied, and the combined image is displayed at its new location.</p>
<?BRFSUM description="Brief Summary" end="tail"?>
<?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?>
<description-of-drawings>
<heading id="h-0005" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading>
<p id="p-0026" num="0025">The patent or application file contains at least one drawing executed in color. Copies of this patent or patent application publication with color drawing(s) will be provided by the Office upon request and payment of the necessary fee.</p>
<p id="p-0027" num="0026">The invention will be described with reference to the accompanying drawings, in which like elements are referenced with like reference numerals, and in which:</p>
<p id="p-0028" num="0027"><figref idref="DRAWINGS">FIG. 1</figref> is a block diagram illustrating one embodiment of a software program for implementing the present invention;</p>
<p id="p-0029" num="0028"><figref idref="DRAWINGS">FIG. 2</figref> is a flow diagram illustrating one embodiment of a method for implementing the present invention;</p>
<p id="p-0030" num="0029"><figref idref="DRAWINGS">FIG. 3</figref> is a color drawing illustrating semblance as a seismic data attribute;</p>
<p id="p-0031" num="0030"><figref idref="DRAWINGS">FIG. 4</figref> is a color drawing illustrating amplitude as a seismic data attribute;</p>
<p id="p-0032" num="0031"><figref idref="DRAWINGS">FIG. 5</figref> is a color drawing illustrating the combined image of both attributes illustrated in <figref idref="DRAWINGS">FIGS. 3 and 4</figref>;</p>
<p id="p-0033" num="0032"><figref idref="DRAWINGS">FIG. 6</figref> is a color drawing illustrating the combined image of <figref idref="DRAWINGS">FIG. 5</figref> with the light source positioned to the left of the image.</p>
<p id="p-0034" num="0033"><figref idref="DRAWINGS">FIG. 7</figref> is a color drawing illustrating the combined image of <figref idref="DRAWINGS">FIG. 5</figref> with the light source positioned perpendicular to the image.</p>
<p id="p-0035" num="0034"><figref idref="DRAWINGS">FIG. 8</figref> is a color drawing illustrating the combined image of <figref idref="DRAWINGS">FIG. 5</figref> with the light source positioned to the right of the image.</p>
</description-of-drawings>
<?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?>
<?DETDESC description="Detailed Description" end="lead"?>
<p id="p-0036" num="0035">While the present invention will be described in connection with presently preferred embodiments, it will be understood that it is not intended to limit the invention to those embodiments. On the contrary, it is intended to cover all alternatives, modifications, and equivalents included within the spirit of the invention.</p>
<heading id="h-0006" level="1">DETAILED DESCRIPTION OF THE PREFERRED EMBODIMENTS</heading>
<p id="p-0037" num="0036">The present invention may not be implemented using hardware, software or a combination thereof, and may be implemented in a computer system or other processing system. The following description applies the present invention to various seismic data attributes which are contained within a specified space or volume referred to as a probe. Each probe comprises voxel data represented by x, y, z, data value. Each data value is associated with a particular seismic data attribute at a specified location (x, y, z). The present invention, therefore, may employ one or more of the hardware and software system components required to display and manipulate the probe as described in U.S. Pat. No. 6,765,570(“’570 Patent”) assigned to Landmark Graphics Corporation and incorporated herein by reference. For a more complete description of the probe requirements, reference is made to the ’570 Patent.</p>
<p id="p-0038" num="0037">In addition to the probe requirements, the present invention may be implemented using current high performance graphics and personal computer commodity hardware in order to insure real time performance. Examples of available hardware for the personal computer include graphics cards like GeForce® marketed by NVIDIA® and 2.4 Ghz×86 instruction set computer processors manufactured by Intel® or AMD®.</p>
<p id="p-0039" num="0038">One embodiment of a software or program structure for implementing the present invention is shown in <figref idref="DRAWINGS">FIG. 1</figref>. At the base of program structure <b>100</b> is an operating system <b>102</b>. Suitable operating systems may include, for example, UNIX® or LINUX® operating systems, Windows NT®, and other operating systems generally known in the art.</p>
<p id="p-0040" num="0039">Menu and interface software <b>104</b> overlays operating system <b>102</b>. Menu and interface software <b>104</b> are used to provide various menus and windows to facilitate interaction with the user, and to obtain user input and instructions. Menu and interface software <b>104</b> may include, for example, Microsoft Windows®, X Free 86®, MOTIF®, and other menu and interface software generally known in the art.</p>
<p id="p-0041" num="0040">A basic graphics library <b>106</b> overlays menu and interface software <b>104</b>. Basic graphics library <b>106</b> is an application programming interface (API) for 3-D computer graphics. The functions performed by basic graphics library <b>106</b> include, for example, geometric and raster primitives, RGBA or color index mode, display list or immediate mode, viewing and modeling transformations, lighting and shading, hidden surface removal, alpha blending (translucency), anti-aliasing, texture mapping, atmospheric effects (fog, smoke, haze), feedback and selection, stencil planes, and accumulation buffer.</p>
<p id="p-0042" num="0041">A particularly useful basic graphics library <b>106</b> is OpenGL®, marketed by Silicon Graphics, Inc. (“SGI®”). The OpenGL® API is a multi-platform industry standard that is hardware, window, and operating system independent. OpenGL® is designed to be callable from C, C++, FORTRAN, Ada and Java programming languages. OpenGL® performs each of the functions listed above for basic graphics library <b>106</b>. Some commands in OpenGL® specify geometric objects to be drawn, and others control how the objects are handled. All elements of the OpenGL® state, even the contents of the texture memory and the frame buffer, can be obtained by a client application using OpenGL®. OpenGL® and the client application may operate on the same or different machines because OpenGL® is network transparent. OpenGL® is described in more detail in the OpenGL® Programming Guide (ISBN: 0-201-63274-8) and the OpenGL® Reference Manual (ISBN: 0-201-63276-4), both of which are incorporated herein by reference.</p>
<p id="p-0043" num="0042">Visual simulation graphics library <b>108</b> overlays the basic graphics library <b>106</b>. Visual simulation graphics library <b>108</b> is an API for creating real-time, multi-processed 3-D visual simulation graphics applications. Visual simulation graphics library <b>108</b> provides functions that bundle together graphics library state control functions such as lighting, materials, texture, and transparency. These functions track state and the creation of display lists that can be rendered later.</p>
<p id="p-0044" num="0043">A particularly useful visual simulation graphics library <b>108</b> is OpenGL Performer®, which is available from SGI®. OpenGL Performer® supports the OpenGL® graphics library discussed above. OpenGL Performer® includes two main libraries (libpf and libpr) and four associated libraries (libpfdu, libpfdb, libpfui, and libpfutil).</p>
<p id="p-0045" num="0044">The basis of OpenGL Performer® is the performance rendering library libpr, a low-level library providing high speed rendering functions based on GeoSets and graphics state control using GeoStates. GeoSets are collections of drawable geometry that group same-type graphics primitives (e.g., triangles or quads) into one data object. The GeoSet contains no geometry itself, only pointers to data arrays and index arrays. Because all the primitives in a GeoSet are of the same type and have the same attributes, rendering of most databases is performed at maximum hardware speed. GeoStates provide graphics state definitions (e.g., texture or material) for GeoSets.</p>
<p id="p-0046" num="0045">Layered above libpr is libpf, a real-time visual simulation environment providing a high-performance multi-process database rendering system that optimizes use of multiprocessing hardware. The database utility library, libpfdu, provides functions for defining both geometric and appearance attributes of 3-D objects, shares state and materials, and generates triangle strips from independent polygonal input. The database library libpfdb uses the facilities of libpfdu, libpf and libpr to import database files in a number of industry standard database formats. The libpfui is a user interface library that provides building blocks for writing manipulation components for user interfaces (C and C++ programming languages). Finally, the libpfutil is the utility library that provides routines for implementing tasks and graphical user interface (GUI) tools.</p>
<p id="p-0047" num="0046">An application program which uses OpenGL Performer® and OpenGL® API typically performs the following steps in preparing for real-time 3-D visual simulation:
<ul id="ul0001" list-style="none">
    <li id="ul0001-0001" num="0000">
    <ul id="ul0002" list-style="none">
        <li id="ul0002-0001" num="0047">1. Initialize OpenGL Performer®;</li>
        <li id="ul0002-0002" num="0048">2. Specify number of graphics pipelines, choose the multiprocessing configuration, and specify hardware mode as needed;</li>
        <li id="ul0002-0003" num="0049">3. Initialize chosen multiprocessing mode;</li>
        <li id="ul0002-0004" num="0050">4. Initialize frame rate and set frame-extend policy;</li>
        <li id="ul0002-0005" num="0051">5. Create, configure, and open windows as required; and</li>
        <li id="ul0002-0006" num="0052">6. Create and configure display channels as required.</li>
    </ul>
    </li>
</ul>
</p>
<p id="p-0048" num="0053">Once the application program has created a graphical rendering environment by carrying out steps 1 through 6 above, then the application program typically iterates through the following main simulation loop once per frame:
<ul id="ul0003" list-style="none">
    <li id="ul0003-0001" num="0000">
    <ul id="ul0004" list-style="none">
        <li id="ul0004-0001" num="0054">1. Compute dynamics, update model matrices, etc.;</li>
        <li id="ul0004-0002" num="0055">2. Delay until the next frame time;</li>
        <li id="ul0004-0003" num="0056">3. Perform latency critical viewpoint updates; and</li>
        <li id="ul0004-0004" num="0057">4. Draw a frame.</li>
    </ul>
    </li>
</ul>
</p>
<p id="p-0049" num="0058">Alternatively, Open Scene Graph® can be used as the visual simulation graphics library <b>108</b>. Open Scene Graph® operates in the same manner as OpenGL Performer®, providing programming tools written in C/C++ for a large variety of computer platforms. Open Scene Graph® is based on OpenGL® and is publicly available.</p>
<p id="p-0050" num="0059">A multi-attribute co-rendering program <b>110</b> of the present invention overlays visual simulation graphics library <b>108</b>. In a manner generally well known in the art, program <b>110</b> interfaces with, and utilizes the functions carried out by, the visual simulation graphics library <b>108</b>, basic graphics library <b>106</b>, menu and interface software <b>104</b>, operating system <b>102</b> and the probe described in the '634 application. Program <b>110</b> is preferably written in an object oriented programming language to allow the creation and use of objects and object functionality. One preferred object oriented programming language is C++.</p>
<p id="p-0051" num="0060">In this particular embodiment, program <b>110</b> stores the 3-D volume data set in a manner generally well known in the art. For example, the format for a particular data volume may include two parts: a volume header followed by the body of data that is as long as the size of the data set. The volume header typically includes information in a prescribed sequence, such as the file path (location) of the data set, size, dimensions in the x, y, and z directions, annotations for the x, y, and z axes, annotations for the data value, etc. The body of data is a binary sequence of bytes and may include one or more bytes per data value. For example, the first byte is the data value at volume location (0,0,0); the second byte is the data value at volume location (1,0,0); and the third byte is the data value at volume location (2,0,0). When the x dimension is exhausted, then the y dimension and the z dimension are incremented, respectively. This embodiment is not limited in any way to a particular data format.</p>
<p id="p-0052" num="0061">The program <b>110</b> facilitates input from a user to identify one or more 3-D volume data sets to use for imaging and analysis. When a plurality of data volumes is used, the data value for each of the plurality of data volumes represents a different physical parameter or attribute for the same geographic space. By way of example, a plurality of data volumes could include a geology volume, a temperature volume, and a water-saturation volume. The voxels in the geology volume can be expressed in the form (x, y, z, seismic amplitude). The voxels in the temperature volume can be expressed in the form (x, y, z, ° C.). The voxels in the water-saturation volume can be expressed in the form (x, y, z, % saturation). The physical or geographic space defined by the voxels in each of these volumes is the same. However, for any specific spatial location (x<sub>0</sub>, y<sub>0</sub>, z<sub>0</sub>), the seismic amplitude would be contained in the geology volume, the temperature in the temperature volume, and the water-saturation in the water-saturation volume. The operation of program <b>110</b> is described in reference to <figref idref="DRAWINGS">FIGS. 2 through 8</figref>.</p>
<p id="p-0053" num="0062">Referring now to <figref idref="DRAWINGS">FIG. 2</figref>, a method <b>200</b> is illustrated for co-rendering multiple attributes in a combined image. The following description refers to certain bump mapping algorithms and techniques discussed in Kilgard.</p>
<p id="p-0054" num="0063">In Step <b>202</b>, a first attribute and a second attribute are selected from the available attributes using the GUI tools (menu/interface software <b>104</b>) described in reference to <figref idref="DRAWINGS">FIG. 1</figref>. Although other available stored attributes may be used, such as frequency and phase, semblance is used as the first attribute illustrated in the probe <b>300</b> of <figref idref="DRAWINGS">FIG. 3</figref>, and amplitude is used as the second attribute illustrated in the probe <b>400</b> of <figref idref="DRAWINGS">FIG. 4</figref>. The seismic data is displayed on the visible planar surfaces of the probe using conventional shading/opacity (texture mapping) techniques, however, may be displayed within the planar surfaces defining the probe using volume rendering techniques generally well known in the art. In order to display seismic data in the manner thus described, voxel data is read from memory and converted into a specified color representing a specific texture. Textures are tiled into 256 pixel by 256 pixel images. For large volumes, many tiles exist on a single planar surface of the probe. This process is commnonly referred to by those skilled in the art as sampling, and is coordinated among multiple CPU's on a per-tile basis. These techniques, and others employed herein, are further described and illustrated in the ’570 Patent.</p>
<p id="p-0055" num="0064">In Step <b>204</b>, a normal map is calculated in order to convert the texture based semblance attribute illustrated in <figref idref="DRAWINGS">FIG. 3</figref>, sometimes referred to as a height field, into a normal map that encodes lighting information that will be used later by the register combiners. This technique enables the application of per-pixel lighting to volumetric data in the same way the probe displays volumetric data. In other words, it is a 2-D object which is actually displayed, however, because it is comprised of voxel data and the speed at which it is displayed, appears as a 3-D object. In short, this step converts the data values representing the semblance attribute into perturbed normalized vectors that are used by the graphics card to calculate lighting effects which give the illusion of depth and geometry when, in fact, a planar surface is displayed.</p>
<p id="p-0056" num="0065">The normal map comprises multiple perturbed normal vectors which, collectively, are used to construct an illusion of height, depth and geometry on a planar surface. Each perturbed normal vector is derived from the cross product of the vertical and horizontal components for each data value on a given surface (e.g., <b>310</b>) in <figref idref="DRAWINGS">FIG. 3</figref>. Each perturbed normal vector is stored in the hardware as a texture unit (normal map) wherein each spatial coordinate (x, y, z) for each perturbed normal vector is assigned a specified color red, green or blue (RGB) value. The coordinate space in which these coordinates are assigned RGB values is generally known as texture coordinate space. Thus, the blue component of the perturbed normal vector represents the spatial coordinate (z). A pixel in the texture that is all blue would therefore, represent a typical tangent vector in planar objects such as the surface <b>310</b> in <figref idref="DRAWINGS">FIG. 3</figref>. As the data values vary, the normal map appearance becomes less blue and appears almost chalky. The techniques necessary to derive a normal map from a height field are generally described in Section 5.3 of Kilgard. By applying the equations referred to in Section 2.6 of Kilgard to the data values shown in the probe <b>300</b> of <figref idref="DRAWINGS">FIG. 3</figref>, a normal map may be constructed. One set of instructions to perform this method and technique is illustrated in Appendix E of Kilgard.</p>
<p id="p-0057" num="0066">In order to obtain a more accurate lighting effect, a vertex program is applied in Step <b>206</b> to the vertices that constrain the planar surface <b>310</b> of the underlying attribute illustrated in <figref idref="DRAWINGS">FIG. 3</figref> and the vertices that constrain the corresponding planar surface of the normal map (not shown). A new coordinate space, tangent space, is contained in a transformation matrix used by the vertex program. The programmable hardware on the graphics card is used for rendering coordinate space transforms that drive the vertex program. The tangent space is constructed on a per-vertex basis, and typically requires the CPU to supply per-vertex light-angle vectors and half-angle vectors as 3-D texture coordinates. The light angle vectors and half angle vectors are likewise converted to tangent space when multiplied by the tangent space matrix. This step employs the techniques generally described in Section 5.1 of Kilgard.</p>
<p id="p-0058" num="0067">For example, normal and tangent vectors are calculated on a per-vertex basis for a given geometric model—like the probe <b>300</b> in <figref idref="DRAWINGS">FIG. 3</figref>. A bi-normal vector is calculated by taking the cross product of the tangent and normal vector components for each vertex. The tangent, normal and bi-normal vectors thus, form an ortho-normal basis at each vertex. The ortho-normal basis represents a matrix used to transform objects, space, light and eye position into tangent space. One set of instructions for performing this technique is illustrated in Appendix C of Kilgard.</p>
<p id="p-0059" num="0068">Register combiners or texture shaders (not shown) are applied by the graphics card in Step <b>208</b> to calculate the lighting equations described in Sections 2.5 through 2.5.1 of Kilgard. The GeForce® and Quadro® register combiners, available through NVIDIA,® provide a configurable, but not programmable, means to determine per-pixel fragment coloring/shading, and replace the standard OpenGL® fixed function texture environment, color sum, and fog operations with an enhanced mechanism for coloring/shading fragments. With multi-textured OpenGL®, filtered texels from each texture unit representing the normal map and the second attribute (amplitude) illustrated in the probe <b>400</b> of <figref idref="DRAWINGS">FIG. 4</figref> are combined with the fragments' current color in sequential order. The register combiners are generally described in Section 4.2 of Kilgard as a sequential application of general combiner stages that culminate in a final combiner stage that outputs an RGBA color for the fragment. One set of instructions for programming OpenGL® register combiners is illustrated in Appendix B of Kilgard.</p>
<p id="p-0060" num="0069">As further explained in Section 5.4 of Kilgard, the register combiners are configured to compute the ambient and diffuse illumination for the co-rendered image that is displayed in Step <b>210</b> by means generally well-known in the art. In short, the register combiners are used to calculate ambient and diffuse lighting effects (illumination) for the normal map, after the vertex program is applied, and the second attribute which are combined to form an enhanced image representing the first and second attributes. The resulting data values for the combined image represent a blended texture or combined texture of both the first and second attributes One set of instructions for programming the register combiners to compute the ambient and diffuse illumination is illustrated in Appendix G of Kilgard.</p>
<p id="p-0061" num="0070">Alternatively, fragment routines, generally well known in the art, may be used with the register combiners to provide a more refined per-pixel lighting effect for the normal map.</p>
<p id="p-0062" num="0071">As illustrated in <figref idref="DRAWINGS">FIG. 3</figref>, certain geological features, such as faults represented by the black color values <b>312</b>, are distinguished from the blue color values <b>314</b> due to discontinuity between the adjacent data values measured along the z-axis. In <figref idref="DRAWINGS">FIG. 4</figref>, the same geological features <b>412</b> are barely distinguishable because they are illustrated by a different attribute (amplitude) that is assigned multiple color values and contains more consistent adjacent data values along the z-axis. The same geological features <b>512</b> are even more readily distinguished in <figref idref="DRAWINGS">FIG. 5</figref> due to the enhanced surface texture which appears to give the planar surface <b>510</b> on the probe <b>500</b> depth and height.</p>
<p id="p-0063" num="0072">In <figref idref="DRAWINGS">FIG. 5</figref>, the first attribute (semblance) is distinguished by shading from the second attribute (amplitude) which is shown by various color values. This illusion is uncharacteristic of the actual geological feature which is substantially indistinguishable in its natural environment. Although both attributes are not visible at the same time over the planar surface <b>510</b> of the probe <b>500</b>, they are imaged in the same space and capable of being simultaneously viewed depending on the angle of the probe <b>500</b> relative to the light source. Thus, as the probe <b>500</b> is rotated, certain voxels representing the first attribute become masked while others representing the second attribute become visible, and vice-versa. This technique is useful for enhancing images of certain features of an object which are substantially indistinguishable in their natural environment. The present invention may also be applied, using the same techniques, to image volume-rendered seismic-data attributes.</p>
<p id="p-0064" num="0073">As the image is displayed in Step <b>210</b>, several options described in reference to Steps <b>212</b> through <b>220</b> may be interactively controlled through the menu/interface software <b>104</b> to compare and analyze any differences between the various images.</p>
<p id="p-0065" num="0074">In Step <b>212</b>, the specular or diffuse lighting coefficients may be interactively controlled to alter the shading/lighting effects applied to the combined image. Accordingly, the register combiners are reapplied in Step <b>208</b> to enhance the image displayed in Step <b>210</b>.</p>
<p id="p-0066" num="0075">In Step <b>214</b>, the imaginary light source may be interactively repositioned or the probe may be interactively rotated to image other geological features revealed by the attributes. The movement of the probe is accomplished by means generally described in the '634 application. In <figref idref="DRAWINGS">FIGS. 6-8</figref>, the planar surface <b>510</b> of the probe <b>500</b> illustrated in <figref idref="DRAWINGS">FIG. 5</figref> is fixed at a position perpendicular to the line of sight as the light source is interactively repositioned. As the light source moves, different voxels become illuminated according to the position of the light source. The effect is similar to that achieved when the probe is rotated. Accordingly, Steps <b>206</b> and <b>208</b> are reapplied to provide different perspectives of the image displayed in Step <b>210</b>.</p>
<p id="p-0067" num="0076">In <figref idref="DRAWINGS">FIG. 6</figref>, for example, the light source is positioned to the left of the probe face <b>610</b> so that voxels <b>612</b>, which are perceived as indentions, appear darker while voxels <b>614</b>, which are perceived as bumps, appear lighter or more illuminated. When the light source is repositioned to the right of the probe face <b>810</b>, as in <figref idref="DRAWINGS">FIG. 8</figref>, different voxels <b>812</b>, <b>814</b> appear darker and lighter than those illustrated in <figref idref="DRAWINGS">FIG. 6</figref>. As illustrated in <figref idref="DRAWINGS">FIG. 7</figref>, the light source is positioned perpendicular to the probe face <b>710</b> and the entire image appears brighter. This effect is attributed to the specular component of the lighting equation, and enhances the illusion of depth and height in the image as the light source is repositioned or the probe is rotated. One set of instructions explaining how to configure the register combiners to compute the specular component is illustrated in Appendix H of Kilgard. In this manner, the combined image can be interactively manipulated to simultaneously reveal multiple attributes with nominal loss in the clarity of each attribute.</p>
<p id="p-0068" num="0077">In Step <b>216</b>, the per-pixel lighting height is interactively controlled to alter the normal depth of the indentions and/or height of the bumps which are shaded and illuminated as described in reference to Step <b>208</b>. The per-pixel lighting height is interactively controlled by scaling each perturbed normal vector from zero which cancels any indentations or bumps. If the per-pixel lighting is scaled in positive increments, then each perturbed normal vector height (bump) or depth (indentation) is increased. Conversely, if the per-pixel lighting is scaled in negative increments, then each perturbed normal vector height or depth is decreased. The net effect produces an image that appears to alter the position of the light source so that different features of the object are enhanced. Accordingly, Steps <b>204</b>, <b>206</b>, and <b>208</b> are reapplied to provide different perspectives of the image displayed in Step <b>210</b>.</p>
<p id="p-0069" num="0078">In Step <b>218</b>, different attributes are interactively selected in the manner described in reference to Step <b>202</b>. Accordingly, Steps <b>204</b>, <b>206</b>, and <b>208</b> are reapplied to provide an entirely new image, illustrating different data values in Step <b>210</b>. Furthermore, the image displayed in Step <b>210</b> may illustrate more than two attributes which are selected in Step <b>218</b>. For example, if the available attributes include amplitude, phase and semblance, then a normal map is created for any two of these attributes in the manner described in reference to Step <b>204</b>. In other words, a normal map is calculated or each of the two selected attributes and the resulting value for each perturbed normal vector in one normal map is then added to the value of each perturbed normal vector in the other normal map, at the same location, to create a single normal map that is used in the manner described in reference to Steps <b>206</b> and <b>208</b>. Alternatively, the voxels for one of the selected attributes can be added to the voxels of the other selected attribute at the same location and a normal map is calculated for the combined voxel values in the manner described in reference to Step <b>204</b>. The normal map is then used in the manner described in reference to Steps <b>206</b> and <b>208</b>. In either application where there are more than two attributes, one attribute will serve as the static attribute until Step <b>208</b>, while the others will be used in the manner thus described.</p>
<p id="p-0070" num="0079">In Step <b>220</b>, the probe is interactively controlled so that it can be resized or moved in a manner more particularly described in the ’570 Patent. This step necessarily alters the voxels displayed on the planar surfaces of the probe for the combined image displayed in Step <b>210</b>. As a result, the first and second attributes must be re-sampled in Step <b>222</b> and Steps <b>204</b>, <b>206</b>, and <b>208</b> must be reapplied to display a new image in Step <b>210</b> illustrating the same attributes at a different location.</p>
<p id="p-0071" num="0080">The techniques described by the foregoing invention remove the extra processing step normally encountered in conventional bump mapping techniques by interactively processing the attributes using hardware graphics routines provided by commodity PC graphics cards. These techniques are therefore, particularly useful to the discovery and development of energy resources.</p>
<p id="p-0072" num="0081">The foregoing disclosure and description of the invention is illustrative and explanatory thereof, and it will be appreciated by those skilled in the art, that various changes in the size, shape and materials, the use of mechanical equivalents, as well as in the details of the illustrated construction or combinations of features of the various elements may be made without departing from the spirit of the invention.</p>
<?DETDESC description="Detailed Description" end="tail"?>
</description>
<us-claim-statement>The invention claimed is:</us-claim-statement>
<claims id="claims">
<claim id="CLM-00001" num="00001">
<claim-text>1. A method for enhancing an image of one or more attributes representing a property of an object, which comprises the steps of:
<claim-text>selecting a first attribute and a second attribute from multiple attributes, the first attribute and the second attribute each having its own vertices;</claim-text>
<claim-text>creating a normal map using at least one of the first and second attributes, the normal map having its own vertices;</claim-text>
<claim-text>converting the normal map vertices and the vertices of the at least one of the first and second attributes used to create the normal map into a matrix representing a tangent space normal map;</claim-text>
<claim-text>calculating a diffuse lighting component from the tangent space normal map and the at least one of the first and second attributes used to create the normal map;</claim-text>
<claim-text>and</claim-text>
<claim-text>combining an ambient lighting component with the diffuse lighting component and at least one of the first and second attributes to form an enhanced image representing at least one property of the object.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00002" num="00002">
<claim-text>2. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein at least one of the first attribute and the second attribute comprise a combination of two or more attributes.</claim-text>
</claim>
<claim id="CLM-00003" num="00003">
<claim-text>3. The method of <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein the combination of two or more attributes form a hybrid attribute.</claim-text>
</claim>
<claim id="CLM-00004" num="00004">
<claim-text>4. The method of <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein the first attribute comprises any combination of two or more attributes comprising amplitude, frequency, phase, power, semblance, coherency, dip, azimuth, gradient, fluid factor, acoustic impedance, velocity, pressure, porosity, permeability, stratigraphy and lithology and the second attribute comprises at least one attribute from amplitude, frequency, phase, power, semblance, coherency, dip, azimuth, gradient, fluid factor, acoustic impedance, velocity, pressure, porosity, permeability, stratigraphy and lithology.</claim-text>
</claim>
<claim id="CLM-00005" num="00005">
<claim-text>5. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the ambient lighting component and diffuse lighting component are combined with the first attribute and the second attribute is used to create the normal map.</claim-text>
</claim>
<claim id="CLM-00006" num="00006">
<claim-text>6. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the ambient lighting component and the diffuse lighting component are combined with the first attribute and the first attribute is used to create the normal map.</claim-text>
</claim>
<claim id="CLM-00007" num="00007">
<claim-text>7. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising the steps of:
<claim-text>selecting a third attribute, the third attribute having its own vertices;</claim-text>
<claim-text>creating another normal map using at least one of the first, second and third attributes, the another normal map having its own vertices;</claim-text>
<claim-text>converting the another normal map vertices and the vertices of the at least one of the first, second and third attributes used to create the another normal map into another matrix representing another tangent space normal map;</claim-text>
<claim-text>calculating another diffuse lighting component from the another tangent space normal map and the at least one of the first, second and third attributes used to create the another normal map; and</claim-text>
<claim-text>combining the ambient lighting component with the another diffuse lighting component and at least one of the first, second and third attributes to form another enhanced image representing another property of the object.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00008" num="00008">
<claim-text>8. The method of <claim-ref idref="CLM-00007">claim 7</claim-ref>, wherein the third attribute comprises the combination of the ambient lighting component, the diffuse lighting component and the at least one of the first and second attributes.</claim-text>
</claim>
<claim id="CLM-00009" num="00009">
<claim-text>9. The method of <claim-ref idref="CLM-00008">claim 8</claim-ref>, wherein the another normal map is created using at least one of the first and second attributes and the third attribute is combined with the ambient lighting component and the another diffuse lighting component to form the another enhanced image.</claim-text>
</claim>
<claim id="CLM-00010" num="00010">
<claim-text>10. The method of <claim-ref idref="CLM-00008">claim 8</claim-ref>, wherein the another normal map is created using the third attribute and the third attribute is combined with the ambient lighting component and the another diffuse lighting component to form the another enhanced image.</claim-text>
</claim>
<claim id="CLM-00011" num="00011">
<claim-text>11. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising the step of displaying at least a portion of the enhanced image to a user.</claim-text>
</claim>
<claim id="CLM-00012" num="00012">
<claim-text>12. The method of <claim-ref idref="CLM-00011">claim 11</claim-ref>, wherein the enhanced image displayed is displayed on at least a portion of one of a plurality of planar surfaces defining a probe.</claim-text>
</claim>
<claim id="CLM-00013" num="00013">
<claim-text>13. The method of <claim-ref idref="CLM-00011">claim 11</claim-ref>, wherein the enhanced image displayed is displayed at least partially within a plurality of planar surfaces defining a probe.</claim-text>
</claim>
<claim id="CLM-00014" num="00014">
<claim-text>14. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the first attribute and the second attribute each comprise multiple data values and associated spatial coordinates, each data value having a three-dimensional spatial coordinate.</claim-text>
</claim>
<claim id="CLM-00015" num="00015">
<claim-text>15. The method of <claim-ref idref="CLM-00014">claim 14</claim-ref>, wherein the normal map comprises multiple perturbed normal vectors that are derived from the cross product of a vertical component and a horizontal component for each data value.</claim-text>
</claim>
<claim id="CLM-00016" num="00016">
<claim-text>16. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein a vertex program is used to convert the normal map vertices and the vertices of the at least one of the first and second attributes used to create the normal map into the matrix representing the tangent space normal map.</claim-text>
</claim>
<claim id="CLM-00017" num="00017">
<claim-text>17. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the diffuse lighting component and the ambient lighting component are each calculated using a register combiner.</claim-text>
</claim>
<claim id="CLM-00018" num="00018">
<claim-text>18. The method of <claim-ref idref="CLM-00017">claim 17</claim-ref>, wherein the ambient lighting component, the diffuse lighting component and the at least one of the first and second attributes are combined using the register combiners to form the enhanced image.</claim-text>
</claim>
<claim id="CLM-00019" num="00019">
<claim-text>19. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the first attribute and the second attribute comprise medical data.</claim-text>
</claim>
<claim id="CLM-00020" num="00020">
<claim-text>20. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the first attribute and the second attribute comprise seismic data.</claim-text>
</claim>
<claim id="CLM-00021" num="00021">
<claim-text>21. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the ambient lighting component is a predetermined constant.</claim-text>
</claim>
<claim id="CLM-00022" num="00022">
<claim-text>22. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising the steps of:
<claim-text>calculating a specular lighting component from the tangent space normal map and the at least one of the first and second attributes used to create the normal map;</claim-text>
<claim-text>and</claim-text>
<claim-text>combining the specular lighting component, the ambient lighting component, the diffuse lighting component and the at least one of the first and second attributes to form the enhanced image.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00023" num="00023">
<claim-text>23. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising the steps of:
<claim-text>applying an imaginary light source to the enhanced image;</claim-text>
<claim-text>displaying a portion of the enhanced image to a user;</claim-text>
<claim-text>interactively repositioning at least one of the imaginary light source and the displayed enhanced image relative to a line of sight of the displayed enhanced image to the user;</claim-text>
<claim-text>and</claim-text>
<claim-text>repeating the converting, calculating and combining steps in <claim-ref idref="CLM-00001">claim 1</claim-ref>.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00024" num="00024">
<claim-text>24. A method for enhancing an image of one or more attributes representing a property of an object which comprises the steps of:
<claim-text>selecting an attribute from multiple attributes, the attribute having its own vertices;</claim-text>
<claim-text>creating a normal map using the attribute, the normal map having its own vertices;</claim-text>
<claim-text>converting the normal map vertices and the vertices of the attribute into a matrix representing a tangent space normal map;</claim-text>
<claim-text>calculating a diffuse lighting component from the tangent space normal map and the attribute; and</claim-text>
<claim-text>combining an ambient lighting component with the diffuse lighting component and the attribute to form an enhanced image representing at least one property of the object.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00025" num="00025">
<claim-text>25. A method for enhancing an image of multiple attributes representing a property of an object, which comprises the steps of:
<claim-text>selecting a first attribute and a second attribute from the multiple attributes, the first attribute and the second attribute each having its own vertices;</claim-text>
<claim-text>creating a normal map using at least one of the first and second attributes, the normal map having its own vertices;</claim-text>
<claim-text>converting the normal map vertices and the vertices of the at least one of the first and second attributes used to create the normal map into a matrix representing a tangent space normal map;</claim-text>
<claim-text>calculating a diffuse lighting component from the tangent space normal map and the at least one of the first and second attributes used to create the normal map;</claim-text>
<claim-text>combining an ambient lighting component with the diffuse lighting component and the first and second attributes to form an enhanced image of the first and second attributes; and</claim-text>
<claim-text>displaying at least a portion of the enhanced image to a user, the portion of the displayed enhanced image comprising at least part of the first attribute and part of the second attribute.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00026" num="00026">
<claim-text>26. The method of <claim-ref idref="CLM-00025">claim 25</claim-ref>, wherein the first and second attributes represent a geophysical property of the object.</claim-text>
</claim>
<claim id="CLM-00027" num="00027">
<claim-text>27. A system comprising a program storage device readable by a machine, the storage device embodying a program of instructions executable by the machine for enhancing an image of one or more attributes representing a property of an object, the instructions comprising the steps of:
<claim-text>selecting a first attribute and a second attribute from multiple attributes, the first attribute and the second attribute each having its own vertices;</claim-text>
<claim-text>creating a normal map derived from at least one of the first and second attributes, the normal map having its own vertices;</claim-text>
<claim-text>converting the normal map vertices and the vertices of the at least one of the first and second attributes used to create the normal map into a matrix representing a tangent space normal map;</claim-text>
<claim-text>calculating a diffuse lighting component from the tangent space normal map and the at least one of the first and second attributes used to create the normal map;</claim-text>
<claim-text>and</claim-text>
<claim-text>combining an ambient lighting component with the diffuse lighting component and at least one of the first and second attributes to form an enhanced image representing at least one property of the object.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00028" num="00028">
<claim-text>28. The system of <claim-ref idref="CLM-00027">claim 27</claim-ref>, wherein at least one of the first attribute and the second attribute comprise a combination of two or more attributes.</claim-text>
</claim>
<claim id="CLM-00029" num="00029">
<claim-text>29. The system of <claim-ref idref="CLM-00028">claim 28</claim-ref>, wherein the combination of two or more attributes form a hybrid attribute.</claim-text>
</claim>
<claim id="CLM-00030" num="00030">
<claim-text>30. The system of <claim-ref idref="CLM-00028">claim 28</claim-ref>, wherein the first attribute comprises any combination of two or more attributes comprising amplitude, frequency, phase, power, semblance, coherency, dip, azimuth, gradient, fluid factor, acoustic impedance, velocity, pressure, porosity, permeability, stratigraphy and lithology and the second attribute comprises at least one attribute from amplitude, frequency, phase, power, semblance, coherency, dip, azimuth, gradient, fluid factor, acoustic impedance, velocity, pressure, porosity, permeability, stratigraphy and lithology.</claim-text>
</claim>
<claim id="CLM-00031" num="00031">
<claim-text>31. The system of <claim-ref idref="CLM-00027">claim 27</claim-ref>, wherein the ambient lighting component and the diffuse lighting component are combined with the first attribute and the second attribute is used to create the normal map.</claim-text>
</claim>
<claim id="CLM-00032" num="00032">
<claim-text>32. The system of <claim-ref idref="CLM-00027">claim 27</claim-ref>, wherein the ambient lighting component and the diffuse lighting component are combined with the first attribute and the first attribute is used to create the normal map.</claim-text>
</claim>
<claim id="CLM-00033" num="00033">
<claim-text>33. The system of <claim-ref idref="CLM-00027">claim 27</claim-ref>, further comprising the steps of:
<claim-text>selecting a third attribute, the third attribute having its own vertices;</claim-text>
<claim-text>creating another normal map derived from at least one of the first, second and third attributes, the another normal map having its own vertices;</claim-text>
<claim-text>converting the another normal map vertices and the vertices of the at least one of the first, second and third attributes used to create the another normal map into another matrix representing another tangent space normal map;</claim-text>
<claim-text>calculating a diffuse lighting component from the another tangent space normal map and the at least one of the first, second and third attributes used to create the another normal map; and</claim-text>
<claim-text>combining the ambient lighting component with the another diffuse lighting component and at least one of the first, second and third attributes to form another enhanced image representing another property of the object.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00034" num="00034">
<claim-text>34. The system of <claim-ref idref="CLM-00033">claim 33</claim-ref>, wherein the third attribute comprises the combination of the ambient lighting component, the diffuse lighting component and the at least one of the first and second attributes.</claim-text>
</claim>
<claim id="CLM-00035" num="00035">
<claim-text>35. The system of <claim-ref idref="CLM-00034">claim 34</claim-ref>, wherein the another normal map is created using at least one of the first and second attributes and the third attribute is combined with the ambient lighting component and the another diffuse lighting component to form the another enhanced image.</claim-text>
</claim>
<claim id="CLM-00036" num="00036">
<claim-text>36. The system of <claim-ref idref="CLM-00034">claim 34</claim-ref>, wherein the another normal map is created using the third attribute and the third attribute is combined with the ambient lighting component and the another diffuse lighting component to form the another enhanced image.</claim-text>
</claim>
<claim id="CLM-00037" num="00037">
<claim-text>37. The system of <claim-ref idref="CLM-00027">claim 27</claim-ref>, further comprising the step of displaying at least a portion of the enhanced image on a monitor to a user.</claim-text>
</claim>
<claim id="CLM-00038" num="00038">
<claim-text>38. The system of <claim-ref idref="CLM-00027">claim 27</claim-ref>, wherein the first attribute and the second attribute each comprise multiple data values and corresponding spatial coordinates, each data value having a thee-dimensional spatial coordinate.</claim-text>
</claim>
<claim id="CLM-00039" num="00039">
<claim-text>39. The system of <claim-ref idref="CLM-00038">claim 38</claim-ref>, wherein the normal map comprises multiple perturbed normal vectors that are derived from the cross product of a vertical component and a horizontal component for each data value.</claim-text>
</claim>
<claim id="CLM-00040" num="00040">
<claim-text>40. The system of <claim-ref idref="CLM-00027">claim 27</claim-ref>, wherein the first attribute and the second attribute comprise medical data.</claim-text>
</claim>
<claim id="CLM-00041" num="00041">
<claim-text>41. The system of <claim-ref idref="CLM-00027">claim 27</claim-ref>, wherein the first attribute and the second attribute comprise seismic data.</claim-text>
</claim>
<claim id="CLM-00042" num="00042">
<claim-text>42. The system of <claim-ref idref="CLM-00027">claim 27</claim-ref>, wherein the ambient lighting component is a predetermined constant.</claim-text>
</claim>
<claim id="CLM-00043" num="00043">
<claim-text>43. The system of <claim-ref idref="CLM-00027">claim 27</claim-ref>, further comprising the steps of:
<claim-text>calculating a specular lighting component from the tangent space normal map and the at least one of the first and second attributes used to create the normal map;</claim-text>
<claim-text>and</claim-text>
<claim-text>combining the specular lighting component, the ambient lighting component, the diffuse lighting component and the at least one of the first and second attributes to form the enhanced image.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00044" num="00044">
<claim-text>44. The system of <claim-ref idref="CLM-00027">claim 27</claim-ref>, further comprising the steps of:
<claim-text>applying an imaginary light source to the enhanced image;</claim-text>
<claim-text>displaying a portion of the enhanced image to a user;</claim-text>
<claim-text>interactively repositioning at least one of the imaginary light source and the displayed enhanced image relative to a line of sight of the displayed enhanced image to the user;</claim-text>
<claim-text>and</claim-text>
<claim-text>repeating the converting, calculating and combining steps in <claim-ref idref="CLM-00027">claim 27</claim-ref>.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00045" num="00045">
<claim-text>45. A system comprising a program storage device readable by a machine, the storage device embodying a program of instructions executable by the machine for enhancing an image of one or more attributes representing a property of an object, the instructions comprising the steps of:
<claim-text>selecting an attribute from multiple attributes, the attribute having its own vertices;</claim-text>
<claim-text>creating a normal map derived from the attribute, the normal map having its own vertices;</claim-text>
<claim-text>converting the normal map vertices and the vertices of the attribute into a matrix representing a tangent space normal map;</claim-text>
<claim-text>calculating a diffuse lighting component from the tangent space normal map and the attribute; and</claim-text>
<claim-text>combining an ambient lighting component with the diffuse lighting component and the attribute to form an enhanced image representing at least one property of the object.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00046" num="00046">
<claim-text>46. A system comprising a program storage device readable by a machine, the storage device embodying a program of instructions executable by the machine for enhancing an image of multiple attributes representing a property of an object, the instructions comprising the steps of:
<claim-text>selecting a first attribute and a second attribute from the multiple attributes, the first attribute and the second attribute each having its own vertices;</claim-text>
<claim-text>creating a normal map derived from at least one of the first and second attributes, the normal map having its own vertices;</claim-text>
<claim-text>converting the normal map vertices and the vertices of at least one of the first and second attributes used to create the normal map into a matrix representing a tangent space normal map;</claim-text>
<claim-text>calculating a diffuse lighting component from the tangent space normal map and the at least one of the first and second attributes used to create the normal map;</claim-text>
<claim-text>combining an ambient lighting component with the diffuse lighting component and the first and second attributes to form an enhanced image of the first and second attributes; and</claim-text>
<claim-text>displaying at least a portion of the enhanced image to a user, the portion of the displayed enhanced image comprising at least part of the first attribute and part of the second attribute.</claim-text>
</claim-text>
</claim>
</claims>
</us-patent-grant>
