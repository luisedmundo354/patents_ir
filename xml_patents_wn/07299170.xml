<us-patent-grant lang="EN" dtd-version="v4.2 2006-08-23" file="US07299170-20071120.XML" status="PRODUCTION" id="us-patent-grant" country="US" date-produced="20071106" date-publ="20071120">
<us-bibliographic-data-grant>
<publication-reference>
<document-id>
<country>US</country>
<doc-number>07299170</doc-number>
<kind>B2</kind>
<date>20071120</date>
</document-id>
</publication-reference>
<application-reference appl-type="utility">
<document-id>
<country>US</country>
<doc-number>10726858</doc-number>
<date>20031202</date>
</document-id>
</application-reference>
<us-application-series-code>10</us-application-series-code>
<priority-claims>
<priority-claim sequence="01" kind="national">
<country>GB</country>
<doc-number>0315350.9</doc-number>
<date>20030628</date>
</priority-claim>
<priority-claim sequence="02" kind="national">
<country>GB</country>
<doc-number>0322325.2</doc-number>
<date>20030924</date>
</priority-claim>
</priority-claims>
<us-term-of-grant>
<us-term-extension>639</us-term-extension>
</us-term-of-grant>
<classifications-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>G</section>
<class>06</class>
<subclass>F</subclass>
<main-group>9</main-group>
<subgroup>455</subgroup>
<symbol-position>F</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20071120</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
</classifications-ipcr>
<classification-national>
<country>US</country>
<main-classification>703 26</main-classification>
<further-classification>703 23</further-classification>
<further-classification>703 24</further-classification>
<further-classification>708495</further-classification>
</classification-national>
<invention-title id="d0e89">Method and apparatus for the emulation of high precision floating point instructions</invention-title>
<references-cited>
<citation>
<patcit num="00001">
<document-id>
<country>US</country>
<doc-number>5341320</doc-number>
<kind>A</kind>
<name>Trissel et al.</name>
<date>19940800</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>708498</main-classification></classification-national>
</citation>
<citation>
<patcit num="00002">
<document-id>
<country>US</country>
<doc-number>5631859</doc-number>
<kind>A</kind>
<name>Markstein et al.</name>
<date>19970500</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>708513</main-classification></classification-national>
</citation>
<citation>
<patcit num="00003">
<document-id>
<country>US</country>
<doc-number>5678016</doc-number>
<kind>A</kind>
<name>Eisen et al.</name>
<date>19971000</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<patcit num="00004">
<document-id>
<country>US</country>
<doc-number>5732005</doc-number>
<kind>A</kind>
<name>Kahle</name>
<date>19980300</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<patcit num="00005">
<document-id>
<country>US</country>
<doc-number>5764959</doc-number>
<kind>A</kind>
<name>Sharangpani et al.</name>
<date>19980600</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<patcit num="00006">
<document-id>
<country>US</country>
<doc-number>6138135</doc-number>
<kind>A</kind>
<name>Karp</name>
<date>20001000</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<patcit num="00007">
<document-id>
<country>US</country>
<doc-number>6163764</doc-number>
<kind>A</kind>
<name>Dulong et al.</name>
<date>20001200</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>703 26</main-classification></classification-national>
</citation>
<citation>
<patcit num="00008">
<document-id>
<country>US</country>
<doc-number>6282634</doc-number>
<kind>B1</kind>
<name>Hinds et al.</name>
<date>20010800</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<patcit num="00009">
<document-id>
<country>US</country>
<doc-number>6697832</doc-number>
<kind>B1</kind>
<name>Kelley et al.</name>
<date>20040200</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>708501</main-classification></classification-national>
</citation>
<citation>
<patcit num="00010">
<document-id>
<country>GB</country>
<doc-number>2 294 565</doc-number>
<kind>A</kind>
<date>19960500</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<nplcit num="00011">
<othercit>Arakawa F., “SH4 RISC Multimedia Microprocessor,” IEEE Micro, Mar./Apr. 1998, pp. 26-34, vol. 18-issue 2.</othercit>
</nplcit>
<category>cited by other</category>
</citation>
</references-cited>
<number-of-claims>27</number-of-claims>
<us-exemplary-claim>1</us-exemplary-claim>
<us-field-of-classification-search>
<classification-national>
<country>US</country>
<main-classification>703 26</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>703 23</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>703 24</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>708495</main-classification>
</classification-national>
</us-field-of-classification-search>
<figures>
<number-of-drawing-sheets>2</number-of-drawing-sheets>
<number-of-figures>2</number-of-figures>
</figures>
<us-related-documents>
<related-publication>
<document-id>
<country>US</country>
<doc-number>20040268324</doc-number>
<kind>A1</kind>
<date>20041230</date>
</document-id>
</related-publication>
</us-related-documents>
<parties>
<applicants>
<applicant sequence="001" app-type="applicant-inventor" designation="us-only">
<addressbook>
<last-name>Walker</last-name>
<first-name>Paul</first-name>
<address>
<city>Lancs</city>
<country>GB</country>
</address>
</addressbook>
<nationality>
<country>GB</country>
</nationality>
<residence>
<country>GB</country>
</residence>
</applicant>
</applicants>
<agents>
<agent sequence="01" rep-type="attorney">
<addressbook>
<orgname>Wilmer Cutler Pickering Hale and Dorr LLP</orgname>
<address>
<country>unknown</country>
</address>
</addressbook>
</agent>
</agents>
</parties>
<assignees>
<assignee>
<addressbook>
<orgname>Transitive Limited</orgname>
<role>03</role>
<address>
<country>GB</country>
</address>
</addressbook>
</assignee>
</assignees>
<examiners>
<primary-examiner>
<last-name>Shah</last-name>
<first-name>Kamini</first-name>
<department>2128</department>
</primary-examiner>
<assistant-examiner>
<last-name>Day</last-name>
<first-name>Herng-der</first-name>
</assistant-examiner>
</examiners>
</us-bibliographic-data-grant>
<abstract id="abstract">
<p id="p-0001" num="0000">A high precision floating point emulator and associated method for emulating subject program code on a target machine where the subject machine base operands possess a different precision than the target machine. The high precision floating point emulator is provided for the emulation of subject program code instructions having a higher precision than that supported by the target machine architecture by utilizing intermediate calculations having values with a higher precision than that supported by the target machine.</p>
</abstract>
<drawings id="DRAWINGS">
<figure id="Fig-EMI-D00000" num="00000">
<img id="EMI-D00000" he="114.55mm" wi="178.48mm" file="US07299170-20071120-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00001" num="00001">
<img id="EMI-D00001" he="142.24mm" wi="182.29mm" file="US07299170-20071120-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00002" num="00002">
<img id="EMI-D00002" he="209.21mm" wi="175.60mm" file="US07299170-20071120-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
</drawings>
<description id="description">
<?BRFSUM description="Brief Summary" end="lead"?>
<heading id="h-0001" level="1">BACKGROUND OF THE INVENTION</heading>
<p id="p-0002" num="0001">1. Technical Field</p>
<p id="p-0003" num="0002">The subject invention relates generally to the field of computers and computer software and, more particularly, to an apparatus and method for emulating high precision floating point instructions.</p>
<p id="p-0004" num="0003">2. Description of Related Art</p>
<p id="p-0005" num="0004">Floating point notation is widely used in digital data processing devices, such as microprocessors, to represent a much larger range of numbers than can be represented in regular binary notation. Various types of floating point notations are used. Typically, a floating point number has a sign bit (s), followed by an exponent field (e) and a mantissa field.</p>
<p id="p-0006" num="0005">Microprocessors typically contain and work together with floating point units (FPU) to perform operations, such as addition and subtraction, on floating point numbers. FPUs have the ability to support complex numerical and scientific calculations on data in floating point format. In order to add or subtract floating point numbers, the decimal points must be aligned. The process is equivalent to addition or subtraction of base ten numbers in scientific notation. Generally, the FPU performs operations on the exponents and mantissas of the values in order to align the decimal points.</p>
<p id="p-0007" num="0006">Once the decimal points are aligned, the mantissas can be added or subtracted in accordance with the sign bits. The result may need to be normalized, or left shifted, so that a one is in the most significant bit position of the mantissa. The result may also be rounded. Many different representations can be used for the mantissa and exponent themselves, where IEEE Standard 754, entitled “IEEE Standard for Binary Floating point Arithmetic (ANSI/IEEE Std 754-1985), provides a standard used by many CPUs and FPUs which defines formats for representing floating point numbers, representations of special values (e.g., infinity, very small values, NaN), exceptions, rounding modes, and a set of floating point operations that will work identically on any conforming system. The IEEE 754 Standard further specifies the formats for representing floating point values with single-precision (32-bit), double-precision (64-bit), single-extended precision (up to 80-bits), and double-extended precision (128-bit).</p>
<p id="p-0008" num="0007">Most microprocessors also typically include integer units for performing integer operations. An integer unit is typically provided to perform integer operations, such as addition and subtraction. While integer units are common in microprocessors, floating point arithmetic performed using integer operations is much more costly than the equivalent floating point operations. Thus, most microprocessor utilize a combination of FPUs and integer units to perform necessary calculations. The precision capable of being achieved in such calculations is determined by the actual architecture of the FPU and integer unit hardware associated with the microprocessor.</p>
<p id="p-0009" num="0008">Across the embedded and non-embedded CPU market, one finds predominant Instruction Set Architectures (ISAs) for which large bodies of software exist that could be “Accelerated” for performance, or “Translated” to a myriad of capable processors that could present better cost/performance benefits, provided that they could transparently access the relevant software. One also finds dominant CPU architectures that are locked in time to their ISA, and cannot evolve in performance or market reach and would benefit from “Synthetic CPU” co-architecture.</p>
<p id="p-0010" num="0009">It is often desired to run program code written for a computer processor of a first type (a “subject” processor) on a processor of a second type (a “target” processor). Here, an emulator or translator is used to perform program code translation, such that the subject program is able to run on the target processor. The emulator provides a virtual environment, as if the subject program were running natively on a subject processor, by emulating the subject processor. The precision of the calculations which can be performed on the values of the subject program have conventionally been limited by the hardware architecture of the target processor.</p>
<heading id="h-0002" level="1">SUMMARY</heading>
<p id="p-0011" num="0010">The following is a summary of various aspects and advantages realizable according to various embodiments of the improved architecture for program code conversion according to the present invention. It is provided as an introduction to assist those skilled in the art to more rapidly assimilate the detailed discussion of the invention that ensues and does not and is not intended in any way to limit the scope of the claims that are appended hereto.</p>
<p id="p-0012" num="0011">In particular, the inventors have developed an improved method and apparatus for expediting program code conversion, particularly useful in connection with an emulator which emulates subject program code on a target machine where the subject machine base operands possess a different precision than the target machine. More particularly, a high precision floating point emulator is provided for the emulation of subject program code instructions having a higher precision than that supported by the target machine architecture by utilizing intermediate calculations having values with a higher precision than that supported by the target machine.</p>
<?BRFSUM description="Brief Summary" end="tail"?>
<?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?>
<description-of-drawings>
<heading id="h-0003" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading>
<p id="p-0013" num="0012">The features of the present invention, which are believed to be novel, are set forth with particularity in the appended claims. The present invention, both as to its organization and manner of operation, together with further advantages, may best be understood by reference to the following description, taken in connection with the accompanying drawings in which the reference numerals designate like parts throughout the figures thereof and wherein:</p>
<p id="p-0014" num="0013"><figref idref="DRAWINGS">FIG. 1</figref> shows a computing environment including subject and target processor architectures; and</p>
<p id="p-0015" num="0014"><figref idref="DRAWINGS">FIG. 2</figref> is an operational flow diagram that describes an example of the high precision floating point emulation performed in accordance with a preferred embodiment of the present invention.</p>
</description-of-drawings>
<?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?>
<?DETDESC description="Detailed Description" end="lead"?>
<heading id="h-0004" level="1">DETAILED DESCRIPTION OF THE PREFERRED EMBODIMENTS</heading>
<p id="p-0016" num="0015">The following description is provided to enable any person skilled in the art to make and use the invention and sets forth the best modes contemplated by the inventors of carrying out their invention. Various modifications, however, will remain readily apparent to those skilled in the art, since the general principles of the present invention have been defined herein specifically to provide an improved high precision floating point emulation apparatus.</p>
<p id="p-0017" num="0016">Referring to <figref idref="DRAWINGS">FIG. 1</figref>, an example computing environment is shown including a subject computing environment <b>1</b> (“subject machine 1”) and a target computing environment <b>2</b> (“target machine 2”). In the subject machine <b>1</b>, subject code <b>10</b> is executable natively on a subject processor <b>12</b>. The subject processor <b>12</b> includes a set of subject registers <b>14</b>. Here, the subject code <b>10</b> may be represented in any suitable language with intermediate layers (e.g., compilers) between the subject code <b>10</b> and the subject processor <b>12</b>, as will be familiar to a person skilled in the art.</p>
<p id="p-0018" num="0017">It is desired in some situations to run the subject code <b>10</b> on the target machine <b>2</b> of the present invention, which includes a target processor <b>22</b> using a set of target registers <b>24</b>. The two processors <b>12</b> and <b>22</b> of the subject machine <b>1</b> and the target machine <b>2</b>, respectively, may be inherently non-compatible, such that these two processors <b>12</b> and <b>22</b> use different instruction sets. The target processor <b>22</b> includes a floating point unit <b>28</b> for computing floating point operations and an integer unit <b>26</b> for performing integer operations. The floating point unit <b>28</b> and the integer unit <b>26</b> may comprise any of a wide variety of types of hardware units, as known to those skilled in the art, where the floating point unit <b>28</b> is preferably IEEE 754 Standard compatible floating point hardware.</p>
<p id="p-0019" num="0018">The two processors <b>12</b> and <b>22</b> may operate with different levels of accuracy and precision depending upon their particular architectures as well as the hardware designs of their respective floating point unit <b>28</b> and the integer unit <b>26</b>. Hence, a floating point emulator <b>20</b> is provided in the target machine <b>2</b>, in order to emulate high precision instructions from the subject code <b>10</b> in the target computing environment <b>2</b>. High precision refers to a level of precision which is higher than that provided by the target machine <b>2</b>, where the base operands in instructions of the subject program code have a higher precision than that supported by the target machine <b>2</b>. The floating point emulator <b>20</b> provides for a higher level of precision during calculations than the target architecture <b>2</b> could otherwise provide, thus providing a higher level of accuracy in the emulated instructions.</p>
<p id="p-0020" num="0019">The floating point emulator <b>20</b> is preferably a software component, i.e., a compiled version of the source code implementing the emulator, run in conjunction with an operating system running on the target processor <b>22</b>, typically a microprocessor or other suitable processing device. It will be appreciated that the structure illustrated in <figref idref="DRAWINGS">FIG. 1</figref> is exemplary only and that, for example, software, methods and processes according to the invention may be implemented in code residing within or beneath an operating system.</p>
<p id="p-0021" num="0020">Referring now to <figref idref="DRAWINGS">FIG. 2</figref>, an operational flow diagram of a method of performing high precision floating point emulation in accordance with a preferred embodiment of the present invention is illustrated. The high precision floating point emulation algorithm described hereafter refers to a single embodiment of the invention that provides for the emulation of high precision floating point accumulated instructions, while it is understood that the floating point emulator <b>20</b> is capable of emulating any types of instructions where the subject machine <b>1</b> base operands are at a different precision than the target machine <b>2</b>. For example, the floating point emulator <b>20</b> would allow the emulation of the addition of two double precision values, such as a subject machine's Double(x)+Double(y) operation, on a target machine <b>2</b> that only supports single precision floating point operations. With this understanding and for ease of discussion, the high precision floating point emulation algorithm will be described hereafter with reference to the emulation of high precision floating point accumulated instructions of the form:
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>d=±</i>(<i>a*b±c</i>)<?in-line-formulae description="In-line Formulae" end="tail"?>
</p>
<p id="p-0022" num="0021">where a, b, c and d are operands which can be expressed as floating point numbers. High precision as referred to in this description means any precision which is higher than that provided by the target machine <b>2</b>. For instance, if the architecture of the target machine <b>2</b> supports IEEE Standard 754 double-precision floating point values, then high precision would refer to any values having a higher precision than double-precision floating point values. It should be noted that the floating point emulator <b>20</b> only calculates the intermediate values of the accumulated instructions at high precision, and the operands themselves and the result are not at high precision.</p>
<p id="p-0023" num="0022">The high precision floating point algorithm embodied in <figref idref="DRAWINGS">FIG. 2</figref> utilizes standard integer techniques to perform the calculation of the accumulated instructions in stages, where, at the end of each stage, the intermediate values are tested at runtime to ascertain whether the intermediate values have reached a point at which the hardware (i.e., the target processor <b>22</b>, integer unit <b>26</b>, and floating point unit <b>28</b>) of the target machine <b>2</b> has enough precision to finish the calculation without loss of accuracy. To achieve this, the high precision floating point algorithm performed by the floating point emulator <b>20</b> works in combination with an integer unit <b>26</b> and an IEEE Standard 754 compatible floating point hardware unit <b>28</b>.</p>
<p id="p-0024" num="0023">A wide number of integer techniques for performing floating point emulation are known to those skilled in the art, where the high precision floating point algorithm of the preferred embodiment described herein accelerates the process by using floating point hardware, namely floating point unit <b>28</b>. These process accelerations are referred to as fast exit points hereinafter, because they exit the testing routine performed at runtime to determine if the intermediate values are at a level such that the target architecture <b>2</b> has enough precision to finish the calculation without loss of accuracy and the hardware of the target architecture <b>2</b> is immediately used to perform the necessary calculations.</p>
<p id="p-0025" num="0024">Fast Exit Points</p>
<p id="p-0026" num="0025">The floating point emulator <b>20</b> begins the high precision floating point algorithm when a floating point accumulated instructions of the form: d=±(a*b±c) is encountered in step <b>200</b>. It is determined in step <b>202</b> if any of the three input operands (a, b, c) can be considered a special value, where special values include zero, infinity or NAN (not a number). For each of these special values, there is a known result, such as dictated by the IEEE Standard 754, that all compatible hardware will produce regardless of the level of precision and hence there is no need for expensive integer emulation. Thus, there is a fast exit point for any operands identified as special values to perform the calculation using the target architecture's floating point unit <b>28</b> in step <b>204</b>.</p>
<p id="p-0027" num="0026">If none of the operands (a, b, c) are special values, it is next determined in step <b>206</b> whether the exponent for the result of the multiplication (a*b) overlaps with the exponent of operand c. Two values will overlap if the addition/subtraction of the significant digits of the two values yields a result different from each of the two values. In this context, non-overlapping refers to the fact that either a*b or c is so large as to make the other insignificant. By way of example, in the situation where a particular FPU is only capable of representing 3 significant digits. If the value 3.10 is added to the value to 0.01, then it can be seen that both values are important to the result, i.e., performing the addition will yield a result different to the sources and the sources thus overlap. Contrarily, for the same FPU only capable of representing 3 significant digits, if the value 310 is added to the value 0.01, the result when using 3 significant figures is 310. Thus, in this situation the result is the same as the first source and the two values did not overlap.</p>
<p id="p-0028" num="0027">When the two values fail to overlap, the addition of the values is not required. Thus, if the exponent for the result of the multiplication (a*b) does not overlap with the exponent of operand c, then the floating point algorithm determines that the addition/subtraction is not required and another fast exit point is provided where the calculation can be performed using the target machine <b>2</b>'s FPU <b>28</b> in step <b>204</b>. It should be noted that thresholds for determining whether two values overlap can either be variably selected or can be thresholds established by formats well-known to those skilled in the art can be utilized, such as the IEEE Standard 754 floating point double-precision format.</p>
<p id="p-0029" num="0028">When the exponents of the result of (a*b) and c do overlap, the mantissa for the result of the multiplication (a*b) is calculated in step <b>208</b> in order to establish how much precision is required by the mantissa. It is determined in step <b>210</b> whether the result of the multiplication (a*b) requires more mantissa bits than is provided by the FPU <b>28</b> of the target machine <b>2</b>. For example, when the FPU <b>28</b> is capable of handling double-precision numbers, it is known that operands containing 52 mantissa bits are utilized for double-precision values. If the number of bits required by the mantissa(a*b) is less than or equal to the number of mantissa bits capable of being handled by the FPU <b>28</b> (e.g., 52 bits in the case of double-precision numbers), then the FPU <b>28</b> has sufficient precision to perform the calculation. Thus, if the mantissa(a*b) requires no more mantissa bits than are provided for by the FPU <b>28</b>, another fast exit point is provided and the result is calculated using the target architecture's FPU <b>28</b> in step <b>204</b>. In the determination made in step <b>210</b>, the precision is determined by comparing the spread of the mantissa, namely the number of bit positions between the most and least significant set bits.</p>
<p id="p-0030" num="0029">When the mantissa(a*b) requires more bits than provided by the FPU <b>28</b>, the full calculation of a*b is completed using the integer unit <b>26</b> in step <b>212</b>. At this point, the high precision floating point algorithm calculates the ±mantissa(a*b)±mantissa(c) in step <b>214</b>. A determination is made in step <b>216</b> whether the resulting mantissa is equal to zero, where upon another fast exit point can be created to use the target architecture <b>2</b>'s most efficient mechanism, namely the FPU <b>28</b>, to set the final result to 0.0 in step <b>218</b>. Thus, this fast exit point is only valid when the mantissa(a*b) and the mantissa(c) are subtracted from one another to yield a resulting mantissa equal to zero. This removes the need to calculate the final exponent and also bypasses any expensive rounding. However, if the final resulting mantissa is not equal to zero, then the remaining parts of the calculation of a*b+c must be calculated using the integer unit <b>26</b> in step <b>220</b>.</p>
<p id="p-0031" num="0030">The various fast exit point provided in the high precision floating point algorithm described above provide for faster and more efficient emulation of accumulated instructions by maximizing the use of the FPU <b>28</b> for performing floating point arithmetic. By implementing the high precision floating point algorithm of the preferred embodiment, accumulated instructions are calculated at a higher precision than the operands are typically capable of being handled by the architecture of the target machine <b>2</b>, resulting in accumulated instructions effectively being calculated with greater precision. If the intermediate result is twice the precision of the sources, then the accumulated instructions can be calculated to an infinite precision (i.e., no loss of accuracy).</p>
<p id="p-0032" num="0031">For the purposes of illustrating the steps performed by the floating point emulator <b>20</b> in implementing the above-described high precision floating point algorithm, the following example is provided without any intention by the inventors of the present invention to limit the scope of their invention to the described example.</p>
<p id="p-0033" num="0032">This example utilizes an accumulated instruction having three IEEE Standard 754 double precision floating point values for operands (a, b, c). The IEEE Standard 754 standard dictates that a double-precision floating point value is 64 bits wide, including 1 sign bit, an 11-bit exponent, and a 52-bit mantissa). The example uses the definitions in the following legend:</p>
<p id="p-0034" num="0033">
<tables id="TABLE-US-00001" num="00001">
<table frame="none" colsep="0" rowsep="0">
<tgroup align="left" colsep="0" rowsep="0" cols="1">
<colspec colname="1" colwidth="217pt" align="center"/>
<thead>
<row>
<entry namest="1" nameend="1" align="center" rowsep="1"/>
</row>
<row>
<entry>Legend</entry>
</row>
<row>
<entry namest="1" nameend="1" align="center" rowsep="1"/>
</row>
</thead>
<tbody valign="top">
<row>
<entry/>
</row>
</tbody>
</tgroup>
<tgroup align="left" colsep="0" rowsep="0" cols="2">
<colspec colname="1" colwidth="49pt" align="left"/>
<colspec colname="2" colwidth="168pt" align="left"/>
<tbody valign="top">
<row>
<entry>a, b, c</entry>
<entry>the input operands</entry>
</row>
<row>
<entry>a * b, a * b − c</entry>
<entry>the intermediate operands</entry>
</row>
<row>
<entry>sign(x)</entry>
<entry>the sign part of the operand x, represented as a</entry>
</row>
<row>
<entry/>
<entry>Boolean value</entry>
</row>
<row>
<entry>exp(x)</entry>
<entry>the exponent part of operand x, represented as</entry>
</row>
<row>
<entry/>
<entry>an integer</entry>
</row>
<row>
<entry>man(x)</entry>
<entry>the mantissa part of the operands x including the implied</entry>
</row>
<row>
<entry/>
<entry>one, represented as a larger integer</entry>
</row>
<row>
<entry>FPU(x)</entry>
<entry>calculate x using the targets floating point unit 28 and</entry>
</row>
<row>
<entry/>
<entry>exit. These are indicative of a fast exit point.</entry>
</row>
<row>
<entry>sub(x, y)</entry>
<entry>x − y</entry>
</row>
<row>
<entry>mul(x, y)</entry>
<entry>x * y</entry>
</row>
<row>
<entry>shift<sub>—</sub></entry>
<entry>Shift x y places to the right</entry>
</row>
<row>
<entry>right(x, y)</entry>
</row>
<row>
<entry namest="1" nameend="2" align="center" rowsep="1"/>
</row>
</tbody>
</tgroup>
</table>
</tables>
</p>
<p id="p-0035" num="0034">In this example, a large integer means larger than that provided by the hardware of the target architecture. The last three operations in the legend are represented as functions as they operate on large integers.</p>
<p id="p-0036" num="0035">The pseudo-code for the high precision floating point algorithm for the accumulated instruction fmsub (i.e., a*b−c) is as follows. Initially, it is determined if any of the operands are special to apply an early exit:</p>
<p id="p-0037" num="0036">
<tables id="TABLE-US-00002" num="00002">
<table frame="none" colsep="0" rowsep="0">
<tgroup align="left" colsep="0" rowsep="0" cols="2">
<colspec colname="offset" colwidth="42pt" align="left"/>
<colspec colname="1" colwidth="175pt" align="left"/>
<thead>
<row>
<entry/>
<entry namest="offset" nameend="1" align="center" rowsep="1"/>
</row>
</thead>
<tbody valign="top">
<row>
<entry/>
<entry>If (a or b or c) == (± infinity or 0.0 or NAN)</entry>
</row>
<row>
<entry/>
<entry>  FPU(a * b − c)</entry>
</row>
<row>
<entry/>
<entry>EndIf</entry>
</row>
<row>
<entry/>
<entry namest="offset" nameend="1" align="center" rowsep="1"/>
</row>
</tbody>
</tgroup>
</table>
</tables>
</p>
<p id="p-0038" num="0037">When the operands are not special, it must then be determine whether to apply a different early exit by determining if the subtraction operation is significant, i.e., it has an effect on the final result within the required emulation accuracy or SMA (Subject Machine Accuracy).</p>
<p id="p-0039" num="0038">Considering (a*b)−c, the subtraction would not be significant for the following two cases:
<ul id="ul0001" list-style="none">
    <li id="ul0001-0001" num="0000">
    <ul id="ul0002" list-style="none">
        <li id="ul0002-0001" num="0039">i) (a*b)−c==(a*b): SMA</li>
        <li id="ul0002-0002" num="0040">ii) (a*b)−c==−c: SMA</li>
    </ul>
    </li>
</ul>
</p>
<p id="p-0040" num="0041">The particular emulation must be taken into account to determine whether accuracy of the SMA is similar to that of the Target Machine Accuracy (TMA). For instance, for the PPC-P4 emulation, it follows that if the subtract operation is insignificant in SMA then it is also insignificant in TMA.</p>
<p id="p-0041" num="0042">A quick and efficient way to test for significance is to test for the mere possibility of the subtract operation affecting the result within the greater SMA. If it is determined that the result does not change within SMA, the calculation can be performed natively in TMA without precision loss. However, if there is a chance that the subtract operation could change the result in a change in SMA, the emulation performed by the high precision floating point algorithm continues.</p>
<p id="p-0042" num="0043">For subtraction, the operand with the lower exponent is first shifted to make its exponent the same as the larger exponent. The mantissas are then subtracted and the exponent remains the same. For the operand with the lower exponent, the initial exponent shift upwards results in the mantissa being shifted downwards. If this results in a zero mantissa then the subtract operation is not significant. A zero mantissa will be produced if the exponent needs to be raised more than the number of bits of accuracy in the mantissa. Thus,</p>
<p id="p-0043" num="0044">
<tables id="TABLE-US-00003" num="00003">
<table frame="none" colsep="0" rowsep="0">
<tgroup align="left" colsep="0" rowsep="0" cols="2">
<colspec colname="offset" colwidth="42pt" align="left"/>
<colspec colname="1" colwidth="175pt" align="left"/>
<thead>
<row>
<entry/>
<entry namest="offset" nameend="1" align="center" rowsep="1"/>
</row>
</thead>
<tbody valign="top">
<row>
<entry/>
<entry>if ((higherExp − lowerExp) &gt; Mantissa Bits)</entry>
</row>
<row>
<entry/>
<entry>  Subtraction not significant</entry>
</row>
<row>
<entry/>
<entry>else</entry>
</row>
<row>
<entry/>
<entry>  Subtraction possibly significant</entry>
</row>
<row>
<entry/>
<entry>fi</entry>
</row>
<row>
<entry/>
<entry namest="offset" nameend="1" align="center" rowsep="1"/>
</row>
</tbody>
</tgroup>
</table>
</tables>
</p>
<p id="p-0044" num="0045">Considering the PPC instruction, fmsub, (a*b)−c, the intermediate result of the multiply is accurate to 106 mantissa bits and the final result of the subtraction is accurate to 53 mantissa bits. Therefore a*b has a maximum 106 mantissa bits and c has a maximum 53 mantissa bits.</p>
<p id="p-0045" num="0046">For PPC, the above pseudo code now becomes:</p>
<p id="p-0046" num="0047">
<tables id="TABLE-US-00004" num="00004">
<table frame="none" colsep="0" rowsep="0">
<tgroup align="left" colsep="0" rowsep="0" cols="2">
<colspec colname="offset" colwidth="14pt" align="left"/>
<colspec colname="1" colwidth="203pt" align="left"/>
<thead>
<row>
<entry/>
<entry namest="offset" nameend="1" align="center" rowsep="1"/>
</row>
</thead>
<tbody valign="top">
<row>
<entry/>
<entry>if(exp(a * b) &gt; exp(c))</entry>
</row>
<row>
<entry/>
<entry>  if(exp(a * b) − exp(c) &gt; 53)</entry>
</row>
<row>
<entry/>
<entry>    // Subtraction not significant, take fast exit</entry>
</row>
<row>
<entry/>
<entry>    FPU(a * b − c)</entry>
</row>
<row>
<entry/>
<entry>  else</entry>
</row>
<row>
<entry/>
<entry>    // Subtraction possibly significant, continue emulation</entry>
</row>
<row>
<entry/>
<entry>  fi</entry>
</row>
<row>
<entry/>
<entry>else // exp(a * b) &lt;= exp(c)</entry>
</row>
<row>
<entry/>
<entry>  if(exp(c) − exp(a * b) &gt; 106)</entry>
</row>
<row>
<entry/>
<entry>    // Subtraction not significant, take fast exit</entry>
</row>
<row>
<entry/>
<entry>    FPU(a * b − c)</entry>
</row>
<row>
<entry/>
<entry>  else</entry>
</row>
<row>
<entry/>
<entry>    // Subtraction possibly significant, continue emulation</entry>
</row>
<row>
<entry/>
<entry>  fi</entry>
</row>
<row>
<entry/>
<entry>fi</entry>
</row>
<row>
<entry/>
<entry namest="offset" nameend="1" align="center" rowsep="1"/>
</row>
</tbody>
</tgroup>
</table>
</tables>
</p>
<p id="p-0047" num="0048">The calculation of exp(a*b) involves the quick addition, exp(a)+exp(b). This fast exit check can therefore be done before the expensive SMA multiplication of a and b. The mantissa of a*b is then calculated:</p>
<p id="p-0048" num="0049">
<tables id="TABLE-US-00005" num="00005">
<table frame="none" colsep="0" rowsep="0">
<tgroup align="left" colsep="0" rowsep="0" cols="2">
<colspec colname="offset" colwidth="56pt" align="left"/>
<colspec colname="1" colwidth="161pt" align="left"/>
<thead>
<row>
<entry/>
<entry namest="offset" nameend="1" align="center" rowsep="1"/>
</row>
</thead>
<tbody valign="top">
<row>
<entry/>
<entry>man(a * b) = mul(man(a), man(b))</entry>
</row>
<row>
<entry/>
<entry namest="offset" nameend="1" align="center" rowsep="1"/>
</row>
</tbody>
</tgroup>
</table>
</tables>
</p>
<p id="p-0049" num="0050">It is known that a double-precision floating point value has 52 bits for its mantissa (plus the implied 1), thus man(x) is 53 bits wide. The result of the multiplication will therefore be a maximum of 106 bits wide. It is then determined if the extra precision is required by examining the spread of the resulting mantissa. If this mantissa would fit within a float double (i.e., 53 bits including the implied one), then the extra precision is not required. This is tested by checking to see if the bottom 53 bits of the resulting mantissa were used.</p>
<p id="p-0050" num="0051">
<tables id="TABLE-US-00006" num="00006">
<table frame="none" colsep="0" rowsep="0">
<tgroup align="left" colsep="0" rowsep="0" cols="2">
<colspec colname="offset" colwidth="49pt" align="left"/>
<colspec colname="1" colwidth="168pt" align="left"/>
<thead>
<row>
<entry/>
<entry namest="offset" nameend="1" align="center" rowsep="1"/>
</row>
</thead>
<tbody valign="top">
<row>
<entry/>
<entry>If ((man(a * b) &amp; 0x1ffffffffffff) == 0)</entry>
</row>
<row>
<entry/>
<entry>  FPU(a * b − c)</entry>
</row>
<row>
<entry/>
<entry>EndIf</entry>
</row>
<row>
<entry/>
<entry>exp(a * b) = exp(a) + exp(b)</entry>
</row>
<row>
<entry/>
<entry>sign(a * b) = sign(a) xor sign(b)</entry>
</row>
<row>
<entry/>
<entry namest="offset" nameend="1" align="center" rowsep="1"/>
</row>
</tbody>
</tgroup>
</table>
</tables>
<br/>
It is now necessary to align a*b and c, in order to perform the subtraction.
</p>
<p id="p-0051" num="0052">
<tables id="TABLE-US-00007" num="00007">
<table frame="none" colsep="0" rowsep="0">
<tgroup align="left" colsep="0" rowsep="0" cols="2">
<colspec colname="offset" colwidth="35pt" align="left"/>
<colspec colname="1" colwidth="182pt" align="left"/>
<thead>
<row>
<entry/>
<entry namest="offset" nameend="1" align="center" rowsep="1"/>
</row>
</thead>
<tbody valign="top">
<row>
<entry/>
<entry>If (exp(a * b) &gt; exp(c))</entry>
</row>
<row>
<entry/>
<entry>  shift_right(man(c), exp(a * b) − exp(c))</entry>
</row>
<row>
<entry/>
<entry>  exp(a * b − c) = exp(a * b)</entry>
</row>
<row>
<entry/>
<entry>Else</entry>
</row>
<row>
<entry/>
<entry>  shift_right(man(a * b), exp(c) − exp(a * b))</entry>
</row>
<row>
<entry/>
<entry>  exp(a * b − c) = exp(c)</entry>
</row>
<row>
<entry/>
<entry>EndIf</entry>
</row>
<row>
<entry/>
<entry>If (man(a * b) &gt; man(c))</entry>
</row>
<row>
<entry/>
<entry>  sub(man(a * b), man(c))</entry>
</row>
<row>
<entry/>
<entry>  sign(a * b − c) = sign(a * b)</entry>
</row>
<row>
<entry/>
<entry>Else</entry>
</row>
<row>
<entry/>
<entry>  sub(man(c), man(a * b))</entry>
</row>
<row>
<entry/>
<entry>  sign(a * b − c) = sign(c)</entry>
</row>
<row>
<entry/>
<entry>EndIf</entry>
</row>
<row>
<entry/>
<entry namest="offset" nameend="1" align="center" rowsep="1"/>
</row>
</tbody>
</tgroup>
</table>
</tables>
<br/>
The resulting mantissa is then checked to see if it equals zero
</p>
<p id="p-0052" num="0053">
<tables id="TABLE-US-00008" num="00008">
<table frame="none" colsep="0" rowsep="0">
<tgroup align="left" colsep="0" rowsep="0" cols="2">
<colspec colname="offset" colwidth="70pt" align="left"/>
<colspec colname="1" colwidth="147pt" align="left"/>
<thead>
<row>
<entry/>
<entry namest="offset" nameend="1" align="center" rowsep="1"/>
</row>
</thead>
<tbody valign="top">
<row>
<entry/>
<entry>If (man(a * b − c) == 0)</entry>
</row>
<row>
<entry/>
<entry>  FPU(0.0)</entry>
</row>
<row>
<entry/>
<entry>EndIf</entry>
</row>
<row>
<entry/>
<entry namest="offset" nameend="1" align="center" rowsep="1"/>
</row>
</tbody>
</tgroup>
</table>
</tables>
</p>
<p id="p-0053" num="0054">At this point, emulation has either exited via a fast exit point or it has been determined that the full precision is required. The result sign, exponent and mantissa have all been calculated, where the only operation remaining is to convert the result into the subject machine's floating point format, which involves aligning the result and rounding.</p>
<p id="p-0054" num="0055">As can be seen from the foregoing, an emulator described in the various embodiments above provide for the high precision emulation of subject program code on a target machine where the subject machine base operands possess a different precision than the target machine. Moreover, the emulation of subject program code instructions having a higher precision than that supported by the target machine architecture is provided by utilizing intermediate calculations having values with a higher precision than that supported by the target machine.</p>
<p id="p-0055" num="0056">The different structures of the high precision floating point emulation apparatus and method of the present invention are described separately in each of the above embodiments. However, it is the full intention of the inventors of the present invention that the separate aspects of each embodiment described herein may be combined with the other embodiments described herein. Those skilled in the art will appreciate that various adaptations and modifications of the just described preferred embodiments can be configured without departing from the scope and spirit of the invention. Therefore, it is to be understood that, within the scope of the appended claims, the invention may be practiced other than as specifically described herein.</p>
<?DETDESC description="Detailed Description" end="tail"?>
</description>
<us-claim-statement>What is claimed is:</us-claim-statement>
<claims id="claims">
<claim id="CLM-00001" num="00001">
<claim-text>1. A method of performing high precision emulation of program code instructions for a subject machine on a target machine including floating point hardware and integer hardware, the method comprising:
<claim-text>determining that operands in instructions of the program code for the subject machine require a higher precision than provided for by the hardware of the target machine; and</claim-text>
<claim-text>applying a floating point emulation algorithm to perform intermediate calculations on the operands of the instructions at a higher precision than the precision supported by the hardware of the target machine to produce intermediate values;</claim-text>
<claim-text>wherein at each stage, the intermediate values are tested to determine whether the intermediate values have reached a point at which the hardware of the target machine has enough precision to finish the calculation without loss of accuracy, such that when it is determined based upon the intermediate calculations that the floating point hardware of the target machine provides sufficient precision to finish the calculations required by the instructions without loss of accuracy, the floating point hardware on the target machine is utilized to finish the calculations; and the integer hardware on the target machine is utilized to perform calculations not selected to be performed by the floating point hardware.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00002" num="00002">
<claim-text>2. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the program code instructions are accumulated instructions that are calculated at a higher precision than the operands capable of being handled by the target machine.</claim-text>
</claim>
<claim id="CLM-00003" num="00003">
<claim-text>3. The method of <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein the program code instructions are floating point accumulated instructions of the form: d=±(a*b±c),
<claim-text>wherein a, b, c and d are operands expressible as floating point numbers.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00004" num="00004">
<claim-text>4. The method of <claim-ref idref="CLM-00003">claim 3</claim-ref>, further comprising identifying whether any of the operands (a, b, or c) are special values having a known result that all compatible hardware will produce regardless of the level of precision of said compatible hardware.</claim-text>
</claim>
<claim id="CLM-00005" num="00005">
<claim-text>5. The method of <claim-ref idref="CLM-00004">claim 4</claim-ref>, wherein said special values include either zero, infinity, or NaN (not a number), wherein the floating point hardware is utilized to calculate the result of the accumulated instructions when any of the operands (a, b, or c) are identified as special values.</claim-text>
</claim>
<claim id="CLM-00006" num="00006">
<claim-text>6. The method of <claim-ref idref="CLM-00003">claim 3</claim-ref>, wherein said floating point emulation algorithm further comprises:
<claim-text>determining whether the exponent for the result of the multiplication of (a*b) overlaps with the exponent of c; and</claim-text>
<claim-text>utilizing the floating point hardware to calculate the result of the accumulated instructions when the exponent for the result of the multiplication of (a*b) fails to overlap with the exponent of c.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00007" num="00007">
<claim-text>7. The method of <claim-ref idref="CLM-00006">claim 6</claim-ref>, wherein, when the exponent for the result of the multiplication of (a*b) overlaps with the exponent of c, said floating point emulation algorithm further comprising:
<claim-text>determining whether the mantissa for the result of the multiplication (a*b) requires more mantissa bits than provided for by said floating point hardware; and</claim-text>
<claim-text>utilizing the floating point hardware to calculate the result of the accumulated instructions when the result of the multiplication (a*b) does not require more mantissa bits than provided for by the floating point hardware.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00008" num="00008">
<claim-text>8. The method of <claim-ref idref="CLM-00007">claim 7</claim-ref>, said floating point emulation algorithm further comprising computing the full calculation of a*b using the integer hardware when mantissa for the result of the multiplication (a*b) requires more mantissa bits than provided for by the floating point hardware.</claim-text>
</claim>
<claim id="CLM-00009" num="00009">
<claim-text>9. The method of <claim-ref idref="CLM-00008">claim 8</claim-ref>, said floating point emulation algorithm further comprising:
<claim-text>determining whether the final resulting mantissa of the mantissa(a*b)−the mantissa (c) equals zero;</claim-text>
<claim-text>utilizing the floating point hardware to make the result of the calculation of a*b+c equal to zero when the resulting mantissa is equal to zero; and</claim-text>
<claim-text>calculating the remaining parts of the calculation of a*b+c using the integer hardware when the final resulting mantissa is not equal to zero.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00010" num="00010">
<claim-text>10. A computer-readable storage medium having software resident thereon in the form of computer-readable code executable by a computer to perform the following steps in the high precision emulation of program code instructions for a subject machine on a target machine including floating point hardware and integer hardware:
<claim-text>determining that operands in instructions of the program code for the subject machine require a higher precision than provided for by the hardware of the target machine; and</claim-text>
<claim-text>applying a floating point emulation algorithm to perform intermediate calculations on the operands of the instructions at a higher precision than the precision supported by the hardware of the target machine to produce intermediate values;</claim-text>
<claim-text>wherein at each stage, the intermediate values are tested to determine whether the intermediate values have reached a point at which the hardware of the target machine has enough precision to finish the calculation without loss of accuracy, such that when it is determined based upon the intermediate calculations that the floating point hardware of the target machine provides sufficient precision to finish the calculations required by the instructions without loss of accuracy, the floating point hardware on the target machine is utilized to finish the calculations; and the integer hardware on the target machine is utilized to perform calculations not selected to be performed by the floating point hardware.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00011" num="00011">
<claim-text>11. The computer-readable storage medium of <claim-ref idref="CLM-00010">claim 10</claim-ref>, wherein the program code instructions are accumulated instructions that are calculated at a higher precision than the operands capable of being handled by the target machine.</claim-text>
</claim>
<claim id="CLM-00012" num="00012">
<claim-text>12. The computer-readable storage medium of <claim-ref idref="CLM-00011">claim 11</claim-ref>, wherein the program code instructions are floating point accumulated instructions of the form: d=±(a*b+c), wherein a, b, c and d are operands expressible as floating point numbers.</claim-text>
</claim>
<claim id="CLM-00013" num="00013">
<claim-text>13. The computer-readable storage medium of <claim-ref idref="CLM-00012">claim 12</claim-ref>, said computer-readable code further executable for identifying whether any of the operands (a, b, or c) are special values having a known result that all compatible hardware will produce regardless of the level of precision of said compatible hardware.</claim-text>
</claim>
<claim id="CLM-00014" num="00014">
<claim-text>14. The computer-readable storage medium of <claim-ref idref="CLM-00013">claim 13</claim-ref>, wherein said special values include either zero, infinity, or NaN (not a number), wherein the floating point hardware is utilized to calculate the result of the accumulated instructions when any of the operands (a, b, or c) are identified as special values.</claim-text>
</claim>
<claim id="CLM-00015" num="00015">
<claim-text>15. The computer-readable storage medium of <claim-ref idref="CLM-00012">claim 12</claim-ref>, wherein said floating point emulation algorithm further comprises:
<claim-text>determining whether the exponent for the result of the multiplication of (a*b) overlaps with the exponent of c; and</claim-text>
<claim-text>utilizing the floating point hardware to calculate the result of the accumulated instructions when the exponent for the result of the multiplication of (a*b) fails to overlap with the exponent of c.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00016" num="00016">
<claim-text>16. The computer-readable storage medium of <claim-ref idref="CLM-00015">claim 15</claim-ref>, wherein, when the exponent for the result of the multiplication of (a*b) overlaps with the exponent of c, said floating point emulation algorithm further comprises:
<claim-text>determining whether the mantissa for the result of the multiplication (a*b) requires more mantissa bits than provided for by said floating point hardware; and</claim-text>
<claim-text>utilizing the floating point hardware to calculate the result of the accumulated instructions when the result of the multiplication (a*b) does not require more mantissa bits than provided for by the floating point hardware.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00017" num="00017">
<claim-text>17. The computer-readable storage medium of <claim-ref idref="CLM-00016">claim 16</claim-ref>, said floating point emulation algorithm further comprising computing the full calculation of a*b using the integer hardware when mantissa for the result of the multiplication (a*b) requires more mantissa bits than provided for by the floating point hardware.</claim-text>
</claim>
<claim id="CLM-00018" num="00018">
<claim-text>18. The computer-readable storage medium of <claim-ref idref="CLM-00017">claim 17</claim-ref>, said floating point emulation algorithm further comprising:
<claim-text>determining whether the final resulting mantissa of the mantissa(a*b)−the mantissa (c) equals zero;</claim-text>
<claim-text>utilizing the floating point hardware to make the result of the calculation of a*b+c equal to zero when the resulting mantissa is equal to zero; and</claim-text>
<claim-text>calculating the remaining parts of the calculation of a*b+c using the integer hardware when the final resulting mantissa is not equal to zero.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00019" num="00019">
<claim-text>19. A system for providing a target computing environment comprising:
<claim-text>a target processor including floating point hardware and integer hardware; and</claim-text>
<claim-text>translator code for performing high precision emulation of program code instructions for a subject machine on a target machine, said translator code comprising code executable by said target processor for performing the following steps:</claim-text>
<claim-text>determining that operands in instructions of the program code for the subject machine require a higher precision than provided for by the hardware of the target machine; and</claim-text>
<claim-text>applying a floating point emulation algorithm to perform intermediate calculations on the operands of the instructions at a higher precision than the precision supported by the hardware of the target machine to produce intermediate values;</claim-text>
<claim-text>wherein at each stage, the intermediate values are tested to determine whether the intermediate values have reached a point at which the hardware of the target machine has enough precision to finish the calculation without loss of accuracy, such that when it is determined based upon the intermediate calculations that the floating point hardware of the target machine provides sufficient precision to finish the calculations required by the instructions without loss of accuracy, the floating point hardware on the target machine is utilized to finish the calculations; and the integer hardware on the target machine is utilized to perform calculations not selected to be performed by the floating point hardware.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00020" num="00020">
<claim-text>20. The system of <claim-ref idref="CLM-00019">claim 19</claim-ref>, wherein the program code instructions are accumulated instructions that are calculated at a higher precision than the operands capable of being handled by the target machine.</claim-text>
</claim>
<claim id="CLM-00021" num="00021">
<claim-text>21. The system of <claim-ref idref="CLM-00020">claim 20</claim-ref>, wherein the program code instructions are floating point accumulated instructions of the form: d=±(a*b±c),
<claim-text>wherein a, b, c and d are operands expressible as floating point numbers.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00022" num="00022">
<claim-text>22. The system of <claim-ref idref="CLM-00021">claim 21</claim-ref>, wherein said floating point emulation algorithm comprises identifying whether any of the operands (a, b, or c) are special values having a known result that all compatible hardware will produce regardless of the level of precision of said compatible hardware.</claim-text>
</claim>
<claim id="CLM-00023" num="00023">
<claim-text>23. The system of <claim-ref idref="CLM-00022">claim 22</claim-ref>, wherein said special values include either zero, infinity, or NaN (not a number), wherein the floating point hardware is utilized to calculate the result of the accumulated instructions when any of the operands (a, b, or c) are identified as special values.</claim-text>
</claim>
<claim id="CLM-00024" num="00024">
<claim-text>24. The system of <claim-ref idref="CLM-00021">claim 21</claim-ref>, wherein said floating point emulation algorithm further comprises:
<claim-text>determining whether the exponent for the result of the multiplication of (a*b) overlaps with the exponent of c; and</claim-text>
<claim-text>utilizing the floating point hardware to calculate the result of the accumulated instructions when the exponent for the result of the multiplication of (a*b) fails to overlap with the exponent of c.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00025" num="00025">
<claim-text>25. The system of <claim-ref idref="CLM-00024">claim 24</claim-ref>, wherein, when the exponent for the result of the multiplication of (a*b) overlaps with the exponent of c, said floating point emulation algorithm further comprises:
<claim-text>determining whether the mantissa for the result of the multiplication (a*b) requires more mantissa bits than provided for by said floating point hardware; and</claim-text>
<claim-text>utilizing the floating point hardware to calculate the result of the accumulated instructions when the result of the multiplication (a*b) does not require more mantissa bits than provided for by the floating point hardware.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00026" num="00026">
<claim-text>26. The system of <claim-ref idref="CLM-00025">claim 25</claim-ref>, said floating point emulation algorithm further comprising computing the full calculation of a*b using the integer hardware when mantissa for the result of the multiplication (a*b) requires more mantissa bits than provided for by the floating point hardware.</claim-text>
</claim>
<claim id="CLM-00027" num="00027">
<claim-text>27. The system of <claim-ref idref="CLM-00026">claim 26</claim-ref>, said floating point emulation algorithm further comprising:
<claim-text>determining whether the final resulting mantissa of the mantissa(a*b)−the mantissa (c) equals zero;</claim-text>
<claim-text>utilizing the floating point hardware to make the result of the calculation of a*b+c equal to zero when the resulting mantissa is equal to zero; and</claim-text>
<claim-text>calculating the remaining parts of the calculation of a*b+c using the integer hardware when the final resulting mantissa is not equal to zero.</claim-text>
</claim-text>
</claim>
</claims>
</us-patent-grant>
