<us-patent-grant lang="EN" dtd-version="v4.2 2006-08-23" file="US07298869-20071120.XML" status="PRODUCTION" id="us-patent-grant" country="US" date-produced="20071106" date-publ="20071120">
<us-bibliographic-data-grant>
<publication-reference>
<document-id>
<country>US</country>
<doc-number>07298869</doc-number>
<kind>B1</kind>
<date>20071120</date>
</document-id>
</publication-reference>
<application-reference appl-type="utility">
<document-id>
<country>US</country>
<doc-number>10894458</doc-number>
<date>20040719</date>
</document-id>
</application-reference>
<us-application-series-code>10</us-application-series-code>
<us-term-of-grant>
<us-term-extension>686</us-term-extension>
</us-term-of-grant>
<classifications-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>G</section>
<class>06</class>
<subclass>K</subclass>
<main-group>9</main-group>
<subgroup>00</subgroup>
<symbol-position>F</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20071120</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
</classifications-ipcr>
<classification-national>
<country>US</country>
<main-classification>382108</main-classification>
<further-classification>701208</further-classification>
<further-classification>382100</further-classification>
<further-classification>382254</further-classification>
<further-classification>382275</further-classification>
<further-classification>324330</further-classification>
<further-classification>324323</further-classification>
</classification-national>
<invention-title id="d0e53">Multispectral data acquisition system and method</invention-title>
<references-cited>
<citation>
<patcit num="00001">
<document-id>
<country>US</country>
<doc-number>3703133</doc-number>
<kind>A</kind>
<name>Yost</name>
<date>19721100</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<patcit num="00002">
<document-id>
<country>US</country>
<doc-number>4853543</doc-number>
<kind>A</kind>
<name>Ozdemir</name>
<date>19890800</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<patcit num="00003">
<document-id>
<country>US</country>
<doc-number>4963742</doc-number>
<kind>A</kind>
<name>Abernathy</name>
<date>19901000</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>2503385</main-classification></classification-national>
</citation>
<citation>
<patcit num="00004">
<document-id>
<country>US</country>
<doc-number>5266799</doc-number>
<kind>A</kind>
<name>Steinitz et al.</name>
<date>19931100</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>250253</main-classification></classification-national>
</citation>
<citation>
<patcit num="00005">
<document-id>
<country>US</country>
<doc-number>5557397</doc-number>
<kind>A</kind>
<name>Hyde et al.</name>
<date>19960900</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>356  501</main-classification></classification-national>
</citation>
<citation>
<patcit num="00006">
<document-id>
<country>US</country>
<doc-number>5629988</doc-number>
<kind>A</kind>
<name>Burt et al.</name>
<date>19970500</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>382276</main-classification></classification-national>
</citation>
<citation>
<patcit num="00007">
<document-id>
<country>US</country>
<doc-number>6549012</doc-number>
<kind>B2</kind>
<name>Stolarczyk</name>
<date>20030400</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<patcit num="00008">
<document-id>
<country>US</country>
<doc-number>6774939</doc-number>
<kind>B1</kind>
<name>Peng</name>
<date>20040800</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>3482314</main-classification></classification-national>
</citation>
<citation>
<patcit num="00009">
<document-id>
<country>US</country>
<doc-number>6873265</doc-number>
<kind>B2</kind>
<name>Bleier</name>
<date>20050300</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>340690</main-classification></classification-national>
</citation>
<citation>
<patcit num="00010">
<document-id>
<country>US</country>
<doc-number>7127348</doc-number>
<kind>B2</kind>
<name>Smitherman et al.</name>
<date>20061000</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>701208</main-classification></classification-national>
</citation>
<citation>
<patcit num="00011">
<document-id>
<country>US</country>
<doc-number>2004/0119020</doc-number>
<kind>A1</kind>
<name>Bodkin</name>
<date>20040600</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<patcit num="00012">
<document-id>
<country>US</country>
<doc-number>2005/0017721</doc-number>
<kind>A1</kind>
<name>McCracken et al.</name>
<date>20050100</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>324330</main-classification></classification-national>
</citation>
<citation>
<patcit num="00013">
<document-id>
<country>WO</country>
<doc-number>WO 02/082181</doc-number>
<kind>A1</kind>
<date>20021000</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<patcit num="00014">
<document-id>
<country>WO</country>
<doc-number>WO 2004/028134</doc-number>
<kind>A2</kind>
<date>20040400</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<nplcit num="00015">
<othercit>Ann Parker, Bird's Eye View Carlifies Research on the Ground, S&amp;TR May 2003, pp. 12-21, Lawrence Livermore National Laboratory.</othercit>
</nplcit>
<category>cited by other</category>
</citation>
</references-cited>
<number-of-claims>3</number-of-claims>
<us-exemplary-claim>1</us-exemplary-claim>
<us-field-of-classification-search>
<classification-national>
<country>US</country>
<main-classification>382108-113</main-classification>
<additional-info>unstructured</additional-info>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>382100</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>701208</main-classification>
</classification-national>
</us-field-of-classification-search>
<figures>
<number-of-drawing-sheets>12</number-of-drawing-sheets>
<number-of-figures>12</number-of-figures>
</figures>
<us-related-documents>
<us-provisional-application>
<document-id>
<country>US</country>
<doc-number>60488826</doc-number>
<kind>00</kind>
<date>20030721</date>
</document-id>
</us-provisional-application>
</us-related-documents>
<parties>
<applicants>
<applicant sequence="001" app-type="applicant-inventor" designation="us-only">
<addressbook>
<last-name>Abernathy</last-name>
<first-name>Donald A.</first-name>
<address>
<street>1208 Rapids Rd.</street>
<city>New Braunfels</city>
<state>TX</state>
<postcode>78130</postcode>
<country>US</country>
</address>
</addressbook>
<nationality>
<country>omitted</country>
</nationality>
<residence>
<country>US</country>
</residence>
</applicant>
</applicants>
<agents>
<agent sequence="01" rep-type="attorney">
<addressbook>
<orgname>Eric W. Cernyar, P.C.</orgname>
<address>
<country>unknown</country>
</address>
</addressbook>
</agent>
</agents>
</parties>
<examiners>
<primary-examiner>
<last-name>Ahmed</last-name>
<first-name>Samir</first-name>
<department>2624</department>
</primary-examiner>
<assistant-examiner>
<last-name>Rush</last-name>
<first-name>Eric</first-name>
</assistant-examiner>
</examiners>
</us-bibliographic-data-grant>
<abstract id="abstract">
<p id="p-0001" num="0000">A portable multispectral data acquisition system for use on a vehicle such as an aircraft comprises a plurality of gyroscope-stabilized remote sensing devices synchronized to simultaneously capture images of a common spatial area in both visible and invisible bands of the electromagnetic spectrum, a computer and digital recorder to record and correlate the captured images with temporospatial reference information, image processing software to stack or layer the images and to extract and compare the images in order to identify and filter hidden or subsurface anomalies that are invisible to the naked eye, and additional image processing software to orthorectify the images to a three-dimensional digital terrain map and to stitch adjacent images together into a mosaic. Methods for using the portable multispectral data acquisition system are also provided.</p>
</abstract>
<drawings id="DRAWINGS">
<figure id="Fig-EMI-D00000" num="00000">
<img id="EMI-D00000" he="184.15mm" wi="167.30mm" file="US07298869-20071120-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00001" num="00001">
<img id="EMI-D00001" he="206.93mm" wi="170.35mm" file="US07298869-20071120-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00002" num="00002">
<img id="EMI-D00002" he="205.91mm" wi="173.74mm" file="US07298869-20071120-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00003" num="00003">
<img id="EMI-D00003" he="158.16mm" wi="155.79mm" file="US07298869-20071120-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00004" num="00004">
<img id="EMI-D00004" he="168.66mm" wi="136.23mm" file="US07298869-20071120-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00005" num="00005">
<img id="EMI-D00005" he="166.71mm" wi="123.78mm" file="US07298869-20071120-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00006" num="00006">
<img id="EMI-D00006" he="203.20mm" wi="146.98mm" file="US07298869-20071120-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00007" num="00007">
<img id="EMI-D00007" he="154.09mm" wi="143.59mm" file="US07298869-20071120-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00008" num="00008">
<img id="EMI-D00008" he="204.55mm" wi="132.08mm" file="US07298869-20071120-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00009" num="00009">
<img id="EMI-D00009" he="219.88mm" wi="100.16mm" file="US07298869-20071120-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00010" num="00010">
<img id="EMI-D00010" he="200.66mm" wi="150.11mm" file="US07298869-20071120-D00010.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00011" num="00011">
<img id="EMI-D00011" he="103.21mm" wi="143.76mm" file="US07298869-20071120-D00011.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00012" num="00012">
<img id="EMI-D00012" he="234.78mm" wi="151.13mm" file="US07298869-20071120-D00012.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
</drawings>
<description id="description">
<?RELAPP description="Other Patent Relations" end="lead"?>
<heading id="h-0001" level="1">RELATED APPLICATIONS</heading>
<p id="p-0002" num="0001">This application claims the benefit of, and incorporates by reference, U.S. Provisional Application No. 60/488,826, filed Jul. 21, 2003, and entitled “Multispectral Data Acquisition System.”</p>
<?RELAPP description="Other Patent Relations" end="tail"?>
<?BRFSUM description="Brief Summary" end="lead"?>
<heading id="h-0002" level="1">FIELD OF THE INVENTION</heading>
<p id="p-0003" num="0002">This invention relates to remote sensing systems, and more particularly to remote sensing systems that employ a plurality of sensing devices to detect both surface and subsurface features of a place, region or object.</p>
<heading id="h-0003" level="1">BACKGROUND</heading>
<p id="p-0004" num="0003">The most familiar system for detecting the surface features of a terrestrial surface or object is the human eye. But there is far more electromagnetic information about an object than meets the eye. It is well known, for example, that all objects having a temperature above absolute zero emit electromagnetic radiation. Hot objects predominantly emit radiation at short wavelengths. Cool objects predominantly emit radiation at longer wavelengths. Objects also absorb and reflect incident electromagnetic radiation. The variation of an object's reflectance with respect to the wavelength of the incident light helps determine what an object looks like. Indeed, different materials and molecular structures emit and reflect electromagnetic energy in different but characteristic ways depending on the temperature of the material or the wavelength of the incident radiation.</p>
<p id="p-0005" num="0004">There are a variety of commercially available sensors and imagers operable to detect various wavelengths and intensities of electromagnetic energy, both visible and invisible, and both reflected and emitted, emanating from various objects and materials. Employing knowledge about known characteristic energy absorption and radiation patterns for different surface and subsurface materials and molecular structures, it is possible to use information collected from such sensors and imagers to detect and identify surface and subsurface features of a place, region or object that are invisible to or hidden from the naked eye. These hidden or invisible features are commonly referred to as “anomalies.”</p>
<p id="p-0006" num="0005">Surface and subsurface imaging technology has a wide field of applications. Imaging technology can and has been used, for example, to survey fields to search for pipeline leaks. In U.S. Pat. No. 4,963,742, I describe an airborne multispectral survey system comprising up to three infrared detectors and two video cameras. Using standard, commercially available video tape recorders, the system simultaneously captured and recorded images in both the visible region and in selected infrared bands of the electromagnetic spectrum of the terrain below. After the recording, analysis could be performed by simultaneously playing the infrared and video recordings to identify anomalies—areas where the infrared recordings detected temperatures outside of a given threshold and which could not readily be explained from the visible features of the terrain. That system, however, heavily on human observation and on-site inspection of anomalies. Moreover, given the limitations of the system, it was difficult to separate anomalies of interest from “false” anomalies (anomalies not of interest). Therefore, there is a need for more sophisticated multispectral remote sensing systems that facilitate improved anomaly analysis capabilities.</p>
<heading id="h-0004" level="1">SUMMARY OF THE INVENTION</heading>
<p id="p-0007" num="0006">A data acquisition system is provided comprising a plurality of remote sensing devices. One embodiment of the data acquisition system comprises a wide angle visible-spectrum camera, a narrow-field-of-view visible-spectrum camera, a short infrared wavelength imager, a long infrared wavelength imager, a laser mapping or scanning device, an ultraviolet imager, a broad-band linear sweep laser, a magnetometer, and means for simultaneously recording the information collected. Means are provided for integrating captured frames into a composite image or mosaic; for overlaying and comparing images captured from one imager with corresponding (i.e., simultaneously captured) images from another; and for identifying and distinguishing surface from subsurface anomalies. In one embodiment, the image capture devices are mounted on an aircraft such as a helicopter, together with gyroscopes or other means for stabilizing the instruments during flight and means for simultaneously recording referential data provided by a global positioning satellite system and an inertial reference system. In other embodiments, the image capture devices are handheld or mounted on terrestrial, sea-borne, or amphibious vehicles.</p>
<p id="p-0008" num="0007">The primary purpose of the data acquisition system is to acquire information in several different electro-magnetic bands or frequencies simultaneously so that data analysis will allow direct interpretation of effect and reaction of varying radiation levels or norms that prove to be valuable information, not available in any single wave length. The data acquisition system allows visual reference, as well as computer generated interpretive uses, and both real time and recorded analysis and review.</p>
<p id="p-0009" num="0008">The data acquisition system works by looking for a combination of radiation levels. The resulting images allow a computer application or trained analyst to see and identify varying geometric digital profiles, or signatures, that violate the logical geometric comparable norm patterns. The several different remote sensors of the data acquisition system are trained on a common terrestrial target area, for viewing selected frequency bands of the electromagnetic field. Perspective data acquired by the remote imaging devices are preferably compiled to create what would appear to be a multi-dimensional (or layered) logic array of “stacked” imaging data. Some of this data is reflective energy, while other is radiated energy. Differences that appear between the various images that may be referred to as “anomalies.” These anomalies are caused by variations in the reflective and radiated energies in different frequency bands. Often, the variations are foreign effects caused by an influencing generator or absorbent of the natural thermodynamic characteristics in the target area. Anomalies are also caused by changes in the density of target mass.</p>
<p id="p-0010" num="0009">The data acquisition system is preferably operable to provide information including, but not limited to, changes in the absorption characteristics of sub-surface mass, changes in geological mass radiation caused by fluids or gasses below or above the surface target area, and changes in radiation influences on ground, water or other surface materials. The data acquisition system also preferably provides high speed assessment of the effect and reactions of varying levels of both radiation and reflected energy of the “target” and the effects caused by virtually anything within the radiation target area that would cause an abnormal radiation ambient or average deviation. Thus, a suitable embodiment of a data acquisition system includes the following equipment: a visual spectrum camera of 0.2-0.90 microns with defined color separation; a 3-14 micron infrared imager with varying ranges and the capacity of a variable or “floating raster” of defined increment range that can manually or automatically sweep the ambient range of the target area; an advanced cesium magnetometer array that provides earth magnetic flux data; a specialized laser imaging system combined with a spectrometer that provides profile and other spectrographic information; a time code generator or a GPS signal to ensure that each frame or sector of data is clearly identified; data recorders; broadband color CCD cameras (0.2-0.9 microns); lenses for the cameras; power distribution box and cables; and color monitors for real time data processing and system status monitoring. Other technologically enhanced embodiments of the data acquisition system may also incorporate an ultra-violet imaging system, a high-resolution variable spectrum solenoid, and side scan pulse sonar.</p>
<p id="p-0011" num="0010">The image analysis process is able to support capture and retention of files that conventional CAD engineering programs can read, thus making the data more usable. The analyst is able to overlay image data onto G.I.S. data. This enables optimal analysis, superior inventorying and precise locating capabilities. Interpretive analysis may be carried out on numerous phenomena ranging from subsurface mass, ground water activity, leachate and pipeline leaks to the density changes in vegetation where drought, disease or photosynthesis problems may exist.</p>
<p id="p-0012" num="0011">I believe my invention to be useful for a wide range of applications, including detection of water and gas pipeline leaks, mechanical failures in industrial machinery, and electrical insulation damage on utility lines. Other applications include detection of crevasses, road surface delamination, dam and dike seepage points, gas leaks and gas seepage, and subsurface dump sites. The invention is also useful for land use studies and regulatory-mandated environmental surveys. Environmental study applications include vegetation discrimination, urban microclimate studies, airflow effects on vegetation near water bodies, crop moisture studies, animal behavior studies, glacial drift studies, estuarine ecology studies, arctic geology studies, groundwater studies, identification of subterranean streams, hot spring detection, river current studies, thermal effluent discharge detection and mapping, strip mining studies, forest fire detection and mapping, geological mapping, and volcano studies.</p>
<p id="p-0013" num="0012">These and other aspects, features, advantages, and applications of various embodiments of the present invention will be readily apparent to those skilled in the art from the following detailed description taken in conjunction with the annexed drawing, which illustrate the invention.</p>
<?BRFSUM description="Brief Summary" end="tail"?>
<?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?>
<description-of-drawings>
<heading id="h-0005" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading>
<p id="p-0014" num="0013"><figref idref="DRAWINGS">FIG. 1</figref> is a block diagram of one embodiment of a multispectral data acquisition system.</p>
<p id="p-0015" num="0014"><figref idref="DRAWINGS">FIG. 2</figref> illustrates an aircraft equipped with another embodiment of a multispectral data acquisition system.</p>
<p id="p-0016" num="0015"><figref idref="DRAWINGS">FIG. 3</figref> illustrates a top level view of an on-board computer and controller for a portable multispectral data acquisition system.</p>
<p id="p-0017" num="0016"><figref idref="DRAWINGS">FIG. 4</figref> is a view of one embodiment of an aircraft-mounted platform for carrying two cameras.</p>
<p id="p-0018" num="0017"><figref idref="DRAWINGS">FIG. 5</figref> illustrates a small hand held controller being held by a person in an ultra-light aircraft equipped with a multispectral data acquisition system.</p>
<p id="p-0019" num="0018"><figref idref="DRAWINGS">FIG. 6</figref> is a scanned image depicting a plurality of corresponding orthorectification points on a pre-orthorectified image and a topographic map to which the pre-orthorectified image is to be fitted.</p>
<p id="p-0020" num="0019"><figref idref="DRAWINGS">FIG. 7</figref> is a scanned image illustrating an infrared image that has been orthorectified to fit over a visible image slice.</p>
<p id="p-0021" num="0020"><figref idref="DRAWINGS">FIG. 8A</figref> is a scanned image of a series of logically stacked spatially calibrated image slices simultaneously captured from different remote sensing devices.</p>
<p id="p-0022" num="0021"><figref idref="DRAWINGS">FIG. 8B</figref> is a line drawing illustrating a series of logically stacked spatially calibrated image slices simultaneously captured from different remote sensing devices.</p>
<p id="p-0023" num="0022"><figref idref="DRAWINGS">FIG. 9</figref> is a functional block diagram of one embodiment of a method of operating a multispectral data acquisition system.</p>
<p id="p-0024" num="0023"><figref idref="DRAWINGS">FIG. 10</figref> is a functional block diagram of one embodiment of a method of orthorectifying and layering images captured with a multispectral data acquisition system.</p>
<p id="p-0025" num="0024"><figref idref="DRAWINGS">FIG. 11</figref> is a functional block diagram of one embodiment of an automated method of processing a combination of visible, infrared, and magnetometer data to identify anomalies.</p>
</description-of-drawings>
<?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?>
<?DETDESC description="Detailed Description" end="lead"?>
<heading id="h-0006" level="1">DETAILED DESCRIPTION</heading>
<p id="p-0026" num="0025">Turning to <figref idref="DRAWINGS">FIG. 1</figref>, an embodiment of a multispectral data acquisition system <b>100</b> is provided comprising a plurality of remote sensing devices <b>110</b>, one or more controllers <b>185</b> operable to control the focus, direction, and other operating characteristics of the remote sensing devices <b>110</b>, an image and data processor <b>190</b>, and one or more digital recorders <b>180</b>. The digital recorders <b>180</b> comprise any suitable media for digital recording, including but not limited to digital <b>8</b> format VCRs, flash memory, optical drives, and standard computer hard drives. The image and data processor <b>190</b> comprises one or more computer processors and random access memory for executing software applications that correlate, filter, and process the data recorded on the digital recorders <b>180</b>. Although the invention is not limited to any one combination of remote sensing devices <b>110</b>, it preferably includes a plurality of passive and active remote sensing devices, including at least one remote subsurface imaging device.</p>
<p id="p-0027" num="0026">As shown and depicted in the drawing, the data acquisition system <b>100</b> comprises a wide angle visual camera <b>115</b>, a narrow field-of-vision camera <b>120</b>, a 3-6 μm mid-range infrared imager <b>130</b>, an 8-14 μm long-wavelength infrared imager <b>135</b>, a three-dimensional narrow-band laser range finder or laser mapping device <b>140</b>, commonly known by the acronym LIDAR (light detection and ranging), an ultraviolet imager <b>145</b>, magnetometer system <b>150</b>, and a 3-axis variometer <b>155</b>. The data acquisition system <b>100</b> as shown and depicted in the drawing also comprises a broadband laser emitter <b>125</b>.</p>
<p id="p-0028" num="0027">In operation, the remote sensing devices <b>110</b> are mounted on an aircraft or other vehicle. The digital recorders <b>180</b> and controllers <b>185</b> are also preferably carried by the same vehicle carrying the remote sensing devices <b>110</b>, but with the help of high-bandwidth wireless communication link, could be housed or mounted elsewhere. The image and data processor <b>190</b> or some portion of it may also carried by the same vehicle carrying the sensing devices <b>110</b> and digital recorders <b>180</b> and in real-time communication with the same. Alternatively, the image and data processor <b>190</b> or some portion of it is housed or mounted elsewhere, such as a ground-based data processing facility.</p>
<p id="p-0029" num="0028"><figref idref="DRAWINGS">FIG. 2</figref> illustrates an aircraft <b>210</b> equipped with an embodiment of a multispectral data acquisition system <b>205</b>. GPS antenna <b>220</b> receives GPS positioning data from GPS satellites <b>222</b>-<b>225</b> and transfers the information to an onboard computer <b>208</b>. An inertial reference system <b>230</b> is communicatively coupled to the computer <b>208</b> so that it can record the roll, pitch, and heading of the aircraft and determine the orientation of the aircraft <b>210</b>. A gyroscope-stabilized platform <b>211</b> for carrying remote sensors is mounted to the aircraft <b>210</b>. The platform <b>211</b> carries a first visible-spectrum camera <b>212</b> having a wide angle field of view <b>213</b> and a second infrared-spectrum camera <b>214</b> having a narrow field of view <b>215</b>. The objective focal axis (not shown in <figref idref="DRAWINGS">FIG. 2</figref> but corresponding to the direction to which the sensor is directed) of the first camera <b>212</b> is substantially parallel to the second camera <b>214</b>, so that the two cameras are trained on the same target.</p>
<p id="p-0030" num="0029">The platform <b>211</b> (or another like it) also carries a laser emitter <b>216</b> and a laser receiver/detector to <b>218</b> scan the surface of the terrain below to record elevation data. This laser emitter and detector pair may comprise a standard range-finding LIDAR, a differential absorption LIDAR (DIAL) (which compares absorbed wavelengths with reflected wavelengths to identify molecules with particular spectrographic signatures), a Doppler LIDAR (to measure the velocity of a target), or a device that combines the capabilities of all three. In pipeline applications, the laser emitter and detector pair is preferably tuned to an infrared, most preferably the near infrared, frequency or range of frequencies to facilitate detection of pipeline leaks and the size of the flume.</p>
<p id="p-0031" num="0030"><figref idref="DRAWINGS">FIG. 3</figref> illustrates a top level view of an on-board computer and controller <b>300</b> for a portable multispectral data acquisition system. The controller <b>300</b> has a digital data recorder <b>310</b> to record images received from its imagers and detectors together with temporospatial information received from the GPS satellites <b>222</b>-<b>225</b> and inertial reference system <b>230</b>. The digital data recorder <b>310</b> can also record verbal information spoken by the person operating the system (e.g., the pilot or an on-board technician) while the images are being recorded. A video screen <b>312</b> on the controller enables an operator of the system to view images received from one or more of the sensing devices in real time.</p>
<p id="p-0032" num="0031">The controller also has a motion control interface <b>320</b> with switches, knobs, and a joystick to control the sensitivity of the gyros and the speed of the servos and to actuate the servos that control the orientation of the sensing devices carried by the platform <b>211</b>. The onboard controller <b>300</b> also includes a camera setting control interface <b>330</b> to control various settings of the remote sensing devices carried on the platform <b>211</b>. Such settings include the digital and optical zoom (or field of view) settings, the camera focus, and, for any IR cameras, the emissivity setting. Other optional adjustable settings (not shown in <figref idref="DRAWINGS">FIG. 3</figref>) include the ranging and power levels for the LIDAR system and any magnetometers and the frequency levels of any scanners and oscillators.</p>
<p id="p-0033" num="0032"><figref idref="DRAWINGS">FIG. 4</figref> is a view of one embodiment of the platform <b>211</b> for carrying two cameras <b>410</b> and <b>420</b>, which for purposes of <figref idref="DRAWINGS">FIG. 4</figref>, is referred to as a camera pack <b>400</b>. Camera pack <b>400</b> comprises a gyroscope stabilized aircraft mount <b>440</b> and cameras <b>410</b> and <b>420</b>. The camera pack <b>400</b> includes a variety of servos that are used to both adjust the position of the cameras <b>410</b> and <b>420</b> and to adjust various settings of those cameras. For example, one servo controls a pulley <b>424</b> and adjusts the focus of the lens <b>422</b>. Another servo turns a pulley <b>434</b> to adjust the camera angle (i.e., the angle of the camera's objective focal axis).</p>
<p id="p-0034" num="0033"><figref idref="DRAWINGS">FIG. 5</figref> illustrates a small hand held controller <b>500</b> being held by a person in an ultra-light aircraft <b>550</b> equipped with a multispectral data acquisition system. This hand held controller <b>500</b> includes a video screen <b>312</b>, a camera motion control interface <b>520</b> and a camera setting control interface <b>530</b>. The small hand held controller <b>500</b> enables an operator to adjust remote sensing device settings (such as the emissivity setting of an IR camera) in real time as multispectral videos are being captured.</p>
<p id="p-0035" num="0034">A suitable data acquisition system <b>100</b> preferably utilizes several mass-produced, “off-the-shelf” components, because they tend to be more light-weight, compact, and economical than custom-made components. For example, a suitable camera <b>115</b> or <b>120</b> comprises a broadband color CCD block camera made by Sony® under the product designation “FCB-EX780SP.” This particular camera is operable to capture wavelengths from between about 0.2 and about 0.9 μm in length, i.e., the visible spectrum and the near infrared and near ultraviolet portions of the spectrum. Because this particular camera extends into the near ultraviolet portion of the spectrum, it is also suitable, if chilled and combined with a filter, to function as the ultraviolet imager <b>145</b>. For example, a 0.24-0.28 μm wavelength pass-through filter would enable the camera to capture images of high-voltage coronas on electric distribution lines and components.</p>
<p id="p-0036" num="0035">A suitable infrared imager <b>130</b> is sold under the trademark ThermoVision Ranger™ by Flir Systems, Inc.™ of Portland Oreg. This particular imager <b>130</b> has a 3-5 μm near infrared spectral range. A suitable infrared imager <b>135</b> is sold under the trademark ThermoVision 2000™ by Flir Systems, Inc. This particular long-range, platform mounted imager has an 8-9 μm far infrared spectral range.</p>
<p id="p-0037" num="0036">The data acquisition system <b>100</b> obtains valuable 3-dimensional information about the topography of a surface through its laser ranger finder <b>140</b>. A suitable laser range finder <b>140</b> is sold by Optech™ of Toronto, Canada, under the product designation “ALTM 30/70”. The laser range finder <b>140</b> is used to obtain detailed and accurate elevation or depth information about a topographic surface.</p>
<p id="p-0038" num="0037">The data acquisition system <b>100</b> obtains important subsurface information through its magnetometer system <b>150</b>. A magnetometer is an instrument for measuring the variations in the magnitude and direction of a magnetic field, and in particular, variations in the earth's magnetic field. Surface and subsurface features such as a buried pipeline can distort the earth's magnetic field in ways that can be remotely sensed with a handheld or aircraft-mounted magnetometer. A suitable magnetometer <b>150</b> comprises a cesium magnetometer sensor and electronics package is sold by Geometrics™ of San Jose, Calif., under the product designation “G-822A”.</p>
<p id="p-0039" num="0038">In a preferred embodiment, an array of at least two of these cesium magnetometers are adjustably mounted on one or more booms attached to a helicopter or ultralight aircraft at a distance of about 40 feet apart. At this distance, two linear data streams can be acquired during a flyover of a pipeline, eliminating the need, in some applications (especially pipeline applications), to make parallel flight lines over the surveyed area. The magnetometers are adjustably mounted to maintain the sensors' equators and long axes at angles of at least 6 degrees away from the earth's field vector.</p>
<p id="p-0040" num="0039">During operation, magnetic field data is acquired from both magnetometers simultaneously at an above-ground altitude of up to 30,000 feet, but most preferably at about 500 to 2,000 feet. The absolute magnetometer data of the two magnetometers are compared to determine the lateral horizontal gradient between the two magnetometers. The magnetometer data is then used to generate a magnetometer survey map of the terrestrial surface. In pipeline survey applications, the magnetometer data is used to detect cathodic breakdown of the pipes.</p>
<p id="p-0041" num="0040">It is expected and anticipated that many other “off-the-shelf” components exist or will become available with advances in technology that would also be suitable for the data acquisition system <b>100</b> described herein and possibly superior to the components described herein. Therefore, it will be understood that the invention disclosed in this application is by no means limited to the specific “off-the-shelf” components noted herein.</p>
<p id="p-0042" num="0041">As shown and depicted in <figref idref="DRAWINGS">FIG. 1</figref>, the data acquisition system <b>100</b> also comprises a broadband laser emitter <b>125</b> utilizing a light source having a range of wavelengths from 0.1 to 18 microns. A light source passes through a very precise prism mounted on a piezo-ceramic wafer that vibrates at frequencies of up to 1 GHz, providing a linear sweep of wavelengths, which can be time-calibrated, that are emitted by the laser emitter <b>125</b>. The other remote sensing devices <b>110</b> may be utilized to detect reflected laser light originating from the broadband laser emitter <b>125</b>. In this manner, the remote sensing devices <b>110</b> can function as both passive and active remote sensors.</p>
<p id="p-0043" num="0042">In alternative embodiments, the broadband laser emitter can be used in place of a conventional laser range finder to generate a three-dimensional topographical map of a surface. Because control of the piezo-ceramic wafer can be used to precisely alter the direction of the laser beam of the broadband laser emitter <b>125</b>, there is no need to pulse the laser light source, as is done with conventional laser range finders.</p>
<p id="p-0044" num="0043">Airborne-based embodiments of the data acquisition system <b>100</b> preferably include means for mounting the remote sensing devices <b>110</b> on the aircraft and means for stabilizing the remote sensing devices <b>110</b> during flight. A suitable gyroscope-stabilized aircraft mount for the remote sensing devices <b>110</b> is marketed by K-Hill, Inc., of Fort Worth Tex. under the Sky Pack™ trademark.</p>
<p id="p-0045" num="0044">The data acquisition system <b>100</b> also comprises a time code generator <b>160</b> and time base corrector <b>165</b> to synchronize the remote sensing devices <b>110</b>. Airborne-based embodiments of the data acquisition system <b>100</b> preferably also include a global positioning satellite (GPS) receiver <b>170</b> to provide information about the precise location of the aircraft. In these embodiments, the GPS data also serves as an input to the time code generator <b>160</b>. Airborne-based embodiments of the data acquisition system <b>100</b> also preferably include an inertial reference system <b>175</b> to record the roll, pitch and heading of the aircraft to determine its orientation in space. Input from the GPS receiver <b>170</b> and inertial reference system <b>175</b> enables the image and data processor <b>190</b> to correlate spatial reference information with the frames and/or data streams captured by the remote sensing devices <b>110</b>.</p>
<p id="p-0046" num="0045">One embodiment of the image and data processor <b>190</b> takes the form of a standard, commercially available computer, in combination with one or more image processing video boards plugged into a hardware slot of the computer. The image and data processor <b>190</b> preferably includes a multispectral image mixer module <b>192</b> operable to stack images captured from the different remote sensing devices <b>110</b>. The image and data processor <b>190</b> also preferably includes a module or application <b>194</b> operable to automatically orthorectify and integrate topographical images into a composite image or mosaic. A preferred process of orthorectification is described further below. A suitable orthorectification and mosaic module or application <b>194</b> is sold under the trademark ER Mapper™ by the Earth Resource Mapping company of San Diego, Calif.</p>
<p id="p-0047" num="0046">The image and data processor <b>190</b> also preferably includes a feature extraction module or application <b>196</b> operable to identify geometric features or profiles in the captured images. Suitable techniques for filtering the images to find anomalies include contrast enhancement, edge detection, two-dimensional fast Fourier transformations and inverse Fourier transformations, image averaging, and de-interlacing to remove field-to-field jitter. The purpose of the filtering is to reduce noise and draw out hidden details contained in the images to improve anomaly detection. A filtering technique suitable for one terrain, such as a lush green field, many not be suitable for a different terrain, such as an arid terrain.</p>
<p id="p-0048" num="0047">The feature extraction module <b>196</b> should also be operable to comparatively analyze the geometric profile of an image captured in one frequency or wavelength (e.g., the infrared spectrum) with the geometric profile of an image captured in any of the other frequency or wavelength (e.g., the visible spectrum), detect differences between the geometric profiles of the various spectrum-specific images, analyze those differences in reference to a rule-based filter or framework (e.g., a set of thresholds), and tag or track those geometric profiles that survive the filtering process of the rule-based framework. In this manner, the feature extraction module <b>196</b> is operable to identify features or geometric profiles that are detectable in one or more spectrums but invisible in one or more other spectrums. In this manner, the feature extraction module <b>196</b> can also be tuned to identify very specific anomalies.</p>
<p id="p-0049" num="0048">The data acquisition system <b>100</b> optionally also includes land or marine based data acquisition modules, such as an electrical conductivity (or resistivity) imaging device to measure terrestrial resistance to current flow. It is known, for example, that areas that are moist or compacted are more conductive than areas that are dry, less compacted, or which contain voids.</p>
<p id="p-0050" num="0049">In general, the multispectral data acquisition system of the present invention is designed for high-speed recording and analysis of data from several frequencies of the electromagnetic spectrum. The remote sensors are preferably mounted on an aircraft such as a helicopter or ultralight aircraft. Alternatively, the remote sensors may be manually carried or mounted on a terrestrial, marine, or amphibious vehicle.</p>
<p id="p-0051" num="0050"><figref idref="DRAWINGS">FIG. 9</figref> illustrates one embodiment of a method of operating the data acquisition system. In step <b>910</b> logistical planning is performed. One of the most important aspects of any given job is logistical planning. This involves determining the scope and location parameters of the job because all calibrations and timing issues are likely to be affected or based upon this information.</p>
<p id="p-0052" num="0051">For some applications it is important to select a time window for data acquisition that minimizes shadow differential signatures, moisture and humidity variances, temperature variances, and the like. The selected time window should also take into account the fact that radiation reflectance and emission characteristics of objects on the surveyed terrain may vary at different times of the day and may affect the quality of the data acquired. Furthermore, it is desirable to select flight times that coincide with observation windows during which GPS satellites will provide the most reliable, error-minimized temporospatial information.</p>
<p id="p-0053" num="0052">Before performing the data acquisition, it is also desirable to have geological and botanical information of the target area. This information can often be determined by viewing a USGS digital terrain or elevation model or existing satellite and aerial imagery of that area.</p>
<p id="p-0054" num="0053">In step <b>920</b>, the remote sensing devices (cameras and detectors) are calibrated and adjusted to optimize the sensitivity of the devices to one or more targeted phenomena. For example, the emissivity setting of any IR imagers are adjusted to facilitate detection of the targeted phenomena. IR cameras with a floating raster are also calibrated or adjusted to target a specific radiation window of interest, so that the thermal characteristics of infrared imaged objects following outside this window of interest will appear saturated. Under Wein's Displacement Law, black body terrain objects which have temperatures ranging from −40 F to +150 F (the range of natural landscape temperatures) have peak emissions from 8.6 to 12.4. Therefore, optic systems and detectors are matched for maximum sensitivity in this region in order to achieve significant terrain component differentiation.</p>
<p id="p-0055" num="0054">The spectral windows of one or more sensing devices (or of the laser emitter of a LIDAR system) are also tuned or filtered as needed to capture a targeted range of electromagnetic phenomena. The sensing device's fields of view, exposure time, and gain levels are also adjusted as needed. The ranging and power levels of the laser and magnetometers and the frequencies of scanners and oscillators are also adjusted as needed.</p>
<p id="p-0056" num="0055">In step <b>930</b>, the sensing devices are time calibrated. It is desirable to relate all of the data tracks to a single coordinated specific time interval. The cameras mounted on the data acquisition system should capture image frames simultaneously. A digital time code generator furnishes regular time signals (60 per second) to logic and data control modules which record the signals on the recording apparatus, thus providing a common frame of reference. Alternatively, time code excitation is done using the GPS signals as the excitation source for timing.</p>
<p id="p-0057" num="0056">In step <b>940</b>, a flight is taken and images are acquired using the data acquisition system. In the operation of an airborne-based embodiment of a data acquisition system, an aircraft is flown at a constant altitude above sea level over the area to be surveyed. The sensing devices of the data acquisition system are directed at the area and synchronized recordings are made of the signals from these devices. The airborne portion of the data acquisition system is carried in the cargo area of the aircraft with the equipment operator operating it by remote control.</p>
<p id="p-0058" num="0057">The data acquisition system preferably captures, records and relates the following data during flight: images and data from each of the sensing devices and camera, a time code, GPS three-dimensional spatial data and inertial reference system data (such as speed and direction) for the aircraft, and the height above ground/target area data using a radar altimeter or laser altimeter. The data acquisition system also optionally records camera setting data (such as the focal or field of view data for one or more cameras) in order to facilitate an automated orthorectification process.</p>
<p id="p-0059" num="0058">During the data acquisition process, the operator (e.g., the pilot, navigator, or technician) pass over the target area and verbally document landmark characteristics and other information onto the data stream. The data acquisition system records an audio of such remarks along with the sensing device data, longitudinal and latitude data (in minutes and seconds) of the aircraft or image target, and other interpretive data. These activities facilitate accuracy.</p>
<p id="p-0060" num="0059">In step <b>950</b>, the operator views the image data and adjusts settings for the remote sensing devices in real time. The operator is desirably trained to make the proper kinds of adjustments in order to more sensitively capture the target data.</p>
<p id="p-0061" num="0060">In step <b>960</b> the image data is orthorectified. The process of orthorectification <b>930</b>, which can take many different forms, can be carried out in either two or three dimensions and is explored further below in conjunction with <figref idref="DRAWINGS">FIG. 10</figref>.</p>
<p id="p-0062" num="0061">In step <b>970</b> images from different electromagnetic spectra are integrated and co-processed to identify and interpret anomalies. This process, which may also take many different forms, is explained further below in conjunction with <figref idref="DRAWINGS">FIG. 11</figref>.</p>
<p id="p-0063" num="0062"><figref idref="DRAWINGS">FIG. 10</figref> is a functional block diagram of one method of spatially coordinating or orthorectifying the visual image data. Orthorectification is performed to remove geometric distortions introduced by imperfect camera lens, such as spherical distortions from using a lens with a short focal length. The aim of the orthorectification process is to make any two overlapping pixels of two “overlapping” captured image frames to represent the same piece of terrain. When completed, the orthorectified image data facilitates automatic computerized anomaly identification through comparative pixel-by-pixel analysis of the images.</p>
<p id="p-0064" num="0063">As illustrated by step <b>1010</b>, the process of orthorectification begins with the identification or selection of a reference image. Then, in step <b>1020</b>, matching landmarks visible in both the captured and reference images are identified. Preferably at least ten corresponding orthorectification points on the captured and reference images are identified. For illustration, <figref idref="DRAWINGS">FIG. 6</figref> depicts a plurality of corresponding orthorectification points (by the letters A, B, C, and D) on a pre-orthorectified image <b>610</b> and a topographic map <b>620</b> chosen as a reference image to which the image <b>610</b> is to be fitted. The process of identifying corresponding orthorectification points can be done manually or automatically using computer-executed geometric profile recognition algorithms. This process is facilitated if the reference image obtained comes pre-populated with orthorectification points or other data (e.g., highway data) from which orthorectification points (e.g., intersections) can easily be identified or algorithmically extracted.</p>
<p id="p-0065" num="0064">In step <b>1030</b>, computer executed algorithms derive various mathematical transformation matrixes, that when multiplied by the captured image data, effectively rotate, stretch and bend the captured image until its identified landmarks perfectly align with the corresponding landmarks identified in the reference image. Once orthorectified, the captured image spatially matches with and can be overlaid onto the reference image. <figref idref="DRAWINGS">FIG. 7</figref> illustrates an infrared image that has been orthorectified to fit over a visible image slice.</p>
<p id="p-0066" num="0065">Orthorectification is sometimes narrowly understood as a mathematical process of removing the distortion caused by relief and the camera within a photograph so that the scale is uniform throughout the output image. In this sense, the process of orthorectification scales and aligns the image to a specified geodetic coordinate system. In some embodiments of the present invention, orthorectification is performed to a geodetic coordinate system. USGS topographical data may be used as a reference image source and the captured images orthorectified to that topographical data. In these embodiments, the reference image selected in step <b>1010</b> comprises a three dimensional topographical map such as a USGS GIS digital terrain or elevation model. In these embodiments, the process of orthorectification permits the captured and now georeferenced images to inherit the elevation data of the topographical map. In these embodiments, each georeferenced orthorectified image slice is preferably stored as a separate layer from the survey map data, and the georeferenced orthorectified images may then be rotated and viewed in three dimensions.</p>
<p id="p-0067" num="0066">But in other embodiments, orthorectification need not be performed to scale and align the captured images to a specified geodetic coordinate system. Instead, the reference image selected in step <b>1010</b> may comprise one of the captured images. The remaining remote sensing image data are orthorectified to the selected reference image.</p>
<p id="p-0068" num="0067">An advantage of this alternative, non-geodetic orthorectification is automation. A technician may be employed to identify the common orthorectification points between multispectral images of a first set of simultaneously-captured frames. Software is then used to derive the transformation matrixes needed to orthorectify each of the images to the chosen reference image. Because all of the cameras are mounted on a common platform and run synchronously, this manual orthorectification-point-identification process need only be done once, with the first set of frames, provided that the cameras remain fixed relative to each other and no changes are applied to the field of view (such as applying zoom) to any of the cameras. The transformation matrixes derived to orthorectify the first set of overlapping image frames may and are preferably reused, in automated fashion, to orthorectify subsequent frame sets.</p>
<p id="p-0069" num="0068">Another method of orthorectification (not illustrated) is to use the recorded GPS spatial reference data, the recorded field of view data for the camera that took the image, and any recorded altimeter data to geometrically calculate the spatial position of the center of the image at the ground and the spatial extent of the boundaries of the captured image. This information can be used to either calculate the amount of stretching, skew, and rotation needed to authomatically orthorectify the image to two dimensions, or to facilitate identification of corresponding topographical map data (including corresponding orthorectification points). Yet another method of orthorectification (not illustrated) is to use an image manipulation algorithm to incrementally rotate, skew, filter and refilter the image data and compare it with the topographical map until it finds a good match.</p>
<p id="p-0070" num="0069"><figref idref="DRAWINGS">FIGS. 8A and 8B</figref> illustrate a series of logically stacked orthorectified image slices simultaneously captured from different remote sensing devices. An orthorectified visible light spectrum image <b>810</b> illustrates what the human eye would see of the surveyed terrain. A spatially-corresponding orthorectified low-band infrared image <b>820</b> illustrates radiation data for the surveyed terrain. The image <b>820</b> may also illustrate reflectivity and absorption data from a LIDAR system. A spatially-corresponding orthorectified high-band infrared image <b>830</b> illustrates radiation and density data for the surveyed terrain. A spatially-corresponding orthorectified ultraviolet band image <b>840</b> reveals the extent and health of photosynthesis taking place in plant life below. A spatially-corresponding orthorectified three-dimensional laser ranging image <b>850</b> provides positional and three-dimensional data for the surveyed terrain. A spatially-corresponding orthorectified magnetic data survey slice <b>860</b> is useful for identifying ferrous objects and geo-magnetic data in the surveyed terrain. Most or all of these orthorectified image slices can be three-dimensionally rotated and comparatively filtered and analyzed to identify anomalies.</p>
<p id="p-0071" num="0070"><figref idref="DRAWINGS">FIG. 11</figref> illustrates an embodiment of a computer-automated process to digitally compare spatially correlated pixel data in the first and second images to identify anomalies and to digitally filter the anomalies to distinguish targeted anomalies from untargeted anomalies. This process works with a set of overlapping data slices comprising a combination of visible, infrared, and optionally also magnetometer and LIDAR elevation mapping data taken of a targeted terrestrial surface area to identify targeted hidden or invisible anomalies on or below the targeted terrestrial surface area. It is assumed that the infrared and visible images have already been orthorectified to each other or some other reference image. In step <b>1110</b>, the pixel resolution of either the visible image slice or the infrared image slice is adjusted through pixel averaging until there is a one-to-one correspondence between pixels in the orthorectified image and infrared image slices. Non-overlapping remnants of the visible and infrared image slices are discarded. Alternatively, step <b>1110</b> is done as a part of the orthorectification process.</p>
<p id="p-0072" num="0071">In step <b>1120</b>, each pixel of the orthorectified visible image slice is converted to a single grayscale or intensity value. The visible image slice is converted to a grayscale resolution (e.g., 6 bits or 64 shades of gray) that matches the grayscale resolution of the infrared image slice. The variables used for normalization and grayscale conversion of the different components of the color model represented by each color image pixel are determined using an iterative process or artificial intelligence techniques to create an optimally close match between the grayscaled visible image slice and the infrared slice. Once the variables for grayscale conversion are determined, the same variables are used to convert each color pixel of the visible image frame to grayscale.</p>
<p id="p-0073" num="0072">In one embodiment, an optimally close match between two images is one in which the greatest number of corresponding pixels have identical intensity values. In another embodiment, an optimally close match between two images is one in which the greatest number of corresponding pixels have nearly-equal intensity values. In yet another embodiment, identical pixel values are given one weight and near-equal intensity values are given weights of a lesser or greater value, and an optimally close match between two images is one which has the greatest or smallest sum of the weights.</p>
<p id="p-0074" num="0073">Because the images are digitally represented, each image may be represented as an array. In step <b>1130</b>, visible and infrared difference arrays or difference images are generated in which corresponding pixels having identical intensity values are given a value of zero, and where corresponding pixels having non-identical intensity values retain their original values. Alternatively, visible and infrared difference arrays or difference images are generated where intensity differences of corresponding pixels less than a given threshold are given a value of zero, and the remaining elements retain their original values. Non-zero values in the difference arrays identify potential anomalies. In yet another alternative, a single difference array is generated in which each element has a value equal to the absolute value of the difference between the intensity values of the corresponding elements of the visible and infrared image arrays.</p>
<p id="p-0075" num="0074">Advantageously, the present invention includes a variety of automated techniques for filtering the difference arrays or images to filter the anomalies. In step <b>1140</b>, standard pattern recognition techniques are used to filter anomalies in the difference images. For example, pattern recognition techniques may be used to identify straight-line anomalies corresponding to a buried pipeline. In step <b>1150</b>, the difference images are compared with magnetometer survey data to separate anomalies that have also been picked up by the magnetometer survey from anomalies that have not been picked up by the magnetometer survey. In step <b>1160</b>, the difference images are compared with LIDAR elevation profiles to intelligently separate surface anomalies from subsurface anomalies. For example, this comparison may be used to separate anomalies found on flat or gently sloping surface portions from anomalies generated by buildings, cars, and other structures on the surface.</p>
<p id="p-0076" num="0075">Each of steps <b>1140</b>, <b>1150</b>, and <b>1160</b> are optional anomaly filtering techniques. Use of any one or any combination of those steps is considered to fall within the scope of the present invention. In step <b>1170</b>, the entire process of steps <b>1130</b>-<b>1160</b> may optionally be repeated iteratively using different difference detection thresholds. In step <b>1180</b>, the filtered anomalies may be filtered yet again by comparing different sets of frames to eliminate anomalies that do not persist in overlapping portions of preceding or subsequent frames. This eliminates noisy anomalies. In step <b>1190</b>, the remaining filtered anomalies are highlighted on one or more of the original image slices. Furthermore, the anomalies may be highlighted on a series of frames to create a video or animation that depicts the anomalies. Preferably, steps <b>1110</b>-<b>1190</b> are computer-automated, meaning that after initialization (which may involve some human input), the steps are automatically performed without intensive ongoing human feedback to assist the image manipulation and pattern recognition processes. This is in contrast to using an image manipulation program like Adobe Photoshop® to manually and inconsistently manipulate images on a frame by frame basis without the assistance of macros or other automatic image rendering batch processes.</p>
<p id="p-0077" num="0076">Although the foregoing specific details describe various embodiments of the invention, persons reasonably skilled in the art will recognize that various changes may be made in the details of the apparatus of this invention without departing from the spirit and scope of the invention as defined in the appended claims. Therefore, it should be understood that, unless otherwise specified, this invention is not to be limited to the specific details shown and described herein.</p>
<?DETDESC description="Detailed Description" end="tail"?>
</description>
<us-claim-statement>What is claimed is:</us-claim-statement>
<claims id="claims">
<claim id="CLM-00001" num="00001">
<claim-text>1. A method of remotely sensing and identifying subsurface characteristics of a terrestrial region, the method comprising:
<claim-text>moving an aircraft equipped with a multispectral data acquisition system over a terrestrial region, said multispectral data acquisition system including:
<claim-text>a computer to receive, store, and process data;</claim-text>
<claim-text>a first imaging sensor communicatively coupled with said computer and operable to capture images in a visible band of the electromagnetic spectrum;</claim-text>
<claim-text>a second imaging sensor communicatively coupled with said computer and operable to capture images in an infrared band of the electromagnetic spectrum, and being mounted relative to the first imaging sensor so that one of the two simultaneously-captured images from the first and second imaging sensors will substantially overlap the other;</claim-text>
<claim-text>a magnetometer array to detect disturbances in the earth's magnetic field caused by objects on or near the terrestrial surface, the magnetometer array being in communication with said computer;</claim-text>
<claim-text>a laser emitter and detector system in communication with said computer to facilitate computation of the distance between the laser emitter and a point on the terrestrial region below, thereby to enable generation of a three-dimensional elevation model of the terrestrial region;</claim-text>
<claim-text>a global positioning satellite receiver tuned to receive positional data derived from global positioning satellites, said global positioning satellite receiver being communicatively coupled with said computer;</claim-text>
</claim-text>
<claim-text>capturing a first image with said first imaging sensor;</claim-text>
<claim-text>simultaneously capturing a second image with said second imaging sensor;</claim-text>
<claim-text>simultaneously capturing magnetic field data with said magnetometer array;</claim-text>
<claim-text>simultaneously surveying the terrestrial region with the laser emitter and detector system;</claim-text>
<claim-text>recording said first image, said second image, said magnetic field data, data from said laser emitter and detector system, and correlative spatial reference information received from the global positioning satellite receiver;</claim-text>
<claim-text>generating a magnetometer survey map with said magnetic field data;</claim-text>
<claim-text>spatially correlating the first and second images and magnetometer survey map with each other to enable spatially correlated comparisons between the first image, second image, and magnetometer survey map;</claim-text>
<claim-text>digitally comparing the spatially correlated first and second images to identify anomalies; and</claim-text>
<claim-text>digitally filtering said anomalies that spatially correspond with recorded distortions in the magnetic field data.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00002" num="00002">
<claim-text>2. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising the step of converting the first image to a grayscale image.</claim-text>
</claim>
<claim id="CLM-00003" num="00003">
<claim-text>3. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising the steps of:
<claim-text>generating a digital elevation model of the terrestrial region using the data recorded from said laser detector and emitter system;</claim-text>
<claim-text>spatially correlating the first and second images and digital elevation model with each other to enable spatially correlated comparisons between the first image, second image, and digital elevation model;</claim-text>
<claim-text>digitally comparing the spatially correlated first and second images to identify anomalies; and</claim-text>
<claim-text>with the aid of said digital elevation model, digitally filtering said anomalies that spatially correspond with flat or gently sloping areas of the terrestrial region.</claim-text>
</claim-text>
</claim>
</claims>
</us-patent-grant>
