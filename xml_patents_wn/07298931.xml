<us-patent-grant lang="EN" dtd-version="v4.2 2006-08-23" file="US07298931-20071120.XML" status="PRODUCTION" id="us-patent-grant" country="US" date-produced="20071106" date-publ="20071120">
<us-bibliographic-data-grant>
<publication-reference>
<document-id>
<country>US</country>
<doc-number>07298931</doc-number>
<kind>B2</kind>
<date>20071120</date>
</document-id>
</publication-reference>
<application-reference appl-type="utility">
<document-id>
<country>US</country>
<doc-number>10650964</doc-number>
<date>20030829</date>
</document-id>
</application-reference>
<us-application-series-code>10</us-application-series-code>
<priority-claims>
<priority-claim sequence="01" kind="national">
<country>KR</country>
<doc-number>10-2002-0062518</doc-number>
<date>20021014</date>
</priority-claim>
<priority-claim sequence="02" kind="national">
<country>KR</country>
<doc-number>10-2003-0023715</doc-number>
<date>20030415</date>
</priority-claim>
</priority-claims>
<us-term-of-grant>
<us-term-extension>789</us-term-extension>
</us-term-of-grant>
<classifications-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>G</section>
<class>06</class>
<subclass>K</subclass>
<main-group>9</main-group>
<subgroup>60</subgroup>
<symbol-position>F</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20071120</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
</classifications-ipcr>
<classification-national>
<country>US</country>
<main-classification>382305</main-classification>
<further-classification>382115</further-classification>
<further-classification>382118</further-classification>
<further-classification>382190</further-classification>
<further-classification>340  552</further-classification>
<further-classification>340  582</further-classification>
</classification-national>
<invention-title id="d0e89">Image retrieval method and apparatus using iterative matching</invention-title>
<references-cited>
<citation>
<patcit num="00001">
<document-id>
<country>US</country>
<doc-number>5586197</doc-number>
<kind>A</kind>
<name>Tsujimura et al.</name>
<date>19961200</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>382162</main-classification></classification-national>
</citation>
<citation>
<patcit num="00002">
<document-id>
<country>US</country>
<doc-number>6002794</doc-number>
<kind>A</kind>
<name>Bonneau et al.</name>
<date>19991200</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>382166</main-classification></classification-national>
</citation>
<citation>
<patcit num="00003">
<document-id>
<country>US</country>
<doc-number>6246804</doc-number>
<kind>B1</kind>
<name>Sato et al.</name>
<date>20010600</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>382284</main-classification></classification-national>
</citation>
<citation>
<patcit num="00004">
<document-id>
<country>US</country>
<doc-number>6351749</doc-number>
<kind>B1</kind>
<name>Brown et al.</name>
<date>20020200</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>707 10</main-classification></classification-national>
</citation>
<citation>
<patcit num="00005">
<document-id>
<country>US</country>
<doc-number>6408293</doc-number>
<kind>B1</kind>
<name>Aggarwal et al.</name>
<date>20020600</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>707  3</main-classification></classification-national>
</citation>
<citation>
<patcit num="00006">
<document-id>
<country>US</country>
<doc-number>6421717</doc-number>
<kind>B1</kind>
<name>Kloba et al.</name>
<date>20020700</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>709219</main-classification></classification-national>
</citation>
<citation>
<patcit num="00007">
<document-id>
<country>US</country>
<doc-number>6430306</doc-number>
<kind>B2</kind>
<name>Slocum et al.</name>
<date>20020800</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>382118</main-classification></classification-national>
</citation>
<citation>
<patcit num="00008">
<document-id>
<country>US</country>
<doc-number>6498863</doc-number>
<kind>B1</kind>
<name>Gaidoukevitch et al.</name>
<date>20021200</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>382173</main-classification></classification-national>
</citation>
<citation>
<patcit num="00009">
<document-id>
<country>US</country>
<doc-number>6957221</doc-number>
<kind>B1</kind>
<name>Hart et al.</name>
<date>20051000</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>707100</main-classification></classification-national>
</citation>
<citation>
<patcit num="00010">
<document-id>
<country>US</country>
<doc-number>6985147</doc-number>
<kind>B2</kind>
<name>Asakawa et al.</name>
<date>20060100</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>345467</main-classification></classification-national>
</citation>
<citation>
<patcit num="00011">
<document-id>
<country>US</country>
<doc-number>7181370</doc-number>
<kind>B2</kind>
<name>Furem et al.</name>
<date>20070200</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>702188</main-classification></classification-national>
</citation>
<citation>
<nplcit num="00012">
<othercit>Wey-Shiun Hwang et al., “Hierarchical Discriminant Regression”, <i>IEEE Transactions on Pattern Analysis and Machine Intelligence</i>, 2000, p. 1277-1293, vol. 22, No. 11, IEEE, New York, New York.</othercit>
</nplcit>
<category>cited by other</category>
</citation>
<citation>
<nplcit num="00013">
<othercit>Christian Breiteneder et al., “<i>Automatic Query Generation for Content-Based Image Retrieval</i>”, IEEE Conference on Multimedia and Expo, 2000, p. 705-708, vol. 2, IEEE, New York, New York.</othercit>
</nplcit>
<category>cited by other</category>
</citation>
<citation>
<nplcit num="00014">
<othercit>Toshio Kamei et al., “<i>Report of the Core Expresiments on Fourier Spectral PCLDA Based Face Descriptor</i>”, ISO/IEC JTC1/SC29/WGII MPEG 2002/M8559, 2002, p. 1-11.</othercit>
</nplcit>
<category>cited by other</category>
</citation>
<citation>
<nplcit num="00015">
<othercit>B.S. Manjunath et al., “<i>Introduction to MPEG-7: Multimedia Content Description Interface</i>”, A Quantitative Evaluation of Visual Descriptors, 2002, p. 183-184, Section 12.3, John Wiley &amp; Sons.</othercit>
</nplcit>
<category>cited by other</category>
</citation>
</references-cited>
<number-of-claims>23</number-of-claims>
<us-exemplary-claim>1</us-exemplary-claim>
<us-field-of-classification-search>
<classification-national>
<country>US</country>
<main-classification>382209</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>382219</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>382305</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>382115</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>382118</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>382190</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>382307</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>707  3</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>707  6</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>707  7</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>340  552</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>340  882</main-classification>
</classification-national>
</us-field-of-classification-search>
<figures>
<number-of-drawing-sheets>6</number-of-drawing-sheets>
<number-of-figures>7</number-of-figures>
</figures>
<us-related-documents>
<related-publication>
<document-id>
<country>US</country>
<doc-number>20040073543</doc-number>
<kind>A1</kind>
<date>20040415</date>
</document-id>
</related-publication>
</us-related-documents>
<parties>
<applicants>
<applicant sequence="001" app-type="applicant-inventor" designation="us-only">
<addressbook>
<last-name>Kim</last-name>
<first-name>Hyun-woo</first-name>
<address>
<city>Kyungki-do</city>
<country>KR</country>
</address>
</addressbook>
<nationality>
<country>KR</country>
</nationality>
<residence>
<country>KR</country>
</residence>
</applicant>
<applicant sequence="002" app-type="applicant-inventor" designation="us-only">
<addressbook>
<last-name>Kim</last-name>
<first-name>Tae-kyun</first-name>
<address>
<city>Kyungki-do</city>
<country>KR</country>
</address>
</addressbook>
<nationality>
<country>KR</country>
</nationality>
<residence>
<country>KR</country>
</residence>
</applicant>
<applicant sequence="003" app-type="applicant-inventor" designation="us-only">
<addressbook>
<last-name>Hwang</last-name>
<first-name>Won-jun</first-name>
<address>
<city>Seoul</city>
<country>KR</country>
</address>
</addressbook>
<nationality>
<country>KR</country>
</nationality>
<residence>
<country>KR</country>
</residence>
</applicant>
<applicant sequence="004" app-type="applicant-inventor" designation="us-only">
<addressbook>
<last-name>Kee</last-name>
<first-name>Seok-cheol</first-name>
<address>
<city>Kyungki-do</city>
<country>KR</country>
</address>
</addressbook>
<nationality>
<country>KR</country>
</nationality>
<residence>
<country>KR</country>
</residence>
</applicant>
</applicants>
<agents>
<agent sequence="01" rep-type="attorney">
<addressbook>
<orgname>Buchanan Ingersoll &amp; Rooney PC</orgname>
<address>
<country>unknown</country>
</address>
</addressbook>
</agent>
</agents>
</parties>
<assignees>
<assignee>
<addressbook>
<orgname>Samsung Electronics Co., Ltd.</orgname>
<role>03</role>
<address>
<city>Suwon, Kyungki-do</city>
<country>KR</country>
</address>
</addressbook>
</assignee>
</assignees>
<examiners>
<primary-examiner>
<last-name>Kassa</last-name>
<first-name>Yosef</first-name>
<department>2624</department>
</primary-examiner>
</examiners>
</us-bibliographic-data-grant>
<abstract id="abstract">
<p id="p-0001" num="0000">An image retrieval method and apparatus using iterative matching are provided. The method comprises: (a) extracting K similar images in order of matching rank by retrieving N reference images stored in an image database through comparison between a query image selected by a user and the reference images; and (b) performing iterative matching for M similar images, which are ranked higher and selected from the K similar images, with a predetermined frequency, and rearranging the matching ranks of the M similar images. According to the method and apparatus, among similar images retrieved by using a query image selected by a user, M similar images that are ranked higher are selected, and by performing iterative matching for the M similar images using the high-ranked similar image, M similar images are rearranged in order of similarity and output. Accordingly, accuracy of the matching can improve greatly and a load to the retrieval engine due to iterative retrieval can be minimized.</p>
</abstract>
<drawings id="DRAWINGS">
<figure id="Fig-EMI-D00000" num="00000">
<img id="EMI-D00000" he="196.17mm" wi="119.89mm" file="US07298931-20071120-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00001" num="00001">
<img id="EMI-D00001" he="181.69mm" wi="165.95mm" file="US07298931-20071120-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00002" num="00002">
<img id="EMI-D00002" he="215.14mm" wi="131.74mm" file="US07298931-20071120-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00003" num="00003">
<img id="EMI-D00003" he="158.33mm" wi="175.26mm" file="US07298931-20071120-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00004" num="00004">
<img id="EMI-D00004" he="144.19mm" wi="183.64mm" file="US07298931-20071120-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00005" num="00005">
<img id="EMI-D00005" he="122.00mm" wi="139.53mm" file="US07298931-20071120-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00006" num="00006">
<img id="EMI-D00006" he="223.86mm" wi="154.94mm" file="US07298931-20071120-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
</drawings>
<description id="description">
<?BRFSUM description="Brief Summary" end="lead"?>
<p id="p-0002" num="0001">This application claims priority from Korean Patent Application Nos. 2002-62518, filed on Oct. 14, 2002 and 2003-23715, filed on Apr. 15, 2003, in the Korean Intellectual Property Office, the contents of which are incorporated herein by reference in their entirety.</p>
<heading id="h-0001" level="1">BACKGROUND OF THE INVENTION</heading>
<p id="p-0003" num="0002">1. Field of the Invention</p>
<p id="p-0004" num="0003">The present invention relates to a multimedia retrieval system, and more particularly, to an image retrieval method and apparatus using iterative matching in order to improve accuracy of retrieval without overload of retrieval time in a content-based image retrieval system.</p>
<p id="p-0005" num="0004">2. Description of the Related Art</p>
<p id="p-0006" num="0005">Recently in the content-based multimedia retrieval trend, the focus has been put on providing a user-friendly interface with overcoming the limitation of keyword retrieval in text-based retrieval. Particularly, the rapid growth of the Internet, personalization of multimedia equipment, and the introduction of digital libraries have been stimulating demands for content-based multimedia retrieval. Meanwhile, the content-based image retrieval includes analyzing image characteristic information such as colors, textures, shapes and faces and arranging images which are visually the more similar to a desired image, in order of visual similarity. Content-based image retrieval comprises a step for extracting characteristics and a step for image matching. In the step for extracting characteristics, predetermined vectors are extracted to describe the characteristics of an image and all images are expressed by respective characteristic vectors and stored in a predetermined database. In the step for image matching, if a query image is given, similarities between the query image and the images in the database are calculated in a characteristic vector domain, and the images in the database are arranged in order of the calculated similarity to the query image.</p>
<p id="p-0007" num="0006">Many methods related to characteristics extracting and image matching have been under development. For the characteristics extracting among the methods, a variety of characteristic descriptors are being developed particularly in an MPEG-7 category. Meanwhile, for the image matching, methods for reducing a search time and fusion of a plurality of characteristics are mainly being studied. For example, among related research articles, there are “Hierarchical Discriminant Regression,” by W. Hwang and J. Weng, IEEE Trans. Pattern Analysis and Machine Intelligence, vol. 22, no. 11, November 2000, pp. 1277-1293, and “Automatic Query Generation for Content-Based Image Retriever,” by C. Breitneder and H. Eidenberger, IEEE Conference on Multimedia and Expo, Jul. 30-Aug. 2, 2000, vol. 2, pp. 705-708.</p>
<p id="p-0008" num="0007">However, though many methods have been tried to reduce a search time or to efficiently fuse a variety of characteristics in the image matching step in the prior art as in the above articles, efforts to improve accuracy of the matching based on knowledge have not been made yet. Accordingly, when an image is retrieved by the above methods, since major characteristic information items of images being determined to be similar are all different, the result is not satisfying sometimes. In addition, even though images have identical contents, changes of illumination or poses can cause different characteristic information items on colors or textures to be stored such that an image may not be retrieved accurately.</p>
<p id="p-0009" num="0008">To solve these problems, there is a method by which a user feeds information on images similar to a desired image back to a retrieval apparatus by using the result of a first retrieval, so that the retrieval apparatus automatically calculates what is a more important characteristic information item and as a result carries out retrieval again by increasing the weighted value of the important characteristic information item. However, if the retrieval is carried out again, all images stored in an image database used in the first retrieval should be retrieved such that calculation of similarities between the query image and stored images becomes more complicated and if the frequency of iteration increases in order to improve retrieval performance, the burden to the retrieval engine also increases.</p>
<heading id="h-0002" level="1">SUMMARY OF THE INVENTION</heading>
<p id="p-0010" num="0009">The present invention provides an image retrieval method using iterative matching, by which, among similar images retrieved by using a query image selected by a user, M similar images that are ranked higher are selected, and by performing iterative matching for the M similar images, M similar images are rearranged in order of similarity and output.</p>
<p id="p-0011" num="0010">The present invention also provides an image retrieval apparatus appropriate to the method using the iterative matching.</p>
<p id="p-0012" num="0011">According to an aspect of the present invention, there is provided an image retrieval method comprising: (a) extracting K similar images in order of matching rank by retrieving N reference images stored in an image database through comparison between a query image selected by a user and the reference images; and (b) performing iterative matching for M similar images, which are ranked higher and selected from the K similar images, with a predetermined frequency, and rearranging the matching ranks of the M similar images.</p>
<p id="p-0013" num="0012">In the method, the step (b) comprises: (b1) storing M similar images whose matching ranks are higher among the K similar images extracted in the step (a); (b2) selecting a similar image ranked the highest among the M similar images, as a new query image, and by performing iterative matching for (M−1) similar images except the image ranked the highest, determining a similar image having the second highest rank next to the highest-ranked image; and (b3) after changing the new query image and a retrieval range in the step (b2), performing iterative matching with the predetermined frequency of iteration and rearranging the matching ranks of the M similar images.</p>
<p id="p-0014" num="0013">According to another aspect of the present invention, there is provided an image retrieval apparatus comprising: a 1st retrieval unit which extracts K similar images in order of matching rank by retrieving N reference images stored in an image database through comparison between a query image selected by a user and the reference images; and a 2nd retrieval unit which performs iterative matching for M similar images, which are ranked higher and selected from the K similar images, with a predetermined frequency, and rearranges the matching ranks of the M similar images.</p>
<p id="p-0015" num="0014">In the apparatus, the 2nd retrieval unit comprises: a 1st storage unit which stores M similar images whose matching ranks are higher among K similar images provided by the 1st retrieval unit; and an iterative matching unit which selects a similar image ranked the highest among the M similar images stored in the 1st storage unit, as a new query image, performs iterative matching for the similar images, except the image ranked the highest, rearranges the ranks of the M similar images, and stores the rearranged M similar images in the 1st storage unit.</p>
<p id="p-0016" num="0015">In the apparatus, if M&lt;K, a 2nd storage unit for storing (K−M) similar images except the M similar images stored in the 1st storage unit is further included.</p>
<p id="p-0017" num="0016">In the apparatus, the 2nd retrieval unit further comprises: a retrieval result combining unit which adds the (K−M) similar images stored in the 2nd storage unit, to the M similar images which are rearranged according to matching ranks by the iterative matching unit and stored in the 1st storage unit, and outputs the result as a retrieval result.</p>
<p id="p-0018" num="0017">According to still another aspect of the present invention, there is provided a computer readable medium having embodied thereon a computer program for the method.</p>
<?BRFSUM description="Brief Summary" end="tail"?>
<?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?>
<description-of-drawings>
<heading id="h-0003" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading>
<p id="p-0019" num="0018">The above objects and advantages of the present invention will become more apparent by describing in detail preferred embodiments thereof with reference to the attached drawings in which:</p>
<p id="p-0020" num="0019"><figref idref="DRAWINGS">FIG. 1</figref> is a diagram to explain the concept of an image retrieval method according to the present invention;</p>
<p id="p-0021" num="0020"><figref idref="DRAWINGS">FIG. 2</figref> is a flowchart to explain a preferred embodiment of an image retrieval method according to the present invention;</p>
<p id="p-0022" num="0021"><figref idref="DRAWINGS">FIG. 3</figref> is a detailed flowchart of step <b>260</b> shown in <figref idref="DRAWINGS">FIG. 2</figref>;</p>
<p id="p-0023" num="0022"><figref idref="DRAWINGS">FIG. 4</figref> is a block diagram of the structure of a preferred embodiment of an image retrieval apparatus according to the present invention;</p>
<p id="p-0024" num="0023"><figref idref="DRAWINGS">FIG. 5</figref> is an example of a face retrieval system to which the image retrieval method of the present invention is applied to evaluate the performance of the method; and</p>
<p id="p-0025" num="0024"><figref idref="DRAWINGS">FIGS. 6</figref><i>a </i>and <b>6</b><i>b </i>are graphs expressing the accuracies of retrievals by average normalized modified retrieval rates (ANMRR) when experiments were carried out in the face retrieval system shown in <figref idref="DRAWINGS">FIG. 5</figref>.</p>
</description-of-drawings>
<?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?>
<?DETDESC description="Detailed Description" end="lead"?>
<heading id="h-0004" level="1">DESCRIPTION OF THE PREFERRED EMBODIMENTS</heading>
<p id="p-0026" num="0025">Referring to <figref idref="DRAWINGS">FIG. 1</figref>, the concept of an image retrieval method according to the present invention comprises a server <b>110</b> having an image database and a client <b>120</b> which has a buffer <b>123</b> or <b>126</b> with a predetermined size (M) and requests retrieval of a selected query image. Though the concept comprises the server <b>110</b> and the client <b>120</b> for convenience of explanation, the concept may also be implemented on a computer and the buffer <b>123</b> or <b>126</b> may also be installed in the server <b>110</b>.</p>
<p id="p-0027" num="0026">Referring to <figref idref="DRAWINGS">FIG. 1</figref>, the server <b>110</b> has an image database storing characteristic vectors extracted from N reference images. If a query image is given by the request of the client <b>120</b>, the server <b>110</b> retrieves reference images corresponding to characteristic vectors, by using the characteristic vectors of the query image, and transmits the reference images to the client <b>120</b>. For convenience of explanation, a characteristic vector that is a mathematical expression of an image for image retrieval will now be referred to as an image.</p>
<p id="p-0028" num="0027">The client <b>120</b> selects a query image q in a retrieval engine (not shown) to be installed in the server <b>110</b> and sets predetermined retrieval conditions such as the number (K) of retrieval images. Then, if a request of retrieval is made, the retrieval engine retrieves the image database of the server <b>110</b>, extracts K similar images <b>111</b> in order of matching score and provides the images <b>111</b> to the client <b>120</b>. Among the K extracted similar images, the M similar images <b>121</b> ranked higher corresponding to the size (M) of the buffer <b>123</b> are stored in the buffer so that each of an image area <b>124</b> corresponds to an area in a matching score area <b>125</b> in the buffer <b>123</b>, and the remaining (K−M) similar images <b>122</b> are stored in a separate storage medium (not shown).</p>
<p id="p-0029" num="0028">Then, a retrieval engine (not shown) to be installed in the client <b>120</b> selects a similar image q′, which is ranked the highest among the M similar images ranked higher stored in the buffer <b>123</b>, as a new query image and performs iterative matching for the M similar images stored in the buffer <b>123</b>. By doing so, the retrieval engine extracts the (M−1) similar images, except the highest-ranked similar image q′, in order of matching score. The highest-ranked similar image q′ and the extracted (M−1) similar images are stored again in the same buffer <b>126</b> so that each of an image area <b>127</b> corresponds to an area in a matching score area <b>128</b>. Then, the retrieval engine selects a similar image q″, which is ranked the highest among the (M−1) similar images ranked higher stored in the buffer <b>126</b>, as a new query image and performs iterative matching for the (M−2) similar images stored in the buffer <b>126</b>. By doing so, the retrieval engine extracts the (M−2) similar images, except the highest-ranked similar image q″, in order of matching score.</p>
<p id="p-0030" num="0029">According to this method, by using a similar image ranked the highest among (M−n+1) similar images, (M−n) similar images are rearranged. Here, n denotes the frequency of iterative matching.</p>
<p id="p-0031" num="0030">As a final retrieval result <b>130</b>, M similar images <b>131</b> obtained by rearranging M similar images <b>121</b>, which are originally retrieved, in order of matching score are output. At this time, the (K−M) similar images <b>122</b>, which are separately stored because the images exceed the size of the buffer <b>123</b>, <b>126</b>, may also be output after being added to the M similar images <b>131</b> rearranged in order.</p>
<p id="p-0032" num="0031">Here, though an embodiment in which the server <b>110</b> and the client <b>120</b> have respective retrieval engines is explained, two retrieval engines may be implemented in a unified retrieval engine installed in the server <b>110</b> or in the client <b>120</b>.</p>
<p id="p-0033" num="0032">When an image is coded as a characteristic vector for image retrieval, the mathematical expression of the image may include environmental changes, such as changes of illumination or poses, in addition to the characteristic itself of the image. Accordingly, images retrieved by a given query image may reflect coding errors of the query image by the environmental changes, and some images may be retrieved by environmental changes similar to the query image. In general, since most of the similar images ranked highest are usually retrieved by the characteristics of images, it can be known that the similar images ranked highest have the same characteristics as the query image selected by a user and different environmental changes. In the present invention, this similar image ranked highest is set as a new query image.</p>
<p id="p-0034" num="0033"><figref idref="DRAWINGS">FIG. 2</figref> is a flowchart to explain a preferred embodiment of an image retrieval method according to the present invention and the method comprises the first retrieval step (steps <b>210</b> through <b>240</b>) and the second retrieval step (steps <b>250</b> through <b>270</b>).</p>
<p id="p-0035" num="0034">Referring to <figref idref="DRAWINGS">FIG. 2</figref>, in the step <b>210</b>, a query image (q) selected by a user is input and in the step <b>220</b> retrieval conditions which are set by the user are input. Here, the retrieval conditions may include a retrieval field, a maximum number (K) of similar images to be displayed as a result of retrieval, the number of similar images to be used in iterative matching, and the frequency (n) of iteration.</p>
<p id="p-0036" num="0035">In the step <b>230</b>, comparison retrieval for the query image (q) input in the step <b>210</b> according to the retrieval conditions set in the step <b>220</b> is performed with reference images {l<sub>i</sub>| i=1, . . . , N} of an image database. For this, first, the characteristic information of the query image (q) is extracted, and the extracted characteristic information is compared with characteristic information of reference images indexed in the image database. By doing so, retrieval is performed. In the step <b>240</b>, according to the retrieval result of the step <b>230</b>, K similar images are extracted in order of matching score.</p>
<p id="p-0037" num="0036">In the step <b>250</b>, among the K similar images extracted in the step <b>240</b>, M similar images {q<sub>i</sub>| i=1, . . . , M} ranked higher are extracted and stored in a buffer (not shown). At this time, M matching scores {s(i)| i=1, . . . , M} are also stored corresponding to M similar images. The matching score may be calculated, for example, by using D(q, q<sub>i</sub>). D(q, q<sub>i</sub>) indicates the absolute value of a characteristic information distance between the query image (q) and a similar image (q<sub>i</sub>). The less the absolute value of a characteristic information distance is, the higher the image is ranked as an image similar to the query image (q). Meanwhile, the step <b>250</b> may further comprise a step for storing (K−M) similar images, obtained by excluding the M similar images {q<sub>i</sub>| i=1, . . . , M} ranked higher among the K similar images, in a separate storage medium.</p>
<p id="p-0038" num="0037">In the step <b>260</b>, iterative matching is performed for the M similar images selected in the step <b>250</b>, with a predetermined frequency of iteration, and then rearranged M similar images are output.</p>
<p id="p-0039" num="0038">For this, first, an image (q<sub>1</sub>) ranked the highest among the M similar images is selected as a new query image (q′) and retrieval is again performed for the (M−1) similar images, except the image (q<sub>1</sub>) ranked the highest, in the same manner as in the step <b>230</b>. Thus, retrieval is again performed not for the entire N reference images stored in the image database, but for the M similar images stored in the buffer, which greatly reduces the complexity of similarity calculation. As a result of the retrieval, rearranged similar images {q<sub>i</sub>′| i=1, . . . , M} and matching scores {s′(i)| i=1, . . . , M} corresponding to respective similar images are obtained. In order to reflect matching by the new query image (q′), a matching score (s′(i)) is updated by the following equation 1:
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>s</i>′(<i>i</i>)=<i>s</i>(<i>i</i>)+<i>w</i><sub>1</sub><i>·D</i>(<i>q′,q</i><sub>i</sub>′)  (1)<?in-line-formulae description="In-line Formulae" end="tail"?>
</p>
<p id="p-0040" num="0039">Here, w<sub>1 </sub>is a weighted value constant. The reason why a matching score D(q′,q<sub>i</sub>′) obtained by iterative retrieval is weighted by using the weighted value constant (w<sub>1</sub>) is to reduce the impact of an incorrectly retrieved image in the next retrieval, if a similar image ranked the highest among the similar images retrieved by an arbitrary frequency of iterative retrieval is incorrectly retrieved.</p>
<p id="p-0041" num="0040">If the iterative retrieval is performed n times, as a result of the retrieval, rearranged similar images {q<sub>i</sub><sup>(n-1)</sup>| i=1, . . . , M} and matching scores {s<sup>(n-1)</sup>(i)| i=1, . . . , M} corresponding to respective similar images are obtained and stored again in the buffer. At this time, the matching score (s<sup>(n-1)</sup>(i)) can be expressed by the following equation 2:
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>s</i><sup>(n-1)</sup>(<i>i</i>)=<i>s</i><sup>(n-2)</sup>(<i>i</i>)+<i>w</i><sub>n-1</sub><i>·D</i>(<i>q</i><sup>(n-1)</sup><i>,q</i><sub>i</sub><sup>(n-1)</sup>)  (2)<?in-line-formulae description="In-line Formulae" end="tail"?>
</p>
<p id="p-0042" num="0041">Here, w is set to between 0.7 and 0.9 and desirably to 0.8. In the equation 2, it is preferable that weighted value parameter (w<sub>n-1</sub>) is (w<sub>1</sub>)<sup>n-1 </sup>and w<sub>1 </sub>is set to w. According to this, the weighted value parameter is decreased exponentially in proportion to the frequency of iteration.</p>
<p id="p-0043" num="0042">According to the iterative retrieval described above, M similar images rearranged in order and stored in the buffer are output as the result of the retrieval.</p>
<p id="p-0044" num="0043">In the step <b>270</b>, if the size of the buffer is smaller than the number of retrieval images requested by the user, that is, if M&lt;K, the (K−M) similar images which exceed the buffer size and are separately stored in the step <b>250</b> are added to the end of the rearranged M similar images in the step <b>260</b> and output.</p>
<p id="p-0045" num="0044">Meanwhile, if a similar framework is used, the equation 2 may be modified as the following equation 3:
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>s</i><sup>(n-1)</sup>(<i>i</i>)=<i>D</i>(<i>q</i><sub>i</sub><i>,q</i>)+. . . +<i>w</i><sub>n-1</sub><i>·D</i>(<i>q</i><sub>i</sub><i>,q</i><sup>(n-1)</sup>)  (3)<?in-line-formulae description="In-line Formulae" end="tail"?>
</p>
<p id="p-0046" num="0045">In the equation 3, in the step <b>240</b>, the matching ranks of a plurality of images ranked higher among the K similar images, which are extracted in order of matching score as the result of retrieval in the step <b>230</b>, are fixed and then iterative matching can be performed.</p>
<p id="p-0047" num="0046"><figref idref="DRAWINGS">FIG. 3</figref> is a detailed flowchart of the step <b>260</b> shown in <figref idref="DRAWINGS">FIG. 2</figref>. In step <b>310</b>, the frequency (n) of iteration is set to 1. In step <b>320</b>, a similar image ranked the highest among (M−n+1) similar images is set to a new query image and (M−n) similar images are rearranged. That is, when the frequency (n) of iteration is 1, by using a similar image ranked the highest among M similar images, (M−1) similar images can be rearranged.</p>
<p id="p-0048" num="0047">In step <b>330</b>, it is determined whether or not the frequency (n) of iteration is the same as the set value (t), and if the frequency (n) of iteration is the same as the set value (t), the step <b>340</b> is performed to output rearranged M similar images. Here, the frequency (n) of iteration may be set by the user when the retrieval conditions are set in the step <b>220</b>. Also, the frequency (n) of iteration may be determined in the retrieval engine according to the number of images whose ranks change compared to the immediately previous ranks, or according to whether or not ranks change after i-th matching ranks. That is, if the number of images whose ranks change is less than a predetermined number P, or if the ranks do not change after i-th matching ranks, the iterative matching is finished.</p>
<p id="p-0049" num="0048">Meanwhile, if the determination result of the step <b>330</b> indicates that the frequency (n) of iteration is less than the set value (t), the step <b>350</b> is performed to increase the frequency (n) of iteration, and the step <b>320</b> is performed such that with changing a new query image and a retrieval range, iterative matching is again performed.</p>
<p id="p-0050" num="0049"><figref idref="DRAWINGS">FIG. 4</figref> is a block diagram of the structure of a preferred embodiment of an image retrieval apparatus according to the present invention and the apparatus comprises a user interface unit <b>410</b>, a 1st retrieval unit <b>420</b>, and a 2nd retrieval unit <b>420</b>. In addition, the user interface unit <b>410</b> comprises a query image input unit <b>411</b>, a retrieval condition setting unit <b>413</b>, and a retrieval result window <b>415</b>. The 1st retrieval unit <b>420</b> comprises an image database <b>421</b> and a similar image extracting unit <b>423</b>, and the 2nd retrieval unit <b>430</b> comprises a 1st storage unit <b>431</b>, a 2nd storage unit <b>433</b>, an iterative matching unit <b>435</b>, and a retrieval result combining unit <b>437</b>.</p>
<p id="p-0051" num="0050">Referring to <figref idref="DRAWINGS">FIG. 4</figref>, in the user interface unit <b>410</b>, the query image input unit <b>411</b> inputs a query image (q) selected by the user and provides it to the similar image extracting unit <b>423</b>. Through the retrieval condition setting unit <b>413</b>, the user sets retrieval conditions, for example, a retrieval field, the maximum number (K) of similar images to be displayed as the result of retrieval, the number (M) of similar images to be used in iterative matching, and the number (n) of frequency. The retrieval condition setting unit <b>413</b> sends the retrieval field, the maximum number (K) of similar images to be displayed as the result of retrieval, and the number (M) of similar images to be used in iterative matching to the similar image extracting unit <b>423</b>, and the number (n) of frequency to the iterative matching unit <b>435</b>. The retrieval result window <b>415</b> displays similar images finally retrieved according to the present invention.</p>
<p id="p-0052" num="0051">In the 1st retrieval unit <b>420</b>, the image database (DB) <b>421</b> stores N reference images. The similar image extracting unit <b>423</b> performs comparison retrieval between the query image (q) input through the query image input unit <b>411</b> and the reference images of the image DB <b>421</b> according to the retrieval conditions set in the retrieval condition setting unit <b>413</b>, and extracts K similar images ranked higher in order of matching score as a result of the retrieval.</p>
<p id="p-0053" num="0052">In the 2nd retrieval unit <b>430</b>, the 1st storage unit <b>431</b> stores M similar images ranked higher among the K similar images extracted by the similar image extracting unit <b>423</b>, and the 2nd storage unit <b>433</b> stores the remaining (K−M) similar images. The iterative matching unit <b>435</b> selects a similar image ranked the highest among the M similar images stored in the 1st storage unit <b>431</b>, as a new query image, and performs iterative matching for the similar images, except the image ranked the highest. By doing so, the iterative matching unit <b>435</b> rearranges the ranks of the M similar images and stores the rearranged similar images in the 1st storage unit <b>431</b>. The retrieval result combining unit <b>437</b> adds the (K−M) similar images stored in the 2nd storage unit <b>433</b>, to the M similar images which are rearranged according to matching ranks by the iterative matching unit <b>435</b> and stored in the 1st storage unit <b>431</b>, and provides the result to the user interface unit <b>410</b>.</p>
<p id="p-0054" num="0053">Next, through an example in which the present invention is applied to face image retrieval, evaluation of the performance of the image retrieval method according to the present invention will now be explained.</p>
<p id="p-0055" num="0054"><figref idref="DRAWINGS">FIG. 5</figref> is an example of a face retrieval system to which the image retrieval method of the present invention is applied, in which 16 similar face images are extracted in order of matching score from a predetermined image database, based on a predetermined face description algorithm, when a query face image is given to the database. For face retrieval, first a face image is expressed by a characteristic vector. To obtain a similarity between two face images, the distance or correlation between the characteristic vectors is calculated. Here, 240 bits are used per each face image as the dimension of a characteristic vector and as a similarity measure, a Euclidian distance between characteristic vectors is used. Meanwhile, characteristic vectors are extracted by using a method proposed in an article, “Report of Core Experiment on Fourier Spectral PCA based Face Description,” by Toshio Kamei, Yusuke Takahashi, and Akio Yamada, in NEC_MM_TR<sub>—</sub>2002<sub>—</sub>330, July 2002.</p>
<p id="p-0056" num="0055">Meanwhile, as a data set, an MPEG-7 face data set formed with 5 databases is used. The five databases are an extended version 1 MPEG-7 face database (E1), Altkom database (A2), an MPEG-7 test set (M3) in XM2VTS database, FERET database (F4), and an MPEG-7 test set (B5) in Banca database. The total number of images is 11,845, among which 3655 images are used only as training images for linear discriminant analysis (LDA) projection learning and 8,190 images are used only as test images for evaluating the performance. 4,190 images among the test images are used as query images, and 4,000 images obtained from F4 are used as obstacle images. Table 1 shows a detailed list of training images and test images and it is assumed that in order to evaluate the performance of an image retrieval algorithm according to the present invention, ground truth similar images for all query images are already known.</p>
<p id="p-0057" num="0056">
<tables id="TABLE-US-00001" num="00001">
<table frame="none" colsep="0" rowsep="0">
<tgroup align="left" colsep="0" rowsep="0" cols="5">
<colspec colname="offset" colwidth="70pt" align="left"/>
<colspec colname="1" colwidth="14pt" align="center"/>
<colspec colname="2" colwidth="56pt" align="center"/>
<colspec colname="3" colwidth="28pt" align="center"/>
<colspec colname="4" colwidth="49pt" align="center"/>
<thead>
<row>
<entry/>
<entry namest="offset" nameend="4" rowsep="1">TABLE 1</entry>
</row>
<row>
<entry/>
<entry namest="offset" nameend="4" align="center" rowsep="1"/>
</row>
<row>
<entry/>
<entry>DB</entry>
<entry>Person</entry>
<entry>Image</entry>
<entry>Total</entry>
</row>
<row>
<entry/>
<entry namest="offset" nameend="4" align="center" rowsep="1"/>
</row>
</thead>
<tbody valign="top">
<row>
<entry/>
</row>
</tbody>
</tgroup>
<tgroup align="left" colsep="0" rowsep="0" cols="5">
<colspec colname="1" colwidth="70pt" align="center"/>
<colspec colname="2" colwidth="14pt" align="center"/>
<colspec colname="3" colwidth="56pt" align="center"/>
<colspec colname="4" colwidth="28pt" align="center"/>
<colspec colname="5" colwidth="49pt" align="char" char="."/>
<tbody valign="top">
<row>
<entry>Train</entry>
<entry>A2</entry>
<entry> 40</entry>
<entry>15</entry>
<entry>  600</entry>
</row>
<row>
<entry>Image I</entry>
<entry>B5</entry>
<entry>—</entry>
<entry>—</entry>
<entry>—</entry>
</row>
<row>
<entry>50 vs 50</entry>
<entry>M1</entry>
<entry>317</entry>
<entry> 5</entry>
<entry>1,585</entry>
</row>
<row>
<entry/>
<entry>M3</entry>
<entry>147</entry>
<entry>10</entry>
<entry>1,470</entry>
</row>
<row>
<entry/>
<entry>F4</entry>
<entry>—</entry>
<entry>—</entry>
<entry>—</entry>
</row>
<row>
<entry>Total</entry>
<entry/>
<entry>504</entry>
<entry/>
<entry>3,655</entry>
</row>
<row>
<entry>Test</entry>
<entry>A2</entry>
<entry> 40</entry>
<entry>15</entry>
<entry>  600</entry>
</row>
<row>
<entry>Image I</entry>
<entry>B5</entry>
<entry> 52</entry>
<entry>10</entry>
<entry>  520</entry>
</row>
<row>
<entry>50 vs 50</entry>
<entry>M1</entry>
<entry>318</entry>
<entry> 5</entry>
<entry>1,590</entry>
</row>
<row>
<entry/>
<entry>M3</entry>
<entry>148</entry>
<entry>10</entry>
<entry>1,480</entry>
</row>
<row>
<entry/>
<entry>F4</entry>
<entry>—</entry>
<entry>—</entry>
<entry>4,000</entry>
</row>
<row>
<entry>Total</entry>
<entry/>
<entry>558</entry>
<entry/>
<entry>8,190</entry>
</row>
<row>
<entry namest="1" nameend="5" align="center" rowsep="1"/>
</row>
</tbody>
</tgroup>
</table>
</tables>
</p>
<p id="p-0058" num="0057">Meanwhile, accuracy of retrieval is evaluated in terms of the average normalized modified retrieval rate (ANMRR). The ANMRR evaluates two criteria at the same time: one is the number of retrieved accurate images and the other is the ranks of the accurate images in the retrieval result. The range of the ANMRR is from 0 to 1, and the smaller value indicates a better accuracy. Details of the ANMRR are described in an article, “Introduction to MPEG-7: Multimedia Content Description Interface,” by B. S. Manjunath, Philippe Salembier, and Thomas Sikora, John Wiley &amp; Sons Ltd., 2002.</p>
<p id="p-0059" num="0058">In the experiments, the size (N) of a retrieved data set is 8,190 as shown in the table 1, and the buffer size (M) for iterative matching is 60. When a high-speed sorting algorithm is used, the increased search time for applying the iterative matching is just 0.007 times (=(60 log 60/8190 log 8190)×2) compared to the prior art algorithm. Here, 2 indicates the iteration frequency corresponding to a best accuracy.</p>
<p id="p-0060" num="0059"><figref idref="DRAWINGS">FIGS. 6</figref><i>a </i>and <b>6</b><i>b </i>are graphs expressing the accuracies of retrievals in terms of the ANMRR when experiments of the image retrieval method according to the present invention were carried out in the face retrieval system shown in <figref idref="DRAWINGS">FIG. 5</figref> according to the experiment conditions described above. In <figref idref="DRAWINGS">FIGS. 6</figref><i>a </i>and <b>6</b><i>b</i>, the first retrieval shows the prior art retrieval algorithm to which the iterative matching is not applied. <figref idref="DRAWINGS">FIG. 6</figref><i>a </i>is a graph showing the accuracy of each retrieval frequency according to the weighted value parameter (w), that is, 0.7, 0.8, 0.9, and 1. The graph shows that irrespective of the weighted value parameters, the third retrieval shows higher accuracies. <figref idref="DRAWINGS">FIG. 6</figref><i>b </i>is a graph showing the accuracy of each retrieval frequency when the weighted value parameters (w) are 0.7, 0.8, 0.9, and 1. The graph shows that when the weighted value parameters are 0.8 and 1, higher accuracies are obtained. That is, when the weighted value parameter is 0.8 and the frequency of retrieval is 3, that is, the frequency of iteration is 2, the highest accuracy is obtained. In conclusion, the graphs show that compared to the prior art retrieval algorithm, that is, the first retrieval, the performance is improved by 0.0168, from 0.3456 to 0.3378, when expressed by the ANMRR. This means that more images equivalent to 1.7% are retrieved by the iterative matching. The retrieval of the 60-sized buffer and complexity of the calculation needed in the matching for this performance improvement can be neglected when compared to the first matching performed with the 8,190 images in the entire database.</p>
<p id="p-0061" num="0060">The present invention may be embodied in a code, which can be read by a computer, on a computer readable recording medium. The computer readable recording medium includes all kinds of recording apparatuses on which computer readable data are stored.</p>
<p id="p-0062" num="0061">The computer readable recording media includes storage media such as magnetic storage media (e.g., ROM's, floppy disks, hard disks, etc.), optically readable media (e.g., CD-ROMs, DVDs, etc.) and carrier waves (e.g., transmissions over the Internet). Also, the computer readable recording media can be scattered on computer systems connected through a network and can store and execute a computer readable code in a distributed mode.</p>
<p id="p-0063" num="0062">According to the present invention as described above, among similar images retrieved by using a query image selected by a user, M similar images that are ranked higher are selected, and by performing iterative matching for the M similar images using the high-ranked similar image, M similar images are rearranged in order of similarity and output. Accordingly, accuracy of the matching can improve greatly and a load to the retrieval engine due to iterative retrieval can be minimized.</p>
<p id="p-0064" num="0063">Optimum embodiments have been explained above and are shown. However, the present invention is not limited to the preferred embodiment described above, and it is apparent that variations and modifications by those skilled in the art can be effected within the spirit and scope of the present invention defined in the appended claims. Therefore, the scope of the present invention is not determined by the above description but by the accompanying claims.</p>
<?DETDESC description="Detailed Description" end="tail"?>
</description>
<us-claim-statement>What is claimed is:</us-claim-statement>
<claims id="claims">
<claim id="CLM-00001" num="00001">
<claim-text>1. An image retrieval method comprising:
<claim-text>(a) comparing a query image to N reference images stored in an image database and extracting K similar images in order of matching rank from the N reference images; and</claim-text>
<claim-text>(b) performing iterative matching for M similar images, which are ranked higher among the K similar images, with a predetermined frequency of iteration, by selecting a similar image ranked the highest among to-be-iteratively-matched similar images as a new query image for each iteration, and rearranging the matching ranks of the M similar images for each iteration by updating a matching score using a predetermined weighted value parameter.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00002" num="00002">
<claim-text>2. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising:
<claim-text>(c) adding (K−M) similar images, which exclude the M similar images in the K similar images extracted in the step (a), to the M similar images rearranged in the step (b), and outputting the result.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00003" num="00003">
<claim-text>3. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the step (b) comprises:
<claim-text>(b1) storing the M similar images whose matching ranks are higher among the K similar images extracted in the step (a);</claim-text>
<claim-text>(b2) selecting a similar image ranked the highest among the M similar images as a new query image, and performing iterative matching for a retrieval range of (M−1) similar images that excludes the image ranked the highest; and</claim-text>
<claim-text>(b3) after changing the new query image and the retrieval range in the step (b2), performing iterative matching with the predetermined frequency of iteration and rearranging the matching ranks of the M similar images.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00004" num="00004">
<claim-text>4. The method of <claim-ref idref="CLM-00003">claim 3</claim-ref>, wherein the step (b) further comprises:
<claim-text>(b4) storing (K−M) similar images excluding the M similar images in the K similar images extracted in the step (a).</claim-text>
</claim-text>
</claim>
<claim id="CLM-00005" num="00005">
<claim-text>5. The method of <claim-ref idref="CLM-00004">claim 4</claim-ref>, wherein the step (b) further comprises:
<claim-text>(b5) adding the (K−M) similar images stored in the step (b4) to the M similar images rearranged in the step (b3) and outputting the result.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00006" num="00006">
<claim-text>6. A computer readable medium having embodied thereon a computer program for executing the method of <claim-ref idref="CLM-00003">claim 3</claim-ref>.</claim-text>
</claim>
<claim id="CLM-00007" num="00007">
<claim-text>7. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the weighted value parameter is a value which decreases exponentially in proportion to the increase of an the iteration frequency.</claim-text>
</claim>
<claim id="CLM-00008" num="00008">
<claim-text>8. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the matching score is updated according to the following equation:
<claim-text>
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>s</i><sup>(n-1)</sup>(<i>i</i>)+<i>W</i><sub>n-1</sub><i>·D</i>(<i>q</i><sup>(n-1)</sup><i>,q</i><sub>i</sub><sup>(n-1)</sup>)<?in-line-formulae description="In-line Formulae" end="tail"?>
</claim-text>
<claim-text>where i is between 1 and M, s<sup>(n-1)</sup>(i) denotes a matching score corresponding to each similar image {q<sub>i</sub><sup>(n-1)</sup>|i=1, . . . , M}which are rearranged when iterative matching is performed n times, w<sub>n-1 </sub>is the weighted value parameter which is applied when iterative matching is performed n times, and D(q<sup>(n-1)</sup>, q<sub>i</sub><sup>(n-1)</sup>denotes the absolute value of a characteristic information distance between a query image q<sup>(n-1)</sup>and the rearranged similar images q<sub>i</sub><sup>(n-1)</sup>.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00009" num="00009">
<claim-text>9. The method of <claim-ref idref="CLM-00008">claim 8</claim-ref>, wherein the weighted value parameter (w<sub>n-1</sub>) is (w<sub>1</sub>)<sup>n-1 </sup>and w<sub>1 </sub>is w.</claim-text>
</claim>
<claim id="CLM-00010" num="00010">
<claim-text>10. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the iteration frequency is set by the user.</claim-text>
</claim>
<claim id="CLM-00011" num="00011">
<claim-text>11. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the iteration frequency is automatically set according to a number of images whose ranks change in the M similar images compared to an existing previous rank.</claim-text>
</claim>
<claim id="CLM-00012" num="00012">
<claim-text>12. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the iteration frequency is automatically set according to whether ranks of the M similar images change after an i-th matching rank.</claim-text>
</claim>
<claim id="CLM-00013" num="00013">
<claim-text>13. A computer readable medium having embodied thereon a computer program for executing the method of <claim-ref idref="CLM-00001">claim 1</claim-ref>.</claim-text>
</claim>
<claim id="CLM-00014" num="00014">
<claim-text>14. An image retrieval apparatus comprising:
<claim-text>a 1st retrieval unit which compares a query image to N reference images stored in an image database and extracts K similar images in order of matching rank from the N reference images; and</claim-text>
<claim-text>a 2nd retrieval unit which performs iterative matching for M similar images, which are ranked higher among the K similar images, with a predetermined frequency of iteration, by selecting a similar image ranked the highest among to-be-iteratively-matched similar images as a new query image for each iteration, and rearranges the matching ranks of the M similar images for each iteration by updating a matching score using a predetermined weighted value parameter.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00015" num="00015">
<claim-text>15. The apparatus of <claim-ref idref="CLM-00014">claim 14</claim-ref>, further comprising:
<claim-text>a user interface unit which receives the query image and retrieval conditions selected by user the be provided to the 1st retrieval unit and the 2nd retrieval unit, and displays the M similar images rearranged by the 2nd retrieval unit.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00016" num="00016">
<claim-text>16. The apparatus of <claim-ref idref="CLM-00014">claim 14</claim-ref>, wherein the 2nd retrieval unit comprises:
<claim-text>a 1st storage unit which stores the M similar images whose matching ranks are higher among the K similar images provided by the 1st retrieval unit; and</claim-text>
<claim-text>an iterative matching unit which selects a similar image ranked the highest among the M similar images stored in the 1st storage unit as a new query image, performs iterative matching for a retrieval range of (M−1) similar images that excludes the image ranked the highest, rearranges the ranks of the M similar images, and stores the rearranged M similar images in the 1st storage unit.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00017" num="00017">
<claim-text>17. The apparatus of <claim-ref idref="CLM-00016">claim 16</claim-ref>, wherein the iterative matching unit changes the new query image and the retrieval range each iteration.</claim-text>
</claim>
<claim id="CLM-00018" num="00018">
<claim-text>18. The apparatus of <claim-ref idref="CLM-00016">claim 16</claim-ref>, wherein if M&lt;K, the apparatus further includes a 2nd storage unit for storing (K−M) similar images except the M similar images stored in the 1st storage unit.</claim-text>
</claim>
<claim id="CLM-00019" num="00019">
<claim-text>19. The apparatus of <claim-ref idref="CLM-00018">claim 18</claim-ref>, wherein the 2nd retrieval unit further comprises:
<claim-text>a retrieval result combining unit which adds the (K−M) similar images stored in the 2nd storage unit to the M similar images which are rearranged according to matching ranks by the iterative matching unit and stored in the 1st storage unit, and outputs the result as a retrieval result.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00020" num="00020">
<claim-text>20. The apparatus of <claim-ref idref="CLM-00014">claim 14</claim-ref>, wherein the weighted value parameter is a value which decreases exponentially in proportion to the increase of the iteration frequency.</claim-text>
</claim>
<claim id="CLM-00021" num="00021">
<claim-text>21. The apparatus of <claim-ref idref="CLM-00014">claim 14</claim-ref>, wherein the iteration frequency is set by the user.</claim-text>
</claim>
<claim id="CLM-00022" num="00022">
<claim-text>22. The apparatus of <claim-ref idref="CLM-00014">claim 14</claim-ref>, wherein the iteration frequency is automatically set according to a number of images whose ranks change in the M similar images compared to an existing previous rank.</claim-text>
</claim>
<claim id="CLM-00023" num="00023">
<claim-text>23. The apparatus of <claim-ref idref="CLM-00014">claim 14</claim-ref>, wherein the iteration frequency is automatically set according whether ranks change after an i-th matching rank.</claim-text>
</claim>
</claims>
</us-patent-grant>
