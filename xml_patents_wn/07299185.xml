<us-patent-grant lang="EN" dtd-version="v4.2 2006-08-23" file="US07299185-20071120.XML" status="PRODUCTION" id="us-patent-grant" country="US" date-produced="20071106" date-publ="20071120">
<us-bibliographic-data-grant>
<publication-reference>
<document-id>
<country>US</country>
<doc-number>07299185</doc-number>
<kind>B2</kind>
<date>20071120</date>
</document-id>
</publication-reference>
<application-reference appl-type="utility">
<document-id>
<country>US</country>
<doc-number>11264488</doc-number>
<date>20051101</date>
</document-id>
</application-reference>
<us-application-series-code>11</us-application-series-code>
<us-term-of-grant>
<disclaimer>
<text>This patent is subject to a terminal disclaimer.</text>
</disclaimer>
</us-term-of-grant>
<classifications-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>G</section>
<class>10</class>
<subclass>L</subclass>
<main-group>21</main-group>
<subgroup>00</subgroup>
<symbol-position>F</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20071120</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
</classifications-ipcr>
<classification-national>
<country>US</country>
<main-classification>704270</main-classification>
</classification-national>
<invention-title id="d0e51">Systems and methods for managing interactions from multiple speech-enabled applications</invention-title>
<references-cited>
<citation>
<patcit num="00001">
<document-id>
<country>US</country>
<doc-number>5001697</doc-number>
<kind>A</kind>
<name>Torres</name>
<date>19910300</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<patcit num="00002">
<document-id>
<country>US</country>
<doc-number>5748974</doc-number>
<kind>A</kind>
<name>Johnson</name>
<date>19980500</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<patcit num="00003">
<document-id>
<country>US</country>
<doc-number>5796401</doc-number>
<kind>A</kind>
<name>Winer</name>
<date>19980800</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<patcit num="00004">
<document-id>
<country>US</country>
<doc-number>5797123</doc-number>
<kind>A</kind>
<name>Chou et al.</name>
<date>19980800</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<patcit num="00005">
<document-id>
<country>US</country>
<doc-number>5838969</doc-number>
<kind>A</kind>
<name>Jacklin et al.</name>
<date>19981100</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<patcit num="00006">
<document-id>
<country>US</country>
<doc-number>5842165</doc-number>
<kind>A</kind>
<name>Raman et al.</name>
<date>19981100</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<patcit num="00007">
<document-id>
<country>US</country>
<doc-number>5854629</doc-number>
<kind>A</kind>
<name>Redpath</name>
<date>19981200</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<patcit num="00008">
<document-id>
<country>US</country>
<doc-number>6058366</doc-number>
<kind>A</kind>
<name>Tarkiainen et al.</name>
<date>20000500</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<patcit num="00009">
<document-id>
<country>US</country>
<doc-number>6061653</doc-number>
<kind>A</kind>
<name>Fisher et al.</name>
<date>20000500</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<patcit num="00010">
<document-id>
<country>US</country>
<doc-number>6065041</doc-number>
<kind>A</kind>
<name>Lum et al.</name>
<date>20000500</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<patcit num="00011">
<document-id>
<country>US</country>
<doc-number>6125347</doc-number>
<kind>A</kind>
<name>Cote et al.</name>
<date>20000900</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<patcit num="00012">
<document-id>
<country>US</country>
<doc-number>6192339</doc-number>
<kind>B1</kind>
<name>Cox</name>
<date>20010200</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<patcit num="00013">
<document-id>
<country>US</country>
<doc-number>6233559</doc-number>
<kind>B1</kind>
<name>Balakrishnan</name>
<date>20010500</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<patcit num="00014">
<document-id>
<country>US</country>
<doc-number>6310629</doc-number>
<kind>B1</kind>
<name>Muthusamy et al.</name>
<date>20011000</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<patcit num="00015">
<document-id>
<country>US</country>
<doc-number>6434529</doc-number>
<kind>B1</kind>
<name>Walker et al.</name>
<date>20020800</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<patcit num="00016">
<document-id>
<country>US</country>
<doc-number>6456305</doc-number>
<kind>B1</kind>
<name>Qureshi et al.</name>
<date>20020900</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<patcit num="00017">
<document-id>
<country>US</country>
<doc-number>6456974</doc-number>
<kind>B1</kind>
<name>Baker et al.</name>
<date>20020900</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<patcit num="00018">
<document-id>
<country>US</country>
<doc-number>6463413</doc-number>
<kind>B1</kind>
<name>Applebaum et al.</name>
<date>20021000</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<patcit num="00019">
<document-id>
<country>US</country>
<doc-number>6469711</doc-number>
<kind>B2</kind>
<name>Foreman et al.</name>
<date>20021000</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<patcit num="00020">
<document-id>
<country>US</country>
<doc-number>6507817</doc-number>
<kind>B1</kind>
<name>Wolfe et al.</name>
<date>20030100</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<patcit num="00021">
<document-id>
<country>US</country>
<doc-number>6701383</doc-number>
<kind>B1</kind>
<name>Wason et al.</name>
<date>20040300</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<patcit num="00022">
<document-id>
<country>US</country>
<doc-number>6785654</doc-number>
<kind>B2</kind>
<name>Cyr et al.</name>
<date>20040800</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<patcit num="00023">
<document-id>
<country>US</country>
<doc-number>6975993</doc-number>
<kind>B1</kind>
<name>Keiller</name>
<date>20051200</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<patcit num="00024">
<document-id>
<country>US</country>
<doc-number>6993508</doc-number>
<kind>B1</kind>
<name>Major et al.</name>
<date>20060100</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<patcit num="00025">
<document-id>
<country>US</country>
<doc-number>7188066</doc-number>
<kind>B2</kind>
<name>Falcon et al.</name>
<date>20070300</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<patcit num="00026">
<document-id>
<country>US</country>
<doc-number>2002/0024539</doc-number>
<kind>A1</kind>
<name>Eleftheriadis</name>
<date>20020200</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<patcit num="00027">
<document-id>
<country>US</country>
<doc-number>2002/0095290</doc-number>
<kind>A1</kind>
<name>Kahn et al.</name>
<date>20020700</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<patcit num="00028">
<document-id>
<country>US</country>
<doc-number>2003/0050777</doc-number>
<kind>A1</kind>
<name>Walker, Jr.</name>
<date>20030300</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<nplcit num="00029">
<othercit>“Winamp 3 Preview”. http://www.mp3newswire.net/stories/2001/winamp3.htm. May 23, 2001.</othercit>
</nplcit>
<category>cited by other</category>
</citation>
<citation>
<nplcit num="00030">
<othercit>“Skin (computing)”. http://en.wikipedia.org/wiki/Skin<sub>—</sub>%28computing%29. Wikipedia entry.</othercit>
</nplcit>
<category>cited by other</category>
</citation>
<citation>
<nplcit num="00031">
<othercit>“Winamp.com Skins”. http://www.winamp.com/skins. (Various selections).</othercit>
</nplcit>
<category>cited by other</category>
</citation>
<citation>
<nplcit num="00032">
<othercit>“Winamp2: Winamp's Subwindows”. http://www.winamp-faq.de/english/wa2/documentation/sub.htm. 2000 DigiTalk.</othercit>
</nplcit>
<category>cited by other</category>
</citation>
<citation>
<nplcit num="00033">
<othercit>Sun Microsystems, “Java Speech API Programmer's Guide”, Version 1.0, Oct. 1998.</othercit>
</nplcit>
<category>cited by other</category>
</citation>
</references-cited>
<number-of-claims>20</number-of-claims>
<us-exemplary-claim>1</us-exemplary-claim>
<us-field-of-classification-search>
<classification-national>
<country>US</country>
<main-classification>704270</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>7042701</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>704275</main-classification>
</classification-national>
</us-field-of-classification-search>
<figures>
<number-of-drawing-sheets>14</number-of-drawing-sheets>
<number-of-figures>16</number-of-figures>
</figures>
<us-related-documents>
<continuation>
<relation>
<parent-doc>
<document-id>
<country>US</country>
<doc-number>10067519</doc-number>
<kind>00</kind>
<date>20020204</date>
</document-id>
<parent-grant-document>
<document-id>
<country>US</country>
<doc-number>7139713</doc-number>
<kind>A </kind>
</document-id>
</parent-grant-document>
</parent-doc>
<child-doc>
<document-id>
<country>US</country>
<doc-number>11264488</doc-number>
</document-id>
</child-doc>
</relation>
</continuation>
<related-publication>
<document-id>
<country>US</country>
<doc-number>20060069571</doc-number>
<kind>A1</kind>
<date>20060330</date>
</document-id>
</related-publication>
</us-related-documents>
<parties>
<applicants>
<applicant sequence="001" app-type="applicant-inventor" designation="us-only">
<addressbook>
<last-name>Falcon</last-name>
<first-name>Stephen Russell</first-name>
<address>
<city>Woodinville</city>
<state>WA</state>
<country>US</country>
</address>
</addressbook>
<nationality>
<country>omitted</country>
</nationality>
<residence>
<country>US</country>
</residence>
</applicant>
<applicant sequence="002" app-type="applicant-inventor" designation="us-only">
<addressbook>
<last-name>Yip</last-name>
<first-name>Clement Chun Pong</first-name>
<address>
<city>Bellevue</city>
<state>WA</state>
<country>US</country>
</address>
</addressbook>
<nationality>
<country>omitted</country>
</nationality>
<residence>
<country>US</country>
</residence>
</applicant>
<applicant sequence="003" app-type="applicant-inventor" designation="us-only">
<addressbook>
<last-name>Banay</last-name>
<first-name>Dan</first-name>
<address>
<city>Seattle</city>
<state>WA</state>
<country>US</country>
</address>
</addressbook>
<nationality>
<country>omitted</country>
</nationality>
<residence>
<country>US</country>
</residence>
</applicant>
<applicant sequence="004" app-type="applicant-inventor" designation="us-only">
<addressbook>
<last-name>Miller</last-name>
<first-name>David Michael</first-name>
<address>
<city>Redmond</city>
<state>WA</state>
<country>US</country>
</address>
</addressbook>
<nationality>
<country>omitted</country>
</nationality>
<residence>
<country>US</country>
</residence>
</applicant>
</applicants>
<agents>
<agent sequence="01" rep-type="attorney">
<addressbook>
<orgname>Lee &amp; Hayes, PLLC</orgname>
<address>
<country>unknown</country>
</address>
</addressbook>
</agent>
</agents>
</parties>
<assignees>
<assignee>
<addressbook>
<orgname>Microsoft Corporation</orgname>
<role>02</role>
<address>
<city>Redmond</city>
<state>WA</state>
<country>US</country>
</address>
</addressbook>
</assignee>
</assignees>
<examiners>
<primary-examiner>
<last-name>Armstrong</last-name>
<first-name>Angela A.</first-name>
<department>2626</department>
</primary-examiner>
</examiners>
</us-bibliographic-data-grant>
<abstract id="abstract">
<p id="p-0001" num="0000">Systems and methods are described for speech systems that utilize an interaction manager to manage interactions—also known as dialogues—from one or more applications. The interactions are managed properly even if multiple applications use different grammars. The interaction manager maintains an interaction list. An application wishing to utilize the speech system submits one or more interactions to the interaction manager. Interactions are normally processed in the order in which they are received. An exception to this rule is an interaction that is configured by an application to be processed immediately, which causes the interaction manager to place the interaction at the front of the interaction list of interactions. If an application has designated an interaction to interrupt a currently processing interaction, then the newly submitted application will interrupt any interaction currently being processed and, therefore, it will be processed immediately.</p>
</abstract>
<drawings id="DRAWINGS">
<figure id="Fig-EMI-D00000" num="00000">
<img id="EMI-D00000" he="231.06mm" wi="192.28mm" file="US07299185-20071120-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00001" num="00001">
<img id="EMI-D00001" he="253.24mm" wi="186.61mm" file="US07299185-20071120-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00002" num="00002">
<img id="EMI-D00002" he="236.98mm" wi="156.29mm" file="US07299185-20071120-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00003" num="00003">
<img id="EMI-D00003" he="231.56mm" wi="193.55mm" file="US07299185-20071120-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00004" num="00004">
<img id="EMI-D00004" he="245.36mm" wi="187.03mm" file="US07299185-20071120-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00005" num="00005">
<img id="EMI-D00005" he="229.28mm" wi="182.96mm" file="US07299185-20071120-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00006" num="00006">
<img id="EMI-D00006" he="252.14mm" wi="199.39mm" file="US07299185-20071120-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00007" num="00007">
<img id="EMI-D00007" he="242.99mm" wi="138.77mm" file="US07299185-20071120-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00008" num="00008">
<img id="EMI-D00008" he="220.98mm" wi="172.72mm" file="US07299185-20071120-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00009" num="00009">
<img id="EMI-D00009" he="258.91mm" wi="195.33mm" file="US07299185-20071120-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00010" num="00010">
<img id="EMI-D00010" he="247.99mm" wi="146.81mm" file="US07299185-20071120-D00010.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00011" num="00011">
<img id="EMI-D00011" he="243.59mm" wi="196.51mm" file="US07299185-20071120-D00011.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00012" num="00012">
<img id="EMI-D00012" he="228.60mm" wi="171.62mm" file="US07299185-20071120-D00012.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00013" num="00013">
<img id="EMI-D00013" he="182.29mm" wi="109.05mm" file="US07299185-20071120-D00013.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00014" num="00014">
<img id="EMI-D00014" he="248.75mm" wi="115.49mm" file="US07299185-20071120-D00014.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
</drawings>
<description id="description">
<?RELAPP description="Other Patent Relations" end="lead"?>
<heading id="h-0001" level="1">RELATED APPLICATIONS</heading>
<p id="p-0002" num="0001">This patent application claims priority to parent U.S. patent application Ser. No. 10/067,519 to Stephen Falcon, filed Feb. 4, 2002 now U.S. Pat. No. 7,139,713, and entitled, “Systems And Methods For Managing Interactions From Multiple Speech-Enabled Applications.”</p>
<?RELAPP description="Other Patent Relations" end="tail"?>
<?BRFSUM description="Brief Summary" end="lead"?>
<heading id="h-0002" level="1">TECHNICAL FIELD</heading>
<p id="p-0003" num="0002">The systems and methods described herein relate to speech systems and speech-enabled applications that run on speech systems. More particularly, the described invention relates to managing interactions from multiple speech-enabled applications that utilize more than one grammar.</p>
<heading id="h-0003" level="1">BACKGROUND</heading>
<p id="p-0004" num="0003">Speech systems have been incorporated into many useful applications so that users may utilize the applications without having to manually operate an input device, such as a mouse or a keyboard. Personal computer systems (desktop, laptop, handheld, etc.) and automobile systems are only two examples of systems, or platforms, that may include integrated speech recognition functions.</p>
<p id="p-0005" num="0004">A single platform may have several applications executing at a given time. For example, in an automobile computer system that utilizes speech recognition software, there may be speech-enabled applications for radio operation, navigational tools, climate controls, mail, etc. Personal computers may include word processors, spreadsheets, databases and/or other programs that utilize speech recognition. Each speech-enabled application has a grammar associated with it that is a set of commands that the application is attempting to detect at any one time.</p>
<p id="p-0006" num="0005">Different applications may have different grammars. For instance, a word processing speech-enabled application may use a grammar that enables it to detect the command “print.” However, an automobile speech-enabled application that controls a car radio would not have such a command. On the other hand, the car radio application may have a grammar that enables the speech system to recognize the command “FM” to set the radio to the FM band. The word processor would not waste overhead by including an “FM” command in its relevant grammar.</p>
<p id="p-0007" num="0006">As the number of speech-enabled applications and grammars has increased, it has become increasingly problematic to run multiple speech-enabled applications on a single platform. Although each speech-enabled application may have its own unique grammar, certain commands may be used in more than one grammar, e.g., “stop.” When a speech system receives such a command, it must be able to determine which application the speaker directed the command to and which application should respond to the user.</p>
<p id="p-0008" num="0007">Similarly, multiple speech-enabled applications may attempt to deliver speech feedback simultaneously. This can result in a garbled communication that a user cannot understand. Such a result renders one or more of the applications useless. Also, if speech feedback from one speech-enabled application interrupts speech feedback from another similar application, the feedback from one or both applications may not be understandable to a user.</p>
<p id="p-0009" num="0008">For example, suppose a first application asks a question of the user and awaits a response. But before the user responds to the first application, a second application asks the user a question. Which application will accept the user's first answer? Will one of the applications accept an answer intended for the other application? Will either application be able to function properly with the response(s) it receives? With no control over specific interactions between the system and the user, there is no certain answer to any of these questions.</p>
<p id="p-0010" num="0009">One method that has been devised to handle this problem is to create a ‘token’ that indicates which application has the right to execute at any given time. When an application is ready to execute it requests a token. When the application receives the token, the application may execute.</p>
<p id="p-0011" num="0010">One of several drawbacks of such a system is that applications may crash or hang. If an application that currently holds the token crashes, then the system may not recover unless the system is prepared for application crashes. If the application hangs, then the system may never be able to regain control. Therefore, a token system is an inadequate solution to the problems encountered when attempting to execute multiple speech-enabled applications.</p>
<p id="p-0012" num="0011">Another problem that is encountered by speech-enabled applications is that when a command is given to an application that is not currently running, the command simply falls on deaf ears, so to speak, and there is no response to the command. Therefore, a user must first manually or vocally launch the application, then speak the desired command for the application. This means that a user must always be aware of which applications are running and which are not, so that the user knows whether she must launch an application before issuing certain commands. For example, if an automobile driver wants to play “song_A.mp3” on a car radio, the driver must first issue a command or manually launch an MP3 player, then command the player to play “song_A.” It would be desirable to minimize the actions required to launch an application and subsequently issue a command.</p>
<heading id="h-0004" level="1">SUMMARY</heading>
<p id="p-0013" num="0012">Systems and methods are described for managing interactions in a speech system that utilizes more than one grammar from more than one speech-enabled application. Multiple speech-enabled applications executing on a platform typically means that the platform must recognize and prioritize different grammars. The invention described herein addresses the problem of managing and prioritizing different grammars on a single platform so that a user can understand the applications and vice-versa.</p>
<p id="p-0014" num="0013">In one or more implementation, a speech system interaction manager (hereinafter, an “interaction manager”) is described. An “interaction” as used herein is defined as a complete exchange between a user and a speech system. The interaction manager manages interactions between multiple speech applications and a user so that (a) it is clear to the user which application the user is speaking to, and (b) it is clear to the applications which application is active.</p>
<p id="p-0015" num="0014">When an application wishes to utilize a speech system, the application submits an interaction to the interaction manager. The submitted interaction is placed at the end of an interaction list containing interactions to be processed by the speech system. The only time that an interaction is placed anywhere other than at the end of the interaction list is when an application indicates in the interaction that the interaction is to be placed at the top of the interaction list, i.e., the interaction is to be processed immediately. If this is indicated, the interaction is placed at the top of the interaction list. In one implementation, this indication is made by the application designating a particular grammar to be used with the interaction that is configured to be processed immediately. This type of grammar is referred to as a global grammar.</p>
<p id="p-0016" num="0015">If an interaction is added to the interaction list while another interaction is being processed, then the interaction waits until the current interaction has concluded processing unless the application specifies in the interaction that the interaction is to interrupt any interaction currently being processed. If this is so indicated, then the current interaction is interrupted so that the interrupting interaction can be processed. After the interrupting interaction is processed, the interrupted interaction may be configured to pick up where it left off, start over, re-prompt somewhere in the middle of the interaction, or cancel itself. Interaction processing then proceeds normally.</p>
<p id="p-0017" num="0016">An application may also indicate that an interaction is not to be added to the interaction list if the interaction list is not empty at the time the interaction is submitted. Such an indication may be used on an interaction that pertains to time-sensitive data that may be stale if it is not processed immediately, but is not of an importance such that it should interrupt another interaction.</p>
<p id="p-0018" num="0017">The interaction manager keeps applications informed as to the status of interactions belonging to the applications. For example, the interaction manager sends messages to applications, such as an interaction activated message, an interaction interrupted message, an interaction self-destructed message, an interaction re-activated message, and an interaction completed message.</p>
<p id="p-0019" num="0018">The interaction manager keeps track of the interactions being processed by the speech system so that the speech system only processes one interaction at a time. In this way, the interactions are processed in an orderly manner that allows multiple applications to run concurrently on the speech system, even if the multiple applications each use a different grammar. As a result, a user can better communicate with each of the applications.</p>
<?BRFSUM description="Brief Summary" end="tail"?>
<?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?>
<description-of-drawings>
<heading id="h-0005" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading>
<p id="p-0020" num="0019">A more complete understanding of exemplary methods and arrangements of the present invention may be had by reference to the following detailed description when taken in conjunction with the accompanying drawings wherein:</p>
<p id="p-0021" num="0020"><figref idref="DRAWINGS">FIG. 1</figref> is a block diagram of a computer system conforming to the invention described herein.</p>
<p id="p-0022" num="0021"><figref idref="DRAWINGS">FIG. 2</figref><i>a </i>is a diagram of an exemplary interaction.</p>
<p id="p-0023" num="0022"><figref idref="DRAWINGS">FIG. 2</figref><i>b </i>is a diagram of an exemplary interaction.</p>
<p id="p-0024" num="0023"><figref idref="DRAWINGS">FIG. 2</figref><i>c </i>is a diagram of an exemplary interaction.</p>
<p id="p-0025" num="0024"><figref idref="DRAWINGS">FIG. 3</figref> is a flow diagram depicting a methodological implementation of interaction processing.</p>
<p id="p-0026" num="0025"><figref idref="DRAWINGS">FIG. 4</figref> is a flow diagram depicting a methodological implementation of interaction interruption.</p>
<p id="p-0027" num="0026"><figref idref="DRAWINGS">FIG. 5</figref> is a flow diagram depicting a methodological implementation of interaction chaining.</p>
<p id="p-0028" num="0027"><figref idref="DRAWINGS">FIG. 6</figref> is a flow diagram depicting a methodological implementation of chained interaction interruption.</p>
<p id="p-0029" num="0028"><figref idref="DRAWINGS">FIG. 7</figref> is a flow diagram depicting a methodological implementation of grace period interruption.</p>
<p id="p-0030" num="0029"><figref idref="DRAWINGS">FIG. 8</figref><i>a </i>is a diagram of an exemplary master grammar table.</p>
<p id="p-0031" num="0030"><figref idref="DRAWINGS">FIG. 8</figref><i>b </i>is a diagram of an exemplary grammar table and its components.</p>
<p id="p-0032" num="0031"><figref idref="DRAWINGS">FIG. 9</figref> is a diagram of an exemplary computing environment within which the present invention may be implemented.</p>
<p id="p-0033" num="0032"><figref idref="DRAWINGS">FIG. 10</figref> is a flow diagram of a question control process.</p>
<p id="p-0034" num="0033"><figref idref="DRAWINGS">FIG. 11</figref> is a flow diagram of an announcer control process.</p>
<p id="p-0035" num="0034"><figref idref="DRAWINGS">FIG. 12</figref><i>a </i>is a block diagram of a command manager control.</p>
<p id="p-0036" num="0035"><figref idref="DRAWINGS">FIG. 12</figref><i>b </i>is a representation of a command manager object interface.</p>
</description-of-drawings>
<?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?>
<?DETDESC description="Detailed Description" end="lead"?>
<heading id="h-0006" level="1">DETAILED DESCRIPTION</heading>
<p id="p-0037" num="0036">This invention concerns a speech system that is able to manage interactions from multiple speech-enabled applications to facilitate meaningful dialogue between a user and the speech system. This invention speech system may be applied to a continuous speech system as well as a discrete speech system.</p>
<p id="p-0038" num="0037">Furthermore, the invention may be described herein as an automobile speech system or systems. However, the invention may also be implemented in non-automobile environments. Reference may be made to one or more of such environments. Those skilled in the art will recognize the multitude of environments in which the present invention may be implemented.</p>
<p id="p-0039" num="0038">General Terms</p>
<p id="p-0040" num="0039">Following is a brief description of some of the terms used herein. Some of the terms are terms of art, while others are novel and unique to the described invention. Describing the terms initially will provide proper context for the discussion of the invention, although the descriptions are not meant to limit the scope of the terms in the event that one or more of the descriptions conflict with how the terms are used in describing the invention.</p>
<p id="p-0041" num="0040">Grammars</p>
<p id="p-0042" num="0041">As previously stated, each speech-enabled application likely has its own specific grammar that a speech system must recognize. There are a variety of different things that applications will want to do with their grammars, such as constructing new grammars, using static grammars, enable/disable rules or entire grammars, persist grammars, make the grammars continually available, etc. The speech system described herein exposes methods to accomplish these things and more.</p>
<p id="p-0043" num="0042">Different grammars can have different attributes. A static grammar is one that will not change after being loaded and committed. A dynamic grammar, to the contrary, is a grammar that may change after a commit. Whether a grammar is static or dynamic must be known when the grammar is created or registered with the speech system. Rules may also be static or dynamic. A static rule cannot be changed after it is committed, while a dynamic rule may be changed after it is committed. A static rule can include a dynamic rule as a part of the static rule.</p>
<p id="p-0044" num="0043">A grammar may, at any time, be an enabled grammar or a disabled grammar. A disabled grammar is still within the speech system, but is not being listened for by the system. An enabled grammar may also be called an active grammar; a disabled grammar may also be referred to as an inactive grammar.</p>
<p id="p-0045" num="0044">Reference is made herein to transient and persistent grammars. A transient grammar is a grammar that is only active while its corresponding application is executing. When the application halts execution, i.e., shuts down, the grammar is removed from the speech system. A persistent grammar is always present in the speech system, whether the application to which the grammar belongs is present in the system. If an utterance is heard that belongs to a persistent grammar and the application is not running to handle it, the speech system launches the application.</p>
<p id="p-0046" num="0045">Furthermore, reference is made herein to global and yielding grammars. A global grammar contains terms that the speech system is always listening for. Global grammars are used sparingly to avoid confusion between applications. An example of a global grammar is a “call 9-1-1” command. A yielding grammar is active unless another grammar takes focus. The reason that another grammar would take focus is that a conversation unrelated to the grammar becomes active and yielding grammars outside the conversation are disabled.</p>
<p id="p-0047" num="0046">Interaction</p>
<p id="p-0048" num="0047">The term “interaction” is used herein to refer to a complete exchange between a speech-enabled application and a user. An interaction is a context of communication that unitizes one or more elements of a dialogue exchange. For example, an application developer may want to program a speech-enabled application to alert a user with a tone, ask the user a question, and await a response from the user. The developer would likely want these three events to occur sequentially, without interruption from another application in order for the sequence to make sense to the user. In other words, the developer would not want the alert tone sounded and the question asked only to be interrupted at that point with a communication from another application. The user may then not know how or when to respond to the question. Therefore, with the present invention, the developer may include the three actions in one interaction that is submitted to a speech system for sequential execution. Only in special circumstances will an interaction be interrupted. Interactions will be discussed in greater detail below.</p>
<p id="p-0049" num="0048">Conversation</p>
<p id="p-0050" num="0049">A series of related interactions may be referred to herein as a “conversation.” A conversation is intended to execute with minimal interruptions.</p>
<p id="p-0051" num="0050">Computer-Executable Instructions/Modules</p>
<p id="p-0052" num="0051">The invention is illustrated in the drawings as being implemented in a suitable computing environment. Although not required, the invention is described in the general context of computer-executable instructions, such as program modules, to be executed by a computing device, such as a personal computer or a hand-held computer or electronic device. Generally, program modules include routines, programs, objects, components, data structures, etc. that perform particular tasks or implement particular abstract data types. Moreover, those skilled in the art will appreciate that the invention may be practiced with other computer system configurations, including multi-processor systems, microprocessor-based or programmable consumer electronics, network PCs, minicomputers, mainframe computers, and the like. The invention may also be practiced in distributed computing environments where tasks are performed by remote processing devices that are linked through a communications network. In a distributed computing environment, program modules may be located in both local and remote memory storage devices.</p>
<p id="p-0053" num="0052">Exemplary Speech System</p>
<p id="p-0054" num="0053"><figref idref="DRAWINGS">FIG. 1</figref> is a block diagram of a computer system <b>100</b> that includes a speech system <b>102</b> and memory <b>104</b>. The computer system <b>100</b> also includes a processor <b>106</b> for executing computer instructions, a display <b>108</b>, an input/output (I/O) module <b>110</b>, a speaker <b>112</b> for speech output, a microphone <b>114</b> for speech input, and miscellaneous hardware <b>116</b> typically required in a computer system <b>100</b>. The computer system <b>100</b> may be designed for use in an automobile or in a non-automobile environment, such as in a desktop computer, a handheld computer, an appliance, etc.</p>
<p id="p-0055" num="0054">The speech system <b>100</b> includes a speech engine <b>118</b> having a text-to-speech (TTS) converter <b>120</b> and a speech recognizer (SR) <b>122</b>. The TTS converter <b>120</b> and the speech recognizer <b>122</b> are components typically found in speech systems. The speech recognizer <b>122</b> is configured to receive speech input from the microphone <b>114</b> and the TTS converter <b>120</b> is configured to receive electronic data and convert the data into recognizable speech that is output by the speaker <b>112</b>.</p>
<p id="p-0056" num="0055">The speech system <b>102</b> also includes a speech server <b>124</b> that communicates with the speech engine <b>118</b> by way of a speech application programming interface (SAPI) <b>126</b>. Since the speech engine <b>118</b> is separate from the speech server <b>124</b>, the speech server <b>124</b> can operate with any number of vendor-specific speech engines via the speech API <b>126</b>. However, such a specific configuration is not required.</p>
<p id="p-0057" num="0056">The SAPI <b>126</b> includes a vocabulary <b>164</b> that is the entire set of speech commands recognizable by the speech system <b>102</b>. It is noted that speech engine <b>118</b> may include the vocabulary <b>164</b> or a copy of the vocabulary <b>164</b> that is contained in the SAPI <b>126</b>. However, the present discussion assumes the vocabulary <b>164</b> is included in the SAPI <b>126</b>.</p>
<p id="p-0058" num="0057">Several applications may be stored in the memory <b>104</b>, including application_<b>1</b> <b>130</b>, application_<b>2</b> <b>132</b> and application_n <b>134</b>. Depending on the components that make up the computer system <b>100</b>, virtually any practical number of applications may be stored in the memory <b>104</b> for execution on the speech server <b>124</b>. Each application <b>130</b>-<b>134</b> is shown including at least one control: Application_<b>1</b> <b>130</b> includes a question control <b>154</b>; application_<b>2</b> includes an announcer control <b>156</b>; and application_n includes a command control <b>156</b> and a word trainer control <b>158</b>.</p>
<p id="p-0059" num="0058">Each control <b>154</b>-<b>158</b> uses a specific grammar: the question control <b>154</b> uses grammar_<b>1</b> <b>136</b>; the announcer control <b>156</b> uses grammar_<b>2</b> <b>138</b>; the command control <b>156</b> uses grammar_<b>3</b> <b>152</b>; and the word trainer control <b>158</b> uses grammar_<b>4</b> <b>140</b>.</p>
<p id="p-0060" num="0059">The controls <b>154</b>-<b>158</b> are designed to provide application developers a robust, reliable set of user-interface tools with which to build applications. The controls <b>154</b>-<b>158</b> are code modules that perform recurring functions desired by application developers. The controls <b>154</b>-<b>158</b> decrease the programming effort required by an original equipment manufacturer or an independent vendor to create a rich application user interface.</p>
<p id="p-0061" num="0060">The question control <b>154</b> gives an application developer an easy way to create various system-initiated interactions, or dialogues. The announcer control <b>155</b> provides a developer a simple way to deliver verbal feedback to users, including short notices and long passages of text-to-speech. The command control <b>156</b> provides a way for applications to specify what grammar it is interested in listening to, and communicates to the applications if and when a recognition occurs. The word trainer control <b>158</b> provides an easy way to implement a speech-oriented word-training interaction with a user. These controls will be discussed in greater detail below.</p>
<p id="p-0062" num="0061">It is noted that the speech server <b>126</b> and the applications <b>130</b>-<b>134</b> are separate processes. In most modern operating systems, each process is isolated and protected from other processes. This is to prevent one application from causing another application that is running to crash. A drawback with utilizing separate processes is that it makes sharing data between two processes difficult, which is what the speech server <b>126</b> needs to do in this case. Therefore, data must be marshaled between the applications <b>130</b>-<b>134</b> and the speech server <b>126</b>.</p>
<p id="p-0063" num="0062">There are various ways to marshal data across process boundaries and any of those ways may be used with the present invention. A common way to marshal data is with the use of a proxy and a stub object. A proxy resides in the application process space. As far as the proxy is concerned, the stub object is the remote object it calls. When an application calls some method on a proxy object, it does so internally, which is necessary to package data passed by the application. into the speech server process space, the stub object receives the data and calls a target object in the speech server. However, it is noted that any method known in the art to marshal data between processes may be used.</p>
<p id="p-0064" num="0063">The speech server <b>124</b> also includes an interaction manager <b>160</b> and a master grammar table <b>164</b>. The master grammar table <b>162</b> contains one or more grammars that are registered with the speech server <b>124</b> by one or more applications. The master grammar table <b>162</b> and the registration of grammars will be discussed in greater detail below, with reference to <figref idref="DRAWINGS">FIG. 3</figref>.</p>
<p id="p-0065" num="0064">The interaction manager <b>160</b> maintains an interaction list <b>168</b> of one or more interactions (interaction_<b>1</b> <b>170</b>, interaction_<b>2</b> <b>172</b>, interaction_<b>3</b> <b>174</b>, interaction_n <b>176</b>) from one or more applications in a particular order for processing by the speech server <b>124</b>. As previously discussed, an interaction is a logical context used by an application to communicate with a user. At any given time, there can be, at most, one active interaction between the user and an application. The interaction manager <b>160</b> processes the interactions <b>170</b>-<b>176</b> in order. Interactions can be inserted at the front of the interaction list <b>168</b>, i.e., before interaction_<b>1</b> <b>170</b>, or at the end of the interaction list <b>168</b>, i.e., interaction_n. If an interaction is inserted at the front of the interaction list <b>168</b>, the processing of interaction_<b>1</b> <b>170</b> will be interrupted. In one implementation, the interrupting interaction will only interrupt a current interaction if the interrupting interaction is configured to take precedence over a currently executing interaction.</p>
<p id="p-0066" num="0065">The interaction manager <b>160</b> is also configured to notify the applications <b>170</b>-<b>176</b> of the following transitions so that the applications <b>170</b>-<b>176</b> may modify the state or content of an interaction as it is processed in the interaction list <b>168</b>: interaction activated, interaction interrupted, interaction self-destructed, interaction re-activated, and interaction completed. As a result, the applications <b>170</b>-<b>176</b> can be aware of the state of the speech system <b>102</b> at all times.</p>
<p id="p-0067" num="0066">As previously noted, an interaction contains one or more elements that represent a “turn” of communication. A turn is a single action taken by either the system of the user during an interaction. For example, the system may announce “Fast or scenic route?” during a turn. In response, the user may answer “Fast,” which is the user's turn.</p>
<p id="p-0068" num="0067">Exemplary Interactions</p>
<p id="p-0069" num="0068"><figref idref="DRAWINGS">FIG. 2</figref> illustrates some examples of interactions. <figref idref="DRAWINGS">FIG. 2</figref><i>a </i>depicts exemplary interaction_A <b>200</b>. Interaction_A <b>200</b>, when executed, will sound a tone, ask a question and await a response from a user. Interaction_A <b>200</b> includes three elements that each represent a turn of communication; the first turn is the tone, the second turn is the question, and the third turn is the waiting. The first element is an EC (earcon) <b>210</b>, which causes an audio file to be played. In this example, the EC <b>210</b> sounds a tone to alert a user that the speech system <b>102</b> is about to ask the user a question. The second element is a TTS (text-to-speech) <b>212</b> element that plays a text file (i.e., speaks), which in this example, asks the user a question. The last element is an SR (speech recognition) <b>214</b> element that listens for a term included in the vocabulary <b>164</b>, <figref idref="DRAWINGS">FIG. 1</figref>. Processing exemplary interaction_A <b>200</b> creates the desired result from the speech system <b>102</b>.</p>
<p id="p-0070" num="0069"><figref idref="DRAWINGS">FIG. 2</figref><i>b </i>depicts exemplary interaction_B <b>220</b> that also includes three elements: an EC <b>222</b>, a TTS <b>224</b> and a WT (word trainer) <b>226</b> element. Processing interaction_B <b>226</b> results in the speech system sounding a tone, asking the user to state a command, and assigns the response stated by the user to a command.</p>
<p id="p-0071" num="0070"><figref idref="DRAWINGS">FIG. 2</figref><i>c </i>depicts exemplary interaction_C <b>230</b> that includes two elements: a TTS <b>232</b> and an EC <b>234</b>. Processing interaction_C <b>230</b> results in the speech system <b>102</b> playing a text file followed by the playing of an audio file.</p>
<p id="p-0072" num="0071">There is another type of element (not shown) that may be inserted into an interaction to cause a delay, or time out, before the system processes subsequent elements. This type of element is referred to as a NULL element. A NULL element would be inserted into an interaction to allow additional time for the interaction to be processed.</p>
<p id="p-0073" num="0072">Referring now back to <figref idref="DRAWINGS">FIG. 1</figref>, the interaction manager <b>160</b> provides for the ordering of interactions, including the elements (EC, TTS, WT, NULL, SR) discussed above. This prevents more than one application from addressing the user simultaneously. The interaction manager <b>160</b> processes the interactions <b>170</b>-<b>176</b> in the interaction list <b>168</b> in the order in which the interactions are submitted to the interaction manager <b>160</b> (i.e., on a first-in-first-out basis). An exception to this is that an application is provided the ability to submit an interaction directly to the beginning of the interaction list <b>168</b> in situations where the application considers the interaction a high priority.</p>
<p id="p-0074" num="0073">Interaction Management: Methodological Implementation</p>
<p id="p-0075" num="0074"><figref idref="DRAWINGS">FIG. 3</figref> is a flow diagram depicting a way in which the interaction manager <b>168</b> functions to manage the interactions <b>170</b>-<b>176</b> in the interaction list <b>168</b>. In the discussion of <figref idref="DRAWINGS">FIG. 3</figref> and the following figures, continuing reference will be made to the features and reference numerals contained in <figref idref="DRAWINGS">FIG. 1</figref>.</p>
<p id="p-0076" num="0075">At block <b>300</b>, interaction_A <b>170</b> is active, while interaction_B <b>172</b> and interaction_C <b>174</b> wait in the interaction list <b>168</b> to be processed. At block <b>302</b>, interaction_n <b>176</b> is added to the end of the interaction list <b>168</b>. Interaction_A <b>170</b> continues processing at block <b>304</b> (“No” branch, block <b>306</b>) until it concludes. Then, interaction_B <b>172</b> becomes active, i.e., begins processing at block <b>308</b> (“Yes” branch, block <b>306</b>).</p>
<p id="p-0077" num="0076">Interruption occurs when an application places an interaction at the beginning of the interaction list <b>168</b> without regard to an interaction already active there. When an interruption occurs, the active interaction is deactivated, and the interrupting interaction is activated.</p>
<p id="p-0078" num="0077">Interaction Interruption: Methodological Implementation</p>
<p id="p-0079" num="0078"><figref idref="DRAWINGS">FIG. 4</figref> is a flow diagram depicting an interaction interruption. On the left side of the figure, a current state of the interaction list <b>168</b> is shown corresponding to the blocks contained in the flow diagram. At block <b>400</b>, interaction_A <b>170</b> is active while interaction_B <b>172</b> and interaction_C <b>174</b> are inactive and waiting in the interaction list <b>168</b> to be processed. While interaction_A <b>170</b> is executing, interaction_n <b>176</b> is submitted by one of the speech-enabled applications <b>130</b>-<b>134</b> (block <b>402</b>). The submitting application wants interaction_n <b>176</b> to be processed immediately without regard to other interactions in the interaction list <b>168</b>, so an interruption flag is set in interaction_n <b>176</b> that tells the interaction manager <b>160</b> to process interaction_n <b>176</b> right away.</p>
<p id="p-0080" num="0079">Interaction_n <b>176</b> is then processed at block <b>406</b> (“No” branch, block <b>404</b>) until it has completed, i.e., actions related to any and all elements contained in interaction_n <b>176</b> have been performed. Only when interaction_n <b>176</b> has completed processing (“Yes” branch, block <b>404</b>), does interaction_A <b>170</b> have the capability to process again.</p>
<p id="p-0081" num="0080">However, interactions submitted to the interaction list <b>168</b> have a self-destruct option that, when used, terminates the interaction in the event that the interaction is interrupted. In some cases, an interaction may need to self-destruct due to internal failure. In such cases, the situation is treated the same as a normal self-destruction.</p>
<p id="p-0082" num="0081">At block <b>408</b>, it is determined whether interaction_A <b>170</b> has set a self-destruct flag that indicates the interaction should self-destruct upon interruption. If the self-destruct flag (not shown) is set (“Yes” branch, block <b>408</b>), interaction_A <b>170</b> terminates (block <b>410</b>). If the self-destruction flag is not set (“No” branch, block <b>408</b>), then interaction_A <b>170</b> finishes processing at block <b>412</b>.</p>
<p id="p-0083" num="0082">Interactions do not have an inherent “priority.” The applications only have an absolute ability to place an interaction at the front of the interaction list <b>168</b>. Such a placement results in interruption of a current interaction being processed.</p>
<p id="p-0084" num="0083">In another implementation, not shown, an interrupting interaction will not be processed until a current interaction has concluded if the current interaction will conclude in a pre-determined period of time. If the current interaction will take a longer amount of time than the pre-determined time to conclude, it is interrupted as described above.</p>
<p id="p-0085" num="0084">For example, suppose that an interrupting interaction will only interrupt a current interaction if the current interaction will not complete within three seconds. If a driver is just completing a long interaction that has taken thirty seconds to process but will conclude in two seconds, it may be desirable to let the interaction finish before interrupting with, say, an engine overheating announcement. If the current interaction is not self-destructing, the driver may have to endure another thirty-two seconds of interaction that he doesn't want to hear if the current interaction is repeated after the overheating announcement concludes. This would become even more irritable if another engine overheating announcement interrupted the current interaction again and the current interaction repeated again.</p>
<p id="p-0086" num="0085">Interaction Chaining: Methodological Interaction</p>
<p id="p-0087" num="0086">Interactions may also be “chained” together by speech-enabled applications using the speech server <b>124</b>. An application may want a certain interaction to establish a chain of interactions that constitutes a conversation. When this is the case, when an interaction concludes processing, the speech server <b>124</b> will wait a pre-determined grace period, or time out, before processing the next interaction in the interaction list <b>168</b>. During the grace period, the application may submit a subsequent interaction.</p>
<p id="p-0088" num="0087">An example of when interaction chaining may be used is when an automobile navigation system queries a driver for a destination. The navigation application may submit an interaction that asks for a destination state. If the state is submitted, the application may then submit an interaction that asks for a destination city. If the driver submits the city, the application may then submit an interaction that asks for the destination address.</p>
<p id="p-0089" num="0088">It is easy to understand why a navigation application would not want these interactions broken up. If the interactions are separated, the driver or the speech system <b>124</b> may become confused as to where the other is in the dialogue.</p>
<p id="p-0090" num="0089"><figref idref="DRAWINGS">FIG. 5</figref> is a flow diagram depicting the methodology of interaction chaining. Similar to <figref idref="DRAWINGS">FIG. 4</figref>, a current state of the interaction list <b>168</b> is shown at each stage of the flow diagram. It is noted that, for this example, one of the applications <b>130</b>-<b>134</b> submits a conversation to be processed. The conversation consists of interaction_A <b>170</b> and interaction_n <b>176</b>.</p>
<p id="p-0091" num="0090">At block <b>500</b>, interaction_A <b>170</b> is active while interaction_B <b>172</b> and interaction_C <b>174</b> are inactive and waiting in the interaction list <b>168</b> to be processed. After interaction_A <b>170</b> concludes processing at block <b>502</b>, the interaction manager <b>160</b> waits for the pre-determined grace period before moving on to processing interaction_B <b>172</b> (block <b>504</b>).</p>
<p id="p-0092" num="0091">At block <b>506</b>, the application that submitted interaction_A <b>170</b> submits interaction_n <b>176</b> to be processed to complete the conversation. The submission of interaction_n <b>176</b> occurs before the grace period has expired. If interaction_n <b>176</b> is not submitted before the grace period expires, interaction_B <b>172</b> will begin processing.</p>
<p id="p-0093" num="0092">When interaction_n <b>176</b> is submitted before the grace period expires (“Yes” branch, block <b>506</b>), interaction_n <b>176</b> is processed immediately at block <b>508</b>. There are no additional interactions to be processed after interaction_n <b>176</b> has completed processing (“No” branch, block <b>506</b>), so interaction_B <b>172</b> begins processing at block <b>510</b>. The desired result is achieved, because the complete conversation (interaction_A <b>170</b> and interaction_n <b>176</b>) was processed without separating the interactions.</p>
<p id="p-0094" num="0093">Although it is not typically desired, chained interactions may be interrupted by another application. If an application submits an interaction that is flagged to be processed immediately, that interaction will be placed at the front of the interaction list <b>168</b>, even if doing so will interrupt a conversation. This is one reason that use of the ‘process immediately’ option should be used sparingly by applications. An example of when the ‘process immediately’ option may be used is when an automobile engine is overheating. It is probably desirable to interrupt any interactions being processed to tell the driver of the situation since the situation requires immediate attention.</p>
<p id="p-0095" num="0094">Chained Interaction Interruption: Methodological Implementation</p>
<p id="p-0096" num="0095"><figref idref="DRAWINGS">FIG. 6</figref> is a flow diagram depicting the process of interrupting a chained interaction. Once again, a current state of the interaction list <b>168</b> is shown corresponding to each portion of the flow diagram. Also, it is assumed that an application wants to process a conversation consisting of interaction_A <b>170</b> and interaction_n <b>176</b>.</p>
<p id="p-0097" num="0096">At block <b>600</b>, interaction_A <b>170</b> is active while interaction_B <b>172</b> and interaction_C <b>174</b> are inactive and waiting in the interaction list <b>168</b> to be processed. When interaction_A <b>170</b> concludes processing at block <b>702</b>, a grace period is established at block <b>604</b>.</p>
<p id="p-0098" num="0097">If no interaction is submitted by the same application (“No” branch, block <b>606</b>), then interaction_B <b>172</b> is processed at block <b>608</b>. However, in this example, interaction_n <b>176</b> is submitted before the grace period expires (“Yes” branch, block <b>606</b>). Therefore, interaction_n <b>176</b> begins processing at block <b>610</b>. At block <b>612</b>, interaction_m <b>198</b> is submitted and is flagged to be processed immediately, so it begins processing at block <b>614</b>. Interaction_m <b>198</b> continues to be processed until it is completed (“No” branch, block <b>616</b>). When interaction_m <b>198</b> has concluded (“Yes” branch, block <b>616</b>), the interaction manager <b>160</b> determines if interaction_n <b>176</b> (which was interrupted) is set to self-destruct in the event that it is interrupted. If interaction_n <b>176</b> is to self-destruct (“Yes” branch, block <b>618</b>), then interaction_B <b>172</b> begins to be processed at block <b>608</b>. If interaction_n <b>176</b> does not self-destruct (“No” branch, block <b>618</b>), then interaction_n <b>176</b> finishes processing at block <b>620</b>.</p>
<p id="p-0099" num="0098">Grace Period Interruption: Methodological Implementation</p>
<p id="p-0100" num="0099">Interruptions may also occur during a grace period, because the grace period does not preclude any application from interrupting. <figref idref="DRAWINGS">FIG. 7</figref> is a flow diagram that depicts the process that takes place when an application submits an interrupting interaction during a grace period. As before, a current state of the interaction list <b>168</b> is shown corresponding to the blocks of the flow diagram.</p>
<p id="p-0101" num="0100">At block <b>700</b>, interaction_A <b>170</b> is active while interaction_B <b>172</b> and interaction_C <b>174</b> are inactive and waiting in the interaction list <b>168</b> to be processed. When interaction_A <b>170</b> concludes processing at block <b>702</b>, a grace period is established at block <b>704</b>.</p>
<p id="p-0102" num="0101">Before the grace period has timed out, interaction_n <b>176</b> interrupts and is placed at the front of the interaction list <b>168</b> (block <b>708</b>). It is noted that interaction_n <b>176</b> is not a part of the conversation that began with interaction_A <b>170</b>. Interaction_n <b>176</b> is processed at block <b>708</b> for as long as the interaction needs to run (“No” branch, block <b>710</b>). Only when interaction_n <b>176</b> has concluded processing (“Yes” branch, block <b>710</b>) will interaction_B <b>172</b>—the second interaction of the conversation—be processed (block <b>712</b>).</p>
<p id="p-0103" num="0102">Do not Add Interaction to Non-Empty List</p>
<p id="p-0104" num="0103">An application may also indicate that an interaction is not to be added to the interaction list if the interaction list is not empty at the time the interaction is submitted. One scenario in which this might be desirable is in the event that an application included a verbal clock that announced a current time every minute. If, during the time where the minute would normally be announced, another application was speaking to the user, the announcement interaction would not be added to the interaction list, because the announcement might be out of date by the time it is processed.</p>
<p id="p-0105" num="0104">Another scenario might be a navigation application that announces a current location, block by block, as one drives, e.g., “You are on 1<sup>st </sup>and Main” . . . “You are on 2<sup>nd </sup>and Main,” etc. It would not be desirable to add such interactions to the interaction list if the driver were speaking to another application.</p>
<p id="p-0106" num="0105">Exemplary Grammar(s) &amp; Grammar Attributes</p>
<p id="p-0107" num="0106">The interaction manager <b>160</b> must also use specific attributes of each grammar that it processes to process grammar interactions correctly. When the speech system <b>102</b> is initially booted, any applications that are present at startup are registered with the master grammar table <b>162</b> (whether running or not) so that the speech system <b>102</b> is aware of each grammar that may possibly be active. Additionally, if an application launches or is added while the speech system <b>102</b> is running, the application will register its grammar in the master grammar table <b>162</b>.</p>
<p id="p-0108" num="0107"><figref idref="DRAWINGS">FIG. 8</figref><i>a </i>is an illustration of a master grammar table <b>800</b> similar to the master grammar table <b>162</b> shown in <figref idref="DRAWINGS">FIG. 1</figref>. The master grammar table <b>800</b> is a table of grammar tables, there being one grammar table for each grammar available to the system.</p>
<p id="p-0109" num="0108">As shown in <figref idref="DRAWINGS">FIG. 8</figref><i>a</i>, a grammar table <b>802</b> for grammar_<b>1</b> <b>136</b> is included in the master grammar table <b>800</b>. Similarly, a grammar table <b>804</b> for grammar_<b>2</b> <b>138</b>, a grammar table <b>806</b> for grammar_<b>3</b> <b>140</b> and a grammar table <b>808</b> for grammar_<b>4</b> <b>152</b> are included in the master grammar table <b>800</b>. It is noted that practically any number of grammar tables may be stored in the master grammar table <b>800</b> between grammar table <b>802</b> and grammar table <b>806</b>.</p>
<p id="p-0110" num="0109"><figref idref="DRAWINGS">FIG. 8</figref><i>b </i>is a more detailed illustration of a grammar table <b>810</b> similar to the grammar tables <b>802</b>-<b>806</b> shown in <figref idref="DRAWINGS">FIG. 8</figref><i>a</i>. Grammar table <b>810</b> includes several members: a grammar identifier <b>820</b>; an executable command <b>822</b>; a global flag <b>826</b>; a persistent flag <b>828</b>; an active flag <b>830</b>; and a static flag <b>832</b>. Each of the members <b>820</b>-<b>832</b> included in the grammar table <b>810</b> specifies an attribute of a grammar associated with the grammar table <b>810</b>.</p>
<p id="p-0111" num="0110">The grammar identifier <b>820</b> is a value that is uniquely associated with a grammar that corresponds to the grammar table <b>810</b>. The grammar identifier <b>820</b> is used with interactions to identify a grammar that is associated with the grammar identifier. Including the grammar identifier <b>820</b> with an interaction solves a problem of latency that is inherent in the speech system <b>102</b>. After an application submits an interaction that is placed in the interaction list <b>168</b> of the interaction manager <b>160</b>, the application must wait until the interaction reaches the front of the interaction list <b>168</b> before it is processed. When the interaction finally reaches the front of the interaction list <b>168</b>, the speech server <b>124</b> immediately knows which grammar from the master grammar table <b>162</b> is associated with and, therefore used with, the interaction. If the grammar identifier <b>820</b> were not included in the interaction, the speech server <b>124</b> would first have to notify the application that the interaction submitted by the application is about to be processed. Then, the speech server <b>124</b> would have to wait for the application to tell it which grammar to utilize. Since the grammar identifier <b>820</b> is included with a submitted interaction, the speech server can begin processing the interaction immediately.</p>
<p id="p-0112" num="0111">The executable command <b>822</b> is a command (including a path if necessary) that may be used to launch an application associated with the grammar table <b>820</b>. This allows the speech server <b>124</b> to launch an application with the executable command <b>822</b> even though the corresponding application is not loaded into the system. If the speech server <b>124</b> receives an indication that a recognition occurs for a particular grammar, the speech server <b>124</b> passes the recognition to an application that has registered the grammar if such an application is running. If, however, no application using the identified grammar is running, the speech server <b>124</b> launches the application and passes the recognition to the application. This solves the problem of having to first launch an application manually before it may receive a command.</p>
<p id="p-0113" num="0112">The Speech</p>
<p id="p-0114" num="0113">For example, suppose an automobile driver is driving down the road when she decides she wants to play an MP3 file by, say, David Bowie, on the automobile radio. Assume for this example, that the executable command <b>822</b> is a typical path such as “\win\ . . . \mp3.exe” and that the recognition term <b>824</b> is “play mp3.”</p>
<p id="p-0115" num="0114">Instead of having to manually activate an MP3 player and then command it to “play David Bowie,” the driver simply commands the system to “play MP3 David Bowie.” Even though the MP3 player may not be running, the speech server <b>124</b> will recognize the command “play MP3” and execute the executable command <b>822</b> to start the MP3 player. The grammar associated with the MP3 player (not shown) will recognize “David Bowie” and play the desired selection that is associated with that command.</p>
<p id="p-0116" num="0115">The global flag <b>826</b> is a value that, when set, indicates that the grammar associated with the grammar table <b>810</b> is a global grammar that may not be interrupted by another application or the speech system <b>102</b> (but only the same application). If the global flag <b>826</b> is not set, then the grammar is a yielding grammar that can be interrupted by other applications or by the speech system <b>102</b>. As will be discussed in greater detail below, a global grammar is always active, although parts of it may be deactivated by the application to which it corresponds.</p>
<p id="p-0117" num="0116">It is noted that the global flag <b>826</b> may be implemented as a yielding flag (not shown) which, when set, indicates that the grammar is not a global grammar. The logic described for utilizing the global flag <b>826</b> would, in that case, simply be reversed.</p>
<p id="p-0118" num="0117">The persistent flag <b>828</b> is a value that, when set, indicates that the grammar associated with the grammar table <b>810</b> is persistent and not transient. A persistent grammar is a grammar that is loaded by default when the speech system <b>102</b> is running, irrespective of the run state of its corresponding application. If the persistent flag <b>828</b> is set, then the grammar associated with the grammar table should not be removed from the master grammar table <b>800</b>.</p>
<p id="p-0119" num="0118">The active flag <b>830</b> is a value that, when set, indicates that the grammar associated with the grammar table <b>810</b> is currently active. When a grammar is active, the speech recognitions system <b>102</b> actively listens for the commands included in the grammar. When an interaction is submitted to the interaction manager <b>160</b>, the interaction manager <b>160</b> indicates to the speech server <b>124</b> that other grammars should yield to a certain grammar if applicable. The speech server <b>124</b> sets the active flag <b>830</b> to a value that indicates the grammar associated with the grammar table <b>810</b> is active. Simultaneously, the interaction manager <b>160</b> will clear the active flag <b>830</b> for each yielding grammar in the master grammar table <b>162</b>. As a result, the set of commands that the speech system <b>102</b> listens for is reduced.</p>
<p id="p-0120" num="0119">When the yielding grammars are de-activated, i.e., the active flags are cleared, any grammar that is global (i.e., the global flag <b>826</b> is set) remains active. This is because a global grammar is always active. Therefore, at any given time that an application is executing, the speech system <b>102</b> is listening for all global grammars in the master grammar table <b>800</b> and one yielding grammar that is currently active (i.e., is associated with the application that is currently executing) in the master grammar table <b>800</b>. If no application is currently executing, the speech system <b>102</b> listens for all grammars, whether global or yielding.</p>
<p id="p-0121" num="0120">In one implementation, the speech server <b>124</b> does not de-activate all yielding grammars other than a grammar associated with a currently executing application unless an interaction in the interaction list <b>168</b> includes a method that informs the speech server <b>124</b> that all other yielding grammars should be de-activated. When the interaction manager <b>160</b> identifies such a method, the interaction manager <b>160</b> sends a message to the speech server <b>124</b> to de-activate all other yielding grammars in the master grammar table <b>162</b>.</p>
<p id="p-0122" num="0121">Finally, the static flag <b>832</b> is a value that, when set, indicates that the grammar associated with the grammar table <b>810</b> is a static grammar and, therefore, will not change after it is registered in the master grammar table <b>162</b>.</p>
<p id="p-0123" num="0122">Miscellaneous Functional Scenarios</p>
<p id="p-0124" num="0123">The functional scenarios that follow are not discussed in detail with respect to the speech system <b>102</b>, but may also be implemented with the features described above. The functional scenarios merely require that the interaction manager <b>160</b> be configured to handle the scenarios.</p>
<p id="p-0125" num="0124"> Push-to-Talk</p>
<p id="p-0126" num="0125">Push-to-talk (PTT) is used to indicate that a command from the user is imminent, which allows a user to initiate a command. For example, a user may PTT and say “lock the doors” to actuate a vehicle's door locks. A push-to-talk (PTT) event instantiated by a user interrupts any current interaction.</p>
<p id="p-0127" num="0126">PTT may also be used to provide a response to a system-initiated interaction. For example, if a navigation application asks “Fast or scenic route,” the user pushes push-to-talk and answers “fast” or “scenic.”</p>
<p id="p-0128" num="0127"> Barge-in</p>
<p id="p-0129" num="0128">The speech server <b>124</b> may also be configured to allow a user to “barge in” with a response. For example, if a navigation application asks “Fast or scenic route,” the user may interrupt—without PTT—and answer “fast” or “scenic.”</p>
<p id="p-0130" num="0129"> Immediate Response to User Command</p>
<p id="p-0131" num="0130">The speech server <b>124</b> may be configured to provide an immediate response to a user command. For example, while an automobile system is announcing a driving instruction to a driver, the driver commands the system to “disconnect.” The speech server <b>124</b> either disconnects immediately or confirms the disconnect command by stating “OK to disconnect”, interrupting the original driving instruction.</p>
<p id="p-0132" num="0131"> Application-Aborted Interaction</p>
<p id="p-0133" num="0132">The applications <b>170</b>-<b>176</b> may also abort an interaction in certain circumstances. For example, a navigation application needs to tell a driver that a point of interest is drawing near, but other applications are currently talking to the driver. By the time the other applications have concluded, the point of interest is passed. The navigation application aborts the announcement interaction before it begins. If the point of interest has not been passed, the announcement is made, delaying only until the other applications have concluded.</p>
<p id="p-0134" num="0133"> Interaction-Specific Grammar</p>
<p id="p-0135" num="0134">The speech server <b>124</b> may also de-activate some grammars and leave active an interaction-specific grammar. For example, a navigation application asks a driver “fast or scenic route.” Since the interaction is expecting a specific reply for a specific grammar, the specific grammar is activated (or remains active) to give the words “fast” and “scenic” priority over other grammars. This reduces the overhead required to process the driver's response, since the speech server <b>124</b> does not have to listen for as many terms.</p>
<p id="p-0136" num="0135"> Enhanced Prompt after Interruption</p>
<p id="p-0137" num="0136">The speech server <b>124</b> may also be configured to enhance a prompt during an interrupted conversation. If, for example, a navigation application asks for the driver's destination by stating first “please say the state.” The driver responds with the destination state. The navigation application then asks “please say the city.” However, during the announcement or before the driver answers with the destination city, the question is interrupted with an important announcement. After the announcement concludes, the original conversation resumes. To make up for the lost context, the speech server <b>124</b> is configured to revise the question to “for your destination, please say the city.” By re-focusing the driver on the navigation application conversation, the driver is less likely to be confused about what the system is saying.</p>
<p id="p-0138" num="0137">Speech Controls</p>
<p id="p-0139" num="0138">The speech controls <b>154</b>-<b>158</b> are provided in the speech server <b>124</b> to provide timesaving tools to developers who create applications to run with the speech server <b>124</b>. The speech controls <b>154</b>-<b>158</b> are computer-executable code modules that provide canned functions for developers to use for common interactions utilized in speech-enabled applications, thereby saving the developers the time and effort required to code the interaction for each use.</p>
<p id="p-0140" num="0139"> Question Control</p>
<p id="p-0141" num="0140">The question control <b>154</b> gives an application developer an easy way to create various modal, system-initiated interactions, or dialogues. Such interactions are used to obtain information from a user by asking the user a question. The following scenarios exemplify common uses of the question control to obtain desirable characteristics.</p>
<p id="p-0142" num="0141">User Interface Consistency: A user tries an in-car computer system in his friend's car. He then goes out to shop for a new car. He notices that although other systems sound a little different, working with their speech user interface dialogues is just the same.</p>
<p id="p-0143" num="0142">Application Compatibility: A user buys a full-featured navigation system software package for her car computer. She then buys a new car of a different make. She is still able to install her navigation software in her new car and it works the same as it did in her old car.</p>
<p id="p-0144" num="0143">Hardware/Software Compatibility: A developer can design a unique speech hardware and/or software subsystem to work in conjunction with the question control without compromising application compatibility or user interface consistency.</p>
<p id="p-0145" num="0144">The question control allows flexible programming so that a variety of question scenarios can be implemented. For example, the question control may be used to ask a driver a simple question that may be answered “yes” or “no”, or a more complex question such as “fast or scenic route” and receive “fast” or “scenic” as appropriate answers.</p>
<p id="p-0146" num="0145">The question control also allows greater flexibility by allowing the use of dynamic grammars. A question control has a grammar associated with it. In the above examples, the grammar may only consist of “yes” and “no” or “fast” or “scenic.” The question control can be configured by a developer or OEM to standardize behavior of certain types of questions that can't be provided with a simple list. For example, a hierarchical grammar such as a time or date grammar may be associated with a question control. Such types of grammars involve too many list choices to practically list for a user.</p>
<p id="p-0147" num="0146">The question control may also be used to provide an interrupting question. For example, while a system is reading a news story via TTS, a car application asks “&lt;ding&gt;—Your gas tank is close to empty; do you want instructions to the nearest gas station?” Similarly, a question programmed with the question control may be interrupted. For example, while an e-mail application is asking “You have mail; do you want to read it now?” a car application announces, “&lt;ding&gt;—Your engine is overheating.”</p>
<p id="p-0148" num="0147">Table 1 lists question control properties and types. Discussion follows.</p>
<p id="p-0149" num="0148">
<tables id="TABLE-US-00001" num="00001">
<table frame="none" colsep="0" rowsep="0">
<tgroup align="left" colsep="0" rowsep="0" cols="3">
<colspec colname="offset" colwidth="28pt" align="left"/>
<colspec colname="1" colwidth="112pt" align="left"/>
<colspec colname="2" colwidth="77pt" align="left"/>
<thead>
<row>
<entry/>
<entry namest="offset" nameend="2" rowsep="1">TABLE 1</entry>
</row>
<row>
<entry/>
<entry namest="offset" nameend="2" align="center" rowsep="1"/>
</row>
<row>
<entry/>
<entry>PROPERTY</entry>
<entry>TYPE</entry>
</row>
<row>
<entry/>
<entry namest="offset" nameend="2" align="center" rowsep="1"/>
</row>
</thead>
<tbody valign="top">
<row>
<entry/>
<entry>Type</entry>
<entry>Enumeration</entry>
</row>
<row>
<entry/>
<entry>Interrupting</entry>
<entry>Boolean</entry>
</row>
<row>
<entry/>
<entry>Prompt</entry>
<entry>String</entry>
</row>
<row>
<entry/>
<entry>Prompt Verbose</entry>
<entry>String</entry>
</row>
<row>
<entry/>
<entry>Earcon Mode</entry>
<entry>Enumeration</entry>
</row>
<row>
<entry/>
<entry>App-Provided Grammar</entry>
<entry>Grammar</entry>
</row>
<row>
<entry/>
<entry>List Choices</entry>
<entry>Boolean</entry>
</row>
<row>
<entry/>
<entry>Selection Feedback</entry>
<entry>Enumeration</entry>
</row>
<row>
<entry/>
<entry namest="offset" nameend="2" align="center" rowsep="1"/>
</row>
</tbody>
</tgroup>
</table>
</tables>
</p>
<heading id="h-0007" level="1">Question Control Properties</heading>
<p id="p-0150" num="0149">TYPE PROPERTY—The question control supports a Type property that can be used to determine the behavioral or content characteristics of the application using the question control. The Type property ultimately determines properties used in defining the application's behavior.</p>
<p id="p-0151" num="0150">INTERRUPTING PROPERTY—The Interrupting property determines whether the application will interrupt other interactions in the interaction list <b>168</b> of the interaction manager <b>160</b>. If the Interrupting property value is true, then the application (i.e., the question created with the question control) interrupts any other interaction in the interaction list <b>168</b>. If the Interrupting property is false, then the application does not interrupt other interactions, but places its interactions at the end of the interaction list <b>168</b>.</p>
<p id="p-0152" num="0151">PROMPT PROPERTY—The question control is able to verbally prompt a user in order to solicit a response. The Prompt property contains what is announced when the application/question is started. The Prompt property value is interpreted according to the value of a PromptType property, which is text-to-speech or pre-recorded. If the prompt is TTS, then the prompt announces the TTS string. If the prompt is pre-recorded, then the prompt announces the contents of a file that contains the recording.</p>
<p id="p-0153" num="0152">PROMPT VERBOSE PROPERTY—The Prompt Verbose property is a prompt that an interaction plays if the application/question is re-activated after it is interrupted. This property may be NULL and, if so, the interaction plays whatever is specified by the Prompt property (the prompt initially stated at the beginning of the interaction (i.e., application/question). Similar to the Prompt property, the Prompt Verbose property includes a PromptType that may be a TTS string or a string stored in a file.</p>
<p id="p-0154" num="0153">EARCON MODE PROPERTY—The Earcon Mode property determines if the question control will play an audio file when the question control is activated or re-activated. The audio file played is determined by a currently selected Type property. The Type property may be “Always,” “After Interruption” or “Never.”</p>
<p id="p-0155" num="0154">If the Type property is “Always,” then the audio file always plays on activation or re-activation. For example, if the audio file is a “ding” then the “ding” will be played when the system initiates a sole interaction or a first interaction in a conversation.</p>
<p id="p-0156" num="0155">If the Type property is “After Interruption,” then the audio file is only played on re-activation. For example, if a car system asks a user “Fast or scenic route” after first being interrupted by a global announcement, the audio file (i.e., “ding”) sounds before the question repeats after the interruption.</p>
<p id="p-0157" num="0156">If the Type property is “Never,” then the audio file is never played. The application may modify the Type property between “Always” and “Never.” The “Never” Type property may be set by an application when the application has a special need not to play the audio file.</p>
<p id="p-0158" num="0157">APPLICATION-PROVIDED GRAMMAR—An application can provide the question control with a list of options from which the user may choose. For each option offered, the application may provide one or more phrases whose recognition constitutes that choice. Any choices added are in addition to any grammars implemented in the question control. For example, a navigation application may provide a list having two options, “fast” and “scenic.” If the words “fast” and “scenic” are not already included in an active grammar, then they are automatically added.</p>
<p id="p-0159" num="0158">In one implementation, the question control provides a ‘spoken choice’ feature. The spoken choice feature may be used when a question is configured to have two or more possible answers for one answer choice. For example, a question may ask “What is the current season?” The answers may be “Spring, Summer, Autumn and Winter.” In addition, the word “Fall” may be used instead of “Autumn.” The question control may be configured to respond to a user inquiry as to possible answers as including either “Autumn” or “Fall.” As a result, the list choices provided to a user would be “Spring, Summer, Autumn and Winter,” or “Spring, Summer, Fall and Winter.”</p>
<p id="p-0160" num="0159">Another user for the spoken choice feature is for speech systems that may mispronounce one or more words. For example, many speech systems will mispronounce Spokane, Wash. as having a long “a” sound, since that is how phonetics rules dictate (instead of the correct short “a” sound). If a speech system is to announce the word “Spokane” to a user, the question control (or another control) can be programmed to play a designated audio file that correctly pronounces Spō-kan instead of using a standard TTS. In another implementation, the correct pronunciation may be specified as a spoken choice string, as described above.</p>
<p id="p-0161" num="0160">The application's various grammars are activated in the system immediately upon starting the control. This provides for the user's ability to barge in (using push-to-talk) and respond to the question control before it is finished.</p>
<p id="p-0162" num="0161">LIST CHOICES PROPERTY—The List Choices property determines whether the question control will automatically TTS the list of valid choices to a user after playing the prompt. This option is particularly useful when the user is likely to be unaware of the valid responses. For example, a navigation application may ask a driver who has just entered a destination “Which route would you like to take, fast or scenic?”</p>
<p id="p-0163" num="0162">SELECTION FEEDBACK PROPERTY—The Selection Feedback property determines if the question control will play feedback automatically when the user answers one of the application-provided or system-provided options that are enumerated by the List Choices property. If the Selection Feedback property has a value of “None,” no feedback is played when the user makes a choice. If the Selection Feedback property has a value of “Earcon,” then a designated satisfaction earcon is played when the user makes a choice. If the Selection Feedback property has a value of “Echo Choice” value, then a TTS of the user's choice is played when the user makes a choice.</p>
<p id="p-0164" num="0163"><figref idref="DRAWINGS">FIG. 10</figref> is a flow diagram depicting a question control process. The question control process depicted in <figref idref="DRAWINGS">FIG. 10</figref> is only one way in which the question control may be implemented.</p>
<p id="p-0165" num="0164">At block <b>1000</b>, the question control is launched. If there is an earcon to be played to indicate a question prompt is about to be asked (“Yes” branch, block <b>1002</b>), then the earcon is played at block <b>1004</b>. Otherwise, no earcon is played (“No” branch, block <b>1002</b>). The question prompt is then played at block <b>1008</b>.</p>
<p id="p-0166" num="0165">The choices with which the user may respond to the question prompt may be announced for the user at block <b>1010</b> (“Yes” branch, block <b>1008</b>). But this may not be desirable and, therefore, the play list choices block may be skipped (“No” branch, block <b>1008</b>.</p>
<p id="p-0167" num="0166">Just as an earcon may be played to alert the user that a question prompt if forthcoming, an earcon may also be played after the question (block <b>1014</b>) prompt to indicate to the user that the system is ready for the user's answer (“Yes” branch, block <b>1012</b>). If this is not desirable, the application may be programmed so that no such earcon is played (“No” branch, block <b>1012</b>).</p>
<p id="p-0168" num="0167">Blocks <b>1016</b>-<b>1026</b> represent the possible user responses to the question prompt (block <b>1008</b>). At block <b>1016</b>, the user may answer “What can I say?” (“Yes” branch, block <b>1016</b>) indicating that the user desires to hear the possible responses to the question prompt. Control of the process then returns to block <b>1010</b>, where the play list choice prompt is repeated to the user.</p>
<p id="p-0169" num="0168">If the user's response is to repeat the question prompt (“Yes” branch, block <b>1018</b>), then control of the process returns to block <b>1006</b>, where the question prompt is repeated to the user. If the user's response is ambiguous, i.e., it is a response that the system does not understand (“Yes” branch, block <b>1020</b>), then the system TTS's “Answer is ambiguous” at block <b>1021</b>. Control of the process returns to block <b>1012</b> to receive a new answer from the user.</p>
<p id="p-0170" num="0169">If the question control receives a valid response from the user (“Yes” branch, block <b>1022</b>), then feedback may be returned to the user to verify that the user has returned a valid response. If there is no feedback (“None” branch, block <b>1034</b>), then the result, i.e., the user's choice, is returned by the question control at block <b>1038</b>. If the feedback is an earcon to indicate a valid response (“EC” branch, block <b>1034</b>), then the earcon is played at block <b>1036</b> and the result is returned to the application at block <b>1038</b>. If the feedback is to play TTS of the user's choice (“Echo” branch, block <b>1034</b>), then the user's response is TTS'd to the user at block <b>1040</b> and the response is returned by the question control to the application at block <b>1038</b>.</p>
<p id="p-0171" num="0170">In one implementation of the question control described herein, a user may have an option to cancel a question process. If the user's response to the question prompt is to cancel (“Yes” branch, block <b>1024</b>), and if canceled is enabled (“Yes” branch, block <b>1044</b>), then the question is canceled. If an earcon is to be played to verify the cancellation (“Yes” branch, block <b>1046</b>) then the appropriate earcon is played at block <b>1048</b> and a ‘cancel’ value is returned to the application to indicate the cancellation. If an earcon is not to be played upon cancellation (“No” branch, block <b>1046</b>, then ‘cancel’ is returned at block <b>1050</b> without playing an earcon.</p>
<p id="p-0172" num="0171">If the cancel option is not enabled (“No” branch, block <b>1044</b>), then the system does not respond to the “cancel” command. If after a pre-determined timeout period elapses without receiving a response from the user (“Yes” branch, block <b>1026</b>), the ‘cancel’ is returned to the application at block <b>1050</b>. ‘Cancel’ is returned after an earcon is played (block <b>1048</b>) if a cancel earcon is enabled (“Yes” branch, block <b>1044</b>). Otherwise (“No” branch, block <b>1048</b>), ‘cancel’ is returned without first playing a cancel earcon. (Note that there is not a “No” branch to block <b>1026</b>; this is due to the fact that if a response is returned, the response will have been handled before a determination is made as to whether a response was received during the timeout period.) Other implementations may handle the process of the control differently.</p>
<p id="p-0173" num="0172"> Announcer Control</p>
<p id="p-0174" num="0173">The announcer control <b>155</b> provides a developer an easy way to deliver verbal feedback to users, including short notices and long passages of text-to-speech. The announcer control <b>155</b> implements a simple mechanism for playing pre-recorded speech or TTS text, and for giving a user standardized control of such playback. Use of the announcer control <b>155</b> significantly decreases the effort required by application developers to build a rich application user interface.</p>
<p id="p-0175" num="0174">The following scenarios exemplify common applications of the announcer control <b>155</b>.</p>
<p id="p-0176" num="0175">READ E-MAIL: A user request that an electronic mail message be read. The system begins TTS'ing the message. The user is able to pause, fast forward, rewind, etc.</p>
<p id="p-0177" num="0176">INTERRUPTING ANNOUNCER: While a navigation application is asking “Fast or scenic route?” the user commands “Read e-mail.” The system begins to read the e-mail immediately.</p>
<p id="p-0178" num="0177">INTERRUPTED ANNOUNCER: While the system is reading a news story via TTS, an automobile application asks “&lt;ding&gt; Your gas tank is close to empty. Do you want instructions to the nearest gas station?”</p>
<p id="p-0179" num="0178">NOTIFICATION: E-mail arrives while a user is driving and the system announces, “&lt;ding&gt; E-mail has arrived.”</p>
<p id="p-0180" num="0179">CONVERSATION STATEMENT: A user answers the last question to specify a navigation destination and the system announces, “Turn right at the next intersection.”</p>
<p id="p-0181" num="0180">REPEATED ANNOUNCEMENT: A navigation application announces, “&lt;ding&gt; Turn right at the next intersection.” But the user did not hear it. The user says, “Repeat” and the system repeats the announcement.</p>
<p id="p-0182" num="0181">The following features, or properties, may be available on the announcer control <b>155</b>. Table 2 lists announcer control properties and types. Discussion follows.</p>
<p id="p-0183" num="0182">
<tables id="TABLE-US-00002" num="00002">
<table frame="none" colsep="0" rowsep="0">
<tgroup align="left" colsep="0" rowsep="0" cols="3">
<colspec colname="offset" colwidth="28pt" align="left"/>
<colspec colname="1" colwidth="112pt" align="left"/>
<colspec colname="2" colwidth="77pt" align="left"/>
<thead>
<row>
<entry/>
<entry namest="offset" nameend="2" rowsep="1">TABLE 2</entry>
</row>
<row>
<entry/>
<entry namest="offset" nameend="2" align="center" rowsep="1"/>
</row>
<row>
<entry/>
<entry>PROPERTY</entry>
<entry>TYPE</entry>
</row>
<row>
<entry/>
<entry namest="offset" nameend="2" align="center" rowsep="1"/>
</row>
</thead>
<tbody valign="top">
<row>
<entry/>
<entry>Type</entry>
<entry>Enumeration</entry>
</row>
<row>
<entry/>
<entry>Interrupting</entry>
<entry>Boolean</entry>
</row>
<row>
<entry/>
<entry>ConversationID</entry>
<entry>String</entry>
</row>
<row>
<entry/>
<entry>Abort When Interrupted</entry>
<entry>Boolean</entry>
</row>
<row>
<entry/>
<entry>Earcon Mode</entry>
<entry>Enumeration</entry>
</row>
<row>
<entry/>
<entry>Announcement</entry>
<entry>String</entry>
</row>
<row>
<entry/>
<entry>Cancel Feedback</entry>
<entry>Boolean</entry>
</row>
<row>
<entry/>
<entry>Post Delay</entry>
<entry>Integer</entry>
</row>
<row>
<entry/>
<entry namest="offset" nameend="2" align="center" rowsep="1"/>
</row>
</tbody>
</tgroup>
</table>
</tables>
</p>
<heading id="h-0008" level="1">Announcer Control Properties</heading>
<p id="p-0184" num="0183">TYPE PROPERTY: The announcer control <b>155</b> supports the Type property that can be used to determine the behavioral or content characteristics of the application/announcement. The Type property ultimately determines the properties used in defining the application's/announcement's behavior. The speech server <b>124</b> defines the Type property's valid values.</p>
<p id="p-0185" num="0184">INTERRUPTING PROPERTY: The Interrupting property determines whether the application/announcement will interrupt other interactions present in the interaction list <b>168</b> of the interaction manager <b>160</b>. If the Interrupting property value is True, an announcement interaction will immediately interrupt any other interactions in the interaction list <b>168</b>. If the value is False, an announcement interaction will be placed at the end of the interaction list <b>168</b>.</p>
<p id="p-0186" num="0185">CONVERSATION ID PROPERTY: The Conversation ID property determines whether the application/announcement will operate in the context of the named conversation. The Conversation ID property is a string associated with a control instance. The interaction queue uses the Conversation ID property o identify which interaction belongs with which conversation.</p>
<p id="p-0187" num="0186">ABORT WHEN INTERRUPTED PROPERTY: The Abort When Interrupted property determines whether the announcement will automatically self-destruct if it is interrupted by another interaction. If the property value is True, then the announcement aborts when interrupted; if the value if False, the announcement does not abort.</p>
<p id="p-0188" num="0187">EARCON MODE Property: The Earcon Mode property determines if the application will play an audio file when it is activated or re-activated. If the Earcon Mode property has a value of “Always” the designated audio file is always played upon activation or re-activation. If the value is “After Interruption” the audio file is only played on re-activation; not on activation. If the value is “Never” an audio file is not played on activation or re-activation.</p>
<p id="p-0189" num="0188">ANNOUNCEMENT PROPERTY: The Announcement property contains what is announced when the control is started. If an Announcement Type associated with the Announcement property is “TTS,” then the Announcement property contains a string that is to be TTS'ed. If the Announcement Type is “Pre-recorded,” then the Announcement property contains a string designating a file to be announced, i.e., a file name. If the Announcement Type is “Combination,” then the Announcement property contains a TTS string and an audio file name.</p>
<p id="p-0190" num="0189">CANCEL EARCON PROPERTY: The Cancel Earcon property determines if the announcer control will play an audio file automatically when the user answers “cancel” (or its equivalent). If the Cancel Earcon property is True, then an earcon is played upon canceling; otherwise, an earcon is not played.</p>
<p id="p-0191" num="0190">POST DELAY PROPERTY. The Post Delay property determines if the application will pause for a definable period of time after the announcement has been completely delivered. This features gives a user some time to issue a “repeat” or “rewind” command. It also provides for a natural pause between interactions. If the Post Delay property value is True, then a post delay is provided when not in the context of a conversation. If the value is False, then a post delay is not provided.</p>
<p id="p-0192" num="0191"><figref idref="DRAWINGS">FIG. 11</figref> is a flow diagram depicting an announcer control process. At block <b>1100</b>, the announcer control is activated at some time other than after an interruption. If an earcon mode associated with the announcer control that may be set to “Always,” “Never,” or “After Interruption.” If the earcon mode is set to “Always” (“Always” branch, block <b>1102</b>), then an earcon is played at block <b>1108</b>, prior to an earcon being played at block <b>1108</b>. If the earcon mode is set to “Never” or “After Interruption” mode (“Never or After Interruption” branch, block <b>1102</b>), then an earcon is not played before an announcement is played at block <b>1108</b>.</p>
<p id="p-0193" num="0192">There may be a post delay after the announcement has completed (“Yes” branch, block <b>1112</b>. If the user asks the system to repeat the announcement during a post delay period (“Yes” branch, block <b>1114</b>), then the announcement is replayed at block <b>1110</b>. If the user does not ask the system to repeat the announcement during the post delay period (“No” branch, block <b>1114</b>), then the process completes at block <b>1116</b>.</p>
<p id="p-0194" num="0193">A post delay may not be activated for the announcement control. If not (“No” branch, block <b>1112</b>), then the process completes at block <b>1116</b> immediately after the announcement is played at block <b>1110</b>.</p>
<p id="p-0195" num="0194">Activation of the announcement control may occur after an interruption at block <b>1104</b>. If an interruption occurs before the announcement control is activated and the announcement control earcon mode is set to play an earcon “Always” or “After Interruption” (“Always or After Interruption” branch, block <b>1106</b>), then an earcon is played at block <b>1108</b> to alert the user that an announcement is forthcoming. The announcement is then played at block <b>1110</b>. If the earcon mode is set to “Never” (“Never” branch, block <b>1106</b>), then the announcement is played at block <b>1110</b> without playing an earcon at block <b>1108</b>.</p>
<p id="p-0196" num="0195">Thereafter, a post delay may be implemented (“Yes” branch, block <b>1112</b>) wherein the user may ask the system to repeat the announcement (“Yes” branch, block <b>1114</b>), in which case the announcement is repeated at block <b>1110</b>. If a post delay is not implemented (“No” branch, block <b>1112</b>), or if no response is received during a post delay period (“No” branch, block <b>1114</b>), then the process concludes at block <b>1106</b>.</p>
<p id="p-0197" num="0196">Command Control</p>
<p id="p-0198" num="0197">The command control <b>156</b> is designed to easily attach command-and-control grammar to an application. The command control <b>156</b> is used for user-initiated speech. At a minimum, the command control <b>156</b> must perform two functions. First, the command control <b>156</b> must provide a way for an application to specify what grammar(s) the application is interested in listening to. Second, the command control <b>156</b> must communicate back to the application that a recognition has occurred. To accomplish these tasks, the command control <b>156</b> is made up of four objects.</p>
<p id="p-0199" num="0198"><figref idref="DRAWINGS">FIG. 12</figref> is a block diagram of a command control <b>1200</b> similar to the command control <b>156</b> shown in <figref idref="DRAWINGS">FIG. 1</figref>. The command control <b>1200</b> includes a command manager object <b>1202</b>, a grammar object <b>1204</b>, a rule object <b>1206</b> and a recognition object <b>1208</b>. For purposes of further discussion, the command control <b>1200</b> is assumed to be an ActiveX control that conforms to ActiveX standards promulgated by Microsoft Corporation.</p>
<p id="p-0200" num="0199">Each of the four objects <b>1202</b>-<b>1208</b> includes an interface: the command manager object interface <b>1210</b>, the grammar object interface <b>1212</b>, the rule object interface <b>1214</b> and the recognition object interface <b>1216</b>. The interfaces <b>1210</b>-<b>1216</b> of each object <b>1202</b>-<b>1208</b> will be discussed separately in greater detail.</p>
<p id="p-0201" num="0200">The command manager object interface <b>1210</b> has three properties: Persistence ID <b>1220</b>; Grammar ID <b>1222</b>; and Grammar <b>1224</b>. The Persistence ID <b>1220</b> is used to identify the application for persistence purposes. The Persistence ID <b>1220</b> must be unique in the system. The Persistence ID <b>1220</b> may be blank if the associated grammar is not persistent. In one implementation, the Persistence ID <b>1220</b> is a ProgID (Microsoft WINDOWS implementation).</p>
<p id="p-0202" num="0201">The Grammar ID <b>1222</b> is an identifier that is used by with interactions <b>170</b>-<b>176</b> submitted to the interaction manager <b>160</b>. As previously explained, the Grammar ID <b>1222</b> is utilized to avoid latency problems inherent in the speech system <b>102</b>. The Grammar <b>1224</b> property is a pointer to the Grammar Object <b>1204</b> that is associated with the Command Control <b>1200</b>.</p>
<p id="p-0203" num="0202">The command manager object interface also includes several methods: Create Grammar <b>1226</b>, Persist <b>1228</b>, Remove Grammar <b>1230</b>, Start <b>1232</b> and Event: Recognition <b>1234</b>. Create Grammar <b>1226</b> is a function that is used to create a new grammar object from a grammar file. A grammar file may be an XML (extended markup language) file or a compiled grammar file (.cfg) or NULL, indicating that a new grammar is to be built. Parameters for Create Grammar <b>1226</b> include a path of a file to be opened or NULL for a new grammar (file), a value that indicates whether a grammar is static or dynamic (Load Options), a value that indicates whether a grammar is yielding or global (Context Options), and a pointer that receives the grammar object (ppGrammar).</p>
<p id="p-0204" num="0203">Persist <b>1228</b> is a method that indicates that a grammar is to be persisted. Persisted grammars recognize even if the application with which they are associated are not running. If a recognition occurs, the application is launched. Persist <b>1228</b> includes two parameters: the grammar under which the ID should be persisted (Persistence ID); and a complete path for an executable that will handle grammar recognitions (Application Path).</p>
<p id="p-0205" num="0204">Remove Grammar <b>1230</b> is a method that removes a grammar from the speech server <b>124</b>. If the grammar is persistent, Remove Grammar <b>1230</b> un-persists the grammar. Start <b>1232</b> is a method that is called to let the speech server <b>124</b> know that an application is ready to start handling events. Event: Recognition is a method that is called by the speech server <b>124</b> when a speech recognition occurs so that an appropriate application may be so notified.</p>
<p id="p-0206" num="0205">A specific implementation of the command manager object interface <b>1210</b> is shown below. The implementation is specific to the WINDOWS family of operating systems by Microsoft Corp. Other interfaces may be added to make the command control and ActiveX control (provided by the ATL wizard) so that a developer can simply drop the control on a form and proceed.</p>
<p id="p-0207" num="0206">
<tables id="TABLE-US-00003" num="00003">
<table frame="none" colsep="0" rowsep="0">
<tgroup align="left" colsep="0" rowsep="0" cols="2">
<colspec colname="offset" colwidth="21pt" align="left"/>
<colspec colname="1" colwidth="196pt" align="left"/>
<thead>
<row>
<entry/>
<entry namest="offset" nameend="1" align="center" rowsep="1"/>
</row>
</thead>
<tbody valign="top">
<row>
<entry/>
<entry>interface ICommandManager : IUnknown, IDispatch</entry>
</row>
<row>
<entry/>
<entry>{</entry>
</row>
<row>
<entry/>
<entry>Properties :</entry>
</row>
</tbody>
</tgroup>
<tgroup align="left" colsep="0" rowsep="0" cols="2">
<colspec colname="offset" colwidth="49pt" align="left"/>
<colspec colname="1" colwidth="168pt" align="left"/>
<tbody valign="top">
<row>
<entry/>
<entry>BSTR PersistenceID; (get/put)</entry>
</row>
<row>
<entry/>
<entry>DWORD GrammarID; (get only)</entry>
</row>
<row>
<entry/>
<entry>IDispatch* Grammar; (get only)</entry>
</row>
</tbody>
</tgroup>
<tgroup align="left" colsep="0" rowsep="0" cols="2">
<colspec colname="offset" colwidth="21pt" align="left"/>
<colspec colname="1" colwidth="196pt" align="left"/>
<tbody valign="top">
<row>
<entry/>
<entry>Methods :</entry>
</row>
<row>
<entry/>
<entry>CreateGrammar (BSTR File, SPEECH_LOAD_OPTIONS</entry>
</row>
</tbody>
</tgroup>
<tgroup align="left" colsep="0" rowsep="0" cols="2">
<colspec colname="offset" colwidth="35pt" align="left"/>
<colspec colname="1" colwidth="182pt" align="left"/>
<tbody valign="top">
<row>
<entry/>
<entry>LoadOptions, SPEECH_CONTEXT_OPTIONS</entry>
</row>
<row>
<entry/>
<entry>ContextOptions, IDispatch** ppGrammar)</entry>
</row>
</tbody>
</tgroup>
<tgroup align="left" colsep="0" rowsep="0" cols="2">
<colspec colname="offset" colwidth="49pt" align="left"/>
<colspec colname="1" colwidth="168pt" align="left"/>
<tbody valign="top">
<row>
<entry/>
<entry>HRESULT Persist (BSTR PersistenceID, BSTR</entry>
</row>
</tbody>
</tgroup>
<tgroup align="left" colsep="0" rowsep="0" cols="2">
<colspec colname="offset" colwidth="63pt" align="left"/>
<colspec colname="1" colwidth="154pt" align="left"/>
<tbody valign="top">
<row>
<entry/>
<entry>ApplicationPath)</entry>
</row>
</tbody>
</tgroup>
<tgroup align="left" colsep="0" rowsep="0" cols="2">
<colspec colname="offset" colwidth="49pt" align="left"/>
<colspec colname="1" colwidth="168pt" align="left"/>
<tbody valign="top">
<row>
<entry/>
<entry>HRESULT RemoveGrammar ( )</entry>
</row>
<row>
<entry/>
<entry>HRESULT Start( ):</entry>
</row>
</tbody>
</tgroup>
<tgroup align="left" colsep="0" rowsep="0" cols="2">
<colspec colname="offset" colwidth="21pt" align="left"/>
<colspec colname="1" colwidth="196pt" align="left"/>
<tbody valign="top">
<row>
<entry/>
<entry>};</entry>
</row>
<row>
<entry/>
<entry>interface _ICommandManagerEvents: IDispatch // this</entry>
</row>
</tbody>
</tgroup>
<tgroup align="left" colsep="0" rowsep="0" cols="2">
<colspec colname="offset" colwidth="35pt" align="left"/>
<colspec colname="1" colwidth="182pt" align="left"/>
<tbody valign="top">
<row>
<entry/>
<entry>interface is the event that is sent back on recognition//</entry>
</row>
</tbody>
</tgroup>
<tgroup align="left" colsep="0" rowsep="0" cols="2">
<colspec colname="offset" colwidth="21pt" align="left"/>
<colspec colname="1" colwidth="196pt" align="left"/>
<tbody valign="top">
<row>
<entry/>
<entry>{</entry>
</row>
<row>
<entry/>
<entry>HRESULT Recognition(IDispatch * Recognition,</entry>
</row>
</tbody>
</tgroup>
<tgroup align="left" colsep="0" rowsep="0" cols="2">
<colspec colname="offset" colwidth="35pt" align="left"/>
<colspec colname="1" colwidth="182pt" align="left"/>
<tbody valign="top">
<row>
<entry/>
<entry>DWORD CountAlternates);</entry>
</row>
</tbody>
</tgroup>
<tgroup align="left" colsep="0" rowsep="0" cols="2">
<colspec colname="offset" colwidth="21pt" align="left"/>
<colspec colname="1" colwidth="196pt" align="left"/>
<tbody valign="top">
<row>
<entry/>
<entry>}</entry>
</row>
<row>
<entry/>
<entry namest="offset" nameend="1" align="center" rowsep="1"/>
</row>
</tbody>
</tgroup>
</table>
</tables>
</p>
<p id="p-0208" num="0207">The Grammar Object Interface <b>1212</b> has an Enabled property <b>1236</b>, a Rule method <b>1238</b>, a Create Rule method <b>1240</b>, and a Commit method <b>1241</b>. The Enabled property <b>1242</b> is used to turn the entire grammar on or off. The Rule method <b>1248</b> selects a rule (by ID or name) and returns it to the caller. The Rule method <b>1248</b> includes a RuleID parameter that is either a numeric ID for the rule or a string for the rule name.</p>
<p id="p-0209" num="0208">The Create Rule method <b>1240</b> creates a new rule in the grammar. The Create Rule method <b>1240</b> also utilizes the RuleID parameter, which is a name or numeric identifier of the rule to be created. Other parameters used in the Create Rule method <b>1240</b> include Rule Level, Rule State, ppRule and Prop. Rule Level is an enumeration determines whether the rule is created as a top level rule or not. Rule State specifies whether the rule is to be created as dynamic. Dynamic rules can be modified after they are committed. ppRule is the rule object that is created. Prop is an optional PropID or PropName that a developer wants to associate with the rule.</p>
<p id="p-0210" num="0209">The Commit method <b>1241</b> method commits all changes made in the grammar and all of the rules.</p>
<p id="p-0211" num="0210">A specific implementation of the grammar object interface <b>1212</b> is shown below. As with the command manager object interface shown above, the implementation is specific to the WINDOWS family of operating systems by Microsoft Corp.</p>
<p id="p-0212" num="0211">
<tables id="TABLE-US-00004" num="00004">
<table frame="none" colsep="0" rowsep="0">
<tgroup align="left" colsep="0" rowsep="0" cols="2">
<colspec colname="offset" colwidth="28pt" align="left"/>
<colspec colname="1" colwidth="189pt" align="left"/>
<thead>
<row>
<entry/>
<entry namest="offset" nameend="1" align="center" rowsep="1"/>
</row>
</thead>
<tbody valign="top">
<row>
<entry/>
<entry>interface IGrammar : IUnknown, IDispatch</entry>
</row>
<row>
<entry/>
<entry>{</entry>
</row>
<row>
<entry/>
<entry>Properties :</entry>
</row>
</tbody>
</tgroup>
<tgroup align="left" colsep="0" rowsep="0" cols="2">
<colspec colname="offset" colwidth="42pt" align="left"/>
<colspec colname="1" colwidth="175pt" align="left"/>
<tbody valign="top">
<row>
<entry/>
<entry>VARIANT_BOOL Enabled (get/put)</entry>
</row>
</tbody>
</tgroup>
<tgroup align="left" colsep="0" rowsep="0" cols="2">
<colspec colname="offset" colwidth="28pt" align="left"/>
<colspec colname="1" colwidth="189pt" align="left"/>
<tbody valign="top">
<row>
<entry/>
<entry>Methods :</entry>
</row>
</tbody>
</tgroup>
<tgroup align="left" colsep="0" rowsep="0" cols="2">
<colspec colname="offset" colwidth="42pt" align="left"/>
<colspec colname="1" colwidth="175pt" align="left"/>
<tbody valign="top">
<row>
<entry/>
<entry>IDispatch * Rule(VARIANT RuleID) (get only)</entry>
</row>
<row>
<entry/>
<entry>HRESULT CreateRule ([in] VARIANT RuleID,</entry>
</row>
</tbody>
</tgroup>
<tgroup align="left" colsep="0" rowsep="0" cols="2">
<colspec colname="offset" colwidth="56pt" align="left"/>
<colspec colname="1" colwidth="161pt" align="left"/>
<tbody valign="top">
<row>
<entry/>
<entry>SPEECH_RULE_LEVEL RuleLevel,</entry>
</row>
<row>
<entry/>
<entry>SPEECH_RULE_STATE</entry>
</row>
<row>
<entry/>
<entry>RuleState, [out, retval] IDispatch **ppRule,</entry>
</row>
<row>
<entry/>
<entry>[in, optional]</entry>
</row>
<row>
<entry/>
<entry>VARIANT Prop)</entry>
</row>
</tbody>
</tgroup>
<tgroup align="left" colsep="0" rowsep="0" cols="2">
<colspec colname="offset" colwidth="42pt" align="left"/>
<colspec colname="1" colwidth="175pt" align="left"/>
<tbody valign="top">
<row>
<entry/>
<entry>HRESULT Commit( );</entry>
</row>
</tbody>
</tgroup>
<tgroup align="left" colsep="0" rowsep="0" cols="2">
<colspec colname="offset" colwidth="28pt" align="left"/>
<colspec colname="1" colwidth="189pt" align="left"/>
<tbody valign="top">
<row>
<entry/>
<entry>};</entry>
</row>
<row>
<entry/>
<entry namest="offset" nameend="1" align="center" rowsep="1"/>
</row>
</tbody>
</tgroup>
</table>
</tables>
</p>
<p id="p-0213" num="0212"> Rule Class Interface</p>
<p id="p-0214" num="0213">The Rule Class interface <b>1214</b> includes an enabled <b>1242</b> property and several methods: Add Rule <b>1244</b>, Add Phrase <b>1246</b>, Add Alternate Rule <b>1248</b> and Add Alternate Phrase <b>1250</b>. Enabled <b>1242</b>, when set, indicates whether a rule is active or inactive. Add Rule <b>1244</b> appends a rule to an existing rule structure. For example, if the rule looks like “Rule→Phrase Rule<b>1</b>” and Rule<b>2</b> is added, then a new structure results, “Rule→Phrase Rule<b>1</b> Rule<b>2</b>”.</p>
<p id="p-0215" num="0214">In the WINDOWS specific implementation shown below, Add Rule <b>1244</b> includes two parameters, plrule, which is a pointer to the rule object that will be added to the rule. Prop is an optional PROPID or PROPNAME that can be associated with the rule.</p>
<p id="p-0216" num="0215">Add Phrase <b>1246</b> appends a phrase to an existing rule structure. In the implementation shown below, the Add Phrase <b>124</b> method includes parameters text and val. Text is the text that is to be added. Val is an optional val or valstr that may be associated with the phrase. For this to be set, the rule must have been created with a property.</p>
<p id="p-0217" num="0216">Add Alternate Rule <b>1248</b> places a new rule as an optional path for the previous rule structure. For example, if the structure is “Rule→Phrase Rule<b>1</b>” and then add alternative rule<b>2</b> results in the new structure “Rule→(Phrase Rule<b>1</b>)|Rule<b>2</b>. Concatenation takes precedence over the ‘or’ operator. Add Alternate Rule <b>1248</b> includes two parameters in the WINDOWS implementation shown below. plrule is a pointer to the rule object that will be added to the rule. prop is an optional PROPID or PROPNAME that may be associated with the rule.</p>
<p id="p-0218" num="0217">Add Alternate Phrase <b>1250</b> places a new string as an optional path for the previous rule structure. If the structure is “Rule→(Phrase Rule<b>1</b>)” and alternative phrase Phrase<b>2</b> is added, the new structure is “Rule→(Phrase Rul<b>1</b>)|Rule<b>2</b>. Concatenation takes precedence over the ‘or’ operator. In the WINDOWS implementation shown below, Add Alternate Phrase <b>1250</b> includes two parameters. Text is the text to be added. Val is an optional VAL or VALSTR that is to be associated with the phrase. The rule must have been created with a property for this to be set.</p>
<p id="p-0219" num="0218">A specific implementation of the Rule Object interface <b>1214</b> is shown below. As with the other interfaces shown above, the implementation is specific to the WINDOWS family of operating systems by Microsoft Corp.</p>
<p id="p-0220" num="0219">
<tables id="TABLE-US-00005" num="00005">
<table frame="none" colsep="0" rowsep="0">
<tgroup align="left" colsep="0" rowsep="0" cols="1">
<colspec colname="1" colwidth="217pt" align="left"/>
<thead>
<row>
<entry namest="1" nameend="1" align="center" rowsep="1"/>
</row>
</thead>
<tbody valign="top">
<row>
<entry>interface IRule : IUnknown, IDispatch</entry>
</row>
<row>
<entry>{</entry>
</row>
<row>
<entry>Properties :</entry>
</row>
</tbody>
</tgroup>
<tgroup align="left" colsep="0" rowsep="0" cols="2">
<colspec colname="offset" colwidth="14pt" align="left"/>
<colspec colname="1" colwidth="203pt" align="left"/>
<tbody valign="top">
<row>
<entry/>
<entry>VARIANT_BOOL Enabled (put only)</entry>
</row>
</tbody>
</tgroup>
<tgroup align="left" colsep="0" rowsep="0" cols="1">
<colspec colname="1" colwidth="217pt" align="left"/>
<tbody valign="top">
<row>
<entry>Methods :</entry>
</row>
</tbody>
</tgroup>
<tgroup align="left" colsep="0" rowsep="0" cols="2">
<colspec colname="offset" colwidth="14pt" align="left"/>
<colspec colname="1" colwidth="203pt" align="left"/>
<tbody valign="top">
<row>
<entry/>
<entry>HRESULT AddRule ([in] IDispatch *piRule, [optional, in]</entry>
</row>
</tbody>
</tgroup>
<tgroup align="left" colsep="0" rowsep="0" cols="2">
<colspec colname="offset" colwidth="28pt" align="left"/>
<colspec colname="1" colwidth="189pt" align="left"/>
<tbody valign="top">
<row>
<entry/>
<entry>VARIANT Prop)</entry>
</row>
</tbody>
</tgroup>
<tgroup align="left" colsep="0" rowsep="0" cols="2">
<colspec colname="offset" colwidth="14pt" align="left"/>
<colspec colname="1" colwidth="203pt" align="left"/>
<tbody valign="top">
<row>
<entry/>
<entry>HRESULT AddPhrase ([in] BSTR Text, [optional, in] VARIANT</entry>
</row>
</tbody>
</tgroup>
<tgroup align="left" colsep="0" rowsep="0" cols="2">
<colspec colname="offset" colwidth="28pt" align="left"/>
<colspec colname="1" colwidth="189pt" align="left"/>
<tbody valign="top">
<row>
<entry/>
<entry>Val)</entry>
</row>
</tbody>
</tgroup>
<tgroup align="left" colsep="0" rowsep="0" cols="2">
<colspec colname="offset" colwidth="14pt" align="left"/>
<colspec colname="1" colwidth="203pt" align="left"/>
<tbody valign="top">
<row>
<entry/>
<entry>HRESULT AddAlternative Rule ([in] IDispatch *pIRule, [optional,l</entry>
</row>
</tbody>
</tgroup>
<tgroup align="left" colsep="0" rowsep="0" cols="2">
<colspec colname="offset" colwidth="28pt" align="left"/>
<colspec colname="1" colwidth="189pt" align="left"/>
<tbody valign="top">
<row>
<entry/>
<entry>in] VARIANT Prop)</entry>
</row>
</tbody>
</tgroup>
<tgroup align="left" colsep="0" rowsep="0" cols="2">
<colspec colname="offset" colwidth="14pt" align="left"/>
<colspec colname="1" colwidth="203pt" align="left"/>
<tbody valign="top">
<row>
<entry/>
<entry>HRESULT AddAlternativePhrase ([in] BSTR Text, [optional, in]</entry>
</row>
</tbody>
</tgroup>
<tgroup align="left" colsep="0" rowsep="0" cols="2">
<colspec colname="offset" colwidth="28pt" align="left"/>
<colspec colname="1" colwidth="189pt" align="left"/>
<tbody valign="top">
<row>
<entry/>
<entry>VARIANT Val)</entry>
</row>
</tbody>
</tgroup>
<tgroup align="left" colsep="0" rowsep="0" cols="1">
<colspec colname="1" colwidth="217pt" align="left"/>
<tbody valign="top">
<row>
<entry>};</entry>
</row>
<row>
<entry namest="1" nameend="1" align="center" rowsep="1"/>
</row>
</tbody>
</tgroup>
</table>
</tables>
</p>
<heading id="h-0009" level="1">EXAMPLE</heading>
<p id="p-0221" num="0220">The Rule Object interface <b>1214</b> is designed for building grammars in a BNF (Backus-Naur Format) format. The rule is composed of a Start component that is constructed of either rules or phrases. The Start component corresponds to a top-level rule. For example:</p>
<p id="p-0222" num="0221">
<tables id="TABLE-US-00006" num="00006">
<table frame="none" colsep="0" rowsep="0">
<tgroup align="left" colsep="0" rowsep="0" cols="2">
<colspec colname="offset" colwidth="63pt" align="left"/>
<colspec colname="1" colwidth="154pt" align="left"/>
<thead>
<row>
<entry/>
<entry namest="offset" nameend="1" align="center" rowsep="1"/>
</row>
</thead>
<tbody valign="top">
<row>
<entry/>
<entry>S → A B | C</entry>
</row>
<row>
<entry/>
<entry>A → “I like”</entry>
</row>
<row>
<entry/>
<entry>B → “Candy” | “Food”</entry>
</row>
<row>
<entry/>
<entry>C → “Orange is a great color”</entry>
</row>
<row>
<entry/>
<entry namest="offset" nameend="1" align="center" rowsep="1"/>
</row>
</tbody>
</tgroup>
</table>
</tables>
</p>
<p id="p-0223" num="0222">There are four rules here (S, A, B, C). There are four phrases: “I like”; “Candy”; “Food”; and “Orange is a great color.” This grammar allows three phrases to be said by the user “I like candy,” “I like food,” or “Orange is a great color.” To construct this, assume four rules have been created by a grammar object and then build the rules.</p>
<p id="p-0224" num="0223">S.AddRule(A)</p>
<p id="p-0225" num="0224">S.AddRule(B)</p>
<p id="p-0226" num="0225">S.AddAlternativeRule(C)</p>
<p id="p-0227" num="0226">A.AddPhrase(“I like”)</p>
<p id="p-0228" num="0227">B.AddPhrase(“Candy”)</p>
<p id="p-0229" num="0228">B.AddAlternativePhrase(“Food”)</p>
<p id="p-0230" num="0229">C.AddPhrase(“Orange is a great color.”</p>
<p id="p-0231" num="0230"> Word Trainer Control</p>
<p id="p-0232" num="0231">The word trainer control <b>158</b> provides an easy way to implement a speech-oriented work-training interaction with a user, in support of tasks that involve voice tags, such as speed-dial entries or radio station names. The entire word training process is implemented with a combination of the word trainer control and other GUI (graphical user interface) or SUI (speech user interface) controls. The word trainer primarily focuses on the process of adding the user's way of saying a phrase or verbally referencing an object in the recognizer's lexicon.</p>
<p id="p-0233" num="0232">It is noted that the Word Trainer control <b>158</b> wraps the word trainer API (application programming interface) provided by MICROSOFT CORP. The features discussed below are available on the word trainer control <b>158</b>.</p>
<p id="p-0234" num="0233">An example of a functional scenario for the word trainer control is a user initiating voice tag training to complete creating a speed-dial entry for “Mom.” The system prompts the user to say the name of the called party. The user responds, “Mom.” Training is then complete.</p>
<p id="p-0235" num="0234">Another example of a functional scenario for the word trainer control is a user who wants to place a call via voice command, but cannot remember the voice tag that was previously trained. The system helps the user using a question control: “Choose who you'd like to call by repeating the name. &lt;Mom.wav&gt;, &lt;Dad.wav&gt; or &lt;work.wav&gt;.</p>
<p id="p-0236" num="0235">The following Tables (Tables 3-5) illustrate possible word training sessions that are supported by the word training control <b>158</b>.</p>
<p id="p-0237" num="0236">
<tables id="TABLE-US-00007" num="00007">
<table frame="none" colsep="0" rowsep="0">
<tgroup align="left" colsep="0" rowsep="0" cols="1">
<colspec colname="1" colwidth="217pt" align="center"/>
<thead>
<row>
<entry namest="1" nameend="1" rowsep="1">TABLE 3</entry>
</row>
</thead>
<tbody valign="top">
<row>
<entry namest="1" nameend="1" align="center" rowsep="1"/>
</row>
<row>
<entry>Scenario “A”</entry>
</row>
</tbody>
</tgroup>
<tgroup align="left" colsep="0" rowsep="0" cols="3">
<colspec colname="1" colwidth="35pt" align="left"/>
<colspec colname="2" colwidth="49pt" align="left"/>
<colspec colname="3" colwidth="133pt" align="left"/>
<tbody valign="top">
<row>
<entry>WHO</entry>
<entry>WHAT</entry>
<entry>DETAIL</entry>
</row>
<row>
<entry namest="1" nameend="3" align="center" rowsep="1"/>
</row>
<row>
<entry>System</entry>
<entry>Prompt</entry>
<entry>“Say name twice; Please say name”</entry>
</row>
<row>
<entry>System</entry>
<entry>Earcon</entry>
<entry>Signals user to start utterance</entry>
</row>
<row>
<entry>System</entry>
<entry>AutoPTT</entry>
<entry>Lets user talk w/o manual PTT</entry>
</row>
<row>
<entry>User</entry>
<entry>Utterance</entry>
<entry>Says “Mom”</entry>
</row>
<row>
<entry>System</entry>
<entry>Feedback</entry>
<entry>Plays &lt;Mom.wav&gt;</entry>
</row>
<row>
<entry>System</entry>
<entry>Prompt</entry>
<entry>“Please say the name again”</entry>
</row>
<row>
<entry>System</entry>
<entry>Earcon</entry>
<entry>Signals user to start utterance</entry>
</row>
<row>
<entry>System</entry>
<entry>AutoPTT</entry>
<entry>Lets user talk w/o manual PTT</entry>
</row>
<row>
<entry>User</entry>
<entry>Utterance</entry>
<entry>Says “Mom”</entry>
</row>
<row>
<entry>System</entry>
<entry>Feedback</entry>
<entry>Plays &lt;Mom.wav&gt;</entry>
</row>
<row>
<entry>System</entry>
<entry>Question</entry>
<entry>“OK to continue?”</entry>
</row>
<row>
<entry>System</entry>
<entry>Announcement</entry>
<entry>“You can now dial by saying &lt;Mom.wav&gt;</entry>
</row>
<row>
<entry namest="1" nameend="3" align="center" rowsep="1"/>
</row>
</tbody>
</tgroup>
</table>
</tables>
</p>
<p id="p-0238" num="0237">
<tables id="TABLE-US-00008" num="00008">
<table frame="none" colsep="0" rowsep="0">
<tgroup align="left" colsep="0" rowsep="0" cols="1">
<colspec colname="1" colwidth="217pt" align="center"/>
<thead>
<row>
<entry namest="1" nameend="1" rowsep="1">TABLE 4</entry>
</row>
</thead>
<tbody valign="top">
<row>
<entry namest="1" nameend="1" align="center" rowsep="1"/>
</row>
<row>
<entry>Scenario “B”</entry>
</row>
</tbody>
</tgroup>
<tgroup align="left" colsep="0" rowsep="0" cols="4">
<colspec colname="offset" colwidth="14pt" align="left"/>
<colspec colname="1" colwidth="35pt" align="left"/>
<colspec colname="2" colwidth="42pt" align="left"/>
<colspec colname="3" colwidth="126pt" align="left"/>
<tbody valign="top">
<row>
<entry/>
<entry>WHO</entry>
<entry>WHAT</entry>
<entry>DETAIL</entry>
</row>
<row>
<entry/>
<entry namest="offset" nameend="3" align="center" rowsep="1"/>
</row>
<row>
<entry/>
<entry>System</entry>
<entry>Prompt</entry>
<entry>“Please say name”</entry>
</row>
<row>
<entry/>
<entry>User</entry>
<entry>PTT</entry>
<entry>User pushes PTT</entry>
</row>
<row>
<entry/>
<entry>System</entry>
<entry>Earcon</entry>
<entry>Signals PTT pushed, ready to record</entry>
</row>
<row>
<entry/>
<entry>User</entry>
<entry>Utterance</entry>
<entry>Says “Mom”</entry>
</row>
<row>
<entry/>
<entry>System</entry>
<entry>Earcon</entry>
<entry>Signals recording successful</entry>
</row>
<row>
<entry/>
<entry namest="offset" nameend="3" align="center" rowsep="1"/>
</row>
</tbody>
</tgroup>
</table>
</tables>
</p>
<p id="p-0239" num="0238">
<tables id="TABLE-US-00009" num="00009">
<table frame="none" colsep="0" rowsep="0">
<tgroup align="left" colsep="0" rowsep="0" cols="1">
<colspec colname="1" colwidth="217pt" align="center"/>
<thead>
<row>
<entry namest="1" nameend="1" rowsep="1">TABLE 5</entry>
</row>
</thead>
<tbody valign="top">
<row>
<entry namest="1" nameend="1" align="center" rowsep="1"/>
</row>
<row>
<entry>Scenario “C”</entry>
</row>
</tbody>
</tgroup>
<tgroup align="left" colsep="0" rowsep="0" cols="3">
<colspec colname="1" colwidth="35pt" align="left"/>
<colspec colname="2" colwidth="56pt" align="left"/>
<colspec colname="3" colwidth="126pt" align="left"/>
<tbody valign="top">
<row>
<entry>WHO</entry>
<entry>WHAT</entry>
<entry>DETAIL</entry>
</row>
<row>
<entry namest="1" nameend="3" align="center" rowsep="1"/>
</row>
<row>
<entry>System</entry>
<entry>GUI Dialogue</entry>
<entry>Includes buttons for two training passes</entry>
</row>
<row>
<entry>User</entry>
<entry>Pushes #1</entry>
<entry>Starts training pass #1</entry>
</row>
<row>
<entry>System</entry>
<entry>Earcon</entry>
<entry>Signals PTT; Ready to record</entry>
</row>
<row>
<entry>System</entry>
<entry>AutoPTT</entry>
<entry>Lets user talk w/o manual PTT</entry>
</row>
<row>
<entry>User</entry>
<entry>Utterance</entry>
<entry>Says “Mom”</entry>
</row>
<row>
<entry>System</entry>
<entry>Feedback</entry>
<entry>Plays .wav of “Mom”</entry>
</row>
<row>
<entry>System</entry>
<entry>Disables #1</entry>
<entry>Shows that pass #1 remains</entry>
</row>
<row>
<entry>User</entry>
<entry>Pushes #1</entry>
<entry>Starts training pass #2</entry>
</row>
<row>
<entry>System</entry>
<entry>Earcon</entry>
<entry>Signals PTT; Ready to record</entry>
</row>
<row>
<entry>System</entry>
<entry>AutoPTT</entry>
<entry>Lets user talk w/o manual PTT</entry>
</row>
<row>
<entry>User</entry>
<entry>Utterance</entry>
<entry>Says “Mom”</entry>
</row>
<row>
<entry>System</entry>
<entry>Feedback</entry>
<entry>Plays .wav of “Mom”</entry>
</row>
<row>
<entry>System</entry>
<entry>Disables #2</entry>
<entry>Shows that pass #2 remains</entry>
</row>
<row>
<entry>System</entry>
<entry>GUI Dialogue</entry>
<entry>“Voice tag created”</entry>
</row>
<row>
<entry namest="1" nameend="3" align="center" rowsep="1"/>
</row>
</tbody>
</tgroup>
</table>
</tables>
</p>
<p id="p-0240" num="0239">Word Trainer is a control, such as an ActiveX control, that a developer can include in an application for the purpose of initiating and managing a training user interface process. All of the interfaces exposed by the Word Trainer API (MICROSOFT CORP.)</p>
<p id="p-0241" num="0240">Table 6 identifies word trainer control <b>158</b> properties. It is noted that these properties are in addition to Word Trainer API (MICROSOFT CORP.) properties and methods wrapped by the word trainer control <b>158</b>.</p>
<p id="p-0242" num="0241">
<tables id="TABLE-US-00010" num="00010">
<table frame="none" colsep="0" rowsep="0">
<tgroup align="left" colsep="0" rowsep="0" cols="3">
<colspec colname="offset" colwidth="35pt" align="left"/>
<colspec colname="1" colwidth="98pt" align="left"/>
<colspec colname="2" colwidth="84pt" align="left"/>
<thead>
<row>
<entry/>
<entry namest="offset" nameend="2" rowsep="1">TABLE 6</entry>
</row>
<row>
<entry/>
<entry namest="offset" nameend="2" align="center" rowsep="1"/>
</row>
<row>
<entry/>
<entry>PROPERTY</entry>
<entry>TYPE</entry>
</row>
<row>
<entry/>
<entry namest="offset" nameend="2" align="center" rowsep="1"/>
</row>
</thead>
<tbody valign="top">
<row>
<entry/>
<entry>Type</entry>
<entry>Enumeration</entry>
</row>
<row>
<entry/>
<entry>Interrupting</entry>
<entry>Boolean</entry>
</row>
<row>
<entry/>
<entry>Feedback</entry>
<entry>Enumeration</entry>
</row>
<row>
<entry/>
<entry>PassesRemaining</entry>
<entry>Integer</entry>
</row>
<row>
<entry/>
<entry namest="offset" nameend="2" align="center" rowsep="1"/>
</row>
</tbody>
</tgroup>
</table>
</tables>
</p>
<heading id="h-0010" level="1">Word Trainer Control Properties</heading>
<p id="p-0243" num="0242">The word trainer control <b>158</b> supports the Type property that can be used to determine the behavioral or content characteristics of the control. It is noted that it is the Type property that ultimately determines the style class and properties used in defining the control's behavior. The Type property's valid values are defined in the system's current speech theme.</p>
<p id="p-0244" num="0243">The Interrupting property determines whether the control will interrupt other interactions in the interaction list <b>168</b> of the interaction manager <b>160</b>. If the Interrupting property has a value of “True,” then the control immediately interrupts any other interaction in the interaction list <b>168</b>. If the value is “False,” then the control does not interrupt, but places interactions at the end of the interaction list <b>168</b>.</p>
<p id="p-0245" num="0244">The Feedback property determines if the word trainer control <b>158</b> will play feedback automatically after the system successfully records the user. If the Feedback property has no value (or a value of ‘none’), then the word trainer control <b>158</b> doesn't play feedback when the user makes a choice. If the Feedback property has a value of “Earcon,” then the word trainer control <b>158</b> plays a completion earcon resource after a successful recording. If the value is “Echo recording,” then the word trainer control <b>158</b> plays a sound file of the user's recording.</p>
<p id="p-0246" num="0245">The PassesRemaining property is a read-only property that tells an application how many recording passes the engine requires before a usable voice tag exists. It is intended that, as this number decrements, the application user interface reflects course progress through the training process.</p>
<p id="p-0247" num="0246">In addition to the foregoing, the word trainer control <b>158</b> includes a StartRecording method. The StartRecording method initiates the recording process for one pass. When recording completes successfully, the PassesRemaining property decrements. It is noted that, in the cases where the speech engine can accept additional recordings, an application may call StartRecording even though PassesRemaining equals zero.</p>
<p id="p-0248" num="0247">It is noted that other speech recognition grammars must be temporarily disabled when the speech engine is in a recording mode.</p>
<p id="p-0249" num="0248">Exemplary Computer Environment</p>
<p id="p-0250" num="0249">The various components and functionality described herein are implemented with a number of individual computers. <figref idref="DRAWINGS">FIG. 9</figref> shows components of typical example of such a computer, referred by to reference numeral <b>900</b>. The components shown in <figref idref="DRAWINGS">FIG. 9</figref> are only examples, and are not intended to suggest any limitation as to the scope of the functionality of the invention; the invention is not necessarily dependent on the features shown in <figref idref="DRAWINGS">FIG. 9</figref>.</p>
<p id="p-0251" num="0250">Generally, various different general purpose or special purpose computing system configurations can be used. Examples of well known computing systems, environments, and/or configurations that may be suitable for use with the invention include, but are not limited to, personal computers, server computers, hand-held or laptop devices, multiprocessor systems, microprocessor-based systems, set top boxes, programmable consumer electronics, network PCs, minicomputers, mainframe computers, distributed computing environments that include any of the above systems or devices, and the like.</p>
<p id="p-0252" num="0251">The functionality of the computers is embodied in many cases by computer-executable instructions, such as program modules, that are executed by the computers. Generally, program modules include routines, programs, objects, components, data structures, etc. that perform particular tasks or implement particular abstract data types. Tasks might also be performed by remote processing devices that are linked through a communications network. In a distributed computing environment, program modules may be located in both local and remote computer storage media.</p>
<p id="p-0253" num="0252">The instructions and/or program modules are stored at different times in the various computer-readable media that are either part of the computer or that can be read by the computer. Programs are typically distributed, for example, on floppy disks, CD-ROMs, DVD, or some form of communication media such as a modulated signal. From there, they are installed or loaded into the secondary memory of a computer. At execution, they are loaded at least partially into the computer's primary electronic memory. The invention described herein includes these and other various types of computer-readable media when such media contain instructions programs, and/or modules for implementing the steps described below in conjunction with a microprocessor or other data processors. The invention also includes the computer itself when programmed according to the methods and techniques described below.</p>
<p id="p-0254" num="0253">For purposes of illustration, programs and other executable program components such as the operating system are illustrated herein as discrete blocks, although it is recognized that such programs and components reside at various times in different storage components of the computer, and are executed by the data processor(s) of the computer.</p>
<p id="p-0255" num="0254">With reference to <figref idref="DRAWINGS">FIG. 9</figref>, the components of computer <b>900</b> may include, but are not limited to, a processing unit <b>920</b>, a system memory <b>930</b>, and a system bus <b>921</b> that couples various system components including the system memory to the processing unit <b>920</b>. The system bus <b>921</b> may be any of several types of bus structures including a memory bus or memory controller, a peripheral bus, and a local bus using any of a variety of bus architectures. By way of example, and not limitation, such architectures include Industry Standard Architecture (ISA) bus, Micro Channel Architecture (MCA) bus, Enhanced ISA (EISAA) bus, Video Electronics Standards Association (VESA) local bus, and Peripheral Component Interconnect (PCI) bus also known as the Mezzanine bus.</p>
<p id="p-0256" num="0255">Computer <b>900</b> typically includes a variety of computer-readable media. Computer-readable media can be any available media that can be accessed by computer <b>900</b> and includes both volatile and nonvolatile media, removable and non-removable media. By way of example, and not limitation, computer-readable media may comprise computer storage media and communication media. “Computer storage media” includes both volatile and nonvolatile, removable and non-removable media implemented in any method or technology for storage of information such as computer-readable instructions, data structures, program modules, or other data. Computer storage media includes, but is not limited to, RAM, ROM, EEPROM, flash memory or other memory technology, CD-ROM, digital versatile disks (DVD) or other optical disk storage, magnetic cassettes, magnetic tape, magnetic disk storage or other magnetic storage devices, or any other medium which can be used to store the desired information and which can be accessed by computer <b>910</b>. Communication media typically embodies computer-readable instructions, data structures, program modules or other data in a modulated data signal such as a carrier wave or other transport mechanism and includes any information delivery media. The term “modulated data signal” means a signal that has one or more if its characteristics set or changed in such a manner as to encode information in the signal. By way of example, and not limitation, communication media includes wired media such as a wired network or direct-wired connection and wireless media such as acoustic, RF, infrared and other wireless media. Combinations of any of the above should also be included within the scope of computer readable media.</p>
<p id="p-0257" num="0256">The system memory <b>930</b> includes computer storage media in the form of volatile and/or nonvolatile memory such as read only memory (ROM) <b>931</b> and random access memory (RAM) <b>932</b>. A basic input/output system <b>933</b> (BIOS), containing the basic routines that help to transfer information between elements within computer <b>900</b>, such as during start-up, is typically stored in ROM <b>931</b>. RAM <b>932</b> typically contains data and/or program modules that are immediately accessible to and/or presently being operated on by processing unit <b>920</b>. By way of example, and not limitation, <figref idref="DRAWINGS">FIG. 9</figref> illustrates operating system <b>934</b>, application programs <b>935</b>, other program modules <b>936</b>, and program data <b>937</b>.</p>
<p id="p-0258" num="0257">The computer <b>900</b> may also include other removable/non-removable, volatile/nonvolatile computer storage media. By way of example only, <figref idref="DRAWINGS">FIG. 9</figref> illustrates a hard disk drive <b>941</b> that reads from or writes to non-removable, nonvolatile magnetic media, a magnetic disk drive <b>951</b> that reads from or writes to a removable, nonvolatile magnetic disk <b>952</b>, and an optical disk drive <b>955</b> that reads from or writes to a removable, nonvolatile optical disk <b>956</b> such as a CD ROM or other optical media. Other removable/non-removable, volatile/nonvolatile computer storage media that can be used in the exemplary operating environment include, but are not limited to, magnetic tape cassettes, flash memory cards, digital versatile disks, digital video tape, solid state RAM, solid state ROM, and the like. The hard disk drive <b>941</b> is typically connected to the system bus <b>921</b> through an non-removable memory interface such as interface <b>940</b>, and magnetic disk drive <b>951</b> and optical disk drive <b>955</b> are typically connected to the system bus <b>921</b> by a removable memory interface such as interface <b>950</b>.</p>
<p id="p-0259" num="0258">The drives and their associated computer storage media discussed above and illustrated in <figref idref="DRAWINGS">FIG. 9</figref> provide storage of computer-readable instructions, data structures, program modules, and other data for computer <b>900</b>. In <figref idref="DRAWINGS">FIG. 9</figref>, for example, hard disk drive <b>941</b> is illustrated as storing operating system <b>944</b>, application programs <b>945</b>, other program modules <b>946</b>, and program data <b>947</b>. Note that these components can either be the same as or different from operating system <b>934</b>, application programs <b>935</b>, other program modules <b>936</b>, and program data <b>937</b>. Operating system <b>944</b>, application programs <b>945</b>, other program modules <b>946</b>, and program data <b>947</b> are given different numbers here to illustrate that, at a minimum, they are different copies. A user may enter commands and information into the computer <b>900</b> through input devices such as a keyboard <b>962</b> and pointing device <b>961</b>, commonly referred to as a mouse, trackball, or touch pad. Other input devices (not shown) may include a microphone, joystick, game pad, satellite dish, scanner, or the like. These and other input devices are often connected to the processing unit <b>920</b> through a user input interface <b>960</b> that is coupled to the system bus, but may be connected by other interface and bus structures, such as a parallel port, game port, or a universal serial bus (USB). A monitor <b>991</b> or other type of display device is also connected to the system bus <b>921</b> via an interface, such as a video interface <b>990</b>. In addition to the monitor, computers may also include other peripheral output devices such as speakers <b>997</b> and printer <b>996</b>, which may be connected through an output peripheral interface <b>995</b>.</p>
<p id="p-0260" num="0259">The computer may operate in a networked environment using logical connections to one or more remote computers, such as a remote computer <b>980</b>. The remote computer <b>980</b> may be a personal computer, a server, a router, a network PC, a peer device or other common network node, and typically includes many or all of the elements described above relative to computer <b>900</b>, although only a memory storage device <b>981</b> has been illustrated in <figref idref="DRAWINGS">FIG. 9</figref>. The logical connections depicted in <figref idref="DRAWINGS">FIG. 9</figref> include a local area network (LAN) <b>971</b> and a wide area network (WAN) <b>973</b>, but may also include other networks. Such networking environments are commonplace in offices, enterprise-wide computer networks, intranets, and the Internet.</p>
<p id="p-0261" num="0260">When used in a LAN networking environment, the computer <b>900</b> is connected to the LAN <b>971</b> through a network interface or adapter <b>970</b>. When used in a WAN networking environment, the computer <b>900</b> typically includes a modem <b>972</b> or other means for establishing communications over the WAN <b>973</b>, such as the Internet. The modem <b>972</b>, which may be internal or external, may be connected to the system bus <b>921</b> via the user input interface <b>960</b>, or other appropriate mechanism. In a networked environment, program modules depicted relative to the computer <b>900</b>, or portions thereof, may be stored in the remote memory storage device. By way of example, and not limitation, <figref idref="DRAWINGS">FIG. 9</figref> illustrates remote application programs <b>985</b> as residing on memory device <b>981</b>. It will be appreciated that the network connections shown are exemplary and other means of establishing a communications link between the computers may be used.</p>
<heading id="h-0011" level="1">CONCLUSION</heading>
<p id="p-0262" num="0261">The systems and methods as described, thus provide a way to manage interactions from multiple applications, even if two or more of the multiple applications use different grammars. Implementation of the systems and methods described herein provide orderly processing of interactions from multiple applications so a user can more easily communicate with the applications.</p>
<p id="p-0263" num="0262">Although details of specific implementations and embodiments are described above, such details are intended to satisfy statutory disclosure obligations rather than to limit the scope of the following claims. Thus, the invention as defined by the claims is not limited to the specific features described above. Rather, the invention is claimed in any of its forms or modifications that fall within the proper scope of the appended claims, appropriately interpreted in accordance with the doctrine of equivalents.</p>
<?DETDESC description="Detailed Description" end="tail"?>
</description>
<us-claim-statement>The invention claimed is:</us-claim-statement>
<claims id="claims">
<claim id="CLM-00001" num="00001">
<claim-text>1. A method for processing speech interactions from multiple speech-enabled applications, comprising:
<claim-text>receiving an interaction submitted by a speech-enabled application;</claim-text>
<claim-text>determining if the interaction is an interrupting interaction;</claim-text>
<claim-text>placing the interaction in an interaction list having a front and a back and containing from one to several interactions to be processed in order from the front to the back, wherein the interaction is placed at the back of the interaction list unless the interaction is an interrupting interaction, in which case the interaction is placed at the front of the interaction list; and</claim-text>
<claim-text>interrupting an interaction currently processing when the interrupting interaction is placed at the front of the list, and the interrupted interaction does not resume processing after the interrupting interaction is processed if a self-destruct flag is set in the interrupted interaction.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00002" num="00002">
<claim-text>2. The method as recited in <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the interaction list is a list of pointers to memory locations that contain the interactions submitted by one of the speech-enabled applications to the interaction list.</claim-text>
</claim>
<claim id="CLM-00003" num="00003">
<claim-text>3. The method as recited in <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising sending one or more status messages to the application that submitted the interrupted interaction of the status of the processing of the submitted interaction.</claim-text>
</claim>
<claim id="CLM-00004" num="00004">
<claim-text>4. The method as recited in <claim-ref idref="CLM-00003">claim 3</claim-ref>, wherein the one or more status messages sent to the application that submitted the interrupted interaction further comprises one or more of the following messages: interaction activated; interaction interrupted; interaction self-destructed; interaction re-activated; and/or interaction completed.</claim-text>
</claim>
<claim id="CLM-00005" num="00005">
<claim-text>5. The method as recited in <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the determining if the interaction is an interrupting interaction further comprises checking for a presence of an interruption flag in the interaction that, if present, indicates that the interaction is an interrupting interaction.</claim-text>
</claim>
<claim id="CLM-00006" num="00006">
<claim-text>6. The method as recited in <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the interrupted interaction is a first interaction and the method further comprising waiting a pre-determined grace period after the first interaction is processed before beginning processing of a second interaction submitted from a different speech-enabled application than the speech-enabled application that submitted the first interaction.</claim-text>
</claim>
<claim id="CLM-00007" num="00007">
<claim-text>7. The method as recited in <claim-ref idref="CLM-00006">claim 6</claim-ref>, further comprising:
<claim-text>receiving a third interaction during the grace period, the third interaction being submitted from the speech-enabled application that submitted the first interaction; and</claim-text>
<claim-text>processing the third interaction prior to processing the second interaction.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00008" num="00008">
<claim-text>8. method for processing speech interactions from multiple speech-enabled applications, comprising:
<claim-text>receiving an interaction submitted by a speech-enabled application;</claim-text>
<claim-text>determining if the interaction is an interrupting interaction;</claim-text>
<claim-text>placing the interaction in an interaction list having a front and a back and containing from one to several interactions to be processed in order from the front to the back,</claim-text>
<claim-text>placing an interaction received from a speech-enabled application at the end of the interaction list unless an interaction manager detects an indication to place the interaction received from the speech-enabled application at the front of the interaction list ahead of other interactions in the interaction list;</claim-text>
<claim-text>providing a pre-determined grace period after processing a first interaction submitted by one of the speech-enabled applications before beginning to process a second interaction; and</claim-text>
<claim-text>placing a chained interaction at the front of the interaction list if the chained interaction is received during the pre-determined grace period after processing the first interaction and if the chained interaction is submitted by the same speech-enabled application that submitted the first interaction, even if the chained interaction does not indicate that it is to be placed at the front of the list and the chained interaction is submitted after the second interaction is submitted.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00009" num="00009">
<claim-text>9. The method as recited in <claim-ref idref="CLM-00008">claim 8</claim-ref>, wherein the interaction list is a list of pointers to memory locations that contain the interactions submitted by the speech-enabled applications to the interaction list.</claim-text>
</claim>
<claim id="CLM-00010" num="00010">
<claim-text>10. The method as recited in <claim-ref idref="CLM-00008">claim 8</claim-ref>, further comprising sending one or more status messages to the speech-enabled application that submitted the interaction of the status of the processing of the submitted interaction.</claim-text>
</claim>
<claim id="CLM-00011" num="00011">
<claim-text>11. The method as recited in <claim-ref idref="CLM-00010">claim 10</claim-ref>, wherein the one or more status messages sent to the speech-enabled application that submitted the interaction further comprises one or more of the following messages: interaction activated; interaction interrupted; interaction self-destructed; interaction re-activated; and/or interaction completed.</claim-text>
</claim>
<claim id="CLM-00012" num="00012">
<claim-text>12. The method as recited in <claim-ref idref="CLM-00008">claim 8</claim-ref>, wherein the determining if the interaction is an interrupting interaction further comprises checking for a presence of an interruption flag in the interaction that, if present, indicates that the interaction is an interrupting interaction.</claim-text>
</claim>
<claim id="CLM-00013" num="00013">
<claim-text>13. The method as recited in <claim-ref idref="CLM-00008">claim 8</claim-ref>, wherein the receiving interaction is a first interaction that is active and the method further comprising waiting a pre-determined grace period after the first interaction is processed before beginning processing of a second interaction submitted from a different speech-enabled application than the speech-enabled application that submitted the first interaction.</claim-text>
</claim>
<claim id="CLM-00014" num="00014">
<claim-text>14. The method as recited in <claim-ref idref="CLM-00013">claim 13</claim-ref>, further comprising:
<claim-text>receiving a third interaction during the grace period, the third interaction being submitted from the speech-enabled application that submitted the first interaction; and</claim-text>
<claim-text>processing the third interaction prior to processing the second interaction.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00015" num="00015">
<claim-text>15. A computer readable medium containing instructions that when executed by a computing device, perform actions, including:
<claim-text>receiving an interaction submitted by a speech-enabled application;</claim-text>
<claim-text>determining if the interaction is an interrupting interaction;</claim-text>
<claim-text>placing the interaction in an interaction list having a front and a back and containing from one to several interactions to be processed in order from the front to the back, wherein the interaction is placed at the back of the interaction list unless the interaction is an interrupting interaction, in which case the interaction is placed at the front of the interaction list; and</claim-text>
<claim-text>interrupting an interaction currently processing when the interaction is placed at the front of the list, and the interrupted interaction does not resume processing after the interrupting interaction is processed if a self destruct flag is set in the interrupted interaction.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00016" num="00016">
<claim-text>16. The computer readable medium as recited in <claim-ref idref="CLM-00015">claim 15</claim-ref>, wherein the interaction list is a list of pointers to memory locations that contain the interactions submitted by one of the speech-enabled applications to the interaction list.</claim-text>
</claim>
<claim id="CLM-00017" num="00017">
<claim-text>17. The computer readable medium as recited in <claim-ref idref="CLM-00015">claim 15</claim-ref>, further comprising instructions for sending one or more status messages to the application that submitted the interrupted interaction of the status of the processing of the submitted interaction.</claim-text>
</claim>
<claim id="CLM-00018" num="00018">
<claim-text>18. The computer readable medium as recited in <claim-ref idref="CLM-00017">claim 17</claim-ref>, wherein the one or more status messages sent to the application that submitted the interrupted interaction further comprises one or more of the following messages: interaction activated; interaction interrupted; interaction self-destructed; interaction re-activated; and/or interaction completed.</claim-text>
</claim>
<claim id="CLM-00019" num="00019">
<claim-text>19. The computer readable medium as recited in <claim-ref idref="CLM-00015">claim 15</claim-ref>, wherein the determining if the interaction is an interrupting interaction further comprises checking for a presence of an interruption flag in the interaction that, if present, indicates that the interaction is an interrupting interaction.</claim-text>
</claim>
<claim id="CLM-00020" num="00020">
<claim-text>20. The computer readable medium as recited in <claim-ref idref="CLM-00015">claim 15</claim-ref>, wherein the receiving interaction is a first interaction and the instructions further comprise instructions for waiting a pre-determined grace period after the first interaction is processed before beginning processing of a second interaction submitted from a different speech-enabled application than the speech-enabled application that submitted the first interaction.</claim-text>
</claim>
</claims>
</us-patent-grant>
