<us-patent-grant lang="EN" dtd-version="v4.2 2006-08-23" file="US07298871-20071120.XML" status="PRODUCTION" id="us-patent-grant" country="US" date-produced="20071106" date-publ="20071120">
<us-bibliographic-data-grant>
<publication-reference>
<document-id>
<country>US</country>
<doc-number>07298871</doc-number>
<kind>B2</kind>
<date>20071120</date>
</document-id>
</publication-reference>
<application-reference appl-type="utility">
<document-id>
<country>US</country>
<doc-number>10165286</doc-number>
<date>20020607</date>
</document-id>
</application-reference>
<us-application-series-code>10</us-application-series-code>
<us-term-of-grant>
<us-term-extension>806</us-term-extension>
</us-term-of-grant>
<classifications-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>G</section>
<class>06</class>
<subclass>K</subclass>
<main-group>9</main-group>
<subgroup>00</subgroup>
<symbol-position>F</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20071120</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
</classifications-ipcr>
<classification-national>
<country>US</country>
<main-classification>382115</main-classification>
</classification-national>
<invention-title id="d0e53">System and method for adapting the ambience of a local environment according to the location and personal preferences of people in the local environment</invention-title>
<references-cited>
<citation>
<patcit num="00001">
<document-id>
<country>US</country>
<doc-number>5745126</doc-number>
<kind>A</kind>
<name>Jain et al.</name>
<date>19980400</date>
</document-id>
</patcit>
<category>cited by other</category>
<classification-national><country>US</country><main-classification>345952</main-classification></classification-national>
</citation>
<citation>
<patcit num="00002">
<document-id>
<country>US</country>
<doc-number>5835616</doc-number>
<kind>A</kind>
<name>Lobo et al.</name>
<date>19981100</date>
</document-id>
</patcit>
<category>cited by other</category>
<classification-national><country>US</country><main-classification>382118</main-classification></classification-national>
</citation>
<citation>
<patcit num="00003">
<document-id>
<country>US</country>
<doc-number>6223992</doc-number>
<kind>B1</kind>
<name>Yasui et al.</name>
<date>20010500</date>
</document-id>
</patcit>
<category>cited by other</category>
<classification-national><country>US</country><main-classification>236 46 R</main-classification></classification-national>
</citation>
<citation>
<patcit num="00004">
<document-id>
<country>US</country>
<doc-number>6400835</doc-number>
<kind>B1</kind>
<name>Lemelson et al.</name>
<date>20020600</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>382118</main-classification></classification-national>
</citation>
<citation>
<patcit num="00005">
<document-id>
<country>US</country>
<doc-number>6548967</doc-number>
<kind>B1</kind>
<name>Dowling et al.</name>
<date>20030400</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>315318</main-classification></classification-national>
</citation>
<citation>
<patcit num="00006">
<document-id>
<country>US</country>
<doc-number>6625503</doc-number>
<kind>B1</kind>
<name>Smith</name>
<date>20030900</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>700 83</main-classification></classification-national>
</citation>
<citation>
<patcit num="00007">
<document-id>
<country>US</country>
<doc-number>6934917</doc-number>
<kind>B2</kind>
<name>Lin</name>
<date>20050800</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>715811</main-classification></classification-national>
</citation>
<citation>
<patcit num="00008">
<document-id>
<country>US</country>
<doc-number>2002/0007510</doc-number>
<kind>A1</kind>
<name>Mann</name>
<date>20020100</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>  4300</main-classification></classification-national>
</citation>
<citation>
<patcit num="00009">
<document-id>
<country>EP</country>
<doc-number>1102500</doc-number>
<date>20010500</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<patcit num="00010">
<document-id>
<country>WO</country>
<doc-number>WO9747066</doc-number>
<date>19971200</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<patcit num="00011">
<document-id>
<country>WO</country>
<doc-number>WO0031560</doc-number>
<date>20000600</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<patcit num="00012">
<document-id>
<country>WO</country>
<doc-number>WO0152478</doc-number>
<date>20010700</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<patcit num="00013">
<document-id>
<country>WO</country>
<doc-number>WO0159622</doc-number>
<date>20010800</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<patcit num="00014">
<document-id>
<country>WO</country>
<doc-number>WO0179952</doc-number>
<date>20011000</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<nplcit num="00015">
<othercit>McKenna, Stephen et al., <i>Tracking Faces</i>, Proceedings of the Second Int'l Conference on Automatic Face and Gesture Recognition, Oct. 14-16, 1996, Killington VT, pp. 271-276.</othercit>
</nplcit>
<category>cited by other</category>
</citation>
<citation>
<nplcit num="00016">
<othercit>Gutta, S., et al., <i>Hand Gesture Recognition Using Ensembles Of Radial Basis Function (RBP) Networks And Decision Trees</i>, Int'l J. Of Pattern Recognition and Artificial Intelligence, vol. 11, No. 6, pp. 845-874 (1997).</othercit>
</nplcit>
<category>cited by other</category>
</citation>
<citation>
<nplcit num="00017">
<othercit>Gutta, S., et al. <i>Mixture Of Experts For Classification of Gender, Ethnic Origin and Pose of Human Faces</i>, IEEE Transactions On Neural Networks, vol. 11, No. 4, (Jul. 2000), pp. 948-960.</othercit>
</nplcit>
<category>cited by other</category>
</citation>
</references-cited>
<number-of-claims>24</number-of-claims>
<us-exemplary-claim>1</us-exemplary-claim>
<us-field-of-classification-search>
<classification-national>
<country>US</country>
<main-classification>382115</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>382118</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>382155</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>700 47</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>700 48</main-classification>
</classification-national>
</us-field-of-classification-search>
<figures>
<number-of-drawing-sheets>3</number-of-drawing-sheets>
<number-of-figures>4</number-of-figures>
</figures>
<us-related-documents>
<related-publication>
<document-id>
<country>US</country>
<doc-number>20030227439</doc-number>
<kind>A1</kind>
<date>20031211</date>
</document-id>
</related-publication>
</us-related-documents>
<parties>
<applicants>
<applicant sequence="001" app-type="applicant-inventor" designation="us-only">
<addressbook>
<last-name>Lee</last-name>
<first-name>Mi-Suen</first-name>
<address>
<city>Ossining</city>
<state>NY</state>
<country>US</country>
</address>
</addressbook>
<nationality>
<country>omitted</country>
</nationality>
<residence>
<country>US</country>
</residence>
</applicant>
<applicant sequence="002" app-type="applicant-inventor" designation="us-only">
<addressbook>
<last-name>Strubbe</last-name>
<first-name>Hugo</first-name>
<address>
<city>Yorktown Heights</city>
<state>NY</state>
<country>US</country>
</address>
</addressbook>
<nationality>
<country>omitted</country>
</nationality>
<residence>
<country>US</country>
</residence>
</applicant>
</applicants>
<agents>
<agent sequence="01" rep-type="attorney">
<addressbook>
<last-name>Glickberg</last-name>
<first-name>Yan</first-name>
<address>
<country>unknown</country>
</address>
</addressbook>
</agent>
</agents>
</parties>
<assignees>
<assignee>
<addressbook>
<orgname>Koninklijke Philips Electronics N.V.</orgname>
<role>03</role>
<address>
<city>Eindohoven</city>
<country>NL</country>
</address>
</addressbook>
</assignee>
</assignees>
<examiners>
<primary-examiner>
<last-name>Bella</last-name>
<first-name>Matthew C.</first-name>
<department>2624</department>
</primary-examiner>
<assistant-examiner>
<last-name>Lu</last-name>
<first-name>Tom Y</first-name>
</assistant-examiner>
</examiners>
</us-bibliographic-data-grant>
<abstract id="abstract">
<p id="p-0001" num="0000">A system and method for automatically controlling systems and devices in a local environment, such as a home. The system comprises a control unit that receives images associated with one or more regions of the local environment. The one or more regions are each serviced by one or more servicing components. The control unit processes the images to identify, from a group of known persons associated with the local environment, any one or more known persons located in the regions. For the regions in which one or more known person is identified, the control unit automatically generates a control signal for at least one of the servicing components associated with the region, the control signal reflecting a preference of at least one of the known persons located in the respective region.</p>
</abstract>
<drawings id="DRAWINGS">
<figure id="Fig-EMI-D00000" num="00000">
<img id="EMI-D00000" he="187.79mm" wi="93.73mm" file="US07298871-20071120-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00001" num="00001">
<img id="EMI-D00001" he="142.66mm" wi="149.01mm" file="US07298871-20071120-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00002" num="00002">
<img id="EMI-D00002" he="225.13mm" wi="147.74mm" file="US07298871-20071120-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00003" num="00003">
<img id="EMI-D00003" he="208.96mm" wi="94.83mm" file="US07298871-20071120-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
</drawings>
<description id="description">
<?BRFSUM description="Brief Summary" end="lead"?>
<heading id="h-0001" level="1">FIELD OF THE INVENTION</heading>
<p id="p-0002" num="0001">The invention relates to adjusting the ambience, such as the lighting, temperature, noise level, etc., in a home or like interior environment.</p>
<heading id="h-0002" level="1">BACKGROUND OF THE INVENTION</heading>
<p id="p-0003" num="0002">Certain home automation systems and techniques are known. Many known home automation systems and techniques may generally be classified as reactive to a real-time physical input. A well-known example are lights having attendant IR sensors (or like motion sensor), which will turn on when a person walks by, such as into a room. Such lights can often have an attendant daylight sensor (another real-time input), which will prevent the light from turning on when there is ambient daylight.</p>
<p id="p-0004" num="0003">Other known home automation systems and techniques may generally be classified as pre-programmed to carry out certain functions when certain criteria are met. Many reactive systems are controlled by timers. For example, heating systems can be initiated automatically at a certain time of day, such as in the morning. Similarly, coffee makers can be automatically initiated at a specified time, so that a person has a cup of brewed coffee ready when he or she walks into the kitchen in the morning.</p>
<p id="p-0005" num="0004">An example of a more complex home automation system is described in European Patent Application EP 1 102 500 A2 of Richton. The position of a wireless mobile unit (such as a wireless phone) carried by a person is used to determine the distance of the person to the home. Messages or instructions to perform certain actions based on the distance between the person and the home are generated and sent to a controller within the home. The controller causes the instruction to be enacted. For example, when the user is within a certain distance of the home, the home heating system may be instructed to turn on. Richton thus has features that are analogous to both a reactive system (i.e., a feature is engaged based upon proximity) and a pre-programmed system (i.e., engagement of a feature when certain pre-stored criteria are met).</p>
<p id="p-0006" num="0005">Another example of a more elaborate pre-programmed type home automation system is described in PCT WO 01/52478 A2 of Sharood et al. In the Sharood system, existing home appliances and systems are connected to a control server. The user may control a selected appliance or system via a user interface that interacts with the server and can present graphic representations of the actual control inputs for the selected appliance or system. The user may therefore access the server and control appliances or systems remotely, for example, through an internet connection. In addition, the control server may be programmed so that certain appliances or systems are initiated and run under certain circumstances. For example, when a “vacation mode” is engaged, the lights are turned on at certain times for security purposes, and the heat is run at a lower temperature.</p>
<p id="p-0007" num="0006">There are numerous deficiencies associated with the known home automation techniques and systems. For example, known reactive-type systems simply provide a fixed response when an input is received. Thus, for example, a motion sensor will switch on a light even if person would not otherwise want it on. Even a reactive system such as Richton, where certain reactions may be programmed, suffer from such a disadvantage. For example, a mobile phone that initiates certain functions in the home at certain distances that reflect a wife's preferences may create conditions that are not agreeable to a husband who is carrying his wife's phone.</p>
<p id="p-0008" num="0007">Similarly, known pre-programmed type home automation systems have numerous deficiencies. For example, a timer that automatically turns on an appliance or system will do so unless it is turned off, thus creating situations that are undesirable or possibly unsafe. For example, if a person forgets to turn the timer of a coffee maker off on the day he or she has an early business meeting, a potential hazard may occur when the coffee maker is turned on later in the morning and remains on for the entire day. Likewise, for example, if the “vacation mode” is selected in Richton and a son or daughter who is unfamiliar with the system controls unexpectedly returns home from college for a weekend while the rest of the family is away, he or she may not be able to operate the lights, heating, etc. to their liking.</p>
<p id="p-0009" num="0008">Other disadvantages of known home automation systems and techniques include an inability to identify a particular person and tailor a setting or response in the house to the preferences of the identified person. In addition, known systems and techniques do not respond with the preferred settings or responses based on the location of a particular person in the home. In addition, known systems and techniques do not respond with the preferred settings or responses of a number of persons based upon where they are located in the house.</p>
<heading id="h-0003" level="1">SUMMARY OF THE INVENTION</heading>
<p id="p-0010" num="0009">It is thus an objective of the invention to provide automatic setting of conditions or ambiance in a local environment, such as a home. It is also an objective to provide automatic detection of the location of a particular person in the local environment and automatic setting of conditions or ambiance in the region of local environment in which the person is detected based on the preferences of the particular person. It is also an objective to provide automatic detection of the location of a particular user in the local environment using image recognition.</p>
<p id="p-0011" num="0010">Accordingly, the invention provides a system comprising a control unit that receives images associated with one or more regions of a local environment. The local environment may be, for example, a home, and the two or more regions may be the rooms of the home, a wing or floor of the home, etc. The one or more regions are each serviced by one or more controllable devices or systems. For example, the controllable devices or systems may be the lights in a room, the heat level for a sector of the home, etc. The control unit processes the images to identify, from a group of known persons associated with the local environment, any known persons located in one or more of the regions. For a known person so identified in a respective region, the control unit retrieves from a database an indicium of the identified person's preference for at least one of the one or more controllable devices or systems that service the respective region in which the known person is located. The control unit generates control signals so that the one or more controllable devices or systems that service the respective region in which the identified person is located is adjusted to reflect the known person's preference.</p>
<p id="p-0012" num="0011">Also, the invention provides a method for adjusting the conditions or ambiance of regions comprising a local environment. The method comprises capturing images associated with each of a number of regions of a local environment. From a group of known persons associated with the local environment, any known persons located in one or more of the regions are identified from the captured images. One or more preferences of an identified person are retrieved. The one or more preferences for the identified person are used to control one or more devices or systems associated with the region in which the identified person is located.</p>
<?BRFSUM description="Brief Summary" end="tail"?>
<?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?>
<description-of-drawings>
<heading id="h-0004" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading>
<p id="p-0013" num="0012"><figref idref="DRAWINGS">FIG. 1</figref> is a representative view of an embodiment of the invention;</p>
<p id="p-0014" num="0013"><figref idref="DRAWINGS">FIG. 2</figref> is a more detailed representative view of the embodiment of the invention shown in <figref idref="DRAWINGS">FIG. 1</figref>;</p>
<p id="p-0015" num="0014"><figref idref="DRAWINGS">FIG. 2</figref><i>a </i>depicts further details of a component of <figref idref="DRAWINGS">FIG. 2</figref>; and</p>
<p id="p-0016" num="0015"><figref idref="DRAWINGS">FIG. 3</figref> is a flow-chart of an embodiment of a method in accordance with the invention</p>
</description-of-drawings>
<?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?>
<?DETDESC description="Detailed Description" end="lead"?>
<heading id="h-0005" level="1">DETAILED DESCRIPTION</heading>
<p id="p-0017" num="0016">Referring to <figref idref="DRAWINGS">FIG. 1</figref>, a local environment comprising a home <b>10</b> is represented that supports an embodiment of the invention. Although a home is focused on in the ensuing description, the local environment may be any setting to which the invention may be applied, such as an office, store, hospital, hotel, camper, etc. The invention may be easily adapted to such other settings by one skilled in the art.</p>
<p id="p-0018" num="0017">The home <b>10</b> is shown to be comprised of rooms R<b>1</b>, R<b>2</b>. Although R<b>1</b>, R<b>2</b> are represented and referred to as rooms, they are generally intended to represent definable regions in the home, not just traditional rooms. For example, any of the regions may alternatively be a kitchen, hallway, stairway, garage, basement, storage space, etc. In addition, rooms R<b>1</b>, R<b>2</b> in <figref idref="DRAWINGS">FIG. 1</figref> are representative of other rooms in the home that have at least one controllable device or system that service the respective room. Any regions in the home that do not include systems or devices that are not controlled in accordance with the invention are not represented in <figref idref="DRAWINGS">FIG. 1</figref>, but it is understood that such regions may exist. For example, the systems and/or devices that are controlled in accordance with the invention may be found in certain regions of the home that are used more frequently, such as the bedrooms, kitchen, den and living room, and may be absent from regions of the home that are used less frequently, such as the hallways, stairways, basement and garage.</p>
<p id="p-0019" num="0018">Each room R<b>1</b>, R<b>2</b> in <figref idref="DRAWINGS">FIG. 1</figref> is shown as having a camera C<b>1</b>, C<b>2</b>, respectively, or like image capturing device, that captures images within the room and, in particular, images of persons in the room. More than one camera may be used to cover a region, but for ease of description only one camera is represented as covering each region in <figref idref="DRAWINGS">FIG. 1</figref> Thus, for example, camera C<b>2</b> will capture images of person X when located as shown in room R<b>2</b>. Each room R<b>1</b>, R<b>2</b> is also shown as having a respective light, L<b>1</b>, L<b>2</b>, that illuminates the room, as well as a respective heating unit H<b>1</b>, H<b>2</b> that heats the room. Room R<b>2</b> is serviced by an audio system that provides music through speaker S<b>2</b>. Light L<b>1</b> and heating unit H<b>1</b> are the systems and components of room R<b>1</b> that are controlled in accordance with the invention. Similarly, light L<b>2</b>, heating unit H<b>2</b> and speaker S<b>2</b> are the systems and components of room R<b>2</b> that are controlled in accordance with the invention. As will be described in more detail below, according to the invention, images from cameras C<b>1</b>, C<b>2</b> are used to identify a known person in a respective room R<b>1</b>, R<b>2</b>. Once identified, the devices and/or systems that service the room in which the identified person is located is automatically adjusted or controlled in accordance with the individual preference of the identified person. For example, if person X is recognized by camera C<b>1</b>, the light L<b>2</b>, heat H<b>2</b> and volume of music from speaker S<b>2</b> in room R<b>2</b> is automatically adjusted to the preferences of X.</p>
<p id="p-0020" num="0019">Before proceeding, it is also noted that the particular devices and/or systems shown that service each room R<b>1</b>, R<b>2</b> and which are controlled in accordance with the invention is for convenience to aid in describing the embodiment of the present invention. However, each room may include more or less and/or different devices or systems that service the room and are controlled according to the invention. One skilled in the art may readily adapt the description applied to the representative devices and systems described below to different and/or additional or fewer devices or systems found in any individual room.</p>
<p id="p-0021" num="0020">In addition, it is also noted that each device and system is ascribed in <figref idref="DRAWINGS">FIG. 1</figref> as servicing a particular room. However, any one device or system may service two or more rooms. For those servicing devices or systems, the servicing area of the device or system defines the room or local region and thus which cameras are used in controlling the device or system. For example, in <figref idref="DRAWINGS">FIG. 1</figref>, heating unit H<b>2</b> may be absent and heating unit H<b>1</b> may service both rooms R<b>1</b>, R<b>2</b>. Thus, for the purposes of the heating unit H<b>1</b>, the local region is rooms R<b>1</b> and R<b>2</b>, and is adjusted according to the preference of a person identified by either C<b>1</b> or C<b>2</b>. Similarly, speaker S<b>2</b> may provide music to both rooms R<b>1</b> and R<b>2</b>, so it's volume will be adjusted to the preference of an identified person located in either R<b>1</b> or R<b>2</b> (i.e., a person identified in an image captured by either C<b>1</b> or C<b>2</b>).</p>
<p id="p-0022" num="0021">Referring to <figref idref="DRAWINGS">FIG. 2</figref>, a more detailed (and somewhat more generalized) representation of the embodiment introduced in <figref idref="DRAWINGS">FIG. 1</figref> is shown. Rooms R<b>1</b>, R<b>2</b> are shown schematically, along with respective cameras C<b>1</b>, C<b>2</b>, respective lights L<b>1</b>, L<b>2</b>, and respective heating units H<b>1</b>, H<b>2</b>. For room R<b>2</b>, speaker S<b>2</b> is also shown. For clarity, devices and/or systems that service a room and which are controlled according to the invention (such as L<b>1</b>, L<b>2</b>, H<b>1</b>, H<b>2</b> and S<b>2</b>) may alternatively be referred to as a “servicing component”.</p>
<p id="p-0023" num="0022"><figref idref="DRAWINGS">FIG. 2</figref> shows additional components of the embodiment of the invention. Control unit <b>20</b> provides the central processing and initiation of control signals for the servicing components of rooms R<b>1</b>, R<b>2</b>. Control unit <b>20</b> may comprise any digital controller, processor, microprocessor, computer, server or the like, and any needed ancillary components (such as a memory, database, etc.) which can carry out the control processing and signal generation of the invention. Control unit <b>20</b> may comprise, for example, a processor <b>22</b> and memory <b>24</b>, as shown further in <figref idref="DRAWINGS">FIG. 2</figref><i>a</i>, and run software for determining and outputting appropriate control signals to the servicing components, which is described in further detail below. Cameras C<b>1</b>, C<b>2</b> are connected to control unit <b>20</b> over data lines <b>1</b>(C<b>1</b>), <b>1</b>(C<b>2</b>), respectively. Data lines <b>1</b>(C<b>1</b>), <b>1</b>(C<b>2</b>) and like lines described below may comprise standard communication wires, optical fibers and like hardwired data lines. They may also represent wireless communication. Each camera C<b>1</b>, C<b>2</b> thus provides images of the respective room R<b>1</b>, R<b>2</b> in which it is located to the processor <b>22</b> of control unit <b>20</b>. Thus, camera C<b>1</b> provides images of room R<b>1</b> to control unit <b>20</b> and camera C<b>2</b> provides images of room R<b>2</b> to control unit <b>20</b>.</p>
<p id="p-0024" num="0023">In addition, processor <b>22</b> of control unit <b>20</b> provides appropriate control signals to lights L<b>1</b>, L<b>2</b> over lines <b>1</b>(L<b>1</b>), <b>1</b>(L<b>2</b>), respectively, for controlling the intensity of the respective lights L<b>1</b>, L<b>2</b>. For convenience, lines <b>1</b>(L<b>1</b>), <b>1</b>(L<b>2</b>) are shown in <figref idref="DRAWINGS">FIG. 2</figref> as directly connected to lights L<b>1</b>, L<b>2</b>, respectively, but it is understood that lines <b>1</b>(L<b>1</b>), <b>1</b>(L<b>2</b>) actually provide control signals to dimming circuitry attached to each respective light L<b>1</b>, L<b>2</b>. Alternatively, lines <b>1</b>(L<b>1</b>), <b>1</b>(L<b>2</b>) may be input to a separate lighting controller that provides the appropriate dimming control signals to L<b>1</b> and/or L<b>2</b> based on the input received from control unit <b>20</b>.</p>
<p id="p-0025" num="0024">Processor <b>22</b> of control unit <b>20</b> also provides control signals over lines <b>1</b>(H<b>1</b>), <b>1</b>(H<b>2</b>) for controlling the temperature provided by heating units H<b>1</b>, H<b>2</b> to rooms R<b>1</b>, R<b>2</b>, respectively. The control signals from control unit <b>20</b> over lines <b>1</b>(H<b>1</b>), <b>1</b>(H<b>2</b>) may comprise an appropriate temperature control signal for heating unit H<b>1</b>, H<b>2</b>, respectively. In a particular example of the heating system of <figref idref="DRAWINGS">FIG. 2</figref>, heating units H<b>1</b>, H<b>2</b> are electric heaters that each have associated thermostats that receive control signals (in the form of a temperature setting) from control unit <b>20</b> over lines <b>1</b>(H<b>1</b>), <b>1</b>(H<b>2</b>), respectively.</p>
<p id="p-0026" num="0025">For other common types of heating systems known in the art, the control signal provided by control unit <b>20</b> to heating elements H<b>1</b>, H<b>2</b> shown in <figref idref="DRAWINGS">FIG. 2</figref> is a more abstract representation of the actual underlying system. For example, for heat provided by a centralized source (such as a gas fired hot water furnace), control unit <b>20</b> may provide a temperature setting over line <b>1</b>(H<b>1</b>) for a thermostat (not shown) in room R<b>1</b>. The thermostat consequently turns on a particular circulator attached to the furnace that provides hot water to baseboard heating elements comprising heating unit H<b>1</b>. In addition, lines <b>1</b>(H<b>1</b>), <b>1</b>(H<b>2</b>) may be input to a separate heating controller that provides the appropriate heating control signals to H<b>1</b> and/or H<b>2</b> based on the input received from control unit <b>20</b>. Whatever the underlying heating system however, the control of the embodiment described with respect to <figref idref="DRAWINGS">FIG. 2</figref> may be readily adapted by one skilled in the art.</p>
<p id="p-0027" num="0026">Control unit <b>20</b> also provides control signals over line <b>1</b>(S) to audio system <b>40</b>. Audio system <b>40</b> provides music to speaker S<b>2</b> in room R<b>2</b> over line <b>1</b>(S<b>2</b>) in accordance with the control signals received from control unit <b>20</b>. The control unit <b>20</b> may provide signals to the audio system that set the volume level of speaker S<b>2</b>, the type of music selected for play (for example, particular CDs, a radio station or webcast, etc.), etc. Audio system <b>40</b> may be located in room R<b>2</b>, such as a stereo, but also may be a centralized audio system that provides music to other rooms in the home. Audio system <b>40</b> may include an internal processor that receives the control signals from control unit <b>20</b> and processes those signals to select the music to play, the volume of speaker S<b>2</b> output over line <b>1</b>(S<b>2</b>), etc.</p>
<p id="p-0028" num="0027">Control unit <b>20</b> further comprises image recognition software that is stored in memory <b>24</b> and run by processor <b>22</b>. The image recognition software processes the incoming images of each room R<b>1</b>, R<b>2</b> received from cameras C<b>1</b>, C<b>2</b>, respectively. For convenience, the ensuing description will focus on the images received from a single camera, selected to be C<b>1</b> of room R<b>1</b>, shown in <figref idref="DRAWINGS">FIG. 2</figref>. The description is also applicable to images received by control unit <b>20</b> from camera C<b>2</b> located in room R<b>2</b>.</p>
<p id="p-0029" num="0028">As noted, camera C<b>1</b> captures images of room R<b>1</b> and transmits the image data to control unit <b>20</b>. The images are typically comprised of pixel data, for example, those from a CCD array in a typical digital camera. The pixel data of the images is assumed to be pre-processed into a known digital format that may be further processed using the image recognition software in control unit <b>20</b>. Such pre-processing of the images may take place in a processor of the camera C<b>1</b>. Such processing of images by digital cameras (which provides the pre-processed image data to the control unit <b>20</b> for further processing by the image recognition software) is well known in the art and, for convenience, it's description will be omitted except to the extent necessary to describe the invention. While such pre-processing of the images of camera C<b>1</b> may take place in the camera C<b>1</b>, it may alternatively take place in the processor <b>22</b> of control unit <b>20</b> itself.</p>
<p id="p-0030" num="0029">Processor <b>22</b> includes known image recognition software loaded therein that analyzes the image data received from camera C<b>1</b> via data line <b>1</b>(C<b>1</b>). If a person is located in room R<b>1</b>, he or she will thus be depicted in the image data. The image recognition software may be used, for example, to recognize the contours of a human body in the image, thus recognizing the person in the image. Once the person's body is located, the image recognition software may be used to locate the person's face in the received image and to identify the person.</p>
<p id="p-0031" num="0030">For example, if control unit <b>20</b> receives a series of images from camera C<b>1</b>, control unit <b>20</b> may detect and track a person that moves into the room R<b>1</b> covered by camera C<b>1</b> and, in particular, may detect and track the approximate location of the person's head. Such a detection and tracking technique is described in more detail in “Tracking Faces” by McKenna and Gong, Proceedings of the Second International Conference on Automatic Face and Gesture Recognition, Killington, Vt., Oct. 14-16, 1996, pp. 271-276, the contents of which are hereby incorporated by reference. (Section 2 of the aforementioned paper describes tracking of multiple motions.)</p>
<p id="p-0032" num="0031">When the person is stationary in region R<b>1</b>, for example, when he or she sits in a chair, the movement of the body (and the head) will be relatively stationary. Where the software of the control unit <b>20</b> has previously tracked the person's movement in the image, it may then initiate a separate or supplementary technique of face detection that focuses on the portion of the subsequent images received from the camera C<b>1</b> where the person's head is located. If the software of the control unit <b>20</b> does not track movements in the images, then the person's face may be detected using the entire image, for example, by applying face detection processing in sequence to segments of the entire image.</p>
<p id="p-0033" num="0032">For face detection, the control unit <b>20</b> may identify a static face in an image using known techniques that apply simple shape information (for example, an ellipse fitting or eigen-silhouettes) to conform to the contour in the image. Other structure of the face may be used in the identification (such as the nose, eyes, etc.), the symmetry of the face and typical skin tones. A more complex modeling technique uses photometric representations that model faces as points in large multi-dimensional hyperspaces, where the spatial arrangement of facial features are encoded within a holistic representation of the internal structure of the face. Face detection is achieved by classifying patches in the image as either “face” or “non-face” vectors, for example, by determining a probability density estimate by comparing the patches with models of faces for a particular sub-space of the image hyperspace. This and other face detection techniques are described in more detail in the aforementioned Tracking Faces paper.</p>
<p id="p-0034" num="0033">Face detection may alternatively be achieved by training a neural network supported within the control unit <b>20</b> to detect frontal or near-frontal views. The network may be trained using many face images. The training images are scaled and masked to focus, for example, on a standard oval portion centered on the face images. A number of known techniques for equalizing the light intensity of the training images may be applied. The training may be expanded by adjusting the scale of the training face images and the rotation of the face images (thus training the network to accommodate the pose of the image). The training may also involve back-propagation of false-positive non-face patterns. The control unit <b>20</b> provides portions of the image to such a trained neural network routine in the control unit <b>20</b>. The neural network processes the image portion and determines whether it is a face image based on its image training.</p>
<p id="p-0035" num="0034">The neural network technique of face detection is also described in more detail in the aforementioned Tracking Faces paper. Additional details of face detection (as well as detection of other facial sub-classifications, such as gender, ethnicity and pose) using a neural network is described in “Mixture of Experts for Classification of Gender, Ethnic Origin and Pose of Human Faces” by Gutta, et al., IEEE Transactions on Neural Networks, vol. 11, no. 4, pp. 948-960 (July 2000), the contents of which are hereby incorporated by reference and referred to below as the “Mixture of Experts” paper.</p>
<p id="p-0036" num="0035">Once a face is detected in the image, the control unit <b>20</b> provides image recognition processing to the face to identify the person. Thus, the image recognition processing is be programmed to recognize particular faces, and each face is correlated to the identity of a person. For example, for the home represented in the embodiment of <figref idref="DRAWINGS">FIGS. 1 and 2</figref>, the image recognition processing is programmed to recognize the faces of the family members and/or other residents that reside in the home, and each face is correlated to the identity of the family member/resident. The neural network technique of face detection described above may be adapted for identification by training the network using the faces of those persons who must be identified. Faces of other persons may be used in the training as negative matches (for example, false-positive indications). Thus, a determination by the neural network that a portion of the image contains a face image will be based on a training image for a known (identified) person, thus simultaneously providing the identification of the person. So programmed, the neural network provides both face detection and identification of the person. Alternatively, where a face is detected in the image using a technique other than a neural network (such as that described above), the neural network procedure may be used to confirm detection of a face and to also provide identification of the face.</p>
<p id="p-0037" num="0036">As another alternative technique of face recognition and processing that may be programmed in control unit <b>20</b>, U.S. Pat. No. 5,835,616, “FACE DETECTION USING TEMPLATES” of Lobo et al, issued Nov. 10, 1998, hereby incorporated by reference herein, presents a two step process for automatically detecting and/or identifying a human face in a digitized image, and for confirming the existence of the face by examining facial features. Thus, the technique of Lobo may be used in lieu of, or as a supplement to, the face detection and identification provided by the neural network technique after the initial tracking of a moving body (when utilized), as described above. The system of Lobo et al is particularly well suited for detecting one or more faces within a camera's field of view, even though the view may not correspond to a typical position of a face within an image. Thus, control unit <b>20</b> may analyze portions of the image for an area having the general characteristics of a face, based on the location of flesh tones, the location of non-flesh tones corresponding to eye brows, demarcation lines corresponding to chins, nose, and so on, as in the referenced U.S. Pat. No. 5,835,616.</p>
<p id="p-0038" num="0037">If a face is detected, it is characterized for comparison with reference faces for family members who reside in the home (which are stored in database <b>22</b>), as in the referenced U.S. Pat. No. 5,835,616. This characterization of the face in the image is preferably the same characterization process that is used to characterize the reference faces, and facilitates a comparison of faces based on characteristics, rather than an ‘optical’ match, thereby obviating the need to have two identical images (current face and reference face) in order to locate a match. In a preferred embodiment, the number of reference faces is relatively small, typically limited to the number of people in a home, office, or other small sized environment, thereby allowing the face recognition process to be effected quickly. The reference faces stored in memory <b>24</b> of control unit <b>20</b> have the identity of the person associated therewith; thus, a match between a face detected in the image and a reference face provides an identification of the person in the image.</p>
<p id="p-0039" num="0038">Thus, the memory <b>24</b> and/or software of control unit <b>20</b> effectively includes a pool of reference images and the identities of the persons associated therewith. Using the images received from camera C<b>1</b>, the control unit <b>20</b> effectively detects and identifies a known person (or persons) when located in room R<b>1</b> by locating a face (or faces) in the image and matching it with an image in the pool of reference images. The “match” may be detection of a face in the image provided by a neural network trained using the pool of reference images, or the matching of facial characteristics in the camera image and reference images as in U.S. Pat. No. 5,835,616, as described above. Using the images received from camera C<b>2</b>, the control unit <b>20</b> likewise detects and identifies a known person (or persons) when located in room R<b>2</b>.</p>
<p id="p-0040" num="0039">When an image of a known person (such as a family member) located in a room is identified in the control unit <b>20</b> by applying the image recognition software to the images received from the camera in the room, the processor <b>22</b> then executes control software so that the servicing components of the room in which the person is located are automatically adjusted by the control unit <b>20</b> according to the individual preference of the identified person. Memory <b>24</b> of control unit <b>20</b> includes each family member's preference for room lighting (i.e., the settings of lights L<b>1</b> and L<b>2</b>), room temperature (i.e., the settings of heating units H<b>1</b> and H<b>2</b>) and audio volume (i.e., the setting of volume of speaker S<b>2</b>). Each family member's set of preferences stored in memory <b>24</b> is correlated to the identity of the family member. Processor <b>22</b> uses the identification of the family member to access the preferences for the identified family member from memory <b>24</b>. The preferences of the family member applicable to the servicing components of the room in which he or she is located is then utilized by the control software of the processor <b>22</b> to format and output control commands to the servicing components in the room.</p>
<p id="p-0041" num="0040">For example, family member X is shown as located in room R<b>2</b> in <figref idref="DRAWINGS">FIGS. 1 and 2</figref>. Images of X are captured by camera C<b>2</b> and sent to control unit <b>20</b> via line <b>1</b>(C<b>2</b>). Processor <b>22</b> uses image recognition software (along with reference image data of family members stored in memory <b>24</b>) to identify the family member as family member X. Processor <b>22</b> then uses the identity of family member X to retrieve preferences for family member X stored in memory for servicing components in the home. Processor <b>22</b> considers the preferences of family member X for those servicing components located in room R<b>2</b>, since the image of X came from camera C<b>2</b> (i.e., over line <b>1</b>(C<b>2</b>)). Thus, processor <b>22</b> retrieves X's preferences for the intensity of light L<b>2</b>, temperature for heating unit H<b>2</b> and volume of speaker S<b>2</b>. Processor <b>22</b> may retrieve all preferences of X from memory <b>24</b> first and then determine which are related to the servicing components associated with R<b>2</b>. Alternatively, processor <b>22</b> may address and retrieve from memory <b>24</b> only those preferences related to the servicing components of R<b>2</b>.</p>
<p id="p-0042" num="0041">For each preference retrieved for X from memory that corresponds to a servicing component in R<b>2</b>, processor <b>22</b> formats a command for the corresponding servicing component in room R<b>2</b> in accordance with the preference and outputs it to the component over the appropriate line. For person X in room R<b>2</b>, X's preference for the intensity of lighting is used by processor <b>22</b> to generate an intensity control command that is formatted to control light L<b>2</b> in accordance with the preference. The intensity control command is output to light L<b>2</b> over line <b>1</b>(L<b>2</b>) and light L<b>2</b> is consequently adjusted to X's preferred intensity automatically. Similarly, X's preference for room temperature is is used by processor <b>22</b> to generate a control command formatted to control heating unit H<b>2</b> in accordance with X's preference. The command is output to H<b>2</b> over line <b>1</b>(H<b>2</b>) and heating unit H<b>2</b> is consequently adjusted to X's preferred temperature automatically.</p>
<p id="p-0043" num="0042">In like manner, the output of stereo speaker S<b>2</b> is adjusted to X's preferred volume automatically. X's preference retrieved from memory for audio volume is used by processor <b>22</b> to generate a volume control command formatted to control audio system <b>40</b> in accordance with the preference. The volume control command is output to audio system <b>40</b> over line <b>1</b>(S), and audio system <b>40</b> consequently adjusts the volume level output by audio system <b>40</b> to speaker S<b>2</b> in room R<b>2</b> according to the preference. Audio system <b>40</b> may also have additional functions that may be controlled externally by processor <b>22</b>, such as power on and off and music selection. In that case, when family member X is identified in room R<b>2</b>, the preferences retrieved from memory <b>24</b> pertaining to audio may include whether X normally wants the audio system turned on, the type of music X likes, for example, the particular CDs in a CD jukebox or a particular CD radio station, and the volume. The control signals formatted and sent to audio system <b>40</b> by processor <b>22</b> reflect the preferences of person X for the additional functions. For example, the control signals reflecting X's preferences may turn on the audio system, tune the audio system tuner to a particular pop radio station, and adjust the volume of speaker S<b>2</b> to a moderately loud level.</p>
<p id="p-0044" num="0043">Images from cameras C<b>1</b> and C<b>2</b> may be captured and transmitted to control unit <b>20</b> continuously or periodically. Control unit <b>20</b> periodically processes the images received to identify known persons in the respective rooms and control servicing components in the manner described. When servicing components in a room are adjusted according to the preferences of a known person in the room, a temporary record may be created indicating such. While the known person remains in the room, subsequent images of the known person in the room are processed by control unit <b>20</b> and again identify the known person in the room. However, the temporary record indicates to the control unit <b>20</b> that the servicing components have previously been adjusted according to the person's preferences and control signals are not again generated and transmitted to the servicing components for the room for those received images. When the known person is no longer found in subsequent images from the room (indicating the person has left the room), the temporary record for the room is erased, and full processing by the control unit for subsequent images continues. This includes generation and transmission of control signals corresponding to preferences of known persons newly identified in the room in subsequent images. If X moves to another room, for example, to room R<b>1</b>, the lighting provided by L<b>1</b> and the heat provided by heating unit H<b>1</b> are automatically adjusted in like manner. That is, images captured by camera C<b>1</b> are transmitted to control unit <b>20</b> over line <b>1</b>(C<b>1</b>). Image recognition software is applied to the images by processor <b>22</b>, thus identifying family member X in room R<b>1</b>. Preferences for family member X are then retrieved by processor <b>22</b> from memory <b>24</b>. Control signals reflecting those preferences are transmitted to light L<b>1</b> and heating unit Hi over lines <b>1</b>(L<b>1</b>) and <b>1</b>(H<b>1</b>) respectively. The intensity of light L<b>1</b> and the room temperature provided by heating unit H<b>1</b> is consequently adjusted to X's preferences automatically.</p>
<p id="p-0045" num="0044">Also, when X vacates R<b>2</b>, the images provided by camera C<b>2</b> and analyzed by the image recognition software of control unit <b>20</b> will not identify a known person currently in room R<b>2</b>. When the control unit <b>20</b> determines that nobody is located in a particular room, the settings of the servicing components may be set to a default setting. For example, when X vacates room R<b>2</b>, control signals may be sent by processor <b>22</b> to turn off light L<b>2</b>, reduce the thermostat setting of heating unit to 65 degrees, and switch off either the audio system <b>40</b> or the volume output by speaker S<b>2</b>. In addition, if the control unit <b>20</b> determines that there is a person located in a room, but cannot identify the person, other default control signals may be sent to one or more of the servicing components. For example, for safety, light L<b>2</b> may be turned on and set to a medium intensity level, so that the unidentified person (for example, a guest) can see where he or she is going.</p>
<p id="p-0046" num="0045">If more than one family member is located in a room, then the image recognition software will consequently identify more than one family member in the image received. The preferences stored in memory <b>24</b> for the various servicing components may be different. For example, if family members X and Y are located in room R<b>2</b>, each may have a different preference for the volume of music output by speaker S<b>2</b>. For such cases, the control software may include priority rules for determining how to control the speaker and other servicing components. For example, if family member X is a parent and family member Y is a child, the preferences for X may be chosen over Y. On the other hand, if X and Y are brothers, and room R<b>2</b> is X's bedroom, then X's preferences may supersede Y's. Preferences may also govern certain servicing components in a room for a person of higher priority located in another room. For example, if parent X is identified in a room adjacent to R<b>2</b>, then the parent's preference for the level of speaker S<b>2</b> in room R<b>2</b> may supersede the preference of child Y actually located in R<b>2</b>. If no preference is defined between persons located in the same room, then certain default rules may apply. For example, for speaker volume, the lowest preferred volume may govern. On the other hand, for room temperature, the highest preferred volume may govern. For lighting, an intensity that reflects the average of the preferences may be used.</p>
<p id="p-0047" num="0046">Preferences may also be a function of other factors, such as time of day, day of the week or year, etc. For example, if room R<b>2</b> has good natural lighting then X's preference for L<b>2</b> may reflect that L<b>2</b> is “off” in the middle of the day, set at medium intensity after 6 p.m. in the winter months, and set at a medium intensity after 8 p.m. in the spring, summer and fall.</p>
<p id="p-0048" num="0047">The image recognition processing may also detect gestures in addition to the identity of a family member. The control unit <b>20</b> may be programmed to detect certain pre-defined gestures and make corresponding adjustments to one or more of the servicing components in the room in which the identified family member is located. A detected gesture may override the family member's stored preference for the servicing component. For example, in the example above, when X is identified as located in room R<b>2</b>, the volume output by speaker S<b>2</b> is automatically adjusted to the X's preference stored in memory <b>24</b>. However, if X does not want to listen to music, X may hold up three fingers toward camera C<b>2</b>. The gesture is captured in the images and detected by the image recognition software of control unit <b>20</b>, which is further programmed to send a corresponding control signal to audio system <b>40</b> to switch off or otherwise mute speaker S<b>2</b>. Further details on recognition of gestures from images are found in “Hand Gesture Recognition Using Ensembles Of Radial Basis Function (RBF) Networks And Decision Trees” by Gutta, Imam and Wechsler, Int'l Journal of Pattern Recognition and Artificial Intelligence, vol. 11, no. 6, pp. 845-872 (1997), the contents of which are hereby incorporated by reference.</p>
<p id="p-0049" num="0048">Preferences for known persons may be manually input to the control unit <b>20</b> and stored in memory <b>24</b>, for example, via a graphic user interface (not shown). The interface provides appropriate menus for the various servicing components in the home or other local environment when processor <b>22</b> is put into a preference input mode. For example, for known person X inputting his or her preferences, the interface may first present a set of thumbnail images of known persons. The images may be selected from the reference images for known persons stored in memory <b>24</b> for the image recognition software. X inputs his or her identity by selecting his or her image from the set of images. Subsequent menus presented to X allow preferences to be selected for lighting, heating, speaker volume, etc. for the various servicing components throughout the home. The input preferences associated with family member X are stored in memory <b>24</b>.</p>
<p id="p-0050" num="0049">Of course, preferences that are input for a family member and stored in memory <b>24</b> need not be the actual personal preferences of the family member. For example, a parent may input some or all of the preferences for the children in the home. For example, the parent may input the children's preferences so that the stereo volume level, room temperature level, etc. associated with the children are reasonable.</p>
<p id="p-0051" num="0050">Alternatively, preferences may be learned by control unit <b>20</b> by monitoring the settings actually input to servicing components themselves in the home by family members. For example, control unit <b>20</b> may be put into a “learning mode” over a number of days or weeks. During that time, when a person enters a room, he or she is identified via the image recognition software, as described above. Lines between the various servicing components in the room and the control unit <b>20</b> transfer data in both directions, thus enabling the control unit <b>20</b> to detect and record the settings selected by the person for the servicing components themselves.</p>
<p id="p-0052" num="0051">Preferences are created for each of the identified persons based upon the manual settings selected for the various servicing components during the learning mode. For example, during the learning mode, control unit <b>20</b> may record that X set the intensity of light L<b>2</b> in room R<b>2</b> twenty times and that the average setting of the light was 7 on a scale of 10. When the learning mode is complete, X's preference for light L<b>2</b> is stored as 7. Where one general lighting preference is stored for a person, not preferences for individual lights in the home, then control unit may record and average the intensity levels selected by X for all lights throughout the home during the learning mode. Later, when a particular light (such as L<b>2</b>) is automatically adjusted for X, processor <b>20</b> retrieves the general lighting preference for X and, based on the retrieved preference, generates a control signal that is formatted to output the preferred intensity for L<b>2</b>. Preferences for other known persons and other servicing components are created in the learning mode in analogous manner.</p>
<p id="p-0053" num="0052">As previously noted, the rooms shown in <figref idref="DRAWINGS">FIGS. 1 and 2</figref> are representative of the above-described embodiment. Home <b>10</b> may have additional rooms, serviced by like and/or different servicing components than the heating units, lights and audio system/speaker shown in <figref idref="DRAWINGS">FIGS. 1 and 2</figref>. Other controllable servicing components may include automated windowshades, televisions and associated DVDs and VCRs, automatic garage door openers, etc. Audio system <b>40</b> that controls speaker S<b>2</b> in room R<b>2</b> may drive other speakers in other rooms, which may be with different music at different volumes depending on the preference of a person in another room. Additional servicing components and rooms (along with the attendant camera(s)) may be added by suitable wiring and programming of control unit <b>20</b>.</p>
<p id="p-0054" num="0053">In addition, certain servicing components may be controlled according to a failure to identify a person in an image. For example, a security system controlled by the system of the invention may be activated when the control unit <b>20</b> detects the image of a person received from anywhere in the home, but fails to identify the person.</p>
<p id="p-0055" num="0054">As also previously noted, the servicing components of the embodiment shown in <figref idref="DRAWINGS">FIGS. 1 and 2</figref> are shown as servicing one room, either R<b>1</b> or R<b>2</b>. Thus, for example, L<b>1</b> provides light for room R<b>1</b>, but not R<b>2</b>. However, one servicing component may service two or more rooms, such as both R<b>1</b> and R<b>2</b>. For example, heating unit H<b>1</b> may provide heat to both room R<b>1</b> and R<b>2</b>. Similarly, the intensity of lights L<b>1</b> and L<b>2</b> may both be controlled by a single dimmer switch. In these examples, the region covered by the servicing component encompasses two traditional rooms (R<b>1</b> and R<b>2</b>) in the home R<b>1</b>. In such cases, the coverage area of the servicing component defines the extent of the local region or “room” in the control unit for the component and which images from cameras are used by the control unit to control the servicing component. For example, if L<b>1</b> and L<b>2</b> are both controlled by a single dimmer switch in <figref idref="DRAWINGS">FIG. 2</figref>, then a single control line extends from the control unit <b>20</b> to the dimmer switch, and a separate line extends from the dimmer switch to each of lights L<b>1</b> and L<b>2</b>. Control unit <b>20</b> therefore controls the intensity of lights L<b>1</b> and L<b>2</b> using the preference of any known person identified in R<b>1</b> and R<b>2</b>, that is, using images received from both cameras C<b>1</b> and C<b>2</b>. If more than one person is identified in rooms R<b>1</b> and R<b>2</b>, then control unit <b>20</b> uses priority rules as described above to determine which identified person's preference governs. It is noted that the composite region of R<b>1</b> and R<b>2</b> defined by the servicing component L<b>1</b> and L<b>2</b> in this example overlaps other regions, in this case region R<b>1</b> (serviced by servicing component H<b>1</b>) and region R<b>2</b> (serviced by servicing components H<b>2</b> and S<b>2</b>).</p>
<p id="p-0056" num="0055"><figref idref="DRAWINGS">FIG. 3</figref> presents a flow-chart of a generalized method included within the present invention and which may be gleaned from the embodiments described above. In step <b>100</b>, images of a region are captured. In step <b>110</b>, a known person, if any, is identified in the images. If a known person is identified, it is determined in step <b>120</b> whether the known person has recently been identified in the region and servicing components have previously been controlled according to the preference of the person. If not, one or more preferences are retrieved for the identified person in step <b>130</b>. One or more servicing components in the region are controlled using the one or more preferences of the known person in step <b>140</b>. The method is repeated for the region under consideration, as represented by the line connecting step <b>140</b> back to step <b>100</b>. A delay of time may be interposed before the method is repeated. Other regions in a local environment may be processed in like manner, either concurrently (in a multiplexed fashion, for example), or in sequence. The steps of the method may be modified and/or additional steps may be readily be incorporated to the method represented in <figref idref="DRAWINGS">FIG. 3</figref> to encompass other aspects of the embodiments discussed above. Thus, for example, when more than one known person is identified in a region, step <b>130</b> may be modified to retrieve the preferences of all known persons identified in the region, and step <b>140</b> may be modified to control the one or more servicing components using the preferences of the known persons in conjunction with a priority scheme, as described above for example.</p>
<?DETDESC description="Detailed Description" end="tail"?>
</description>
<us-claim-statement>What is claimed is:</us-claim-statement>
<claims id="claims">
<claim id="CLM-00001" num="00001">
<claim-text>1. A system comprising a control unit that receives images associated with one or more regions of a home, the one or more regions each being serviced by one or more servicing components, the control unit processing the images to identify, from a group of known persons associated with the home, any one or more known persons located in the regions and, for the regions in which one or more known person is identified, automatically generating a control signal for at least one of the servicing components associated with the region, the control signal reflecting a preference of at least one of the known persons located in the respective region.</claim-text>
</claim>
<claim id="CLM-00002" num="00002">
<claim-text>2. A system comprising a control unit that receives images associated with one or more regions of a stationary structure, the one or more regions each being serviced by one or more servicing components, the control unit processing the images to identify, from a group of known persons associated with the stationary structure, any one or more known persons located in the regions and, for the regions in which one or more known person is identified, automatically generating a control signal for at least one of the servicing components associated with the region, the control signal reflecting a preference of at least one of the known persons located in the respective region, the system further comprising at least one camera associated with each of the one or more regions of the stationary structure that provide the control unit with the images associated with the one or more regions, images captured by the at least one camera associated with each region being processed to identify any known persons located in the region.</claim-text>
</claim>
<claim id="CLM-00003" num="00003">
<claim-text>3. The system of <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein the control unit applies image recognition software to the received images to identify any one or more known persons located in the regions.</claim-text>
</claim>
<claim id="CLM-00004" num="00004">
<claim-text>4. The system of <claim-ref idref="CLM-00003">claim 3</claim-ref>, wherein the image recognition software uses stored reference image data of the known persons associated with the stationary structure to identify any one or more known persons located in the regions.</claim-text>
</claim>
<claim id="CLM-00005" num="00005">
<claim-text>5. The system as in <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the control unit retrieves preferences stored for the one or more known persons identified as located in the regions.</claim-text>
</claim>
<claim id="CLM-00006" num="00006">
<claim-text>6. The system as in <claim-ref idref="CLM-00005">claim 5</claim-ref>, wherein, for the regions in which one or more known person is identified, the control signal generated for the at least one of the servicing components associated with the region is based upon at least one of the preferences retrieved for at least one of the known persons located in the region.</claim-text>
</claim>
<claim id="CLM-00007" num="00007">
<claim-text>7. The system as in <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising a memory that interfaces with the control unit, the memory storing preferences for each of the group of known persons relating to the one or more servicing components servicing the one or more regions, the preferences for an identified person being retrievable from the memory by the control unit.</claim-text>
</claim>
<claim id="CLM-00008" num="00008">
<claim-text>8. The system as in <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the servicing components are selected from a group comprised of lights, heating units and audio systems.</claim-text>
</claim>
<claim id="CLM-00009" num="00009">
<claim-text>9. The system of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein one of the servicing components associated with one of the regions is a heating unit, the control signal generated for the heating unit reflecting a room temperature preference of at least one of the known persons located in the one region.</claim-text>
</claim>
<claim id="CLM-00010" num="00010">
<claim-text>10. The system of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein one of the servicing components associated with one of the regions is a light, the control signal generated for the light reflecting a lighting preference of at least one of the known persons located in the one region.</claim-text>
</claim>
<claim id="CLM-00011" num="00011">
<claim-text>11. The system of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein one of the servicing components associated with one of the regions is an audio system, the control signal generated for the audio system reflecting at least one of an audio volume preference and a music type preference of at least one of the known persons located in the one region.</claim-text>
</claim>
<claim id="CLM-00012" num="00012">
<claim-text>12. A method for controlling a servicing component located in a region of a stationary structure, the method comprising the steps of:
<claim-text>a) capturing images associated with the region;</claim-text>
<claim-text>b) identifying, from a group of known persons each associated with the stationary structure, a known person in the region from the captured images associated with the region;</claim-text>
<claim-text>c) retrieving a preference of the identified person, the preference related to the servicing component, wherein the step of retrieving a preference includes retrieving the preference from a memory using the identity determined in the identifying step; and</claim-text>
<claim-text>d) controlling the servicing component according to the retrieved preference of the identified person;</claim-text>
<claim-text>wherein steps a)-d) are performed for one or more additional regions in the stationary structure.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00013" num="00013">
<claim-text>13. The method as in <claim-ref idref="CLM-00012">claim 12</claim-ref>, wherein the step of identifying a known person in the region from the captured images includes the step of processing the captured images with image recognition software.</claim-text>
</claim>
<claim id="CLM-00014" num="00014">
<claim-text>14. The method as in <claim-ref idref="CLM-00012">claim 12</claim-ref>, wherein the step of controlling the servicing component according to the retrieved preference of the identified person includes generating a control signal for the servicing component that reflects the preference and transmitting the control signal to the servicing component.</claim-text>
</claim>
<claim id="CLM-00015" num="00015">
<claim-text>15. The method as in <claim-ref idref="CLM-00012">claim 12</claim-ref>, wherein steps c) and d) are performed for one or more additional servicing components in the region.</claim-text>
</claim>
<claim id="CLM-00016" num="00016">
<claim-text>16. A computer readable storage medium including instructions which, when executed by a computer, carries out a method comprising:
<claim-text>receiving an image input associated with one or more regions of a home, the one or more regions each being serviced by one or more servicing components;</claim-text>
<claim-text>processing the images input in conjunction with data comprising a group of known persons associated with the home, to identify any one or more known persons located in the regions;</claim-text>
<claim-text>for the regions in which one or more known person is identified, accessing a memory to retrieve a preference of at least one of the known persons located in the respective region for at least one of the servicing components associated with the region; and</claim-text>
<claim-text>generating a control signal for the at least one of the servicing components associated with the region based upon the retrieved preference.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00017" num="00017">
<claim-text>17. The system of <claim-ref idref="CLM-00002">claim 2</claim-ref> wherein the stationary structure is selected from the group comprising a dwelling and a building.</claim-text>
</claim>
<claim id="CLM-00018" num="00018">
<claim-text>18. The system of <claim-ref idref="CLM-00001">claim 1</claim-ref> wherein the preference is a function of a time of day, a day of the week, or a time of the year.</claim-text>
</claim>
<claim id="CLM-00019" num="00019">
<claim-text>19. The method of <claim-ref idref="CLM-00016">claim 16</claim-ref> including using, for a region in which more than one person is identified, a priority rule to establish a preference, and wherein the control signal is based on the established preference.</claim-text>
</claim>
<claim id="CLM-00020" num="00020">
<claim-text>20. A system comprising a control unit that receives images associated with one or more regions of a building, the one or more regions each being serviced by one or more servicing components, the control unit processing the images to identify, from a group of known persons associated with the building, any one or more known persons located in the regions and, for the regions in which one or more known person is identified, automatically generating a control signal for at least one of the servicing components associated with the region, the control signal reflecting a preference of at least one of the known persons located in the respective region.</claim-text>
</claim>
<claim id="CLM-00021" num="00021">
<claim-text>21. The system according to <claim-ref idref="CLM-00020">claim 20</claim-ref>, wherein the preferences are pre-established and the system includes a memory that interfaces with the control unit, the memory storing pre-established preferences for each of the group of known persons relating to the one or more servicing components servicing the one or more regions, the pre-established preferences for an identified person being retrievable from the memory by the control unit, wherein the controller includes a preference input mode, and wherein the pre-established preferences for each of the group of known persons relating to the one or more servicing components servicing the one or more regions are input manually via an external input interface with the control unit when the controller is in the preference input mode.</claim-text>
</claim>
<claim id="CLM-00022" num="00022">
<claim-text>22. The system according to <claim-ref idref="CLM-00020">claim 20</claim-ref>, wherein the system includes a memory that interfaces with the control unit, the memory storing preferences for each of the group of known persons relating to the one or more servicing components servicing the one or more regions, the preferences for an identified person being retrievable from the memory by the control unit, and wherein the preferences for each of the group of known persons relating to the one or more servicing components servicing the one or more regions are generated by the control unit in a learning mode.</claim-text>
</claim>
<claim id="CLM-00023" num="00023">
<claim-text>23. The system according to <claim-ref idref="CLM-00020">claim 20</claim-ref>, wherein at least one of the one or more regions of the building is defined by a room of the building.</claim-text>
</claim>
<claim id="CLM-00024" num="00024">
<claim-text>24. The system of <claim-ref idref="CLM-00023">claim 23</claim-ref>, wherein the control unit receives images associated with a first room of the building and wherein at least one of the servicing components has an extent of coverage which includes the first room and a second room of the building.</claim-text>
</claim>
</claims>
</us-patent-grant>
