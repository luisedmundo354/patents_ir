<us-patent-grant lang="EN" dtd-version="v4.2 2006-08-23" file="US07299417-20071120.XML" status="PRODUCTION" id="us-patent-grant" country="US" date-produced="20071106" date-publ="20071120">
<us-bibliographic-data-grant>
<publication-reference>
<document-id>
<country>US</country>
<doc-number>07299417</doc-number>
<kind>B1</kind>
<date>20071120</date>
</document-id>
</publication-reference>
<application-reference appl-type="utility">
<document-id>
<country>US</country>
<doc-number>10630061</doc-number>
<date>20030730</date>
</document-id>
</application-reference>
<us-application-series-code>10</us-application-series-code>
<us-term-of-grant>
<us-term-extension>916</us-term-extension>
</us-term-of-grant>
<classifications-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>G</section>
<class>06</class>
<subclass>F</subclass>
<main-group>3</main-group>
<subgroup>00</subgroup>
<symbol-position>F</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20071120</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>G</section>
<class>06</class>
<subclass>F</subclass>
<main-group>3</main-group>
<subgroup>14</subgroup>
<symbol-position>L</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20071120</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
</classifications-ipcr>
<classification-national>
<country>US</country>
<main-classification>715719</main-classification>
<further-classification>715738</further-classification>
<further-classification>715765</further-classification>
<further-classification>715781</further-classification>
</classification-national>
<invention-title id="d0e53">System or method for interacting with a representation of physical space</invention-title>
<references-cited>
<citation>
<patcit num="00001">
<document-id>
<country>US</country>
<doc-number>6040841</doc-number>
<kind>A</kind>
<name>Cohen et al.</name>
<date>20000300</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>345473</main-classification></classification-national>
</citation>
<citation>
<patcit num="00002">
<document-id>
<country>US</country>
<doc-number>6121966</doc-number>
<kind>A</kind>
<name>Teodosio et al.</name>
<date>20000900</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>715838</main-classification></classification-national>
</citation>
<citation>
<patcit num="00003">
<document-id>
<country>US</country>
<doc-number>6268864</doc-number>
<kind>B1</kind>
<name>Chen et al.</name>
<date>20010700</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>345428</main-classification></classification-national>
</citation>
<citation>
<patcit num="00004">
<document-id>
<country>US</country>
<doc-number>6285365</doc-number>
<kind>B1</kind>
<name>Nalwa</name>
<date>20010900</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>715835</main-classification></classification-national>
</citation>
<citation>
<patcit num="00005">
<document-id>
<country>US</country>
<doc-number>6330976</doc-number>
<kind>B1</kind>
<name>Dymetman et al.</name>
<date>20011200</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>235487</main-classification></classification-national>
</citation>
<citation>
<patcit num="00006">
<document-id>
<country>US</country>
<doc-number>6593969</doc-number>
<kind>B1</kind>
<name>Discoll et al.</name>
<date>20030700</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>348335</main-classification></classification-national>
</citation>
<citation>
<patcit num="00007">
<document-id>
<country>US</country>
<doc-number>6987520</doc-number>
<kind>B2</kind>
<name>Criminisi et al.</name>
<date>20060100</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>345629</main-classification></classification-national>
</citation>
<citation>
<patcit num="00008">
<document-id>
<country>US</country>
<doc-number>7082572</doc-number>
<kind>B2</kind>
<name>Pea et al.</name>
<date>20060700</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>715720</main-classification></classification-national>
</citation>
<citation>
<patcit num="00009">
<document-id>
<country>US</country>
<doc-number>7096428</doc-number>
<kind>B2</kind>
<name>Foote et al.</name>
<date>20060800</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>715721</main-classification></classification-national>
</citation>
<citation>
<patcit num="00010">
<document-id>
<country>US</country>
<doc-number>7120197</doc-number>
<kind>B2</kind>
<name>Lin et al.</name>
<date>20061000</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>37524029</main-classification></classification-national>
</citation>
<citation>
<patcit num="00011">
<document-id>
<country>US</country>
<doc-number>7167840</doc-number>
<kind>B1</kind>
<name>Seidman et al.</name>
<date>20070100</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>705 52</main-classification></classification-national>
</citation>
<citation>
<patcit num="00012">
<document-id>
<country>US</country>
<doc-number>2003/0076822</doc-number>
<kind>A1</kind>
<name>Shalom et al.</name>
<date>20030400</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>370378</main-classification></classification-national>
</citation>
<citation>
<patcit num="00013">
<document-id>
<country>US</country>
<doc-number>2004/0225968</doc-number>
<kind>A1</kind>
<name>Look et al.</name>
<date>20041100</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>715778</main-classification></classification-national>
</citation>
<citation>
<patcit num="00014">
<document-id>
<country>WO</country>
<doc-number>WO 97/42601</doc-number>
<date>19971100</date>
</document-id>
</patcit>
<category>cited by examiner</category>
</citation>
<citation>
<nplcit num="00015">
<othercit>Xiang Zhang et al, “Taking AR into Large Scale Industrial Environments: Navigation and Information Access with Mobile Computers”, 2001, IEEE Comput. Soc., Los Alamitos, CA pp. 179-180.</othercit>
</nplcit>
<category>cited by examiner</category>
</citation>
<citation>
<nplcit num="00016">
<othercit>Jim Mallory, “Hike the Rocky Mountains ‘virtual reality’ landscape”, May 1994, Newsbytes, 1 page.</othercit>
</nplcit>
<category>cited by examiner</category>
</citation>
<citation>
<nplcit num="00017">
<othercit>Joe Stoddard, “Enhancing CAD”, Jan. 2003, Builder v26, n1, pp. 369-370.</othercit>
</nplcit>
<category>cited by examiner</category>
</citation>
<citation>
<nplcit num="00018">
<othercit>M. P. Halio, “Helping users navigate in multimedia documents: the affective domain”, 1992, ACM, p. 233-6.</othercit>
</nplcit>
<category>cited by examiner</category>
</citation>
<citation>
<nplcit num="00019">
<othercit>Samuel Greengard, “How technology is advancing HR”, Sep. 1993, Personnel Journal, v72, n9, p. 80(9).</othercit>
</nplcit>
<category>cited by examiner</category>
</citation>
<citation>
<nplcit num="00020">
<othercit>Nelson Heerema et al, “Prediction, visualization, and auralization of noise in industrial workrooms during computer ‘walk-through’”, Mar. 1999, Inst. Noise Control Eng. v47, n2, p. 65-70.</othercit>
</nplcit>
<category>cited by examiner</category>
</citation>
<citation>
<nplcit num="00021">
<othercit>Jonathan Foote et al, “FlyCam: Practical Panoramic Video and Automatic Camera Control”, Aug. 2000, IEEE, vol. III, p. 1419-1422.</othercit>
</nplcit>
<category>cited by examiner</category>
</citation>
<citation>
<nplcit num="00022">
<othercit>M. Nicolescu et al, “Segmentation, tracking and interpretation using panoramic video”, 2000, IEEE, p. 169-174.</othercit>
</nplcit>
<category>cited by examiner</category>
</citation>
<citation>
<nplcit num="00023">
<othercit>Internet Pictures Corporation website located at www.ipix.com, 2003.</othercit>
</nplcit>
<category>cited by other</category>
</citation>
<citation>
<nplcit num="00024">
<othercit>Imatronics Panarama Express website located at www.imatronics.com, 2003.</othercit>
</nplcit>
<category>cited by other</category>
</citation>
<citation>
<nplcit num="00025">
<othercit>3D Vista Panoramic Virtual Tour and Streaming Media Software website located as www.3dvista.com/main.htm, 2004.</othercit>
</nplcit>
<category>cited by other</category>
</citation>
<citation>
<nplcit num="00026">
<othercit>iSeeMedia website located at www.iseemedia.com, 2004.</othercit>
</nplcit>
<category>cited by other</category>
</citation>
<citation>
<nplcit num="00027">
<othercit>VR Toolbox, Inc. website located at www.vrtoolbox.com/vrthome.html, 2003.</othercit>
</nplcit>
<category>cited by other</category>
</citation>
<citation>
<nplcit num="00028">
<othercit>PixAround website located at www.pixaround.com, 2003.</othercit>
</nplcit>
<category>cited by other</category>
</citation>
</references-cited>
<number-of-claims>3</number-of-claims>
<us-exemplary-claim>2</us-exemplary-claim>
<us-field-of-classification-search>
<classification-national>
<country>US</country>
<main-classification>715716</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>715719</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>715733</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>715738</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>715764</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>715781</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>715786</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>715810</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>715835</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>7155011</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>715765</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>715846</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>345649</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>345650</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>345655</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>345659</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>345660</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>345661</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>345666</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>345619</main-classification>
</classification-national>
</us-field-of-classification-search>
<figures>
<number-of-drawing-sheets>17</number-of-drawing-sheets>
<number-of-figures>22</number-of-figures>
</figures>
<parties>
<applicants>
<applicant sequence="001" app-type="applicant-inventor" designation="us-only">
<addressbook>
<last-name>Barris</last-name>
<first-name>Joel M.</first-name>
<address>
<street>4528 Valleyview Dr.</street>
<city>West Bloomfield</city>
<state>MI</state>
<postcode>48323</postcode>
<country>US</country>
</address>
</addressbook>
<nationality>
<country>omitted</country>
</nationality>
<residence>
<country>US</country>
</residence>
</applicant>
<applicant sequence="002" app-type="applicant-inventor" designation="us-only">
<addressbook>
<last-name>DeWitt</last-name>
<first-name>Gabriel L.</first-name>
<address>
<street>134 Seaview Ave. #3</street>
<city>Santa Cruz</city>
<state>CA</state>
<postcode>95062</postcode>
<country>US</country>
</address>
</addressbook>
<nationality>
<country>omitted</country>
</nationality>
<residence>
<country>US</country>
</residence>
</applicant>
<applicant sequence="003" app-type="applicant-inventor" designation="us-only">
<addressbook>
<last-name>Dasu</last-name>
<first-name>Dhiren P.</first-name>
<address>
<street>910 Hammon Gulch Ave.</street>
<city>Boulder Creek</city>
<state>CA</state>
<postcode>95006</postcode>
<country>US</country>
</address>
</addressbook>
<nationality>
<country>omitted</country>
</nationality>
<residence>
<country>US</country>
</residence>
</applicant>
</applicants>
</parties>
<examiners>
<primary-examiner>
<last-name>Bautista</last-name>
<first-name>X. L.</first-name>
<department>2179</department>
</primary-examiner>
</examiners>
</us-bibliographic-data-grant>
<abstract id="abstract">
<p id="p-0001" num="0000">The interactive video system and method (collectively the “system”) provides users with the ability to navigate and otherwise interact with a representation of physical space. Each representation can include a number of viewpoints and objects that can be selected by the user to provide interactive functionality. For example, a user can decide to examine the contents a bookshelf, tilt upwards to look at a chandelier, or zoom in on a tree visible from an exterior window. The motion of tilting, panning, or zooming is conveyed in the form of video clips that accurately resemble the way a human being would perceive such activities. The viewing of objects and viewpoints can be accompanied by the display of information relevant to the user. The various representations can be stored in highly compact files that allow even low-speed dial-up Internet users to effectively navigate a representation in a real-time manner.</p>
</abstract>
<drawings id="DRAWINGS">
<figure id="Fig-EMI-D00000" num="00000">
<img id="EMI-D00000" he="118.70mm" wi="203.71mm" file="US07299417-20071120-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00001" num="00001">
<img id="EMI-D00001" he="236.90mm" wi="164.25mm" orientation="landscape" file="US07299417-20071120-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00002" num="00002">
<img id="EMI-D00002" he="237.49mm" wi="164.93mm" orientation="landscape" file="US07299417-20071120-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00003" num="00003">
<img id="EMI-D00003" he="221.15mm" wi="168.06mm" orientation="landscape" file="US07299417-20071120-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00004" num="00004">
<img id="EMI-D00004" he="230.38mm" wi="155.19mm" orientation="landscape" file="US07299417-20071120-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00005" num="00005">
<img id="EMI-D00005" he="219.03mm" wi="142.41mm" orientation="landscape" file="US07299417-20071120-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00006" num="00006">
<img id="EMI-D00006" he="196.26mm" wi="135.04mm" orientation="landscape" file="US07299417-20071120-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00007" num="00007">
<img id="EMI-D00007" he="208.45mm" wi="143.93mm" orientation="landscape" file="US07299417-20071120-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00008" num="00008">
<img id="EMI-D00008" he="217.68mm" wi="168.40mm" orientation="landscape" file="US07299417-20071120-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00009" num="00009">
<img id="EMI-D00009" he="232.58mm" wi="176.28mm" orientation="landscape" file="US07299417-20071120-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00010" num="00010">
<img id="EMI-D00010" he="210.90mm" wi="169.93mm" orientation="landscape" file="US07299417-20071120-D00010.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00011" num="00011">
<img id="EMI-D00011" he="191.09mm" wi="174.75mm" orientation="landscape" file="US07299417-20071120-D00011.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00012" num="00012">
<img id="EMI-D00012" he="172.89mm" wi="157.82mm" orientation="landscape" file="US07299417-20071120-D00012.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00013" num="00013">
<img id="EMI-D00013" he="222.00mm" wi="163.41mm" orientation="landscape" file="US07299417-20071120-D00013.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00014" num="00014">
<img id="EMI-D00014" he="217.09mm" wi="155.70mm" orientation="landscape" file="US07299417-20071120-D00014.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00015" num="00015">
<img id="EMI-D00015" he="163.66mm" wi="151.98mm" orientation="landscape" file="US07299417-20071120-D00015.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00016" num="00016">
<img id="EMI-D00016" he="138.43mm" wi="150.71mm" orientation="landscape" file="US07299417-20071120-D00016.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00017" num="00017">
<img id="EMI-D00017" he="229.19mm" wi="168.32mm" orientation="landscape" file="US07299417-20071120-D00017.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
</drawings>
<description id="description">
<?BRFSUM description="Brief Summary" end="lead"?>
<heading id="h-0001" level="1">BACKGROUND OF THE INVENTION</heading>
<p id="p-0002" num="0001">The invention is a system or method for providing access to interactive video content (collectively, “video system” or simply the “system”). More specifically, the invention is a video system for interacting with a representation of physical space.</p>
<p id="p-0003" num="0002">The use of digital media has increased significantly because the tools for capturing and transferring digital sounds, still images, and video clips have become increasingly advanced and inexpensive. However, the tools for transforming digital media content into interactive digital video content have not kept up with the advancements in digital media. Existing techniques fail to display representations of physical space in a photo-realistic manner as seen by the human eye. Two types of image distortion, bent edges and blurry digital zooms degrade the quality of the interactive video. Those distortions result in a display that is significantly different from how a human being would perceive the subject matter embodied in the digital media. In summary, the existing interactive video systems fail to accurately capture the human experience. It would be desirable for an interactive video system to incorporate true video processing in a photo-realistic manner. Instead of relying on stitched-together still pictures or image interpolation applications, it would be desirable to use actual video or film</p>
<p id="p-0004" num="0003">Internet-based video systems typically require additional Java or other additional external applications in order to run. Such specific configurations preclude a universal approach to interactive media over the Internet. Different environments require different supporting software, and the resulting fragmentation precludes the market success required for a successful universal system. It would be desirable for an interactive video system to compatible across all major web browsers and platforms.</p>
<p id="p-0005" num="0004">Existing interactive video systems also suffer from substantial performance problems. These problems are particularly acute in the context of invoking interactive video functionality from a web site or some other Internet-based mechanism. Although the use of high-speed connectivity mechanisms such as cable modems, DSL, satellite, and other mechanisms (collectively “high-speed connections”), most web surfers continue to rely on much slower dial-up connections (collectively “low-speed connections”). Existing interactive video systems involve large files that preclude meaningful real-time processing. It would be desirable for the media files of an interactive video system to be sufficiently compact as to allow real-time performance over a low-speed connection.</p>
<heading id="h-0002" level="1">SUMMARY OF THE INVENTION</heading>
<p id="p-0006" num="0005">The invention is a system or method for providing access to interactive video content (collectively, “video system” or simply the “system”). More specifically, the invention is a video system for interacting with a representation of physical space.</p>
<p id="p-0007" num="0006">The system provides access to a representation of physical space (the “representation”) that can be navigated and interacted with through the navigation tool interface provided by the system.</p>
<p id="p-0008" num="0007">The representation can include one or more viewpoints. Each viewpoint can be associated with a location within the representation. One or more video clips can be associated with each viewpoint location, with each video clip representing a series of panoramic, tilt, or zoom activities associated with the particular viewpoint location. Each frame within the video clip can be a view of the system.</p>
<p id="p-0009" num="0008">Representations can also include one or more objects. Each object can be associated with a location within the representation. One or more video clips and/or still images can be associated with the object. Other object-based functionality can be customized to fit the nature of the object.</p>
<p id="p-0010" num="0009">The navigation of representations can further be assisted by a blueprint of the representation area, and a compass. In most embodiments, the representation area is presumed not to move, so the blueprint and compass for a particular representation area is fixed. However, the system can include systems where the observer “moves” within the representation and the representation itself “moves” within the context of the outside world. For example, a representation area may represent the internal space of a vehicle such as a boat or car.</p>
<p id="p-0011" num="0010">In a preferred embodiment, the system is used to provide Internet users with access to interactive digital video content. In other embodiments, the system can function across different types of networks or in a stand-alone environment.</p>
<?BRFSUM description="Brief Summary" end="tail"?>
<?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?>
<description-of-drawings>
<heading id="h-0003" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading>
<p id="p-0012" num="0011">In the drawings:</p>
<p id="p-0013" num="0012"><figref idref="DRAWINGS">FIG. 1</figref> is process flow diagram illustrating an example of an environmental view of an interactive video system.</p>
<p id="p-0014" num="0013"><figref idref="DRAWINGS">FIG. 2</figref> is a block diagram illustrating an example of a representation of physical space that includes two viewpoints and two objects.</p>
<p id="p-0015" num="0014"><figref idref="DRAWINGS">FIG. 3</figref> is a hierarchical diagram illustrating several examples of components that can be included as part of a viewpoint.</p>
<p id="p-0016" num="0015"><figref idref="DRAWINGS">FIG. 4</figref> is a block diagram illustrating an example of a zoom clip and a zoom view.</p>
<p id="p-0017" num="0016"><figref idref="DRAWINGS">FIG. 5</figref><i>a </i>is a block diagram illustrating an example of a pan clip and two pan views.</p>
<p id="p-0018" num="0017"><figref idref="DRAWINGS">FIG. 5</figref><i>b </i>is a diagram illustrating one example of a standard pan range of 360°.</p>
<p id="p-0019" num="0018"><figref idref="DRAWINGS">FIG. 5</figref><i>c </i>is a diagram illustrating one example of a standard pan range of 270°.</p>
<p id="p-0020" num="0019"><figref idref="DRAWINGS">FIG. 5</figref><i>d </i>is a diagram illustrating one example of a standard pan range of 180°.</p>
<p id="p-0021" num="0020"><figref idref="DRAWINGS">FIG. 5</figref><i>e </i>is a diagram illustrating one example of a standard pan range of 90°.</p>
<p id="p-0022" num="0021"><figref idref="DRAWINGS">FIG. 6</figref> is a block diagram illustrating an example of a tilt clip and two tilt views.</p>
<p id="p-0023" num="0022"><figref idref="DRAWINGS">FIG. 7</figref> is a hierarchical diagram illustrating an example of the components that can be included in an object.</p>
<p id="p-0024" num="0023"><figref idref="DRAWINGS">FIG. 8</figref> is a diagram illustrating an example of an interface that can be incorporated into the system.</p>
<p id="p-0025" num="0024"><figref idref="DRAWINGS">FIG. 9</figref> is a block diagram illustrating an example of a subsystem-level view of an interactive video system.</p>
<p id="p-0026" num="0025"><figref idref="DRAWINGS">FIG. 10</figref> is a block diagram illustrating an example of a subsystem-level view of an interactive video system.</p>
<p id="p-0027" num="0026"><figref idref="DRAWINGS">FIG. 11</figref> is a flow chart illustrating an example of a method for interacting with a representation of physical space.</p>
<p id="p-0028" num="0027"><figref idref="DRAWINGS">FIG. 12</figref> is a flow chart illustrating an example of a method for creating a file for an interactive video system.</p>
<p id="p-0029" num="0028"><figref idref="DRAWINGS">FIG. 13</figref> is a detailed flow chart illustrating an example of a method for creating a file for an interactive video system.</p>
<p id="p-0030" num="0029"><figref idref="DRAWINGS">FIG. 14</figref> is a flow chart illustrating an example of a shoot heuristic.</p>
<p id="p-0031" num="0030"><figref idref="DRAWINGS">FIG. 15</figref> is a flow chart illustrating an example of an object shoot heuristic.</p>
<p id="p-0032" num="0031"><figref idref="DRAWINGS">FIG. 16</figref> is a flow chart illustrating an example of a capture heuristic.</p>
<p id="p-0033" num="0032"><figref idref="DRAWINGS">FIG. 17</figref> is a flow chart illustrating an example of an encode heuristic.</p>
<p id="p-0034" num="0033"><figref idref="DRAWINGS">FIG. 18</figref> is a flow chart illustrating an example of an animate heuristic.</p>
</description-of-drawings>
<?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?>
<?DETDESC description="Detailed Description" end="lead"?>
<heading id="h-0004" level="1">DETAILED DESCRIPTION</heading>
<heading id="h-0005" level="1">I. Environmental View</heading>
<p id="p-0035" num="0034"><figref idref="DRAWINGS">FIG. 1</figref> is process flow diagram illustrating an example of an environmental view of an interactive video system <b>100</b>. A wide variety of different architectures can be incorporated into the system <b>100</b>, and the system <b>100</b> can function in a wide variety of different embodiments.</p>
<p id="p-0036" num="0035">A. User</p>
<p id="p-0037" num="0036">A user <b>102</b> interacts with the system <b>100</b> through an access device <b>104</b>. In a preferred embodiment, the user <b>102</b> is a human being. In some embodiments, the user <b>102</b> can be a robot, a neural network, an expert system, or some other form of automated intelligence (collectively, “intelligence technology”). Any human being, non-human organism, organism, application, or machinery capable of manipulating the controls of the system <b>100</b> to interact with the video content of the system <b>100</b> can be a user <b>102</b>.</p>
<p id="p-0038" num="0037">B. Access Device</p>
<p id="p-0039" num="0038">An access device <b>104</b> is any device that allows the user <b>102</b> to interact with the system <b>100</b>. In a preferred embodiment, the access device <b>104</b> is a commercially available device such as a desktop computer, laptop computer, mainframe computer, cell phone, personal digital assistant (PDA), or any other device that allows the user <b>102</b> to access the Internet. In a preferred embodiment (an “Internet embodiment”), users <b>102</b> access the system <b>100</b> through a commercially available web browser and a media player, such as the MACROMEDIA FLASH 5.0 software by MACROMEDIA.</p>
<p id="p-0040" num="0039">In some embodiments, the access device <b>104</b> is a client device connected to a network, such as a LAN, WAN, or some other non-Internet based network (collectively “computer network embodiment”). In other embodiments, the access device <b>104</b> is a server accessed by the user <b>102</b> through some other client device. In still other embodiments, the access device <b>104</b> is a stand-alone device that is not connected to any other device.</p>
<p id="p-0041" num="0040">1. Display Component</p>
<p id="p-0042" num="0041">A display component <b>106</b> is typically some type of monitor or display device that is part of or connected to the access device <b>104</b>. The display component <b>106</b> displays the representation of physical space (“representation”) <b>112</b> navigated by the system <b>100</b>, as well as the navigation tools provided by the system <b>100</b>. The system interface and navigations tools are illustrated in <figref idref="DRAWINGS">FIGS. 8 and 10</figref>, and are described in greater detail below.</p>
<p id="p-0043" num="0042">2. Interactive Component</p>
<p id="p-0044" num="0043">An interactive component <b>108</b> is what allows the user <b>102</b> to interact with the system <b>100</b> and the video content made accessible by the system <b>100</b>. The interactive component <b>108</b> is what allows the user <b>102</b> to transmit instructions to the system <b>100</b> so that the system <b>100</b> can perform the appropriate interactive activity for the user <b>102</b>. A common example of an interactive component <b>108</b> is a keyboard. Devices such as a mouse, light pen, track ball, voice recognition technology component, or other input devices can also serve as interactive components <b>108</b>.</p>
<p id="p-0045" num="0044">C. File</p>
<p id="p-0046" num="0045">A file <b>110</b> is the mechanism by which the system <b>100</b> allows users to interact with a representation of physical space (“representation”) <b>112</b>. In a preferred embodiment, the file <b>110</b> is in a format such as avi, that is platform independent with respect to its player, and is accessible using a wide variety of different browsers, applications, and technical architectures. Although the creation of the particular file <b>110</b> embodying the representation <b>112</b> may require several different software applications, as described in greater detail below.</p>
<p id="p-0047" num="0046">D. Representation</p>
<p id="p-0048" num="0047">A representation of physical space (a “representation”) <b>112</b> is embodied in the file <b>110</b> that is accessed by the system <b>100</b>. In many embodiments of the system <b>100</b>, the representation <b>112</b> will be of a place that has an actual physical existence, such as the rooms of a house that is for sale. In other embodiments, the representation <b>112</b> can be of a fictional place, such as a representation generated by altering digitally captured media or even through animation and other techniques.</p>
<p id="p-0049" num="0048">1. Types of Representations</p>
<p id="p-0050" num="0049">There are a wide variety of different types of representations <b>112</b>.</p>
<p id="p-0051" num="0050">a. True Representations</p>
<p id="p-0052" num="0051">One type of representation can be referred to as a “true representation” <b>112</b>. True representations <b>112</b> represent physical environments that actually exist. True representations <b>112</b> can represent a wide variety of different actually existing physical environments, including the rooms of a house; the interior of a vehicle; an outdoor setting such as a field, forest, or jungle; a molecule or similar microscopic view; a solar system or similar telescopic view; or any other environment that physically exists. True representations <b>112</b> are incorporated into the system <b>100</b> by various cameras, such as video cameras and still image cameras.</p>
<p id="p-0053" num="0052">B. Fictional Representations</p>
<p id="p-0054" num="0053">Another type of representation <b>112</b> can be referred to as a “fictional representation” <b>112</b>. Fictional representations <b>112</b> are created by combining different aspects of true representations in a way that does not physically exist. One example of a fictional representation <b>112</b>, would be a representation <b>112</b> with Mount Rushmore located within 10 feet of an Egyptian pyramid. Fictional representations <b>112</b> are derived from true representations <b>112</b>.</p>
<p id="p-0055" num="0054">C. Fantasy Representations</p>
<p id="p-0056" num="0055">A third type of representation <b>112</b> can be referred to as a “fantasy representation” <b>112</b>. Fantasy representations involve images and video clips that made up, through computer or other animation techniques. Video clips used in a fantasy representation <b>112</b> cannot involve actual video footage because what is being represented is purely fictional.</p>
<p id="p-0057" num="0056">2. Representation Areas</p>
<p id="p-0058" num="0057">The disclosure in <figref idref="DRAWINGS">FIG. 2</figref> is a block diagram of a single representation area. Some embodiments of the system <b>100</b> may involve more simulated space that can be viewable at one time using the interface of the system <b>100</b>. Such embodiments involve representations <b>112</b> that include more than one representation area <b>112</b>. This is often a result of the physical space being represented. For example, in navigating a house, only one room or hallway should be viewable at a time because a person in such a house, could only see the contents of a single room or hallway at one time (unless a door is open or there is a window to see through). In such a context, each room or hallway can be referred to as a representation area with the overall representation. Outdoor representations can similarly be divided up into a variety of different representation areas. For example, a view of a waterfall could be blocked by a rock formation.</p>
<p id="p-0059" num="0058">Representations <b>112</b> may also need to be broken down into multiple representation areas due to the interface limitations of the system <b>100</b>. In certain contexts, the limitations of filming and the limitations of the display component may make it desirable to divide up the representation <b>112</b> into two or more representation areas.</p>
<p id="p-0060" num="0059">In some embodiments of the system <b>100</b>, the entire representation <b>112</b> is made up of only one representation area.</p>
<p id="p-0061" num="0060"><figref idref="DRAWINGS">FIG. 2</figref> is a block diagram illustrating an example of a representation <b>112</b> that includes two viewpoints <b>114</b> and two objects <b>116</b>. Viewpoints <b>114</b> and objects are discussed below. In a preferred embodiment, a representation <b>112</b> has at least one viewpoint <b>114</b> but need not include any objects <b>116</b>. In alternative embodiments, a wide variety of different configurations could be used.</p>
<heading id="h-0006" level="1">II. Viewpoints</heading>
<p id="p-0062" num="0061">A. Introduction to Viewpoints</p>
<p id="p-0063" num="0062">The representations <b>112</b> contained in the file <b>110</b> accessed by the system <b>100</b> can include one or more viewpoints <b>114</b>. The representation <b>112</b> disclosed in the example provided in <figref idref="DRAWINGS">FIG. 2</figref> includes two viewpoints <b>114</b>. Viewpoints <b>114</b> are locations within the representation <b>112</b> from which the user <b>102</b> to free “look around” the representation <b>112</b>. Each representation area requires at least one viewpoint <b>114</b>, or else the user <b>102</b> is not able to interact with or even view the particular representation area.</p>
<p id="p-0064" num="0063">In the example of a representation area being an exterior office in a office building, it would probably be desirable to locate a viewpoint <b>114</b> at the door of the room. This allows the user <b>102</b> to view the room as if the user had just entered the office. It may also be desirable to place a viewpoint <b>114</b> at the position of the chair by the desk so that the user <b>102</b> can view the office from the perspective of someone sitting at the desk. It may also be desirable to place a viewpoint <b>114</b> at any exterior window so that the user <b>102</b> can use the system <b>100</b> to simulate “looking” outside the window. In such an embodiment, in may also be desirable to place viewpoints <b>114</b> at the site of windows within the rooms or other points of interest within the room.</p>
<p id="p-0065" num="0064">The particular decisions as to where to place viewpoints <b>114</b> will depend heavily of the type of representation <b>112</b> being simulated, the user's likely purposes in navigating and interacting with the representation <b>112</b>.</p>
<p id="p-0066" num="0065">B. Components of a Viewpoint</p>
<p id="p-0067" num="0066"><figref idref="DRAWINGS">FIG. 3</figref> is a hierarchical diagram illustrating several examples of components that can be included as part of a viewpoint <b>114</b>.</p>
<p id="p-0068" num="0067">1. Viewpoint Locations</p>
<p id="p-0069" num="0068">Each viewpoint <b>114</b> in the system <b>100</b> can be associated with one or more viewpoint locations <b>122</b> within one or more representation areas. A viewpoint location <b>122</b> is what ties together the particular location within the representation <b>112</b> and video footage representing what the user <b>102</b> can see from a particular vantage point or location. In most embodiments of the system <b>100</b>, a viewpoint <b>114</b> needs to have a viewpoint location <b>122</b>, and the viewpoint location is preferably predefined. In certain alternative embodiments (such as video games or other embodiments where the “reality” of the situation is highly flexible), the viewpoint location <b>122</b> can be dynamically determined, and the viewpoint <b>114</b> need not even be associated with a particular viewpoint location <b>122</b>.</p>
<p id="p-0070" num="0069">2. Viewpoint Icons</p>
<p id="p-0071" num="0070">A viewpoint icon <b>120</b> can be used to identify the viewpoint location <b>122</b> within the representation area for benefit of the user <b>102</b>. The existence of the viewpoint icon <b>120</b> lets the user <b>102</b> know that one or more viewpoint activities can be invoked from that particular location within the representation. The placement of a viewpoint icon <b>120</b> typically requires the existence of a viewpoint location <b>122</b>, but a viewpoint location <b>122</b> need not have a viewpoint icon <b>120</b> associated with it. Viewpoint locations <b>122</b> can be identified in other ways, or need not be identified at all. Viewpoint icons <b>120</b> can be displayed in the within the representation <b>112</b> itself, as well as in supplementary information sources such as a blueprint, which is discussed in greater detail below.</p>
<p id="p-0072" num="0071">3. Video Clip (Viewpoint Clips)</p>
<p id="p-0073" num="0072">Many viewpoints <b>114</b> will have one or more video clips <b>118</b>. However, a viewpoint <b>114</b> need not have any video clips associated with it. A viewpoint <b>114</b> can simply be associated with a still-image. A video clip <b>118</b> is what a user <b>102</b> accesses at a particular viewpoint location <b>122</b> in order to experience the simulation of “moving” through or “experiencing” the representation <b>112</b> in a way that closely resembles how a human being can navigate or experience actual physical space. For example, a video clip <b>118</b> could allow the user <b>102</b> to simulate the activities of tilting upward to look at the ceiling lamp, to turn his or her head from left to right or right to left to purvey a room, or to focus one's attention (zoom in) on an item of interest. The use of video clips <b>1118</b> allows the user <b>102</b> to experience the starting point of the activity, numerous interim points, and the final destination view in a highly linear fashion.</p>
<p id="p-0074" num="0073">With respect to viewpoints <b>114</b>, there are typically three different types of viewpoint <b>114</b> related video clips <b>118</b>: zoom clips <b>124</b>; pan clips <b>126</b>; and tilt clips <b>128</b>. As illustrated in <figref idref="DRAWINGS">FIG. 3</figref>, each type of video clip <b>118</b> can be made up of various views <b>130</b>. Views <b>130</b> are the individual frames within a video clip <b>118</b> and views <b>130</b> can also be referred to as frames <b>130</b>. The individual frames (e.g. views <b>130</b>) that make up a video clip <b>118</b> differ from still-frame images in that frames (e.g. views <b>130</b>) are captured and created at the onset for use as video footage <b>118</b>. Views <b>130</b> are not still-images that are stitched together or subjected to interpolation (collectively a “distorted image”) in order to mimic video images. Thus, a still image is not a frame (e.g. a view <b>130</b>) and a frame or view <b>130</b> is not a still image.</p>
<p id="p-0075" num="0074">a. Zoom Clips and Views</p>
<p id="p-0076" num="0075"><figref idref="DRAWINGS">FIG. 4</figref> is a block diagram illustrating an example of the zoom clip <b>124</b> and the zoom view <b>132</b>. The action of “zooming in” is well known in the art of photography and video footage. The action of zooming in provides the viewer with a closer look at what they are looking at. Thus, one or more viewpoints <b>114</b> in a representation <b>112</b> may be configured to allow the user <b>102</b> to zoom in on the subject matter at hand. Examples of zooming functionality can include looking outside a window to take a closer look at a tree in the yard; examining the title of a book on a bookshelf at the other end of the room; or any other example involving taking a closer look at what is already within view. Unlike the panoramic and tilt activities described below, zooming in does not change what the user <b>102</b> is viewing, but instead, narrows the scope of what is being seen in support of focusing on a particular subset of what is otherwise visible. In a preferred embodiment, the zoom is an optical zoom, not a digital zoom. Other embodiments may include digital zooms or even combinations of both optical and digital zooms.</p>
<p id="p-0077" num="0076">In the example in <figref idref="DRAWINGS">FIG. 4</figref>, the view <b>130</b> associated with the particular viewpoint <b>114</b> is that of an office that includes various objects such as a lamp, a chair, and a file cabinet. The zoom clip <b>124</b> contains video footage that zooms in on the chair as is indicated by the zoom view <b>132</b> at the right end of the figure. A video clip <b>118</b> (a zoom clip <b>124</b>) instead of one or more still-pictures is used to support the functionality of zooming in so that a user <b>102</b> zooming in experiences the representation <b>112</b> in a way that closely mimics the way a human being would experience the physical space being represented. There can be numerous zoom views <b>132</b> on the zoom clip <b>124</b> between the initial view <b>130</b> and the maximum zoom view <b>132</b>.</p>
<p id="p-0078" num="0077">b. Pan Clips and Pan Views</p>
<p id="p-0079" num="0078"><figref idref="DRAWINGS">FIG. 5</figref><i>a </i>is a block diagram illustrating an example of the pan clip <b>126</b> and two pan views <b>134</b>. Invocation of the pan clip <b>126</b> by the user <b>102</b> occurs when the user <b>102</b> wants to look to the left or to the right of the initial view <b>130</b> visible for the particular viewpoint <b>114</b>. As is visible in the figure, items on the far left side of the initial view <b>130</b> are on the far right side of the maximum left pan view <b>134</b>. Correspondingly, items of the far right side of the initial view <b>130</b> are located on the far left side of the maximum right pan view <b>130</b>. There are potentially numerous pan views <b>134</b> between the maximum left pan view <b>134</b> and the maximum right pan view <b>134</b>. The pan clip <b>126</b> includes in the form of video footage, the images in the various pan views <b>134</b>.</p>
<p id="p-0080" num="0079">Pan clips <b>126</b> vary with respect to the angles between a maximum left pan view <b>134</b> and a maximum right pan view <b>134</b>. In some embodiments, standard ranges of angles are used. However, non-standard angle ranges can also be used in those embodiments. <figref idref="DRAWINGS">FIG. 5</figref><i>b </i>is a diagram illustrating one example of a standard pan range of 360°. <figref idref="DRAWINGS">FIG. 5</figref><i>c </i>is a diagram illustrating one example of a standard pan range of 270°.</p>
<p id="p-0081" num="0080"><figref idref="DRAWINGS">FIG. 5</figref><i>d </i>is a diagram illustrating one example of a standard pan range of 180°. <figref idref="DRAWINGS">FIG. 5</figref><i>e </i>is a diagram illustrating one example of a standard pan range of 90°.</p>
<p id="p-0082" num="0081">C. Tilt Clips and Tilt Views</p>
<p id="p-0083" num="0082"><figref idref="DRAWINGS">FIG. 6</figref> is a block diagram illustrating an example of the tilt clip <b>128</b> and two tilt views. As panoramic activity is horizontal, tilting is a vertical activity. Tilt clips <b>128</b> can include looking downwards, looking upwards, or looking both upwards and downwards. Just as pan clips <b>126</b> can be described in light of the range of angles, so can tilt clips <b>128</b>. In the example provided in <figref idref="DRAWINGS">FIG. 6</figref>, tilting occurs in both the upward and downward directions in relation to the initial view <b>130</b>.</p>
<p id="p-0084" num="0083">Items at the bottom of the initial view <b>130</b> are at the top of the maximum downward tilt view <b>136</b>. Correspondingly, items at the top of the initial view <b>130</b> are at the bottom of the maximum upward tilt view <b>136</b>. Just as with the zoom and panoramic activities, the use of a video clip makes the experience of tilting mimic what a human being would experience tilting their head upwards or downwards in physical space. There can be numerous tilt views <b>136</b> between a maximum downward tilt view <b>136</b> and a maximum upward tilt view <b>136</b>. The movement amongst tilt views <b>136</b> occurs in a liner manner to preserve the “human” experience.</p>
<heading id="h-0007" level="1">III. Objects</heading>
<p id="p-0085" num="0084">Unlike viewpoints <b>114</b>, objects <b>118</b> are items that facilitate the possibility of more that merely perception-related interactions. For example, an object <b>118</b> could be: a desk drawer capable of being opened; a piece of merchandise that can be examined and purchased; a top that can be spun; or any other item or object to which it would be desirable to associate the possibility of certain actions. Objects <b>118</b> can be visible from one or more viewpoints <b>114</b>. <figref idref="DRAWINGS">FIG. 7</figref> is a hierarchical diagram illustrating an example of the components that can be included in an object.</p>
<p id="p-0086" num="0085">A. Object Locations</p>
<p id="p-0087" num="0086">Each object <b>116</b> in the system <b>100</b> can be associated with one or more object locations <b>144</b> within one or more representation areas. An object location <b>144</b> is what ties together the particular location within the representation <b>112</b> and the various attributes and functionality associated with the object <b>118</b>. In most embodiments of the system <b>100</b>, the object <b>118</b> needs to be associated with an object location <b>144</b>, and the object location <b>144</b> is preferably predefined. In certain alternative embodiments (such as video games or other embodiments where the “reality” of the situation is highly flexible), the object location <b>144</b> can be dynamically determined, and the object <b>116</b> need not even be associated with a particular object location <b>144</b>.</p>
<p id="p-0088" num="0087">B. Object Icon</p>
<p id="p-0089" num="0088">An object icon <b>142</b> can be used to identify the object location <b>122</b> for the user <b>102</b>. The placement of a object icon <b>142</b> typically requires the existence of an object location <b>144</b>, but an object location <b>144</b> need not have an object icon <b>142</b> associated with it. Object locations <b>144</b> can be identified in other ways, or need not be identified at all. Object icons <b>142</b> can be displayed in the within the representation <b>112</b> itself, as well as in supplementary information sources such as a blueprint, which is discussed in greater detail below.</p>
<p id="p-0090" num="0089">C. Object Functionality</p>
<p id="p-0091" num="0090">As mentioned above, objects <b>116</b> can involve functionality beyond the mere viewing of the object <b>116</b> in a way that occurs by navigating the representation <b>112</b> through one or more viewpoints <b>114</b>. The range of object functionality <b>140</b> is virtually limitless. Examples of object functionality <b>140</b> include: invoking one or more object clips <b>146</b>; invoking one or more still images <b>150</b>; displaying object information <b>152</b>; initiating a communication <b>154</b>; or invoking any other action that can be invoked through the access device <b>104</b> being used by the user <b>102</b>. Object functionality <b>140</b> can also be referred to as ancillary content items that exist within the representation <b>112</b>. For example, in a virtual store embodiment, purchased items can be objects, and the ancillary content can be a price, an audio comment about the item, a website link to the vendor of the item, or any other form of information.</p>
<p id="p-0092" num="0091">1. Object Clips and Object Views</p>
<p id="p-0093" num="0092">An object clip <b>146</b> is a video clip <b>118</b> that corresponds to an object <b>116</b>, and not a viewpoint <b>114</b>, within the representation <b>112</b>. Any video clip <b>118</b> that could of interest to the user <b>102</b> in the context of the particular representation <b>112</b>, the particular object <b>116</b>, and/or the particular user <b>102</b> can be the object clip <b>146</b>. Examples of object clips <b>146</b> include video footage of: merchandise that can be purchased; an item being rotated 360° on a turnkey; news reports relating to the object <b>114</b>; advertisements for the purchase of the particular object <b>114</b>; and any other video clip <b>118</b> that would be of potential interest to the user <b>102</b> or the organization responsible for the representation <b>112</b>.</p>
<p id="p-0094" num="0093">Many objects <b>142</b> may have one or more video clips <b>118</b> associated with them. However, an object <b>116</b> need not have any video clips <b>118</b> associated with it. As an alternative to video footage, the object <b>114</b> can simply be associated with one or more still-images <b>150</b> of the object <b>114</b>. However, it is the video clip <b>118</b> of the object <b>114</b> (e.g. the object clip <b>146</b>) that allows the user <b>102</b> to access photo-realistic video of the object <b>116</b> in a way that allows the user to “experience” the object <b>114</b> in way that human being could experience the object <b>114</b> in the physical world. For example, an object clip <b>146</b> of the object <b>114</b> spinning on a variable speed turntable would allow the user <b>102</b> to view such an activity in a photo-realistic manner.</p>
<p id="p-0095" num="0094">Just as viewpoint video clips <b>118</b> are made up of various views <b>130</b>, so to are object clips <b>146</b>. Each individual frame in an object clip <b>146</b> can be referred to as an object view <b>148</b>. Object views <b>148</b> are different than still-images <b>150</b> because object views <b>148</b> are part of video footage, and need not be artificially stitched together or extrapolated in order to generate video footage.</p>
<p id="p-0096" num="0095">2. Still Images</p>
<p id="p-0097" num="0096">Regardless of whether an object <b>114</b> is associated with one or more object clips <b>146</b>, the object can also be associated with one or more still images <b>150</b>. Still images <b>150</b> can be effective ways to display items that are essentially two-dimensional, such as a painting or photograph. However, a virtual representation <b>112</b> of an art gallery could use still images <b>150</b> for the paintings while relying on object clips <b>148</b> to provide footage relating to the artist and the work. The combinations of still images <b>150</b> and object clips <b>146</b> that can be incorporated into the system <b>100</b> are limitless.</p>
<p id="p-0098" num="0097">3. Object Information</p>
<p id="p-0099" num="0098">In addition to an object <b>116</b> being associated with still images <b>150</b> and object clips <b>146</b>, objects <b>116</b> can also be associated with non-image based formats of object information <b>152</b>. Examples of object information can include: narrative text describing the object <b>114</b>; sounds such as narration describing the object <b>114</b> or even “sound effects” relating to the object <b>116</b>; links to related web sites; contact information; or any other format of information that is not limited to a visual image or video footage. In the example of a representation <b>112</b> of an art gallery, the object <b>116</b> could be a vase and the object information <b>152</b> would be text identifying the artist, the name of the work, and the purchase price.</p>
<p id="p-0100" num="0099">4. Communications</p>
<p id="p-0101" num="0100">Objects <b>116</b> can be preconfigured to facilitate communications <b>154</b> between the user <b>102</b> and some other third party. In the example of an art gallery, object functionality <b>140</b> can include the ability of the user <b>102</b> to send an e-mail inquiring about a particular piece of artwork or even initiating a transaction to purchase a particular piece of artwork. The range of automatically initiated communications <b>154</b> can include e-mail, facsimile, paging, instant messaging, chat room postings, paper correspondence, telephone calls with simulated voices, and any other form of communication <b>154</b> that can potentially be invoked by the user <b>102</b> from the access device <b>104</b>.</p>
<heading id="h-0008" level="1">IV. Interface View</heading>
<p id="p-0102" num="0101"><figref idref="DRAWINGS">FIG. 8</figref> is a diagram illustrating an example of an interface <b>160</b> that can be incorporated into the system <b>120</b>. The interface <b>160</b> can also be referred to as a navigation interface <b>160</b> because it allows the user <b>102</b> to navigate the representation <b>112</b>. The system <b>100</b> can incorporate a wide variety of different interfaces and a wide variety of different interface configurations. In some embodiments of the system <b>100</b>, there are multiple ways to perform the same action. For example, a button can be activated by clicking a mouse, typing on a keyboard, speaking into a voice recognition device, or any other technology that can receive user <b>102</b> inputs. Various buttons and displays on the interface <b>160</b> can be enabled or disabled as appropriate given the characteristics of the representation <b>112</b> and the relevant representation locations. The system <b>100</b> can incorporate a wide variety of different interfaces with a wide variety of different buttons, windows, and other controls.</p>
<p id="p-0103" num="0102">A. Display Window</p>
<p id="p-0104" num="0103">A display window <b>162</b> is the mechanism by which the user <b>102</b> views a representation area and any object icons <b>142</b> and viewpoint icons. Video clips <b>118</b> are viewed through the display window <b>162</b>. There can be multiple display windows <b>162</b> in a single interface <b>160</b>. In a multiple-display window <b>162</b> embodiment, the user <b>102</b> can view the representation area from more than one viewpoint <b>114</b> in a simultaneous manner.</p>
<p id="p-0105" num="0104">B. Compass</p>
<p id="p-0106" num="0105">A compass <b>164</b> provides users <b>102</b> with directional information at each viewpoint <b>114</b> and object <b>116</b> within the representation area. The direction indicated by the compass <b>164</b> is the direction that the user <b>102</b> is facing within the representation area. The compass <b>164</b> moves as the user <b>102</b> moves within the representation area.</p>
<p id="p-0107" num="0106">C. Legend Button</p>
<p id="p-0108" num="0107">A legend button <b>166</b> can be clicked on by the user <b>102</b> to identify how the various icons (such as viewpoint icons <b>120</b> and object icons <b>142</b>) appear on the navigation interface. The legend button <b>166</b> can be used as a “key” or an “index.” In some embodiments, pressing the legend button <b>166</b> will cause a display area to appear on the navigation interface <b>160</b>, and the display area will list the various icons and the names for the various icons.</p>
<p id="p-0109" num="0108">D. “More Information” Button</p>
<p id="p-0110" num="0109">A “more information” button <b>168</b> can be activated by the user <b>102</b> to obtain more information about the representation area, viewpoint <b>114</b>, or object <b>116</b> that is the current “focus” of processing. For example, in a virtual store embodiment of the system <b>100</b>, the object <b>116</b> could be an item for sale. Pressing the “more information” button <b>168</b> would in that context, cause information about the item (such as purchase price, manufacturer, etc.) to appear on screen.</p>
<p id="p-0111" num="0110">E. Blueprint</p>
<p id="p-0112" num="0111">A blueprint <b>170</b> can be used to display a diagram of the representation <b>112</b> or a representation area. In an embodiment where the representation area is the interior of a building, the blueprint <b>170</b> will resemble a “floor plan.” Different embodiments of the system <b>100</b> may incorporate different degrees of detail in the blueprint <b>170</b>. Typically, the blueprint <b>170</b> is a two-dimensional diagram. If the blueprint <b>170</b> is a two-dimensional diagram, then multiple blueprints <b>170</b> can be used to convey information describing the entire three-dimensional representation area. For example, one blueprint <b>170</b> can relate to the x-y plane, while another blueprint <b>170</b> could describe the y-z plane.</p>
<p id="p-0113" num="0112">In a preferred embodiment, viewpoint locations <b>122</b> and object locations <b>144</b> are identified in the blueprint by the icons. In a preferred embodiment, the various icons can be activated by the user <b>102</b> through the blueprint <b>170</b> as well as through the display window <b>162</b>.</p>
<p id="p-0114" num="0113">F. Zoom Button</p>
<p id="p-0115" num="0114">A zoom button <b>172</b> allows the user <b>102</b> to invoke the zoom clip <b>124</b> associated with a particular viewpoint location <b>122</b>. After activating the zoom button <b>172</b>, manipulation of a slider <b>178</b> allows the user <b>102</b> to zoom in or zoom out (e.g. navigate between the various views <b>130</b> making up the zoom clip <b>124</b>) with respect to the representation area embodied in the zoom clip <b>124</b>. In a preferred embodiment, the zoom button <b>172</b> invokes a photo-realistic zoom clip <b>124</b>. If a zoom clip <b>124</b> does not exist for a particular viewpoint <b>114</b>, the zoom button <b>172</b> should be invisible (or at least disabled) when the user <b>102</b> is viewing the representation area from that particular viewpoint <b>114</b>.</p>
<p id="p-0116" num="0115">F. Tilt Button</p>
<p id="p-0117" num="0116">A tilt button <b>174</b> allows the user <b>102</b> to invoke the tilt clip <b>128</b> associated with a particular viewpoint location <b>122</b>. After activating the tilt button <b>174</b>, manipulation of a slider <b>178</b> allows the user <b>102</b> to navigate the representation area in the display window <b>162</b> as if the user <b>102</b> were in the room tilting his or her head in a vertical manner. In a preferred embodiment, the tilt button <b>174</b> invokes a photo-realistic tilt clip <b>128</b>. If a tilt clip <b>128</b> does not exist for a particular viewpoint <b>114</b>, the tilt button <b>174</b> should be invisible (or at least disabled) when the user <b>102</b> is viewing the representation area from that particular viewpoint <b>114</b>.</p>
<p id="p-0118" num="0117">G. Pan Button</p>
<p id="p-0119" num="0118">A pan button <b>176</b> allows the user <b>102</b> to invoke the pan clip <b>126</b> associated with a particular viewpoint location <b>122</b>. After activating the pan button <b>176</b>, manipulation of a slider <b>178</b> allows the user <b>102</b> to navigate the representation area in the display window <b>162</b> as if the user <b>102</b> were turning his or her head in a horizontal manner. In a preferred embodiment, the pan button <b>176</b> invokes a photo-realistic pan clip <b>126</b>. If a pan clip <b>126</b> does not exist for a particular viewpoint <b>114</b>, the pan button <b>174</b> should be invisible (or at least disabled) when the user <b>102</b> is viewing the representation area from the particular viewpoint <b>114</b>.</p>
<p id="p-0120" num="0119">H. Slider</p>
<p id="p-0121" num="0120">A slider control <b>178</b> allows the user to “navigate” video clips <b>118</b> representing the representation area(s). The slider control <b>178</b> will offer the user different options, depending on the characteristics of the viewpoint location <b>122</b> or object location <b>144</b> that is currently “active.” For example, if the tilt button <b>174</b> is pressed, the slider <b>178</b> will be a vertical control allowing the user <b>102</b> to tilt the view in the display window <b>162</b> in a vertical manner. Similarly, if the pan button <b>176</b> is pressed, the slider <b>178</b> will be a horizontal control, and so on and so forth. In situations involving object functionality <b>140</b>, the slider control <b>178</b> can involve multiple different controls for interacting with the object clip <b>146</b>, still images <b>150</b>, object information <b>152</b>, communications <b>152</b>, or other object functionality <b>140</b>.</p>
<p id="p-0122" num="0121">I. Interactivity Buttons</p>
<p id="p-0123" num="0122">Various interactivity buttons <b>182</b> can allow the user <b>182</b> to interact with the individual or organization (collectively “sponsor” or “host”) making available the representation <b>112</b>. For example, the interactivity buttons <b>182</b> can be used by the user <b>102</b> to send an e-mail, instant message, facsimile, or other communication to the sponsor or host of the representation <b>112</b>. Pre-defined “help” functionality can be provided to users <b>102</b> through the use of the interactivity buttons <b>182</b>. The content accessed by the interactivity buttons can be customized in accordance with the particular viewpoint location <b>122</b> or object location <b>144</b> that is currently active in the display window <b>162</b>, or the content can be generic to the representation area or even representation <b>112</b> as a whole.</p>
<p id="p-0124" num="0123">J. Description Field</p>
<p id="p-0125" num="0124">A description field <b>180</b> can be used to provide the user <b>102</b> with information about the representation <b>112</b> as they navigate the representation <b>112</b> or representation area. In a preferred embodiment, the description field <b>180</b> is always on, providing users <b>102</b> with information relating to the currently “active” viewpoint location <b>122</b> or object location <b>144</b>. In such an embodiment, pressing the “more information” button <b>168</b> will result in more detailed information. The description field <b>180</b> provides descriptive information without otherwise blocking the view to the other buttons, controls, and windows of the navigation interface <b>160</b>. The description field <b>180</b> can also be referred to as a label.</p>
<heading id="h-0009" level="1">V. Subsystem-Level View</heading>
<p id="p-0126" num="0125"><figref idref="DRAWINGS">FIG. 9</figref> is a block diagram illustrating an example of a subsystem-level view of the interactive video system <b>100</b>. The functionality of the system <b>100</b> can be divided up into two subsystems, a display subsystem <b>200</b> and a navigation subsystem <b>202</b>. <figref idref="DRAWINGS">FIG. 10</figref> provides a more detailed example of one subsystem-level view of the system <b>100</b>.</p>
<p id="p-0127" num="0126">A. Display Subsystem</p>
<p id="p-0128" num="0127">The display subsystem <b>200</b> is the means by which the representation <b>112</b> is made available to the user <b>102</b> for viewing. As discussed above, the representation <b>112</b> and the various representation areas can be viewed by the user <b>102</b> through one or more display windows <b>162</b>. The display subsystem <b>200</b> may also include the compass <b>164</b> and blueprints <b>170</b> describing the representation <b>112</b>. All video clips <b>118</b> and views <b>130</b>, whether relating to objects <b>116</b> or viewpoints <b>114</b>, are part of the display subsystem <b>200</b>.</p>
<p id="p-0129" num="0128">B. Navigation Subsystem</p>
<p id="p-0130" num="0129">The navigation subsystem <b>202</b> can include any and all of the tools used to interact with the system <b>100</b> and navigate the representation <b>112</b>. The navigation subsystem is responsible for capturing interactions with the user <b>102</b> so that the system <b>100</b> can provide an interactive experience through the display subsystem <b>200</b> for the user <b>102</b>.</p>
<p id="p-0131" num="0130">A zoom tool <b>204</b> allows the user to invoke zoom clips <b>124</b> and zoom views <b>132</b>. A variety of different interface mechanisms can be included to allow the user <b>102</b> to manipulate the zoom tool <b>204</b>, such as the zoom button <b>172</b> and slider <b>178</b> discussed above.</p>
<p id="p-0132" num="0131">A tilt tool <b>206</b> allows the user <b>102</b> to invoke tilt clips <b>128</b> and tilt views <b>136</b>. A wide variety of different interface mechanisms can be included to allow the user <b>102</b> to manipulate the tilt tool <b>206</b>, such as the tilt button <b>174</b> and slider <b>178</b> discussed above.</p>
<p id="p-0133" num="0132">A pan tool <b>208</b> allows the user <b>102</b> to invoke pan clips <b>126</b> and pan views <b>134</b>. A variety of different interface mechanisms can be included to allow the user <b>102</b> to manipulate the pan tool <b>208</b>, such as the pan button <b>176</b> and slider <b>178</b> discussed above.</p>
<p id="p-0134" num="0133">An object tool <b>210</b> allows the user <b>102</b> to invoke object functionality <b>140</b>, object clops <b>146</b>, object views <b>148</b>, object information <b>152</b>, and communications <b>154</b>. A variety of different interface mechanisms can be included to allow the user <b>102</b> to manipulate the object tool <b>210</b>. In a preferred embodiment, those interface mechanisms are customized with respect to the particular object <b>116</b> or type of object. For example, items to be purchased from a virtual store can involve a common set of interface mechanisms that differ from the interface mechanisms of objects <b>116</b> in a video game. The object tool <b>210</b> can include a click tool, an active clickable area, and a scripted behavior relating to the active clickable area in the representation <b>112</b>.</p>
<p id="p-0135" num="0134">The navigation subsystem <b>202</b> should include some mechanism for storing the relationships between the representation <b>112</b> and the various viewpoints <b>114</b>, objects <b>116</b>, video clips <b>118</b>, and views <b>130</b>. Such a mechanism can be referred to as a map <b>212</b>. The map <b>212</b> can be embodied in a wide variety of different formats using a wide variety of different mapping heuristics. In a preferred embodiment, the map <b>212</b> is embedded in the file <b>110</b>, discussed above. In alternative embodiments, a series of files <b>110</b> can be used.</p>
<p id="p-0136" num="0135">In many embodiments, the file <b>110</b> is viewable through the use of a browser such as a Web browser. In preferred embodiments, no external application (e.g. no application besides the browser) is required to support the functionality of the system <b>100</b>. In a preferred embodiment, the browser is a platform independent browser. In some embodiments, the browser will include a build it media player. In other embodiments, an external the system <b>100</b> will interface with an external application.</p>
<p id="p-0137" num="0136">Many different processes and heuristics can be used to create a file <b>110</b> for the system <b>100</b>. In a preferred embodiment, the file <b>110</b> is a compressed .avi file that is less than 1% of the size of the initial avi file used to create the compressed avi file. Depending on the embodiment, the compression percentage can range from 10% to 0.1%. The process for encoding the video clips <b>118</b> into the file <b>110</b> can be referred to as an encode heuristic. By using compressed files <b>110</b> that are small in size, the user <b>102</b> can access the system <b>100</b> through a low-speed “dial-up” connection and still invoke photo-realistic interactions with the system <b>100</b>.</p>
<heading id="h-0010" level="1">VI. Process-Level Views</heading>
<p id="p-0138" num="0137">A. Process for Using the System</p>
<p id="p-0139" num="0138"><figref idref="DRAWINGS">FIG. 11</figref> is a flow chart illustrating an example of a method for interacting with a representation <b>112</b> of physical space. The user <b>102</b> accesses the file containing the representation <b>112</b> at <b>300</b>. As discussed above, this can preferably be done through a platform-independent web browser without requiring external applications such as Java applets. The navigation tools are enabled at <b>302</b> through the invocation of the file <b>110</b>. In a preferred embodiment, the user <b>102</b> invokes the file <b>110</b> through the browser, typically by clicking on a selection of some type. At <b>304</b>, the user <b>102</b> selects one of the navigation tools found in the navigation subsystem <b>202</b> or the navigation interface <b>160</b>. Through the use of the navigation subsystem <b>202</b> or the navigation interface <b>160</b>, video content is selected and invoked at <b>306</b>.</p>
<p id="p-0140" num="0139">B. Process for Creating a File for the System</p>
<p id="p-0141" num="0140"><figref idref="DRAWINGS">FIG. 12</figref> is a flow chart illustrating an example of a method for creating the file <b>110</b> for the interactive video system <b>100</b>. The various reference locations (for objects <b>116</b> and viewpoints <b>114</b>) are defined at <b>310</b>. In some embodiments, there may be only one reference location.</p>
<p id="p-0142" num="0141">One or more blueprints <b>170</b> can be defined at <b>312</b>. The system <b>100</b> does not need to incorporate blueprint <b>170</b> functionality in order for the system <b>100</b> to function, but it is helpful to include such functionality.</p>
<p id="p-0143" num="0142">One or more video clips <b>118</b> can be captured at <b>314</b>. Those video clips <b>118</b> are preferably enhanced at <b>316</b>, a process that is described below. At <b>318</b>, the various video clips <b>118</b> and other forms of information relating to the representation <b>112</b> are embedded into the file <b>110</b>.</p>
<p id="p-0144" num="0143">C. A Second Example of a Process for Creating a File</p>
<p id="p-0145" num="0144"><figref idref="DRAWINGS">FIG. 13</figref> is a detailed flow chart illustrating an example of a method for creating a file <b>110</b> for an interactive video system <b>100</b>.</p>
<p id="p-0146" num="0145">1. Suggested Equipment</p>
<p id="p-0147" num="0146">The equipment and files necessary to perform this process include a digital video camera. The camera should include a tripod, and a three chip camera with a fluid head tripod is preferred. In some embodiments, a one chip camera with a standard tripod can also yield excellent results. A compass is recommended with respect to embodiments of the system <b>100</b> that include compass <b>164</b> functionality. Some type computer device is needed to create the necessary file(s) <b>110</b>. Current applications that are used in one embodiment of the system <b>100</b> include After Effects 4.0 by ADOBE. Premier 6.5 by ADOBE, Flash Turbine 1.0 by BLUE PACIFIC, Flash 5.0 by MICROMEDIA, and Dreamweaver 3.0 by MACROMEDIA. The system <b>100</b> is highly flexible and configurable. A wide variety of different software applications can be used to create files <b>110</b> for the system <b>100</b>. If Flash movie files (.fla files) are used, there are preferably three files: one being a premovie file (premovie.fla), one being a template for the controller (alpha_ctr.fla) and one being a template for actions and graphics (alpha_ptz.fla).</p>
<p id="p-0148" num="0147">2. Blueprint Heuristic</p>
<p id="p-0149" num="0148">In a preferred embodiment, blueprint <b>170</b> functionality is included in the system <b>100</b>. Even if a blueprint <b>170</b> is not displayed on the interface <b>160</b>, it is preferable to begin the process of creating a file <b>110</b> of the representation <b>112</b> by defining what the representation <b>112</b> is at <b>320</b>.</p>
<p id="p-0150" num="0149">For true representations, this process involves reviewing the representation area that is to be shot and decide upon camera angles, lighting, etc. Then a rough sketch of a blueprint <b>170</b> of the representation area and any relevant objects <b>114</b>. It is desirable to use a standardized naming convention for identifying various shots.</p>
<p id="p-0151" num="0150">3. Shooting Heuristic</p>
<p id="p-0152" num="0151">After the blueprint <b>170</b> is created (or at least a rough sketch is created), a shoot heuristic for the representation area (or even the entire representation <b>112</b>) can be performed at <b>322</b>. The system <b>100</b> can incorporate the results of a wide variety of different shooting heuristics at <b>322</b>. In a true representation embodiment, videographic and photographic expertise should be applied in order to judge lighting, manage mirrors, and to get optimal angles. Such expertise can take many forms, depending on the context of the shot. Prior to any filming the digital camera must have several features configured properly. These features include, white balancing, setting the exposure, and disabling the digital zoom. Once the camera is properly configured, specific procedural steps can be followed for filming.</p>
<p id="p-0153" num="0152"><figref idref="DRAWINGS">FIG. 14</figref> is a flow chart illustrating a detailed example of the shoot heuristic at <b>322</b>. At <b>322</b>.<b>02</b>, the entrance shot is taken. In an interior representation <b>112</b> embodiment, the first clip <b>118</b> of a physical space is generally a pan shot from directly in front of the entry door. The aim is to accurately simulate the human experience of walking through the physical space to be represented.</p>
<p id="p-0154" num="0153">At <b>322</b>.<b>04</b>, subsequent viewpoints <b>114</b> are shot in a counter-clockwise rotation around the physical space to be represented. The various clip locations are identified and selected and <b>322</b>.<b>06</b>.</p>
<p id="p-0155" num="0154">a. Pan Clip</p>
<p id="p-0156" num="0155">If the clip <b>118</b> at <b>322</b>.<b>08</b> is to be a pan clip <b>126</b>, the camera person locks the tripod to pan mode only. Once the tripod has been positioned the camera person decides on either a standard pan angles which includes 90, 180, 270, and 360 degrees (as illustrated in <figref idref="DRAWINGS">FIGS. 5</figref><i>b</i>, <b>5</b><i>c</i>, <b>5</b><i>d</i>, and <b>5</b><i>e </i>as discussed above) or a non-standard pan. Once the pan degrees are decided, a compass <b>164</b> reading is taken at the absolute left and absolute right of the pan such that the camera is facing the center of the physical space when the pan is at its middle position. The name of the shot, the type of pan used (number of degrees), and the compass readings from the absolute left, middle, and absolute right pan positions are written down. At <b>322</b>.<b>10</b>, the camera person should preferably starts recording at the absolute left pan position of the shot and moves the camera from absolute left to absolute right, which in some cases can be a complete 360 degree pan.</p>
<p id="p-0157" num="0156">b. Tilt Clip</p>
<p id="p-0158" num="0157">If the clip <b>118</b> at <b>322</b>.<b>14</b> is to be a tilt clip <b>128</b>, the camera person locks the tripod to tilt mode only at <b>322</b>.<b>16</b>. Once the tripod has been positioned the camera person takes a compass <b>164</b> reading of the direction the camera is facing during the tilt and writes down the name of the shot and the compass reading. Next the camera is tilted to the absolute bottom then, while shooting, the camera is then tilted up to its absolute upwards facing position. This angle from absolute bottom to absolute top is between 0-360 degrees. The tilt clip <b>128</b> should be captured in such a way as to mimic the impression of person looking upwards.</p>
<p id="p-0159" num="0158">C. Zoom Clip</p>
<p id="p-0160" num="0159">If the clip <b>118</b> at <b>322</b>.<b>18</b> is a zoom clip <b>124</b>, the camera person at <b>322</b>.<b>20</b> determines the end point of the zoom and centers camera accordingly, locking both the pan and tilt to ensure stability of the shot. The camera person then takes a compass reading of the direction the camera is facing and writes it down along with the name of the shot. Next the camera person records a zoom from as wide as possible and zooms into the selected spot.</p>
<p id="p-0161" num="0160">Each of the recordings identified above should be taken at least twice to make sure that a usable shot “in the can.” However, the compass <b>164</b> reading need only be taken once since it does not vary from one take of a shot to another. The video clips <b>118</b> discussed above relate to viewpoints <b>114</b>. A different procedure can be followed with respect to object clips <b>146</b>.</p>
<p id="p-0162" num="0161">d. Object Clip</p>
<p id="p-0163" num="0162">A wide variety of different object shoot heuristics at <b>322</b>.<b>22</b> can be incorporated into the processing of the system <b>100</b>. <figref idref="DRAWINGS">FIG. 15</figref> is a flow chart illustrating an example of an object shoot heuristic that can be performed at <b>322</b>.<b>22</b>.</p>
<p id="p-0164" num="0163">At <b>322</b>.<b>22</b><i>a</i>, the object <b>116</b> can be placed on a variable speed turntable in a blue screen studio. At <b>322</b>.<b>22</b><i>b</i>, the object <b>116</b> can be centered on the turntable to accurately simulate the object's <b>116</b> center of gravity. At <b>322</b>.<b>22</b><i>c</i>, the camera can be placed on the tripod, and aimed directly at the object <b>116</b> at a variable angle (typically between −45 degrees to +45 degrees). At <b>322</b>.<b>22</b><i>d</i>, the camera person can start recording. The turntable is preferably turned on to a motion level of 2 (slow). The recording is stopped once a full 360 degree rotation has been completed. After all objects <b>116</b> in the representation area have been filmed, the process then returns to <b>322</b>.<b>13</b> of <figref idref="DRAWINGS">FIG. 14</figref>.</p>
<p id="p-0165" num="0164">In a preferred embodiment, each viewpoint <b>112</b> is associated with a video clip <b>118</b>. However, there are many instances where an object clip <b>144</b> may not be necessary. In some contexts (such as where the object <b>116</b> is a painting on a wall), a still image <b>150</b> will be desirable. In other contexts (such as an online virtual store), the object clip <b>146</b> might be a “commercial” relating to the object.</p>
<p id="p-0166" num="0165">Returning to <figref idref="DRAWINGS">FIG. 14</figref>, a determination is made at <b>322</b>.<b>12</b> whether additional clips are needed. If additional clips <b>118</b> need to be taken, the process returns to the selection step at <b>322</b>.<b>06</b>. Otherwise, a compass reading can be captured at <b>322</b>.<b>24</b>. In some embodiments, the compass reading can be captured before the various video clips <b>118</b> are shot.</p>
<p id="p-0167" num="0166">After all of the steps in <figref idref="DRAWINGS">FIG. 14</figref> are performed, the process can then return to the process flow disclosed in <figref idref="DRAWINGS">FIG. 13</figref>.</p>
<p id="p-0168" num="0167">3. Capture Heuristic</p>
<p id="p-0169" num="0168">A capture heuristic <b>324</b> can be performed at <b>324</b> after all of the video clips <b>118</b> have been taken. <figref idref="DRAWINGS">FIG. 16</figref> is a flow chart illustrating an example of a capture heuristic that can be performed at <b>324</b>. The capture heuristic at <b>324</b> can vary widely from embodiment to embodiment, so long as the end result is a video file consistent with the file format of the system <b>100</b>. In a preferred embodiment, the video camera is connected to a computer that has a license copy of the ADOBE Premier application to capture the raw video data in a MiniDV format. Importing video via firewire compresses the resulting digital video file (preferably an avi file). In the capture heuristic <b>324</b> illustrated in <figref idref="DRAWINGS">FIG. 16</figref>, each step results in an incremental compression of the source video file size. By the end of the process, the resulting file is typically less than 1% of the initial size of the file. In almost all circumstances, the resulting file is no greater than 10% of the initial size of the file.</p>
<p id="p-0170" num="0169">At <b>324</b>.<b>02</b>, the frames are resized in accordance with the desired output format. In a typical embodiment, the desired output formats can be 360×240, 660×360, 720×480, or some other format known in the art.</p>
<p id="p-0171" num="0170">At <b>324</b>.<b>04</b>, the frame rate can be optimized. For example, the frame rate of the video file can be changed from the raw MiniDV (29.97 fps) footage according to the desired compression and playback usage. Different types of encoded video will involve different optimal settings. For example, relatively higher frame rates should be used for zoom shots, and relatively lower frame rates should be used for pans and tilts. The goal is to present the user <b>102</b> with a photo-realistic experience of the representation <b>112</b>.</p>
<p id="p-0172" num="0171">At <b>324</b>.<b>06</b>, the duration of each video clip <b>118</b> can be changed. In a preferred embodiment, each clip <b>118</b> is time-compressed between a range of 50% to 25%. This maintains the photo-realistic nature of the user's experience, while supporting significant file compression.</p>
<p id="p-0173" num="0172">A new file (an .avi file in a preferred embodiment) can then be exported at <b>324</b>.<b>08</b>.</p>
<p id="p-0174" num="0173">In a preferred embodiment, each outputted Microsoft Audio/Video (clip) file is saved at <b>324</b>.<b>10</b> in a folder named after the space captured in that clip. The files follow a naming convention for ease of use later on, that being: spaceName_p#.avi for pan shots, spaceName_t#.avi for tilt shots or spaceName_z#.avi for zoom shots, or spaceName_o#.avi for objects.</p>
<p id="p-0175" num="0174">The process then returns to the process flow disclosed in <figref idref="DRAWINGS">FIG. 13</figref>.</p>
<p id="p-0176" num="0175">4. Encode Heuristic</p>
<p id="p-0177" num="0176">Returning to <figref idref="DRAWINGS">FIG. 13</figref>, the process continues with the performance of an encode heuristic at <b>326</b>. An example of the encode heuristic at <b>326</b> is disclosed in <figref idref="DRAWINGS">FIG. 17</figref>.</p>
<p id="p-0178" num="0177">In the performance of the encode heuristic, each captured clip is converted into a shockwave flash format (a .swf file) through the use of a Flash Turbine software application. Different embodiments may utilize different software applications to generate end results of varying formats.</p>
<p id="p-0179" num="0178">At <b>326</b>.<b>02</b>, each captured video clip <b>118</b> is opened with the encoding application. Various configuration parameters are selected at <b>326</b>.<b>04</b>. Such parameters can include video quality, JPEG quality, frame rate, frame size, smoothness, bit Rate limit, audio encoding, transient type, transient quality, collapse type, collapse quality, span type, span quality, span size, and image darken. The parameter selection process at <b>326</b>.<b>04</b> includes the selecting of the amount of time from the original file to encode.</p>
<p id="p-0180" num="0179">The new file <b>110</b> can then be created as an output at <b>326</b>.<b>06</b>. In some embodiments, the outputted file is in a format of a .swf file (the file format for the Shockwave Flash Player). Each outputted Shockwave Flash Player (player) file is saved in a folder named after the space captured in that clip. The files follow a naming convention of ease of use later on, that being: spaceName_p#.swf for pan shots, spaceName_t#.swf for tilt shots or spaceName_z#.swf for zoom shots, or spaceName_o#.swf for objects.</p>
<p id="p-0181" num="0180">The process then returns to the process flow disclosed in <figref idref="DRAWINGS">FIG. 13</figref>.</p>
<p id="p-0182" num="0181">5. Animate Heuristic</p>
<p id="p-0183" num="0182">After the performance of the encode heuristic at <b>326</b>, an animate heuristic can be performed at <b>328</b>. <figref idref="DRAWINGS">FIG. 18</figref> is a flow chart illustrating an example of an animate heuristic. In the performance of the animate heuristic <b>326</b>, each of the encoded Flash Player (*.swf) files are loaded into the Flash 5.0 application (by MACROMEDIA) or some other similar application tool providing a desirable authoring environment and the ability to position the viewable data for the controller which displays it to the user. The compass directions, as well as all hotspots (e.g. clickable areas in the representation <b>112</b>) are also incorporated into each FlashPlayer file. The animate heuristic is the process by which user interactivity is integrated into the file <b>110</b>. <figref idref="DRAWINGS">FIG. 18</figref> discloses one example of the processing that can be included in the animate heuristic.</p>
<p id="p-0184" num="0183">At <b>328</b>.<b>02</b>, the previous encoded file can be imported to the relevant application. In a preferred embodiment, the application is Flash 5.0 by MACROMEDIA.</p>
<p id="p-0185" num="0184">At <b>328</b>.<b>04</b>, viewable content is moved to a particular X,Y coordinate as represented in the map <b>212</b>, discussed above. In the Flash 5.0 application, selecting “all keyframes” moves the viewable content to a particular (X,Y) numeric position based upon the controller.</p>
<p id="p-0186" num="0185">At <b>328</b>.<b>08</b>, layers and viewpoints <b>114</b> are added. In a Flash 5.0 application, two new layers and add hotspot buttons from a graphic template library to the relevant areas of the movie and add the action that will happen when the button is clicked.</p>
<p id="p-0187" num="0186">At <b>328</b>.<b>10</b>, compass directions are integrated into the encoded file <b>110</b>. This allows users <b>102</b> to experience the correct orientation with respect to the display area <b>162</b> and the compass <b>164</b>.</p>
<p id="p-0188" num="0187">At <b>328</b>.<b>12</b>, the output file is created. In a preferred embodiment, the outputted file is in the form of a single Flash Movie file. In a preferred embodiment, the file <b>110</b> is located in a folder named after the room in a folder called movies, and with the same naming convention used through the process, spaceName_p#.fla for example (a pan), note that now the file is a Flash Movie not a Flash Player file (.fla not .swf). The .swf files are deleted once they have been outputted as fla files.</p>
<p id="p-0189" num="0188">After all encoded files have been animated with hotspots, compass directions, and position alignment, and saved as Flash Movies, each movie file can be exported back into Shockwave Flash Player format in a separate folder (outside the movies folder) of the same name as the space (Player files are saved with the same naming convention but (.swf) at the end, for example, spaceName_p#.swf). At <b>328</b>.<b>14</b>, the controller template can be opened and configured.</p>
<p id="p-0190" num="0189">Such a process includes setting a list of variables at <b>328</b>.<b>16</b>. For each animated movie we set a list of variables at the beginning of the controller, these variables are: obj, mov_num, track_dir, pan_r_tog, tilt_z_tog, layer1._visible, layer2._visible, startup, the two or three movies to load (including the premovie which plays which the movie is loading), scroller._x, scroll._y.</p>
<p id="p-0191" num="0190">A master template can then be opened for the desired controller, and customized pursuant to the steps described below.</p>
<p id="p-0192" num="0191">At <b>328</b>.<b>18</b>, the graphical representation of the blueprint <b>164</b> of the representation <b>112</b> is created. At <b>328</b>.<b>20</b>, action buttons can be added from the graphics template library, assigning actions to correspond to each button. At <b>328</b>.<b>22</b>, ancillary and support functionality is incorporated into the file <b>110</b>. For example, the text for the “more information” button <b>168</b>, the legend button <b>166</b>, interactivity buttons <b>182</b> (including e-mail and other communications) are added at this stage in the process.</p>
<p id="p-0193" num="0192">At <b>328</b>.<b>24</b>, the results (the controller Movie) is then outputted as a file (a Shockwave Flash Player file in a preferred embodiment) into the folder of which matches the name of the representation area.</p>
<p id="p-0194" num="0193">6. Additional Representation Areas</p>
<p id="p-0195" num="0194">Returning to <figref idref="DRAWINGS">FIG. 13</figref>, the process flow from the blueprint heuristic at <b>320</b> through the animate heuristic at <b>328</b> is repeated for each representation area (subspace) in the representation <b>112</b>.</p>
<p id="p-0196" num="0195">For each subspace in a physical space (i.e. rooms in a house) each physical space contains X number of subspaces each of which has X number of different hotspots which can link to either a pan movie, tilt movie, zoom movie, object movie, webpage, or other scripted action (such as sending an email or interfacing with a database, or interfacing with an ecommerce system). At the end we have the following directories and files:</p>
<p id="p-0197" num="0196">One main folder with the name of the application (say, “Demo).</p>
<p id="p-0198" num="0197">In the Demo folder is at least two folders, one which is called “Movies” and one folder for each room in the Demo house which contains Shockwave Flash Player files.</p>
<p id="p-0199" num="0198">In the Movies folder is one folder for each room in the Demo house containing Flash Movie files.</p>
<p id="p-0200" num="0199">7. Template Customization</p>
<p id="p-0201" num="0200">At <b>332</b>, template customization is performed. A web template is picked and customized to contain the Shockwave Flash Player files, this web template file (and all necessary images) are placed in each of the room folders, but not inside the Movies folder.</p>
<p id="p-0202" num="0201">8. Configure Access Mechanism</p>
<p id="p-0203" num="0202">The file <b>110</b> including the representation <b>112</b> is ready to run after the access mechanism is configured at <b>334</b>. In a preferred embodiment, each subspace (e.g. representation area) folder contains all necessary files to display on any browser. The subspace folders can be loaded onto a CD-ROM for browsing locally, or the folders can be uploaded to a web server or other network configuration for viewing online.</p>
<p id="p-0204" num="0203">Should changes need be made on any file <b>110</b>, each of the processes above can be repeated so that the desired change is incorporated into the file <b>110</b>. At any point, implementers of the system <b>100</b> can return and reassign actions, buttons, graphics, or text. In a preferred embodiment, the coding of all button actions refer an external database. In an e-commerce application, Storeowners could always maintain their graphical interface and keep changing the items that are in the database to allow for a scalable system.</p>
<heading id="h-0011" level="1">VII. Alternative Embodiments</heading>
<p id="p-0205" num="0204">The above description is intended to be illustrative and not restrictive. Many embodiments and applications other than the examples provided would be apparent to those of skill in the art upon reading the above description. The scope of the invention should be determined, not with reference to the above description, but should instead be determined with reference to the appended claims, along with the full scope of equivalents to which such claims are entitled. It is anticipated and intended that future developments will occur in image alignment systems and methods, and that the invention will be incorporated into such future embodiments.</p>
<?DETDESC description="Detailed Description" end="tail"?>
</description>
<us-claim-statement>The invention claimed is:</us-claim-statement>
<claims id="claims">
<claim id="CLM-00001" num="00001">
<claim-text>1. An interactive video system comprising:
<claim-text>a representation of physical space, said representation comprising:
<claim-text>a first viewpoint, said first viewpoint including:
<claim-text>a first video clip;</claim-text>
<claim-text>a plurality of views, including a first view and a second view;</claim-text>
</claim-text>
</claim-text>
<claim-text>a navigation tool, said navigation tool providing for a first transition from said first view to said second view, wherein said first transition is said first video clip; and</claim-text>
<claim-text>a file storing said representation, wherein said file is less than 1% of the size of an initial .avi file used to create said file.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00002" num="00002">
<claim-text>2. An interactive video system comprising:
<claim-text>a representation of physical space, said representation comprising:
<claim-text>a first viewpoint, said first viewpoint including:
<claim-text>a first video clip;</claim-text>
<claim-text>a plurality of views, including a first view and a second view;</claim-text>
</claim-text>
</claim-text>
<claim-text>a navigation tool, said navigation tool providing for a first transition from said first view to said second view, wherein said first transition is said first video clip;</claim-text>
<claim-text>a configuration heuristic;</claim-text>
<claim-text>a frame rate;</claim-text>
<claim-text>a bit rate; and</claim-text>
<claim-text>a compression factor, wherein said configuration heuristic is invoked to automatically determine said frame rate, said bit rate, and said compression factor, and wherein said representation is stored in accordance with said frame rate, said bit rate, and said compression factor.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00003" num="00003">
<claim-text>3. An interactive video system, comprising:
<claim-text>a display subsystem, including:
<claim-text>a plurality of views, comprising a first view, a zoom view, a tilt view, and a pan view;</claim-text>
<claim-text>a plurality of video clips, comprising a pan clip, a zoom clip, and a tilt clip, and an object clip;</claim-text>
<claim-text>a plurality of viewpoints, each said viewpoint comprising a subset of said plurality of views, a subset of said plurality of video clips, a viewpoint location and a viewpoint icon;</claim-text>
</claim-text>
<claim-text>a photo-realistic object, said object comprising said object clip, an object location and an object icon;</claim-text>
<claim-text>a photo-realistic representation of physical space, said representation comprising said plurality of viewpoints and said object, wherein said object icon identifies said object location, and wherein said viewpoint icons identify said viewpoints locations for said viewpoints;</claim-text>
<claim-text>a blueprint, wherein said blueprint is a two-dimensional cross-sectional view of said representation, wherein said blueprint includes said viewpoint icon and said object icon; and</claim-text>
<claim-text>a compass, comprising a direction of a current view, wherein said compass indicates said current direction of said current view; and</claim-text>
<claim-text>a navigation subsystem, including:</claim-text>
<claim-text>a zoom tool, wherein said zoom tool provides for the navigation from said first view to said zoom view using said zoom clip;</claim-text>
<claim-text>a tilt tool, wherein said title tool provides for the navigation from said first view to said tilt view using said tilt clip; and</claim-text>
<claim-text>a pan tool, wherein said pan tool provides for the navigation from said first view to said tilt view using said pan clip.</claim-text>
</claim-text>
</claim>
</claims>
</us-patent-grant>
