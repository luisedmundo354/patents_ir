<us-patent-grant lang="EN" dtd-version="v4.2 2006-08-23" file="US07299182-20071120.XML" status="PRODUCTION" id="us-patent-grant" country="US" date-produced="20071106" date-publ="20071120">
<us-bibliographic-data-grant>
<publication-reference>
<document-id>
<country>US</country>
<doc-number>07299182</doc-number>
<kind>B2</kind>
<date>20071120</date>
</document-id>
</publication-reference>
<application-reference appl-type="utility">
<document-id>
<country>US</country>
<doc-number>10142406</doc-number>
<date>20020509</date>
</document-id>
</application-reference>
<us-application-series-code>10</us-application-series-code>
<us-term-of-grant>
<us-term-extension>636</us-term-extension>
</us-term-of-grant>
<classifications-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>G</section>
<class>10</class>
<subclass>L</subclass>
<main-group>13</main-group>
<subgroup>00</subgroup>
<symbol-position>F</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20071120</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>G</section>
<class>10</class>
<subclass>L</subclass>
<main-group>13</main-group>
<subgroup>02</subgroup>
<symbol-position>L</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20071120</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
</classifications-ipcr>
<classification-national>
<country>US</country>
<main-classification>704258</main-classification>
<further-classification>704260</further-classification>
<further-classification>704267</further-classification>
</classification-national>
<invention-title id="d0e53">Text-to-speech (TTS) for hand-held devices</invention-title>
<references-cited>
<citation>
<patcit num="00001">
<document-id>
<country>US</country>
<doc-number>4377345</doc-number>
<kind>A</kind>
<name>Yamada et al.</name>
<date>19830300</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>368245</main-classification></classification-national>
</citation>
<citation>
<patcit num="00002">
<document-id>
<country>US</country>
<doc-number>4389121</doc-number>
<kind>A</kind>
<name>Hashimoto et al.</name>
<date>19830600</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>368 63</main-classification></classification-national>
</citation>
<citation>
<patcit num="00003">
<document-id>
<country>US</country>
<doc-number>4701862</doc-number>
<kind>A</kind>
<name>Washizuka et al.</name>
<date>19871000</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>704274</main-classification></classification-national>
</citation>
<citation>
<patcit num="00004">
<document-id>
<country>US</country>
<doc-number>4985697</doc-number>
<kind>A</kind>
<name>Boulton</name>
<date>19910100</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<patcit num="00005">
<document-id>
<country>US</country>
<doc-number>5386493</doc-number>
<kind>A</kind>
<name>Degen et al.</name>
<date>19950100</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>704267</main-classification></classification-national>
</citation>
<citation>
<patcit num="00006">
<document-id>
<country>US</country>
<doc-number>5611018</doc-number>
<kind>A</kind>
<name>Tanaka et al.</name>
<date>19970300</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>704215</main-classification></classification-national>
</citation>
<citation>
<patcit num="00007">
<document-id>
<country>US</country>
<doc-number>5615380</doc-number>
<kind>A</kind>
<name>Hyatt</name>
<date>19970300</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>345520</main-classification></classification-national>
</citation>
<citation>
<patcit num="00008">
<document-id>
<country>US</country>
<doc-number>5694521</doc-number>
<kind>A</kind>
<name>Shlomot et al.</name>
<date>19971200</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>704262</main-classification></classification-national>
</citation>
<citation>
<patcit num="00009">
<document-id>
<country>US</country>
<doc-number>5771273</doc-number>
<kind>A</kind>
<name>McAllister et al.</name>
<date>19980600</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>379 8801</main-classification></classification-national>
</citation>
<citation>
<patcit num="00010">
<document-id>
<country>US</country>
<doc-number>5812977</doc-number>
<kind>A</kind>
<name>Douglas</name>
<date>19980900</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>704275</main-classification></classification-national>
</citation>
<citation>
<patcit num="00011">
<document-id>
<country>US</country>
<doc-number>5826231</doc-number>
<kind>A</kind>
<name>Vigier</name>
<date>19981000</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>704262</main-classification></classification-national>
</citation>
<citation>
<patcit num="00012">
<document-id>
<country>US</country>
<doc-number>5850629</doc-number>
<kind>A</kind>
<name>Holm et al.</name>
<date>19981200</date>
</document-id>
</patcit>
<category>cited by other</category>
<classification-national><country>US</country><main-classification>704260</main-classification></classification-national>
</citation>
<citation>
<patcit num="00013">
<document-id>
<country>US</country>
<doc-number>6009398</doc-number>
<kind>A</kind>
<name>Mueller et al.</name>
<date>19991200</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>704275</main-classification></classification-national>
</citation>
<citation>
<patcit num="00014">
<document-id>
<country>US</country>
<doc-number>6182041</doc-number>
<kind>B1</kind>
<name>Li et al.</name>
<date>20010100</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>704260</main-classification></classification-national>
</citation>
<citation>
<patcit num="00015">
<document-id>
<country>US</country>
<doc-number>6236622</doc-number>
<kind>B1</kind>
<name>Blackman</name>
<date>20010500</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>368 10</main-classification></classification-national>
</citation>
<citation>
<patcit num="00016">
<document-id>
<country>US</country>
<doc-number>6310833</doc-number>
<kind>B1</kind>
<name>Guyett et al.</name>
<date>20011000</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>368 63</main-classification></classification-national>
</citation>
<citation>
<patcit num="00017">
<document-id>
<country>US</country>
<doc-number>6324511</doc-number>
<kind>B1</kind>
<name>Kiraly et al.</name>
<date>20011100</date>
</document-id>
</patcit>
<category>cited by other</category>
<classification-national><country>US</country><main-classification>704260</main-classification></classification-national>
</citation>
<citation>
<patcit num="00018">
<document-id>
<country>US</country>
<doc-number>6557173</doc-number>
<kind>B1</kind>
<name>Hendricks</name>
<date>20030400</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>725142</main-classification></classification-national>
</citation>
<citation>
<patcit num="00019">
<document-id>
<country>US</country>
<doc-number>6633741</doc-number>
<kind>B1</kind>
<name>Posa et al.</name>
<date>20031000</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>434317</main-classification></classification-national>
</citation>
<citation>
<patcit num="00020">
<document-id>
<country>US</country>
<doc-number>6748358</doc-number>
<kind>B1</kind>
<name>Iwasaki et al.</name>
<date>20040600</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>704258</main-classification></classification-national>
</citation>
<citation>
<patcit num="00021">
<document-id>
<country>US</country>
<doc-number>6838994</doc-number>
<kind>B2</kind>
<name>Gutta et al.</name>
<date>20050100</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>340575</main-classification></classification-national>
</citation>
<citation>
<patcit num="00022">
<document-id>
<country>US</country>
<doc-number>6876969</doc-number>
<kind>B2</kind>
<name>Nakao</name>
<date>20050400</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>704260</main-classification></classification-national>
</citation>
<citation>
<patcit num="00023">
<document-id>
<country>US</country>
<doc-number>6925437</doc-number>
<kind>B2</kind>
<name>Hayashi</name>
<date>20050800</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>704260</main-classification></classification-national>
</citation>
<citation>
<patcit num="00024">
<document-id>
<country>US</country>
<doc-number>7240005</doc-number>
<kind>B2</kind>
<name>Chihara</name>
<date>20070700</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>704267</main-classification></classification-national>
</citation>
<citation>
<patcit num="00025">
<document-id>
<country>US</country>
<doc-number>2001/0027395</doc-number>
<kind>A1</kind>
<name>Sakai et al.</name>
<date>20011000</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>704258</main-classification></classification-national>
</citation>
<citation>
<patcit num="00026">
<document-id>
<country>US</country>
<doc-number>2002/0107591</doc-number>
<kind>A1</kind>
<name>Gabai et al.</name>
<date>20020800</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>700 94</main-classification></classification-national>
</citation>
<citation>
<patcit num="00027">
<document-id>
<country>US</country>
<doc-number>2002/0184189</doc-number>
<kind>A1</kind>
<name>Hay et al.</name>
<date>20021200</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>707  1</main-classification></classification-national>
</citation>
<citation>
<patcit num="00028">
<document-id>
<country>US</country>
<doc-number>2003/0004723</doc-number>
<kind>A1</kind>
<name>Chihara</name>
<date>20030100</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>704260</main-classification></classification-national>
</citation>
<citation>
<patcit num="00029">
<document-id>
<country>US</country>
<doc-number>2003/0009337</doc-number>
<kind>A1</kind>
<name>Rupsis</name>
<date>20030100</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>704260</main-classification></classification-national>
</citation>
<citation>
<patcit num="00030">
<document-id>
<country>US</country>
<doc-number>2003/0014252</doc-number>
<kind>A1</kind>
<name>Shizuka et al.</name>
<date>20030100</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>704258</main-classification></classification-national>
</citation>
<citation>
<patcit num="00031">
<document-id>
<country>EP</country>
<doc-number>0 339 316</doc-number>
<kind>A</kind>
<date>19891100</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<patcit num="00032">
<document-id>
<country>WO</country>
<doc-number>WO 01/01373</doc-number>
<kind>A</kind>
<date>20010100</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<nplcit num="00033">
<othercit>International Search Report for International Application No. PCT/US03/14301, Jul. 14, 2003.</othercit>
</nplcit>
<category>cited by other</category>
</citation>
</references-cited>
<number-of-claims>23</number-of-claims>
<us-exemplary-claim>12</us-exemplary-claim>
<us-field-of-classification-search>
<classification-national>
<country>US</country>
<main-classification>704258</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>704260</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>704261</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>704267</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>704268</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>704269</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>704270</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>704271</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>704274</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>368244</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>368245</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>340575</main-classification>
</classification-national>
</us-field-of-classification-search>
<figures>
<number-of-drawing-sheets>5</number-of-drawing-sheets>
<number-of-figures>5</number-of-figures>
</figures>
<us-related-documents>
<related-publication>
<document-id>
<country>US</country>
<doc-number>20030212559</doc-number>
<kind>A1</kind>
<date>20031113</date>
</document-id>
</related-publication>
</us-related-documents>
<parties>
<applicants>
<applicant sequence="001" app-type="applicant-inventor" designation="us-only">
<addressbook>
<last-name>Xie</last-name>
<first-name>Jianlei</first-name>
<address>
<city>Carmel</city>
<state>IN</state>
<country>US</country>
</address>
</addressbook>
<nationality>
<country>omitted</country>
</nationality>
<residence>
<country>US</country>
</residence>
</applicant>
</applicants>
<agents>
<agent sequence="01" rep-type="attorney">
<addressbook>
<last-name>Laks</last-name>
<first-name>Joseph J.</first-name>
<address>
<country>unknown</country>
</address>
</addressbook>
</agent>
<agent sequence="02" rep-type="attorney">
<addressbook>
<last-name>Kurdyla</last-name>
<first-name>Ronald H.</first-name>
<address>
<country>unknown</country>
</address>
</addressbook>
</agent>
<agent sequence="03" rep-type="attorney">
<addressbook>
<last-name>Dorini</last-name>
<first-name>Brian J.</first-name>
<address>
<country>unknown</country>
</address>
</addressbook>
</agent>
</agents>
</parties>
<assignees>
<assignee>
<addressbook>
<orgname>Thomson Licensing</orgname>
<role>03</role>
<address>
<city>Boulogne</city>
<country>FR</country>
</address>
</addressbook>
</assignee>
</assignees>
<examiners>
<primary-examiner>
<last-name>Lerner</last-name>
<first-name>Martin</first-name>
<department>2626</department>
</primary-examiner>
</examiners>
</us-bibliographic-data-grant>
<abstract id="abstract">
<p id="p-0001" num="0000">There is provided an Ebook. The Ebook includes a memory device, a text-to-speech (TTS) module, and at least one speaker. The memory device stores files. The files include text. The TTS module synthesizes speech corresponding to the text. The at least one speaker outputs the speech.</p>
</abstract>
<drawings id="DRAWINGS">
<figure id="Fig-EMI-D00000" num="00000">
<img id="EMI-D00000" he="180.00mm" wi="158.92mm" file="US07299182-20071120-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00001" num="00001">
<img id="EMI-D00001" he="202.01mm" wi="165.27mm" orientation="landscape" file="US07299182-20071120-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00002" num="00002">
<img id="EMI-D00002" he="152.65mm" wi="156.72mm" orientation="landscape" file="US07299182-20071120-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00003" num="00003">
<img id="EMI-D00003" he="223.86mm" wi="162.39mm" file="US07299182-20071120-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00004" num="00004">
<img id="EMI-D00004" he="219.79mm" wi="160.36mm" file="US07299182-20071120-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00005" num="00005">
<img id="EMI-D00005" he="219.88mm" wi="159.26mm" file="US07299182-20071120-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
</drawings>
<description id="description">
<?BRFSUM description="Brief Summary" end="lead"?>
<heading id="h-0001" level="1">CROSS-REFERENCE TO RELATED APPLICATIONS</heading>
<p id="p-0002" num="0001">This application is related to U.S. patent application Ser. No. 10/154,147, entitled “Talking Ebook”, filed on May 22, 2002, U.S. patent application Ser. No. 10/146,406, entitled “Voice Command and Voice Recognition for Hand-Held Devices”, filed on May 15, 2002, and U.S. patent application Ser. No. 10/135,151, entitled “Mixing Music and Text-To-Speech (TTS) for Hand-Held Devices”, filed on Apr. 23, 2002, which are commonly assigned and concurrently filed herewith, and the disclosures of which are incorporated herein by reference.</p>
<heading id="h-0002" level="1">BACKGROUND OF THE INVENTION</heading>
<p id="p-0003" num="0002">1. Field of the Invention</p>
<p id="p-0004" num="0003">The present invention generally relates to hand-held devices and, more particularly, to text-to-speech (TTS) for hand-held devices.</p>
<p id="p-0005" num="0004">2. Background of the Invention</p>
<p id="p-0006" num="0005">An electronic book (also referred to as an “Ebook”) is an electronic version of a traditional print book (or other printed material such as, for example, a magazine, newspaper, and so forth) that can be read by using a personal computer or by using an Ebook reader. Unlike PCs or handheld computers, Ebook readers deliver a reading experience comparable to traditional paper books, while adding powerful electronic features for note taking, fast navigation, and key word searches. However, such actions, irrespective of whether or not they are performed on a PC, handheld computer, or Ebook reader, generally require the user to read the text from a display. Thus, the use of an Ebook generally requires the user to focus his or her visual attention on a display to read the text content (e.g., book, magazine, newspaper, and so forth) of the Ebook. Moreover, the use of any hand-held device requires the user to focus his or her visual attention on a display for one purpose or another.</p>
<p id="p-0007" num="0006">Accordingly, it would be desirable and highly advantageous to have a hand-held device such as, for example, an Ebook, that allows a user to assimilate content without having to look at a display.</p>
<heading id="h-0003" level="1">SUMMARY OF THE INVENTION</heading>
<p id="p-0008" num="0007">The problems stated above, as well as other related problems of the prior art, are solved by the present invention, a hand-held device having text-to-speech (TTS) capabilities.</p>
<p id="p-0009" num="0008">According to an aspect of the present invention, there is provided an Ebook. The Ebook comprises a memory device, a text-to-speech (TTS) module, and at least one speaker. The memory device stores files. The files include text. The TTS module synthesizes speech corresponding to the text. The at least one speaker outputs the speech.</p>
<p id="p-0010" num="0009">According to another aspect of the present invention, there is provided a method for using an Ebook. At least one file is stored in the Ebook. The at least one file includes text. Speech corresponding to the text is synthesized and output from the Ebook.</p>
<p id="p-0011" num="0010">These and other aspects, features and advantages of the present invention will become apparent from the following detailed description of preferred embodiments, which is to be read in connection with the accompanying drawings.</p>
<?BRFSUM description="Brief Summary" end="tail"?>
<?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?>
<description-of-drawings>
<heading id="h-0004" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading>
<p id="p-0012" num="0011"><figref idref="DRAWINGS">FIG. 1</figref> is a block diagram illustrating a computer system <b>100</b> to which the present invention may be applied, according to an illustrative embodiment of the present invention;</p>
<p id="p-0013" num="0012"><figref idref="DRAWINGS">FIG. 2</figref> is a block diagram illustrating an Ebook <b>200</b>, according to an illustrative embodiment of the present invention;</p>
<p id="p-0014" num="0013"><figref idref="DRAWINGS">FIG. 3</figref> is a flow diagram illustrating a method for using an Ebook having text-to-speech (TTS) capabilities, according to an illustrative embodiment of the present invention;</p>
<p id="p-0015" num="0014"><figref idref="DRAWINGS">FIG. 4</figref> is a flow diagram illustrating a method for using an Ebook as an audible storyteller, according to an illustrative embodiment of the present invention; and</p>
<p id="p-0016" num="0015"><figref idref="DRAWINGS">FIG. 5</figref> is a flow diagram illustrating a method for using an Ebook as an wake-up alarm, according to an illustrative embodiment of the present invention.</p>
</description-of-drawings>
<?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?>
<?DETDESC description="Detailed Description" end="lead"?>
<heading id="h-0005" level="1">DETAILED DESCRIPTION OF THE INVENTION</heading>
<p id="p-0017" num="0016">The present invention is directed to a hand-held device having text-to-speech (TTS) capabilities and to a method for using a hand-held device having text-to-speech (TTS) capabilities. It is to be appreciated that the present invention is directed to any type of hand-held device including, but not limited to, electronic books (Ebooks), personal digital assistants (PDAs), and so forth. However, for the purposes of describing the present invention, the following description will be provided with respect to Ebooks.</p>
<p id="p-0018" num="0017">It is to be understood that the present invention may be implemented in various forms of hardware, software, firmware, special purpose processors, or a combination thereof. Preferably, the present invention is implemented as a combination of hardware and software. Moreover, the software is preferably implemented as an application program tangibly embodied on a program storage device. The application program may be uploaded to, and executed by, a machine comprising any suitable architecture. Preferably, the machine is implemented on a computer platform having hardware such as one or more central processing units (CPU), a random access memory (RAM), and input/output (I/O) interface(s). The computer platform also includes an operating system and microinstruction code. The various processes and functions described herein may either be part of the microinstruction code or part of the application program (or a combination thereof) which is executed via the operating system. In addition, various other peripheral devices may be connected to the computer platform such as an additional data storage device and a printing device.</p>
<p id="p-0019" num="0018">It is to be further understood that, because some of the constituent system components and method steps depicted in the accompanying Figures are preferably implemented in software, the actual connections between the system components (or the process steps) may differ depending upon the manner in which the present invention is programmed. Given the teachings herein, one of ordinary skill in the related art will be able to contemplate these and similar implementations or configurations of the present invention.</p>
<p id="p-0020" num="0019"><figref idref="DRAWINGS">FIG. 1</figref> is a block diagram illustrating a computer system <b>100</b> to which the present invention may be applied, according to an illustrative embodiment of the present invention. The computer processing system <b>100</b> includes at least one processor (CPU) <b>102</b> operatively coupled to other components via a system bus <b>104</b>. A read only memory (ROM) <b>106</b>, a random access memory (RAM) <b>108</b>, a display adapter <b>110</b>, an I/O adapter <b>112</b>, and a user interface adapter <b>114</b> are operatively coupled to the system bus <b>104</b>.</p>
<p id="p-0021" num="0020">A display device <b>116</b> is operatively coupled to system bus <b>104</b> by display adapter <b>110</b>. A disk storage device (e.g., a magnetic or optical disk storage device) <b>118</b> is operatively coupled to system bus <b>104</b> by I/O adapter <b>112</b>.</p>
<p id="p-0022" num="0021">A mouse <b>120</b> and keyboard <b>122</b> are operatively coupled to system bus <b>104</b> by user interface adapter <b>114</b>. The mouse <b>120</b> and keyboard <b>122</b> are used to input and output information to and from system <b>100</b>.</p>
<p id="p-0023" num="0022">The computer system <b>100</b> further includes a text-to-speech (TTS) module <b>194</b> and a speaker <b>196</b>.</p>
<p id="p-0024" num="0023"><figref idref="DRAWINGS">FIG. 2</figref> is a block diagram illustrating an Ebook <b>200</b>, according to an illustrative embodiment of the present invention. The Ebook <b>200</b> includes the following elements interconnected by bus <b>201</b>: at least one memory device (hereinafter “memory device” <b>230</b>); at least one processor (hereinafter “processor” <b>240</b>); a user input device <b>250</b> (e.g., keyboard, keypad, and/or remote control); a display <b>260</b>; a text-to-speech (TTS) module <b>270</b>; and a speaker <b>290</b>. Given the teachings of the present invention provided herein, one of ordinary skill in the related art will contemplate these and various other configurations of the computer system <b>100</b> and Ebook <b>200</b> respectively shown in <figref idref="DRAWINGS">FIGS. 1 and 2</figref>, while maintaining the spirit and scope of the present invention. It is to be appreciated that as used herein the term “Ebook” refers to either a standalone Ebook device (e.g., Ebook <b>200</b>) or an Ebook included in a computer system (e.g., computer system <b>100</b>).</p>
<p id="p-0025" num="0024"><figref idref="DRAWINGS">FIG. 3</figref> is a flow diagram illustrating a method for using an Ebook having text-to-speech (TTS) capabilities, according to an illustrative embodiment of the present invention.</p>
<p id="p-0026" num="0025">One or more files (hereinafter “file”) is input into the Ebook (step <b>310</b>). The file includes, at the least, text. The file may be provided via a memory device (e.g., floppy disk, compact disk, flash memory, and so forth), downloaded from the Internet, and so forth. The file may be an Ebook application file, an e-mail file, a Web page, a word processor document, and so forth. The file is then stored in the Ebook (step <b>320</b>).</p>
<p id="p-0027" num="0026">Optionally, at step <b>325</b>, a choice is provided to a user of the Ebook to select between a strictly visual mode where the text is displayed on the display, a strictly audio mode where the text is synthesized by the TTS module and output by the speaker, and a combined visual-audio mode where the text is displayed on the display and simultaneously synthesized by the TTS module and output by the speaker (<b>260</b>, <b>270</b>).</p>
<p id="p-0028" num="0027">One or more commands are received by the Ebook (step <b>330</b>). Preferably, the commands correspond to a playback of the file. The commands may include, for example: a command to begin synthesizing speech corresponding to the text included in the file so that the text is reproduced audibly; a command to end the synthesis; a command to preset a start-up time and/or an end time for the speech synthesis; a command to select/change a voice(s) used in the speech synthesis; a command to select/change the speed of the synthesized speech; a command corresponding to navigation through the file (e.g., to skip one or more pages, sections, chapters, and so forth); and so forth.</p>
<p id="p-0029" num="0028">With respect to the selection of different voices, many different types of voices may be used in the synthesis of speech such as, for example, a man's voice, a woman's voice, an adolescent's voice, or even a funny sounding voice (e.g., chipmunk, etc.). Moreover, different voices may be used in a single playback of a single file. The selection of a particular voice may be made based on, for example, the preference of the user, the different application parameters/circumstances, and/or on a random basis.</p>
<p id="p-0030" num="0029">Further, it is to be appreciated that some of the commands received at step <b>330</b> may not correspond to the playback of the text file. For example, if other functions are integrated with the Ebook such as, for example, a calendar function with a daily reminder schedule, then information relating to the calendar function (or any other function) may be received by the Ebook.</p>
<p id="p-0031" num="0030">The commands are then acted upon to control operations of the Ebook having TTS capabilities (step <b>340</b>). Step <b>340</b> may include the step of synthesizing speech corresponding to the text and/or displaying the text (step <b>340</b><i>a</i>). It is to be appreciated that step <b>340</b> may include acting upon any type of command received at step <b>330</b> including those in support of synthesizing the speech corresponding to the text and/or displaying the text, as well as other functions that may be integrated into the Ebook.</p>
<p id="p-0032" num="0031"><figref idref="DRAWINGS">FIG. 4</figref> is a flow diagram illustrating a method for using an Ebook as an audible storyteller, according to an illustrative embodiment of the present invention. Preferably, the method of <figref idref="DRAWINGS">FIG. 4</figref> is used to reproduce a story to a child at about the child's bedtime. However, the method of <figref idref="DRAWINGS">FIG. 4</figref> may be used for adults as well as children, and may be used at any time of day or night.</p>
<p id="p-0033" num="0032">First and second inputs are received specifying a start time and an end time for a playback of a file on the Ebook (step <b>410</b>). A third input is received specifying the actual file to be played back (step <b>420</b>). A fourth input is received specifying a voice for the playback (step <b>430</b>). It is to be appreciated that steps <b>420</b> and <b>430</b> may be performed randomly by the Ebook, upon simply receiving the first and second inputs. Alternatively, all (or some combination amounting to less than all) of the inputs may be user provided.</p>
<p id="p-0034" num="0033">Playback is commenced at the selected start time, including synthesizing speech corresponding to the file so that the text file is audibly reproduced (step <b>440</b>). Optionally, the text included in the file may be displayed concurrently with the outputting of the synthesized speech. After a random or a pre-specified time period has elapsed, but before the selected end time, the playback volume and/or the speech speed are/is decreased (step <b>450</b>). Step <b>450</b> may be repeated a pre-specified or random number of times so as to gradually decrease the volume and/or speech speed in increments. The reduced playback volume and/or speech speed are intended to render a listener drowsy. The playback is terminated at the specified end time (step <b>460</b>).</p>
<p id="p-0035" num="0034"><figref idref="DRAWINGS">FIG. 5</figref> is a flow diagram illustrating a method for using an Ebook as an wake-up alarm, according to an illustrative embodiment of the present invention.</p>
<p id="p-0036" num="0035">A first input is received specifying a start time for a playback of a file on the Ebook (step <b>510</b>). A second input is received specifying the actual file to be played back (step <b>520</b>). A third input is received specifying a voice for the playback (step <b>530</b>). It is to be appreciated that steps <b>520</b> and <b>530</b> may be performed randomly by the Ebook, upon simply receiving the first input. Alternatively, all (or some combination amounting to less than all) of the inputs may be user provided.</p>
<p id="p-0037" num="0036">Playback is commenced at the selected start time, including synthesizing speech corresponding to the text file so that the text file is audibly reproduced (step <b>540</b>). Optionally, the text included in the file may be displayed concurrently with the outputting of the synthesized speech. After a random or a pre-specified time period(s) has elapsed, the playback volume and/or the speech speed are/is increased (step <b>550</b>). Step <b>550</b> may be repeated so as to incrementally increase the playback volume and/or the speech speed at predefined or random intervals until a stop playback input has been received. The playback is terminated when the stop playback input has been received (step <b>560</b>).</p>
<p id="p-0038" num="0037">Thus, the present invention advantageously allows the use of an Ebook with TTS for applications where reading is not convenient or desirable. For example, the present invention may be used to read while driving, for audibly reading stories to children, for a daily schedule reminder, and so forth. Given the teachings of the present invention provided herein, one of ordinary skill in the related art will contemplate these and various other scenarios in which the present invention may be advantageously employed while maintaining the spirit and scope of the present invention.</p>
<p id="p-0039" num="0038">Although the illustrative embodiments have been described herein with reference to the accompanying drawings, it is to be understood that the present invention is not limited to those precise embodiments, and that various other changes and modifications may be affected therein by one skilled in the art without departing from the scope or spirit of the invention. All such changes and modifications are intended to be included within the scope of the invention as defined by the appended claims.</p>
<?DETDESC description="Detailed Description" end="tail"?>
</description>
<us-claim-statement>What is claimed is:</us-claim-statement>
<claims id="claims">
<claim id="CLM-00001" num="00001">
<claim-text>1. An Ebook, comprising:
<claim-text>a memory device for storing files, the files including text;</claim-text>
<claim-text>a text-to-speech (TTS) module for synthesizing speech corresponding to the text; and</claim-text>
<claim-text>at least one speaker for outputting the speech,</claim-text>
<claim-text>a processor, and</claim-text>
<claim-text>wherein said Ebook is employed as a bedtime story teller such that said TTS module reduces over time a speed at which the speech is output from said TTS module.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00002" num="00002">
<claim-text>2. The Ebook of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein said TTS module has a capability of switching between any one of a plurality of voices in synthesizing the speech, based on at least one of a random basis) user-specified selections, and parameters of a current one of the files.</claim-text>
</claim>
<claim id="CLM-00003" num="00003">
<claim-text>3. The Ebook of <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein the plurality of voices include at least one of a male voice, a female voice, an adolescent voice, and an intentionally funny sounding voice.</claim-text>
</claim>
<claim id="CLM-00004" num="00004">
<claim-text>4. The Ebook of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein said TTS module lies a capability of synthesizing the speech in accordance with at least one of a pre-designated start time and a predesignated end time.</claim-text>
</claim>
<claim id="CLM-00005" num="00005">
<claim-text>5. The Ebook of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein said Ebook commences operations as the bedtime reader based on a pre-designated start time or a receipt of a start input.</claim-text>
</claim>
<claim id="CLM-00006" num="00006">
<claim-text>6. The Ebook of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein said Ebook terminates operations as the bedtime reader based on a pit-designated end time or a receipt of an end input.</claim-text>
</claim>
<claim id="CLM-00007" num="00007">
<claim-text>7. The Ebook of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein said Ebook is employed as the bedtime story teller such that said processor reduces over time a volume of the speech output from the speaker.</claim-text>
</claim>
<claim id="CLM-00008" num="00008">
<claim-text>8. A method for using an Ebook, comprising the steps of:
<claim-text>storing least one file in the Ebook, the at least one file including text;</claim-text>
<claim-text>synthesizing speech corresponding to the text; and</claim-text>
<claim-text>outputting the speech,</claim-text>
<claim-text>wherein said Ebook is employed as a wake-up alarm such that said synthesizing step synthesizes the speech at a pre-designated start time and said method further comprises the step of adjusting a volume of the speech to increase over an open-ended period of time until a stop input has been received, and</claim-text>
<claim-text>wherein the method further comprises the step of adjusting the speed at which the speech is output, based on a random basis.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00009" num="00009">
<claim-text>9. The method of <claim-ref idref="CLM-00008">claim 8</claim-ref>, wherein the Ebook comprises a display and a speaker, and said method further comprises the steps of:
<claim-text>providing to a user of the Ebook a choice between a strictly visual mode where the text is displayed on the display, a strictly audio mode where the text is synthesized and output by the speaker, and a combined visual-audio mode where the text is displayed on the display and simultaneously synthesized and output by the speaker; and operating the Ebook in accordance with the choice of the user.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00010" num="00010">
<claim-text>10. The method of <claim-ref idref="CLM-00008">claim 8</claim-ref>, wherein said Ebook has a calendar function capability, and said synthesizing step synthesizes the speech to include information corresponding to a daily reminder schedule.</claim-text>
</claim>
<claim id="CLM-00011" num="00011">
<claim-text>11. The method of <claim-ref idref="CLM-00008">claim 8</claim-ref>, wherein the stop input comprises a contemporaneous stop input for effecting a contemporaneous termination of the outputting of the speech from the at least one speaker.</claim-text>
</claim>
<claim id="CLM-00012" num="00012">
<claim-text>12. A method for using an Ebook, comprising the steps of:
<claim-text>storing at least one file in the Ebook, the at least one file including text;</claim-text>
<claim-text>synthesizing speech corresponding to the text; and</claim-text>
<claim-text>outputting the speech,</claim-text>
<claim-text>wherein said Ebook is employed as a bedtime story teller such that said synthesizing step reduces over time a speed at which the speech is output.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00013" num="00013">
<claim-text>13. The method of <claim-ref idref="CLM-00012">claim 12</claim-ref>, further comprising the step of switching between any one of a plurality of voices in synthesizing the speech, based on at least one of a random basis, user-specified selections, and parameters of a current one of the files.</claim-text>
</claim>
<claim id="CLM-00014" num="00014">
<claim-text>14. The method of <claim-ref idref="CLM-00013">claim 13</claim-ref>, wherein the plurality of voices include at least one of a male voice, a female voice, an adolescent voice, and an intentionally funny sounding voice.</claim-text>
</claim>
<claim id="CLM-00015" num="00015">
<claim-text>15. The method of <claim-ref idref="CLM-00012">claim 12</claim-ref>, wherein said synthesizing step is performed in accordance with at least one of a pre-designated start time and a pre-designated end time.</claim-text>
</claim>
<claim id="CLM-00016" num="00016">
<claim-text>16. The method of <claim-ref idref="CLM-00012">claim 12</claim-ref>, wherein said Ebook commences operations as the bedtime reader based on a pit-designated start time or a receipt of a start input.</claim-text>
</claim>
<claim id="CLM-00017" num="00017">
<claim-text>17. The method of <claim-ref idref="CLM-00012">claim 12</claim-ref>, wherein said Ebook terminates operations as the bedtime reader based on a pre-designated end time or a receipt of an end input.</claim-text>
</claim>
<claim id="CLM-00018" num="00018">
<claim-text>18. The method of <claim-ref idref="CLM-00012">claim 12</claim-ref>, wherein said Ebook is employed as the bedtime story teller such that said method further comprises the step of reducing over time a volume of the speech.</claim-text>
</claim>
<claim id="CLM-00019" num="00019">
<claim-text>19. A hand-held device, comprising:
<claim-text>a memory device for storing files, the files including text;</claim-text>
<claim-text>a text-to-speech (TTS) module for synthesizing speech corresponding to the text; and</claim-text>
<claim-text>at least one speaker for outputting the speech,</claim-text>
<claim-text>a processor, and</claim-text>
<claim-text>wherein said Ebook is employed as a bedtime story teller such that said TTS module reduces over time a speed at which the speech is output from said TTS module.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00020" num="00020">
<claim-text>20. The hand-held device of <claim-ref idref="CLM-00019">claim 19</claim-ref>, further comprising a display for displaying the text, and wherein a choice is provided to a user of the hand-held device to select between a strictly visual mode where the text is displayed on the display, a strictly audio mode where the text is synthesized by said TTS module and output by said speaker, and a combined visual-audio mode where the text is displayed on the display and simultaneously synthesized by said TTS module and output by said speaker.</claim-text>
</claim>
<claim id="CLM-00021" num="00021">
<claim-text>21. The hand-held device of <claim-ref idref="CLM-00019">claim 19</claim-ref>, wherein said TTS module has a capability of switching between any one of a plurality of voices in synthesizing the speech, based on at least one of a random basis, user-specified selections, and parameters of a current one of the files.</claim-text>
</claim>
<claim id="CLM-00022" num="00022">
<claim-text>22. The hand-held device of <claim-ref idref="CLM-00019">claim 19</claim-ref>, wherein said TTS module has a capability of adjusting a speed at which the speech is output there from, based on at least one of a random basis, user-specified selections, and parameters of a current one of the files.</claim-text>
</claim>
<claim id="CLM-00023" num="00023">
<claim-text>23. The hand-held device of <claim-ref idref="CLM-00019">claim 19</claim-ref>, wherein said hand-held device has a calendar function capability, and said TTS module synthesizes the speech to include information corresponding to a daily reminder schedule.</claim-text>
</claim>
</claims>
</us-patent-grant>
