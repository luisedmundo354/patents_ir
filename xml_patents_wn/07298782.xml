<us-patent-grant lang="EN" dtd-version="v4.2 2006-08-23" file="US07298782-20071120.XML" status="PRODUCTION" id="us-patent-grant" country="US" date-produced="20071106" date-publ="20071120">
<us-bibliographic-data-grant>
<publication-reference>
<document-id>
<country>US</country>
<doc-number>07298782</doc-number>
<kind>B2</kind>
<date>20071120</date>
</document-id>
</publication-reference>
<application-reference appl-type="utility">
<document-id>
<country>US</country>
<doc-number>10355704</doc-number>
<date>20030131</date>
</document-id>
</application-reference>
<us-application-series-code>10</us-application-series-code>
<us-term-of-grant>
<us-term-extension>893</us-term-extension>
</us-term-of-grant>
<classifications-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>H</section>
<class>04</class>
<subclass>N</subclass>
<main-group>7</main-group>
<subgroup>12</subgroup>
<symbol-position>F</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20071120</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
</classifications-ipcr>
<classification-national>
<country>US</country>
<main-classification>37524026</main-classification>
</classification-national>
<invention-title id="d0e53">Method and apparatus for improved memory management of video images</invention-title>
<references-cited>
<citation>
<patcit num="00001">
<document-id>
<country>US</country>
<doc-number>5561780</doc-number>
<kind>A</kind>
<name>Glew et al.</name>
<date>19961000</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>711126</main-classification></classification-national>
</citation>
<citation>
<patcit num="00002">
<document-id>
<country>US</country>
<doc-number>5907500</doc-number>
<kind>A</kind>
<name>Nadehara</name>
<date>19990500</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<patcit num="00003">
<document-id>
<country>US</country>
<doc-number>6078690</doc-number>
<kind>A</kind>
<name>Yamada et al.</name>
<date>20000600</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<patcit num="00004">
<document-id>
<country>US</country>
<doc-number>6208350</doc-number>
<kind>B1</kind>
<name>Herrera</name>
<date>20010300</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<patcit num="00005">
<document-id>
<country>US</country>
<doc-number>6259741</doc-number>
<kind>B1</kind>
<name>Chen et al.</name>
<date>20010700</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<patcit num="00006">
<document-id>
<country>US</country>
<doc-number>6326984</doc-number>
<kind>B1</kind>
<name>Chow et al.</name>
<date>20011200</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>715764</main-classification></classification-national>
</citation>
<citation>
<nplcit num="00007">
<othercit>Bilas, Angelos et al., <i>Real Time Parallel MPEG2 Decoding in Software</i>, Princeton Univ., Depts. of Comp. Sci and Elect. Eng., pp. 1-14, IPPS Apr. 1997.</othercit>
</nplcit>
<category>cited by other</category>
</citation>
<citation>
<nplcit num="00008">
<othercit>Coelho, R. and Hawash, M., <i>DirectX, RDX, RSX and MMX Technology</i>, Chapter 22.10 Speed Up Graphics Writes with Write Combining, Addison-Wesley; Reading, MA., 1998, pp. 369-371.</othercit>
</nplcit>
<category>cited by other</category>
</citation>
<citation>
<nplcit num="00009">
<othercit>Bilas, Angelos et al., <i>Real Time Parallel MPEG 2 Decoding in Software</i>, Princeton Uiversity, Departments of Computer Science and Electrical Engineering, pp. 1-14.</othercit>
</nplcit>
<category>cited by other</category>
</citation>
<citation>
<nplcit num="00010">
<othercit>Chapter 22.10 <i>Speed Up Graphics Writes with Write Combining</i>, pp. 369-371.</othercit>
</nplcit>
<category>cited by other</category>
</citation>
</references-cited>
<number-of-claims>6</number-of-claims>
<us-exemplary-claim>1</us-exemplary-claim>
<us-field-of-classification-search>
<classification-national>
<country>US</country>
<main-classification>348420</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>348413</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>348402</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>348421</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>348416</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>348400</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>348699</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>3484051</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>3484191</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>348424</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>348441</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>348425</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>348453</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>348449</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>382107</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>382235</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>382234</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>382238</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>382244</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>386109</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>386111</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>37524003</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>37524021</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>37524026</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>37524025</main-classification>
</classification-national>
</us-field-of-classification-search>
<figures>
<number-of-drawing-sheets>15</number-of-drawing-sheets>
<number-of-figures>26</number-of-figures>
</figures>
<us-related-documents>
<division>
<relation>
<parent-doc>
<document-id>
<country>US</country>
<doc-number>09607825</doc-number>
<kind>00</kind>
<date>20000630</date>
</document-id>
<parent-grant-document>
<document-id>
<country>US</country>
<doc-number>6961063</doc-number>
<kind>A </kind>
</document-id>
</parent-grant-document>
</parent-doc>
<child-doc>
<document-id>
<country>US</country>
<doc-number>10355704</doc-number>
</document-id>
</child-doc>
</relation>
</division>
<related-publication>
<document-id>
<country>US</country>
<doc-number>20030151610</doc-number>
<kind>A1</kind>
<date>20030814</date>
</document-id>
</related-publication>
</us-related-documents>
<parties>
<applicants>
<applicant sequence="001" app-type="applicant-inventor" designation="us-only">
<addressbook>
<last-name>Kuriakin</last-name>
<first-name>Valery</first-name>
<address>
<city>Nizhny Novgorod</city>
<country>RU</country>
</address>
</addressbook>
<nationality>
<country>RU</country>
</nationality>
<residence>
<country>RU</country>
</residence>
</applicant>
<applicant sequence="002" app-type="applicant-inventor" designation="us-only">
<addressbook>
<last-name>Knyazev</last-name>
<first-name>Alexander</first-name>
<address>
<city>Nizhny Novgorod</city>
<country>RU</country>
</address>
</addressbook>
<nationality>
<country>RU</country>
</nationality>
<residence>
<country>RU</country>
</residence>
</applicant>
<applicant sequence="003" app-type="applicant-inventor" designation="us-only">
<addressbook>
<last-name>Belenov</last-name>
<first-name>Roman</first-name>
<address>
<city>Nizhny Novgorod</city>
<country>RU</country>
</address>
</addressbook>
<nationality>
<country>RU</country>
</nationality>
<residence>
<country>RU</country>
</residence>
</applicant>
<applicant sequence="004" app-type="applicant-inventor" designation="us-only">
<addressbook>
<last-name>Chen</last-name>
<first-name>Yen-Kuang</first-name>
<address>
<city>Franklin Park</city>
<state>NJ</state>
<country>US</country>
</address>
</addressbook>
<nationality>
<country>omitted</country>
</nationality>
<residence>
<country>US</country>
</residence>
</applicant>
</applicants>
<agents>
<agent sequence="01" rep-type="attorney">
<addressbook>
<orgname>Blakely, Sokoloff, Taylor &amp; Zafman LLP</orgname>
<address>
<country>unknown</country>
</address>
</addressbook>
</agent>
</agents>
</parties>
<assignees>
<assignee>
<addressbook>
<orgname>Intel Corporation</orgname>
<role>02</role>
<address>
<city>Santa Clara</city>
<state>CA</state>
<country>US</country>
</address>
</addressbook>
</assignee>
</assignees>
<examiners>
<primary-examiner>
<last-name>Vo</last-name>
<first-name>Tung</first-name>
<department>2621</department>
</primary-examiner>
<assistant-examiner>
<last-name>Senfi</last-name>
<first-name>Behrooz</first-name>
</assistant-examiner>
</examiners>
</us-bibliographic-data-grant>
<abstract id="abstract">
<p id="p-0001" num="0000">A novel storage format enabling a method for improved memory management of video images is described. The method includes receiving an image consisting of a plurality of color components. Once received, the plurality of color components is converted to a mixed format of planar format and packed format. The mixed packet format is implemented by storing one or more of the plurality of color components in a planar format and storing one or more of the plurality of color components in a packed format. A method for writing out video images is also described utilizing a write combining (WC) fame buffer. The decoding method motion compensates groups of macroblocks in order to eliminate partial writes from the WC frame buffer.</p>
</abstract>
<drawings id="DRAWINGS">
<figure id="Fig-EMI-D00000" num="00000">
<img id="EMI-D00000" he="148.17mm" wi="217.42mm" file="US07298782-20071120-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00001" num="00001">
<img id="EMI-D00001" he="190.50mm" wi="139.02mm" file="US07298782-20071120-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00002" num="00002">
<img id="EMI-D00002" he="173.82mm" wi="144.27mm" file="US07298782-20071120-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00003" num="00003">
<img id="EMI-D00003" he="193.55mm" wi="138.26mm" orientation="landscape" file="US07298782-20071120-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00004" num="00004">
<img id="EMI-D00004" he="233.93mm" wi="164.68mm" orientation="landscape" file="US07298782-20071120-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00005" num="00005">
<img id="EMI-D00005" he="239.18mm" wi="77.72mm" orientation="landscape" file="US07298782-20071120-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00006" num="00006">
<img id="EMI-D00006" he="153.08mm" wi="170.26mm" file="US07298782-20071120-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00007" num="00007">
<img id="EMI-D00007" he="143.34mm" wi="150.71mm" file="US07298782-20071120-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00008" num="00008">
<img id="EMI-D00008" he="193.04mm" wi="92.54mm" orientation="landscape" file="US07298782-20071120-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00009" num="00009">
<img id="EMI-D00009" he="206.50mm" wi="154.35mm" file="US07298782-20071120-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00010" num="00010">
<img id="EMI-D00010" he="198.88mm" wi="130.22mm" file="US07298782-20071120-D00010.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00011" num="00011">
<img id="EMI-D00011" he="199.90mm" wi="144.61mm" orientation="landscape" file="US07298782-20071120-D00011.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00012" num="00012">
<img id="EMI-D00012" he="187.28mm" wi="127.34mm" orientation="landscape" file="US07298782-20071120-D00012.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00013" num="00013">
<img id="EMI-D00013" he="195.66mm" wi="160.87mm" orientation="landscape" file="US07298782-20071120-D00013.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00014" num="00014">
<img id="EMI-D00014" he="236.81mm" wi="157.82mm" file="US07298782-20071120-D00014.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00015" num="00015">
<img id="EMI-D00015" he="192.11mm" wi="134.28mm" orientation="landscape" file="US07298782-20071120-D00015.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
</drawings>
<description id="description">
<?RELAPP description="Other Patent Relations" end="lead"?>
<p id="p-0002" num="0001">The application is a divisional of U.S. patent application, Ser. No. 09/607,825, filed Jun. 30, 2000 now U.S. Pat. No. 6,961,063.</p>
<?RELAPP description="Other Patent Relations" end="tail"?>
<?BRFSUM description="Brief Summary" end="lead"?>
<heading id="h-0001" level="1">FIELD OF THE INVENTION</heading>
<p id="p-0003" num="0002">The present invention relates to video images, and, in particular, to a novel storage format for enabling improved memory management of video images.</p>
<heading id="h-0002" level="1">BACKGROUND OF THE INVENTION</heading>
<p id="p-0004" num="0003">In accordance with the NTSC (National Television Standards Committee) and PAL (Phase Alternating Line) standard, video images are presented in the YUV color space. The Y signal represents a luminance value while the U and V signals represent color difference or chrominance values. YUV video image data may be transmitted in packed format or planar format. In packed format, all the data for a given set of pixels of the video image is transmitted before any data for another set of pixels is transmitted. As a result, in packed format, YUV data is interleaved in the transmitted pixel data stream <b>100</b>, as depicted in <figref idref="DRAWINGS">FIG. 1</figref> (YUY2 packed format). In planar format, Y, U and V data values are stored into separate Y, U and V memory areas (planes) in system memory <b>110</b>, as depicted in <figref idref="DRAWINGS">FIG. 1B</figref>.</p>
<p id="p-0005" num="0004"><figref idref="DRAWINGS">FIGS. 2A</figref>, <b>2</b>B and <b>2</b>C are diagrams illustrating three different formats for representing video images in the YUV color space. A video image frame may consist of three rectangular matrices representing the luminance Y and the two-chrominance values U and V. Y matrices <b>120</b>, <b>130</b> and <b>140</b> have an even number of rows and columns. In YUV 4:2:0 color space format, chrominance component matrices <b>122</b> and <b>124</b> may be one half in size of Y matrix <b>120</b> in horizontal and vertical directions as depicted in <figref idref="DRAWINGS">FIG. 2A</figref>. In YUV 4:2:2 format, chrominance component matrices <b>132</b> and <b>134</b> may be one half in size of Y matrix <b>130</b> in the horizontal direction and the same size in the vertical direction as depicted in <figref idref="DRAWINGS">FIG. 2B</figref>. Finally, in YUV 4:4:4 format, chrominance component matrices <b>142</b> and <b>144</b> may be the same size as Y matrix <b>140</b> in the horizontal and vertical directions as depicted in <figref idref="DRAWINGS">FIG. 2C</figref>.</p>
<p id="p-0006" num="0005">To store video data efficiently, conventional digital video systems contain a data compressor that compresses the video image data using compression techniques. Many conventional compression techniques are based on compressing the video image data by processing the different pixel components separately. For example, in accordance with Motion Picture Experts Group (MPEG) or International Telecommunications Union (ITU) video compression standards, a YUV-data compressor may encode the Y data independently of encoding U data and encoding V data. Such a compressor preferable receives video data in planar format, in which the Y, U, and V data for multiple pixels are separated and grouped together in three distinct data streams of Y only, U only and V only data, as described above (<figref idref="DRAWINGS">FIG. 1B</figref>).</p>
<p id="p-0007" num="0006">Although planar format provides significant advantages for data compression, several disadvantages arise when storing or processing data received in planar format. For example, a video decoder that receives video image data in YUV planar format requires three pointers to the Y, U and V component values. For basic DVD (Digital Versatile Disk) and HDTV (High Definition Television) mode, each macroblock has three blocks of pixels: Y:16×16, U:8×8 and V:8×8. In addition, the U and V components are located in different memory locations. In terms of code size, three blocks of code are required for conventional motion compensation of the video image data. Moreover, a separate memory page usually must be opened for each YUV component.</p>
<p id="p-0008" num="0007">In terms of cache efficiency, for YUV video in the 4:2:0 format (<figref idref="DRAWINGS">FIG. 2A</figref>), the useful area (in a cache line) for the Y-component is about sixteen-bytes per cache line. For the U and V components, the useful area is eight-bytes per line per color-component. Therefore, two rows in a macroblock potentially occupy four cache lines since the U and V components are vertically and horizontally sub-sampled in 4:2:0 format (two Y cache lines, one U cache line, and one V cache line). For YUV video in 4:2:2 format (<figref idref="DRAWINGS">FIG. 2B</figref>), six cache lines are required (2 Y cache lines, 2 U cache lines, and 2 V cache lines.) Although YUY2 packed format (<figref idref="DRAWINGS">FIG. 1A</figref>), as described above, uses only two cache lines and could be used to overcome this cache inefficiency problem, conventional motion compensation of data in YUY2 format is inefficient.</p>
<p id="p-0009" num="0008">Therefore, there remains a need to overcome the limitations in the above described existing art, which is satisfied by the inventive structure and method described hereinafter.</p>
<?BRFSUM description="Brief Summary" end="tail"?>
<?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?>
<description-of-drawings>
<heading id="h-0003" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading>
<p id="p-0010" num="0009">Additional features and advantages of the invention will be more apparent from the following detailed description and appended claims when taken in conjunction with the drawings, in which:</p>
<p id="p-0011" num="0010"><figref idref="DRAWINGS">FIG. 1A</figref> illustrates a video image data stream in YUV2 packed format as known in the art.</p>
<p id="p-0012" num="0011"><figref idref="DRAWINGS">FIG. 1B</figref> illustrates video image data stored in YV12 pure planar format, as known in the art.</p>
<p id="p-0013" num="0012"><figref idref="DRAWINGS">FIGS. 2A-C</figref> illustrate the YUV 4:2:0 color space format, the YUV 4:2:2 color space format and the YUV 4:4:4 color space format, as known in the art.</p>
<p id="p-0014" num="0013"><figref idref="DRAWINGS">FIG. 3</figref> illustrates a block diagram of a conventional computer system as known in the art.</p>
<p id="p-0015" num="0014"><figref idref="DRAWINGS">FIG. 4</figref> is a block diagram illustrating a video decoder in accordance with one embodiment of the present invention.</p>
<p id="p-0016" num="0015"><figref idref="DRAWINGS">FIGS. 5A and 5B</figref> illustrate YUV color space storage formats as known in the art.</p>
<p id="p-0017" num="0016"><figref idref="DRAWINGS">FIG. 5C</figref> illustrates a block diagram of a mixed storage format according to an exemplary embodiment of the present invention.</p>
<p id="p-0018" num="0017"><figref idref="DRAWINGS">FIGS. 6A-6B</figref> illustrate motion compensation of decoded blocks according to a further embodiment of the present invention.</p>
<p id="p-0019" num="0018"><figref idref="DRAWINGS">FIG. 7A</figref> is a block diagram illustrating steps for encoding and decoding MPEG image data as known in the art.</p>
<p id="p-0020" num="0019"><figref idref="DRAWINGS">FIG. 7B</figref> is a block diagram illustrating a video encoder in accordance with a further embodiment of the invention.</p>
<p id="p-0021" num="0020"><figref idref="DRAWINGS">FIG. 8</figref> is a block diagram illustrating a write combining (WC) buffer as known in the art.</p>
<p id="p-0022" num="0021"><figref idref="DRAWINGS">FIG. 9</figref> illustrates method steps for decoding an encoded MPEG video bit stream as known in the art.</p>
<p id="p-0023" num="0022"><figref idref="DRAWINGS">FIG. 10</figref> illustrates method steps for decoding an encoded MPEG video bit stream in accordance with the novel motion compensation technique as taught by the present invention.</p>
<p id="p-0024" num="0023"><figref idref="DRAWINGS">FIG. 11A</figref> illustrates a stride of an image frame represented in the mixed storage format as taught by the present invention.</p>
<p id="p-0025" num="0024"><figref idref="DRAWINGS">FIG. 11B</figref> is a block diagram illustrating a write combining (WC) buffer including a cache line containing a Y-component and a UV component represented in the mixed storage format as taught by the present invention.</p>
<p id="p-0026" num="0025"><figref idref="DRAWINGS">FIG. 12</figref> illustrates method steps for improved memory management of video images utilizing a mixed storage format according to an embodiment of the present invention.</p>
<p id="p-0027" num="0026"><figref idref="DRAWINGS">FIG. 13</figref> illustrates additional method steps for improved memory management of video images utilizing a mixed storage format according to a further embodiment of the present invention.</p>
<p id="p-0028" num="0027"><figref idref="DRAWINGS">FIG. 14</figref> illustrates additional method steps for improved memory management of video images utilizing a mixed storage format according to a further embodiment of the present invention.</p>
<p id="p-0029" num="0028"><figref idref="DRAWINGS">FIG. 15</figref> illustrates additional method steps for improved memory management of video images utilizing a mixed storage format according to a further embodiment of the present invention.</p>
<p id="p-0030" num="0029"><figref idref="DRAWINGS">FIG. 16</figref> illustrates method steps for decoding an encoded bit stream according to an embodiment of the present invention.</p>
<p id="p-0031" num="0030"><figref idref="DRAWINGS">FIG. 17</figref> illustrates additional method steps for decoding an encoded bit stream according to a further embodiment of the present invention.</p>
<p id="p-0032" num="0031"><figref idref="DRAWINGS">FIG. 18</figref> illustrates additional method steps for decoding an encoded bit stream utilizing a mixed storage format according to a further embodiment of the present invention.</p>
</description-of-drawings>
<?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?>
<?DETDESC description="Detailed Description" end="lead"?>
<heading id="h-0004" level="1">DETAILED DESCRIPTION</heading>
<heading id="h-0005" level="1">System Architecture</heading>
<p id="p-0033" num="0032">The present invention overcomes the problems in the existing art described above by providing a novel storage format enabling a method for improved memory management of video images. In the following detailed description, numerous specific details are set forth in order to provide a thorough understanding of the present invention. However, one having ordinary skill in the art should recognize that the invention may be practiced without these specific details. In addition, the following description provides examples, and the accompanying drawings show various examples for the purposes of illustration. However, these examples should not be construed in a limiting sense as they are merely intended to provide examples of the present invention rather than to provide an exhaustive list of all possible implementations of the present invention. In some instances, well-known structures, devices, and techniques have not been shown in detail to avoid obscuring the present invention.</p>
<p id="p-0034" num="0033">Referring to <figref idref="DRAWINGS">FIG. 3</figref>, a block diagram illustrating major components of a computer system <b>200</b> in which the inventive storage format may be implemented is now described. The computer system <b>200</b> includes a display controller <b>220</b>. The display controller <b>220</b> is, for example a Video Graphics Adapter (VGA), Super VGA (SVGA) or the like. Display controller <b>120</b> generates pixel data for display <b>290</b>, which is, for example, a CRT, flat panel display or the like. The pixel data is generated at a rate characteristic of the refresh of display <b>290</b> (e.g., 60 Hz, 72 Hz, 75 Hz or the like) and horizontal and vertical resolution of a display image (e.g., 640×480 pixels, 1024×768 pixels, 800×600 or the like). Display controller <b>220</b> may generate a continuous stream of pixel data at the characteristic rate of display <b>290</b>.</p>
<p id="p-0035" num="0034">Display controller <b>220</b> is also provided with a display memory <b>222</b>, which stores pixel data in text, graphics, or video modes for output to display <b>290</b>. Host CPU <b>210</b> is be coupled to display controller <b>220</b> through bus <b>270</b> and updates the content of display memory <b>222</b> when a display image for display <b>290</b> is altered. Bus <b>270</b> may comprise, for example, a PCI bus or the like. System memory <b>280</b> may be coupled to Host CPU <b>210</b> for storing data. Hardware video decoder <b>230</b> is provided to decode video data such as, for example, MPEG video data. MPEG video data is received from an MPEG video data source (e.g., CD-ROM or the like). Alternatively, the video decoder <b>230</b> is implemented as, for example, a conventional software decoder <b>282</b> stored in the system memory <b>280</b>. As such, one of ordinary skill in the art will recognize that the teaching of the present invention may be implemented in either software or hardware video decoders. Once decoded, the decoded video data is outputted to system memory <b>270</b> or directly to display memory <b>230</b>.</p>
<p id="p-0036" num="0035">Referring to <figref idref="DRAWINGS">FIG. 4</figref>, the components of a video decoder <b>240</b> according to a first embodiment of present invention are further described. The video decoder <b>240</b> may be utilized as the hardware decoder <b>230</b> or software decoder <b>282</b> within the computer system <b>200</b>. MPEG data received from an MPEG data source may be decoded and decompressed as follows. The video decoder <b>240</b> receives an MPEG bit stream <b>242</b> at a Variable Length Decoding (VLD) block <b>244</b>. The VLD block <b>244</b> decodes the MPEG bit stream <b>242</b> and generates a quanitized block <b>246</b> that is transferred to an Inverse Quantanization Block (IQ block) <b>266</b>. The IQ block <b>266</b> performs inverse quantization on the quantized block <b>246</b> to generate a frequency spectrum <b>268</b> for the quantized block. An Inverse Discrete Cosine Transform (IDCT) block <b>246</b> performs inverse discrete cosine transformation of the quantized block <b>246</b> using the frequency spectrum <b>268</b> to generate a decoded block <b>252</b> that is transferred to the motion compensation block (MCB) <b>248</b>. Motion compensation is performed by the MCB <b>248</b> to recreate the MPEG data <b>256</b>. Finally, color conversion block <b>262</b> converts the MPEG data <b>256</b> into the Red, Green, Blue (RGB) color space in order to generate pictures <b>264</b>.</p>
<p id="p-0037" num="0036">Conventional MPEG decoders, such hardware video decoder <b>230</b> or software video decoder <b>282</b>, decode a compressed MPEG bit stream into a storage format depending on the particular compression format used to encode the MPEG bit stream. For the reasons described above, YUV planar format is the preferred format for compression of MPEG images within conventional MPEG decoders. Consequently, the decoded block <b>252</b> outputted by the IDCT block <b>250</b> as well as the MPEG data <b>256</b> outputted by the MCB <b>254</b> are generated in YUV planar format within conventional MPEG decoders. Unfortunately, YUV planar format is an inefficient format during motion compensation of the decoded block <b>252</b>.</p>
<p id="p-0038" num="0037">Accordingly, <figref idref="DRAWINGS">FIG. 5C</figref> depicts a novel mixed storage format <b>300</b> described by the present invention that is utilized by the video decoder <b>240</b>. Careful review of <figref idref="DRAWINGS">FIGS. 5A-5C</figref> illustrates that Y component values are stored in a planar array <b>300</b>A while the U and V components are interleaved in a packed array <b>300</b>B. Using the mixed storage format <b>300</b>, decoded block <b>252</b> received from the IDCT block <b>246</b> is converted from planar format (<figref idref="DRAWINGS">FIG. 5B</figref>) to the mixed storage format <b>300</b>. Storage of reference frames <b>260</b> and MPEG data <b>256</b> in the mixed storage format <b>300</b> optimizes motion compensation of the decoded block <b>252</b> as depicted in <figref idref="DRAWINGS">FIGS. 6A and 6B</figref>.</p>
<p id="p-0039" num="0038"><figref idref="DRAWINGS">FIG. 6A</figref> depicts a previous frame <b>260</b> that is stored in a reference frames block <b>258</b>. The previous frame <b>260</b> includes a 320×280 pixel Y-component <b>310</b>, a 160×140 pixel U-component <b>312</b> and a 160×140 pixel V-component <b>314</b>, represented in planar format <b>304</b> (<figref idref="DRAWINGS">FIG. 5B</figref>). <figref idref="DRAWINGS">FIG. 6B</figref> depicts a portion of the Video Decoder <b>240</b>. Together <figref idref="DRAWINGS">FIGS. 6A and 6B</figref> depict the combination of the decoded block <b>252</b> (<b>252</b>A, <b>252</b>B, and <b>252</b>C) and corresponding YUV components, as determined by a motion vector (V) <b>248</b>, of the previous frame <b>260</b> during motion compensation. During the decoding process, the VLD block <b>244</b> generates the motion vector (V) <b>248</b>, which corresponds to the decoded block <b>252</b> generated by the IDCT Block <b>250</b>. Referring again to <figref idref="DRAWINGS">FIGS. 6A and 6B</figref>, the motion vector (V) <b>248</b> identifies Y-block <b>316</b>, U-block <b>318</b> and V-block <b>320</b> of the previous frame <b>260</b>. In order to motion compensate the decoded block <b>252</b>, Y data <b>252</b>A is combined with Y-block <b>316</b>, U data is combined with U-block <b>318</b> and V data is combined with V-block <b>320</b> in order to generate MPEG data <b>256</b> (<b>322</b>A, <b>322</b>B and <b>322</b>C).</p>
<p id="p-0040" num="0039">However, if the previous frame <b>260</b> and the decoded block <b>252</b> are stored in the mixed storage format <b>300</b> (<figref idref="DRAWINGS">FIG. 5C</figref>), the motion compensation process is streamlined. Referring again to <figref idref="DRAWINGS">FIG. 6A</figref>, the previous frame can be stored in the mixed storage format <b>300</b> (<figref idref="DRAWINGS">FIG. 5C</figref>) as indicated by the 320×140 UV-component <b>330</b>, such that a UV-block <b>335</b> is formed. Referring again to <figref idref="DRAWINGS">FIG. 6B</figref>, the decoded block can be stored in the mixed storage format <b>300</b> (<figref idref="DRAWINGS">FIG. 5B</figref>) as indicated by UV data <b>340</b>. The UV-block <b>335</b> is then combined with the UV data <b>340</b> in order to generate UV-MPEG data <b>350</b>.</p>
<p id="p-0041" num="0040">Careful review of <figref idref="DRAWINGS">FIGS. 6A and 6B</figref> illustrates the advantages of using the mixed packed format <b>300</b> (<figref idref="DRAWINGS">FIG. 5C</figref>) during motion compensation. Motion compensation using the planar format <b>304</b> (<figref idref="DRAWINGS">FIG. 5B</figref>) requires (1) opening three memory pages, (2) using three pointers, and (3) performing three operations for each YUV component to motion compensate the decoded block <b>252</b>. In contrast, motion compensation using the mixed storage format <b>300</b> (<figref idref="DRAWINGS">FIG. 5C</figref>) requires (1) opening two memory pages, (2) using two pointers, and (3) performing two operations for each Y and UV component to motion compensate the decoded block <b>252</b>. In addition, referring again to <figref idref="DRAWINGS">FIGS. 6A and 6B</figref>, storage of YUV data in cache memory (not shown) requires thirty-two cache lines in planar format <b>304</b> (<figref idref="DRAWINGS">FIG. 5B</figref>) (eight-cache lines for the Y-component, eight cache lines for each U and V component). In contrast, storage of the Y and UV components in the mixed storage format <b>300</b> (<figref idref="DRAWINGS">FIG. 5C</figref>) requires twenty-four cache lines (sixteen cache lines for the Y-component and eight cache lines for the UV component).</p>
<p id="p-0042" num="0041">Moreover, benefits from using of the mixed storage format <b>300</b> (<figref idref="DRAWINGS">FIG. 5C</figref>) are not limited to video decoders. <figref idref="DRAWINGS">FIG. 7A</figref> depicts the steps for encoding and decoding video images <b>400</b>. Careful review of <figref idref="DRAWINGS">FIG. 7A</figref> illustrates that the encoding <b>402</b> and decoding <b>420</b> of video images essentially mirror each other. Consequently, referring to <figref idref="DRAWINGS">FIG. 7B</figref>, the mixed storage format may be used in a Video Encoder <b>450</b> during motion estimation <b>452</b>, motion compensation <b>454</b> and storage of reference frames <b>456</b>. For the reasons described above use of the mixed storage format <b>300</b> (<figref idref="DRAWINGS">FIG. 5C</figref>) will reduce hardware costs of the Video encoder (less pointers), and speed up the encoding process (less operations and memory access). In addition, cache line efficiency results from using the mixed storage format <b>300</b> (<figref idref="DRAWINGS">FIG. 5C</figref>), as described above.</p>
<p id="p-0043" num="0042">Although use of the mixed storage format <b>300</b> (<figref idref="DRAWINGS">FIG. 5C</figref>) provides improved cache line efficiency, there remains a need to improve access speed to graphics memory. One technique for overcoming this problem is using a Write Combining (WC) buffer in order to accelerate writes to the video frame buffer, as depicted in <figref idref="DRAWINGS">FIG. 8</figref>. <figref idref="DRAWINGS">FIG. 8</figref> depicts a memory address space <b>500</b> including a WC region <b>502</b>. The WC buffer <b>512</b> utilizes the fact that thirty-two byte burst writes are faster than individual byte or word writes since burst writes consume less bandwidth from the system bus. Hence, applications can write thirty-two bytes of data to the WC frame buffer <b>512</b> before burst writing the data to its final destination <b>508</b>.</p>
<p id="p-0044" num="0043">Nonetheless, not all applications take advantage of WC buffers. One problem associated with WC buffers is that WC buffers generally contain only four or six entries in their WC region <b>502</b>. Consequently, any memory stores to an address that is not included in the current WC buffer <b>512</b> will flush out one or some entry in the WC buffer <b>512</b>. As a result, partial writes will occur and reduce the system performance.</p>
<p id="p-0045" num="0044">However, by writing data sequentially and consecutively into the WC region <b>502</b> of the system memory <b>500</b>, partial memory writes to the WC region <b>502</b> are eliminated. Referring again to <figref idref="DRAWINGS">FIG. 8</figref>, if we write the first pixel <b>504</b> in line one of an image, then the first pixel <b>506</b> in line two of the image, it is very likely that the WC buffer <b>512</b> (holding only one byte) will be flushed out. This occurs because we are writing to the WC region <b>502</b> in a vertical manner, such that the second write does not map to the same entry of the WC buffer <b>512</b>. In contrast, when we write to the WC region <b>502</b> in a sequential and consecutive manner, the first thirty-two pixels <b>508</b> of line one of the image may be written to the WC buffer <b>512</b> before the first thirty-two bytes of pixels <b>508</b> are burst written to their final destination in the WC region <b>502</b>. Once we completely write the thirty-second pixel byte <b>508</b> in the WC buffer <b>512</b>, the entire thirty-two bytes of pixels <b>508</b> can be burst written to their final destination.</p>
<p id="p-0046" num="0045">In MPEG video decoding, it is important to reduce partial writes during motion compensation and during frame output to graphics memory. A novel method for re-ordering of operations during motion compensation of video frames in order to reduce partial writes is now described with reference to <figref idref="DRAWINGS">FIG. 9</figref> and <figref idref="DRAWINGS">FIG. 10</figref>.</p>
<p id="p-0047" num="0046"><figref idref="DRAWINGS">FIG. 9</figref> is a block diagram depicting steps for decoding an MPEG bit stream. Instead of motion compensating a block after a block, we propose motion compensation of blocks in groups of four block (as shown in <figref idref="DRAWINGS">FIG. 10</figref>). MPEG video bits streams are generally decoded as follows: (1) VLD the motion vector and the DCT coefficients of a block <b>522</b>; (2) IQ and IDCT of the DCT coefficients of the block <b>524</b>; and (3) MC the residue of the block with the displaced reference block <b>526</b>. One problem of this approach is that it causes many partial writes.</p>
<p id="p-0048" num="0047">Generally, after a block is MC, the resulting block is written back to the frame buffer. Assuming the video image is stored in a linear fashion and the width of the video is larger than the size of an entry in WC buffer, the resulting marcoblock is written to the frame as follows. As the block is written back to the frame buffer line after line, each eight-byte write starts a new cache line. Thus, after storing four lines, the application is forcing the WC buffer to flush some of its entries. That is, partials writes (16 bytes out of 32 bytes) occur.</p>
<p id="p-0049" num="0048">However by motion compensating four blocks together, partial writes are eliminated. That is, Step one (<b>522</b>) and Step two (<b>524</b>) are repeated four times before Step three (<b>526</b>) is performed as depicted in <figref idref="DRAWINGS">FIG. 10</figref>. Furthermore, instead of writing out the second line of a first block after the first line of the first block, the first line of a second block is written out. This is because the first line of the first block and the first line of the second block belong to the same cache lines. Consequently, the WC buffer <b>500</b> can easily combine the write operations in a burst operation. However, those skilled in the art will recognize that various numbers of blocks may be chose as the plurality of block, such the number of blocks is dependent on the line size of the write combining buffer.</p>
<p id="p-0050" num="0049">The real advantages of writing the data out in a multiple of four blocks comes from using the mixed storage format <b>300</b> (<figref idref="DRAWINGS">FIG. 5C</figref>). In this format that, Y components of a video image have the same stride as the UV components of the video image. Referring to <figref idref="DRAWINGS">FIG. 11A</figref>, an image frame <b>550</b> is depicted utilizing the mixed storage format <b>300</b>. The image <b>550</b> includes a 320×280 pixel Y-component <b>552</b> and a 320×140 pixel UV-component <b>554</b>. As such a Y-block <b>556</b> has an equal stride <b>560</b> as a UV-block <b>558</b>. As a result, whenever we finish writing to a full cache line <b>572</b> (thirty-two bytes) of a WC buffer <b>570</b> (<figref idref="DRAWINGS">FIG. 11B</figref>), we generate a “full write” of Y-components <b>574</b> (thirty-two bytes) and corresponding UV components <b>576</b> (thirty-two bytes). We only need four blocks to guarantee a “full-write” of the cache line. That is, we don't need four extra blocks to guarantee a “full-write” of the cache line. This property provides a distinct advantage over previous pure planner YV12 format. Procedural method steps for implementing the inventive mixed storage format <b>300</b> (<figref idref="DRAWINGS">FIG. 5C</figref>) and a modified method for decoding an encoded bit stream are now described.</p>
<heading id="h-0006" level="1">Operation</heading>
<p id="p-0051" num="0050">Referring now to <figref idref="DRAWINGS">FIG. 12</figref>, a method <b>600</b> is depicted for improved memory management of image data, for example, in the Video Decoder <b>240</b> as depicted in <figref idref="DRAWINGS">FIG. 4</figref>. At step <b>602</b>, an image consisting of a plurality of color components <b>242</b> is received. The plurality of color components <b>242</b> contained in the image are received in a planar format <b>304</b> (<figref idref="DRAWINGS">FIG. 5B</figref>). At step <b>604</b>, the each of the plurality of color components <b>242</b> is converted into a mixed format <b>300</b> (<figref idref="DRAWINGS">FIG. 5C</figref>) of planar format <b>304</b> and packed format <b>302</b>. In order to convert the plurality of color components to the mixed format <b>300</b>, one or more of the plurality of color components <b>242</b> are stored in a planar format <b>304</b> and one or more of the plurality of color components are stored in a packed format <b>302</b>.</p>
<p id="p-0052" num="0051"><figref idref="DRAWINGS">FIG. 13</figref> depicts additional method steps <b>610</b> for converting the plurality of color components <b>242</b> of step <b>604</b>, such that the plurality of color components are presented in a YUV color space. At step <b>612</b>, luminance components (Y) of the image are stored in a planar array <b>300</b>A (<figref idref="DRAWINGS">FIG. 5C</figref>). At step <b>614</b>, chrominance components (UV) of the image are stored in a packed array <b>300</b>B (<figref idref="DRAWINGS">FIG. 5C</figref>). The image is, for example, a video image consisting of a plurality of pixels.</p>
<p id="p-0053" num="0052"><figref idref="DRAWINGS">FIG. 14</figref> depicts additional method steps <b>620</b> for converting color components <b>242</b> in the mixed format <b>300</b> into the packed format <b>302</b>. At step <b>622</b>, the luminance components (Y) are accessed within the planar array <b>300</b>A. At step <b>624</b>, the chominance components (UV) are accessed within the packed array <b>300</b>B. Finally at step <b>626</b>, an interleaving write-out of Y components and UV components is performed to a YUV packed array <b>302</b>. The packed format is, for example, one of YUV2 packed format, UYVY packed format, YUV12 packed format, YUV16 packed format, or YUV9 packed format.</p>
<p id="p-0054" num="0053"><figref idref="DRAWINGS">FIG. 15</figref> depicts additional method steps <b>640</b> for converting color components <b>242</b> stored in the mixed format <b>300</b> into the planar format <b>304</b>. A step <b>642</b>, a memory copy is performed of the luminance components (Y) within the planar array <b>300</b>A to a Y-plane <b>304</b>A of YUV planar arrays <b>304</b>. At step <b>644</b>, an alternate read/write-out coping of the UV components is performed into respective planes of the YUV planar arrays <b>304</b>. In other words, the U components are written to a U-plane <b>304</b>B of the planar arrays <b>304</b> and the V component is written to a V-plane <b>304</b>B of the planar arrays <b>304</b>. The planar format is, for example, chosen as one of YV12 planar format, YUV12 planar format, YUV16 planar format, or YUV9 planar format. In addition, color components are presented in a color space chosen as, for example, one of a YUV color space, a YCrCb color space, a YIQ color space, or an RGB color space.</p>
<p id="p-0055" num="0054">Referring now to <figref idref="DRAWINGS">FIG. 16</figref>, a method <b>700</b> is depicted for decoding an encoded bit stream, for example, in the Video Decoder <b>240</b> as depicted in <figref idref="DRAWINGS">FIG. 4</figref>. At step <b>704</b>, a portion of the encoded bit stream is received representing an encoded block. Alternatively, a quanitized block <b>246</b> may be received. At step <b>706</b>, the encoded block is variable length decoded (VLD) to generate a quantized block. When the quanitized block is received at step <b>704</b>, step <b>706</b> is not performed. Those skilled in the art will appreciate that the encoded block may be decoded in various ways and remain with in the scope of this invention. At step <b>708</b>, inverse quantization (IQ) is performed on the quantized block to generate a frequency spectrum for the quantized block. At step <b>710</b>, inverse discrete cosine transformation (IDCT) of the quantized block is performed using the frequency spectrum to generate a decoded block. At step <b>712</b>, steps <b>704</b> through <b>710</b> are repeated for a plurality of encoded blocks. As a result, a plurality of decoded blocks, representing a plurality of macroblocks, are formed. At step <b>714</b>, the plurality of macroblocks are motion compensated as a group in order to generate a plurality of motion compensated (MC) macroblocks. Finally at step <b>740</b>, steps <b>704</b> through <b>714</b> are repeated for each encoded block represented by the encoded bit stream. The encoded bit streams is, for example, an encoded MPEG video bit stream.</p>
<p id="p-0056" num="0055"><figref idref="DRAWINGS">FIG. 17</figref> depicts additional method steps <b>716</b> for motion compensating the plurality of macroblocks of step <b>714</b>. At step <b>718</b>, four blocks are used as the plurality of blocks. Finally at step <b>720</b>, pixel data of the four MC blocks is written as a group and in a sequential manner to a frame buffer, such that prior to being burst written to the frame buffer, the pixel data is temporarily held in an entry of a write-combining (WC) buffer (the first line of a second block is written out after the first line of the first block). As a result, partial writes from the WC buffer are eliminated. The WC buffer is, for example, a four-cache line WC buffer having a thirty-two byte cache line length, however, alternate numbers of cache lines and cache line lengths are within the scope of the present invention.</p>
<p id="p-0057" num="0056"><figref idref="DRAWINGS">FIG. 18</figref> depicts additional method steps <b>730</b> for motion compensating the plurality of macroblocks of step <b>714</b>, such that the decoded blocks are represented in a YUV color space, planar storage format. At step <b>732</b>, luminance components (Y) of the decoded blocks are stored in a planar array. Finally at step <b>734</b>, chrominance components (UV) of the decoded blocks are stored in a packed array. In other words, the decoded blocks are converted into a mixed storage format of planar format and packed format.</p>
<p id="p-0058" num="0057">The methods described above can be provided in applications (e.g., video/graphic applications) to potentially increase the performance of the applications by decreasing the time to motion compensate decoded blocks. Moreover, applications that include the method as described above, can be stored in memory of a computer system as a set of instructions to be executed. In addition, the instructions to perform the methods as described above could alternatively be stored on other forms of computer-readable medium, including magnetic and optical disks. For example, method of the present invention can be stored on computer-readable mediums, such as magnetic disks or optical disks, that are accessible via a disk drive (or computer-readable medium drive).</p>
<p id="p-0059" num="0058">Alternatively, the logic to perform the methods as discussed above, could be implemented in additional computer and/or machine readable mediums. Such mediums include discrete hardware components such as large-scale integrated circuits (LSI's), application-specific integrated circuits (ASIC's), firmware such as electrically erasable programmable read-only memory (EEPROM's); and, electrical, optical, acoustical or other forms of propagated signals (e.g., carrier waves, infrared signals, digital signals, etc.)</p>
<p id="p-0060" num="0059">It is to be understood that even though numerous characteristics and advantages of various embodiments of the present invention have been set forth in the foregoing description, together with details of the structure and function of various embodiment of the invention, this disclosure is illustrative only. Changes may be made in detail, especially matters of structure and management of parts within the principles of the present invention to the full extent indicated by the broad general meaning of the terms in which the appended claims are expressed. For example, the particular element may vary depending on the particular application for the novel mixed storage format while maintaining substantially the same functionality without departing from the scope and spirit of the present invention.</p>
<p id="p-0061" num="0060">In addition, although the preferred embodiment described herein is directed to a novel mixed storage format for enabling improved memory management of video images, it will be appreciated by those skilled in the art that the teaching of the present invention can be applied to other system. In fact, systems from low-bit-rate Internet based communications to high-bit-rate video broadcasting as well as software/hardware video encoder/decoders are within the teachings of the present invention, without departing from the scope and spirit of the present invention.</p>
<p id="p-0062" num="0061">Having disclosed exemplary embodiments and the best mode, modifications and variations may be made to the disclosed embodiments while remaining within the scope of the invention as defined by the following claims.</p>
<?DETDESC description="Detailed Description" end="tail"?>
</description>
<us-claim-statement>What is claimed is:</us-claim-statement>
<claims id="claims">
<claim id="CLM-00001" num="00001">
<claim-text>1. A method comprising:
<claim-text>(1) receiving a quantized block of an image;</claim-text>
<claim-text>(2) performing inverse quantization on the quantized block to generate a frequency spectrum for the quantized block;</claim-text>
<claim-text>(3) performing inverse discrete cosine transformation of the quantized block using the frequency spectrum to generate a decoded block;</claim-text>
<claim-text>(4) repeating for a plurality of encoded blocks, such that a plurality (1)-(3) of decoded blocks are formed;</claim-text>
<claim-text>(5) storing color components of the decoded blocks of the image of a first color component type in a planar format;</claim-text>
<claim-text>(6) storing color components of the decoded blocks of the image of a second color component type and a third color component type in a packed format, such that the color components of the decoded blocks of the image are stored in a mixed format of planar format and packed format;</claim-text>
<claim-text>(7) motion compensating the color components of (1)-(3) the decoded blocks as a group in the mixed format of planar format and packed format thereby generating a plurality of motion compensated (MC) blocks and;</claim-text>
<claim-text>(8) repeating (1)-(7) for each quanitized block of the image.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00002" num="00002">
<claim-text>2. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, where the motion compensating of the plurality of blocks further comprises:
<claim-text>using as the plurality of blocks four blocks, such that four MC blocks are generated as the plurality of MC blocks; and</claim-text>
<claim-text>writing pixel data of the four MC blocks as a group and in a sequential manner to a frame buffer, such that prior to being burst written to the frame buffer, the pixel data is temporarily held in an entry of a write-combining (WC) buffer, thereby eliminating partial writes from the WC buffer.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00003" num="00003">
<claim-text>3. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>,
<claim-text>wherein (5) further comprises:</claim-text>
<claim-text>storing luminance components (Y) of the decoded blocks in a planar array; and</claim-text>
<claim-text>wherein (6) further comprises:</claim-text>
<claim-text>storing chrominance components (UV) of the decoded blocks in a packed array, such that the decoded blocks are converted into a mixed storage format of planar format and packed format.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00004" num="00004">
<claim-text>4. A computer-readable medium having stored thereon a set of instructions, the set of instruction, which when executed by a processor, cause the processor to perform a method comprising:
<claim-text>(1) receiving a quantized block of an image;</claim-text>
<claim-text>(2) performing inverse quantization on the quantized block to generate a frequency spectrum for the quantized block;</claim-text>
<claim-text>(3) performing inverse discrete cosine transformation of the quantized block using the frequency spectrum to generate a decoded block;</claim-text>
<claim-text>(4) repeating (1)-(3) for a plurality of encoded blocks, such that a plurality of decoded blocks are formed;</claim-text>
<claim-text>(5) storing color components of the decoded blocks the image of a first color component type in a planar format;</claim-text>
<claim-text>(6) storing color components of the decoded clocks of the image of a second color component type and a third color component type in a packed format, such that the color components of the decoded blocks of the image are stored in a mixed format of planar format and packed format;</claim-text>
<claim-text>(7) motion compensating the color components of the decoded blocks as a group in the mixed format of planar format and packed format thereby generating a plurality of motion compensated (MC) blocks and;</claim-text>
<claim-text>(8) repeating (1)-(7) for each quanitized block of the image.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00005" num="00005">
<claim-text>5. The computer-readable medium of <claim-ref idref="CLM-00004">claim 4</claim-ref>, where the motion compensating of the plurality of blocks further comprises:
<claim-text>using as the plurality of blocks four blocks, such that four MC blocks are generated as the plurality of MC blocks; and</claim-text>
<claim-text>writing pixel data of the four MC blocks as a group and in a sequential manner to a frame buffer, such that prior to being burst written to the frame buffer, the pixel data is temporarily held in an entry of a write-combining (WC) buffer, thereby eliminating partial writes from the WC buffer.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00006" num="00006">
<claim-text>6. The computer-readable medium of <claim-ref idref="CLM-00004">claim 4</claim-ref>, wherein (5) further comprises:
<claim-text>storing luminance components (Y) of the decoded blocks in a planar array; and</claim-text>
<claim-text>wherein (6) further comprises:</claim-text>
<claim-text>storing chrominance components (UV) of the decoded blocks in a packed array, such that the decoded blocks are converted into a mixed storage format of planar format and packed format.</claim-text>
</claim-text>
</claim>
</claims>
</us-patent-grant>
