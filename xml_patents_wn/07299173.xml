<us-patent-grant lang="EN" dtd-version="v4.2 2006-08-23" file="US07299173-20071120.XML" status="PRODUCTION" id="us-patent-grant" country="US" date-produced="20071106" date-publ="20071120">
<us-bibliographic-data-grant>
<publication-reference>
<document-id>
<country>US</country>
<doc-number>07299173</doc-number>
<kind>B2</kind>
<date>20071120</date>
</document-id>
</publication-reference>
<application-reference appl-type="utility">
<document-id>
<country>US</country>
<doc-number>10060511</doc-number>
<date>20020130</date>
</document-id>
</application-reference>
<us-application-series-code>10</us-application-series-code>
<us-term-of-grant>
<us-term-extension>1024</us-term-extension>
</us-term-of-grant>
<classifications-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>G</section>
<class>10</class>
<subclass>L</subclass>
<main-group>21</main-group>
<subgroup>02</subgroup>
<symbol-position>F</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20071120</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
</classifications-ipcr>
<classification-national>
<country>US</country>
<main-classification>704215</main-classification>
<further-classification>704233</further-classification>
</classification-national>
<invention-title id="d0e53">Method and apparatus for speech detection using time-frequency variance</invention-title>
<references-cited>
<citation>
<patcit num="00001">
<document-id>
<country>US</country>
<doc-number>4222115</doc-number>
<kind>A</kind>
<name>Cooper et al.</name>
<date>19800900</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>375130</main-classification></classification-national>
</citation>
<citation>
<patcit num="00002">
<document-id>
<country>US</country>
<doc-number>4461024</doc-number>
<kind>A</kind>
<name>Rengger et al.</name>
<date>19840700</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<patcit num="00003">
<document-id>
<country>US</country>
<doc-number>4827519</doc-number>
<kind>A</kind>
<name>Fujimoto et al.</name>
<date>19890500</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>704250</main-classification></classification-national>
</citation>
<citation>
<patcit num="00004">
<document-id>
<country>US</country>
<doc-number>5097510</doc-number>
<kind>A</kind>
<name>Graupe</name>
<date>19920300</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>704233</main-classification></classification-national>
</citation>
<citation>
<patcit num="00005">
<document-id>
<country>US</country>
<doc-number>5617508</doc-number>
<kind>A</kind>
<name>Reaves</name>
<date>19970400</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<patcit num="00006">
<document-id>
<country>US</country>
<doc-number>5659622</doc-number>
<kind>A</kind>
<name>Ashley</name>
<date>19970800</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<patcit num="00007">
<document-id>
<country>US</country>
<doc-number>5692104</doc-number>
<kind>A</kind>
<name>Chow et al.</name>
<date>19971100</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<patcit num="00008">
<document-id>
<country>US</country>
<doc-number>5732392</doc-number>
<kind>A</kind>
<name>Mizuno et al.</name>
<date>19980300</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>704233</main-classification></classification-national>
</citation>
<citation>
<patcit num="00009">
<document-id>
<country>US</country>
<doc-number>5826230</doc-number>
<kind>A</kind>
<name>Reaves</name>
<date>19981000</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>704233</main-classification></classification-national>
</citation>
<citation>
<patcit num="00010">
<document-id>
<country>US</country>
<doc-number>5963901</doc-number>
<kind>A</kind>
<name>Vahatalo et al.</name>
<date>19991000</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>704233</main-classification></classification-national>
</citation>
<citation>
<patcit num="00011">
<document-id>
<country>US</country>
<doc-number>5991718</doc-number>
<kind>A</kind>
<name>Malah</name>
<date>19991100</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<patcit num="00012">
<document-id>
<country>US</country>
<doc-number>6278972</doc-number>
<kind>B1</kind>
<name>Bi et al.</name>
<date>20010800</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>704248</main-classification></classification-national>
</citation>
<citation>
<patcit num="00013">
<document-id>
<country>US</country>
<doc-number>6397050</doc-number>
<kind>B1</kind>
<name>Peterson et al.</name>
<date>20020500</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>455221</main-classification></classification-national>
</citation>
<citation>
<patcit num="00014">
<document-id>
<country>US</country>
<doc-number>6591234</doc-number>
<kind>B1</kind>
<name>Chandran et al.</name>
<date>20030700</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>704225</main-classification></classification-national>
</citation>
<citation>
<patcit num="00015">
<document-id>
<country>US</country>
<doc-number>6711536</doc-number>
<kind>B2</kind>
<name>Rees</name>
<date>20040300</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>704210</main-classification></classification-national>
</citation>
<citation>
<patcit num="00016">
<document-id>
<country>EP</country>
<doc-number>0 945 854</doc-number>
<kind>A2</kind>
<date>19990900</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<patcit num="00017">
<document-id>
<country>WO</country>
<doc-number>JP94/01181</doc-number>
<date>19960200</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<patcit num="00018">
<document-id>
<country>WO</country>
<doc-number>WO 01/11606</doc-number>
<kind>A1</kind>
<date>20010200</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<nplcit num="00019">
<othercit>John G. Proakis; “1.1.3 Statistical Averages of Random Variables”; <i>Digital Communications, Second Edition</i>; 1989; McGraw-Hill, Inc., pp. 17.</othercit>
</nplcit>
<category>cited by other</category>
</citation>
</references-cited>
<number-of-claims>10</number-of-claims>
<us-exemplary-claim>1</us-exemplary-claim>
<us-field-of-classification-search>
<classification-national>
<country>US</country>
<main-classification>704215</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>704233</main-classification>
</classification-national>
</us-field-of-classification-search>
<figures>
<number-of-drawing-sheets>2</number-of-drawing-sheets>
<number-of-figures>3</number-of-figures>
</figures>
<us-related-documents>
<related-publication>
<document-id>
<country>US</country>
<doc-number>20030144840</doc-number>
<kind>A1</kind>
<date>20030731</date>
</document-id>
</related-publication>
</us-related-documents>
<parties>
<applicants>
<applicant sequence="001" app-type="applicant-inventor" designation="us-only">
<addressbook>
<last-name>Ma</last-name>
<first-name>Changxue</first-name>
<address>
<city>Barrington</city>
<state>IL</state>
<country>US</country>
</address>
</addressbook>
<nationality>
<country>omitted</country>
</nationality>
<residence>
<country>US</country>
</residence>
</applicant>
<applicant sequence="002" app-type="applicant-inventor" designation="us-only">
<addressbook>
<last-name>Randolph</last-name>
<first-name>Mark</first-name>
<address>
<city>Kildeer</city>
<state>IL</state>
<country>US</country>
</address>
</addressbook>
<nationality>
<country>omitted</country>
</nationality>
<residence>
<country>US</country>
</residence>
</applicant>
</applicants>
<agents>
<agent sequence="01" rep-type="attorney">
<addressbook>
<last-name>Chen</last-name>
<first-name>Sylvia Y.</first-name>
<address>
<country>unknown</country>
</address>
</addressbook>
</agent>
</agents>
</parties>
<assignees>
<assignee>
<addressbook>
<orgname>Motorola Inc.</orgname>
<role>02</role>
<address>
<city>Schaumburg</city>
<state>IL</state>
<country>US</country>
</address>
</addressbook>
</assignee>
</assignees>
<examiners>
<primary-examiner>
<last-name>Abebe</last-name>
<first-name>Daniel</first-name>
<department>2626</department>
</primary-examiner>
</examiners>
</us-bibliographic-data-grant>
<abstract id="abstract">
<p id="p-0001" num="0000">Speech presence is detected by first bandpass filtering (<b>141, 143, 145</b>) the speech to split it into banks of sub-bands. A matrix of shift registers (<b>150</b>) store each sub-band of speech. A power determining circuit (<b>259</b>) then determines individual power measurements of the speech stored in each shift register element. A variance combining circuit (<b>160</b>) combines the individual power measurements to provide a variance for the individual shift registers. A comparator circuit (<b>170</b>) finally compares the variance with at least one threshold to indicate whether speech is detected.</p>
</abstract>
<drawings id="DRAWINGS">
<figure id="Fig-EMI-D00000" num="00000">
<img id="EMI-D00000" he="86.95mm" wi="119.04mm" file="US07299173-20071120-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00001" num="00001">
<img id="EMI-D00001" he="262.89mm" wi="163.83mm" file="US07299173-20071120-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00002" num="00002">
<img id="EMI-D00002" he="243.42mm" wi="161.04mm" file="US07299173-20071120-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
</drawings>
<description id="description">
<?BRFSUM description="Brief Summary" end="lead"?>
<heading id="h-0001" level="1">BACKGROUND OF THE INVENTION</heading>
<p id="p-0002" num="0001">1. Technical Field</p>
<p id="p-0003" num="0002">The present invention relates to speech detection and, more particularly, relates to improved approaches to efficiently detect speech presence in a noisy environment by way of frequency and temporal considerations.</p>
<p id="p-0004" num="0003">2. Description of the Related Art</p>
<p id="p-0005" num="0004">In some applications, automatic speech recognition needs to be activated by uttering a particular word sequence such as keywords. For example, if a desktop personal computer has a speech recognizer for dictation or command control, it is desirable to activate the recognizer in the middle of the conversations in his or her office by uttering a keyword. This process of recognizing the keyword from continuous speech waveform is called keyword scanning. This would require the recognizer constantly recognizing the incoming speech and spotting those keywords. Nevertheless, the recognizer cannot be used to constantly monitor the incoming speech because it takes huge computational resources. Some other techniques that demand much less computations and memories have to be utilized to reduce the burden of speech recognizer. It is known that speech detection techniques are ways of eliminating silence segments from speech utterances so that speech recognizer can be speed up and do not wasting a lot of time on those silences or even misrecognize silence as speech. Speech detection techniques are often based on the speech waveform and utilize features such as short-time energy, zero crossing and etc. The same can be used to hypothesize keyword if some other features such as pitch, duration and voicing can be used in junction with word end-pointing techniques. Although the keyword hypothesis will be over generated, it still can reduce a large proportion of computations since the recognizer will only process these hypotheses.</p>
<p id="p-0006" num="0005">Most speech recognition applications today face the challenging task of segmenting speech based on voice, unvoice &amp; silence detection. A conventional approach is detecting short-term energy and zero crossings of a speech signal. These approaches are not reliable for noisy telephone speech signals due, in part, to the greater noise in a background environment of most telephone conversations. For example, stationary noise such as motor or wind noise and non-stationary noise such as door openings, closing or respiratory exhalation are present in telephone speech.</p>
<p id="p-0007" num="0006">Accurate speech presence detection also conserves power and processing time for portable electronic devices such as cellular telephones. When reliable speech detection approaches are used, a speech recognition algorithm must find the utterances to determine if they are in fact language. This places a burden on computational complexity of processors and is a resource drain on portable electronic devices. A speech detection approach having computational efficiency as well as accuracy is needed.</p>
<heading id="h-0002" level="1">SUMMARY OF THE INVENTION</heading>
<p id="p-0008" num="0007">The inventors of the present invention have discovered that there is a high variance associated with voiced speech such as vowels and the low variance associated with silences and wide-band noise. Speech presence can be efficiently detected in a noisy environment by way of frequency and temporal considerations using this variance.</p>
<p id="p-0009" num="0008">Speech presence is detected by first bandpass filtering the speech to split it into banks of sub-bands. A matrix of shift registers secondly store each sub-band of speech. A power determining circuit then determines individual power measurements of the speech stored in each shift register element. A combining circuit combines the individual power measurements to provide a variance for the individual shift registers. A comparator circuit finally compares the variance with at least one threshold to indicate whether speech is detected. The present invention can be implemented by software in a microprocessor, digital signal processor or combinations with discrete components.</p>
<p id="p-0010" num="0009">The details of the preferred embodiments of the invention will be readily understood from the following detailed description when read in conjunction with the accompanying drawings wherein:</p>
<?BRFSUM description="Brief Summary" end="tail"?>
<?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?>
<description-of-drawings>
<heading id="h-0003" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading>
<p id="p-0011" num="0010"><figref idref="DRAWINGS">FIG. 1</figref> illustrates a schematic block diagram of a time-frequency matrix and variance circuit for speech detection according to the present invention;</p>
<p id="p-0012" num="0011"><figref idref="DRAWINGS">FIG. 2</figref> illustrates a detailed schematic block diagram of one matrix element of <figref idref="DRAWINGS">FIG. 1</figref> for determining power measurements used in the speech detection according to the present invention; and</p>
<p id="p-0013" num="0012"><figref idref="DRAWINGS">FIG. 3</figref> illustrates a flow chart diagram for performing time-frequency matrix to detect speech according to the present invention.</p>
</description-of-drawings>
<?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?>
<?DETDESC description="Detailed Description" end="lead"?>
<heading id="h-0004" level="1">DETAILED DESCRIPTION OF THE PREFERRED EMBODIMENTS</heading>
<p id="p-0014" num="0013"><figref idref="DRAWINGS">FIG. 1</figref> illustrates a schematic block diagram of the time-frequency matrix and variance circuit for speech detection according to the present invention. A microphone <b>110</b> gathers speech often in a noisy environment. In amplifier and analog to digital converter <b>120</b> amplifies and conditions the electrical speech signal received by the microphone <b>110</b> and converts the electrical speech signal to digital speech sampled in time. In the preferred embodiment, the digital speech is sampled at preferably an 8 kHz sampling frequency and stored in frames preferably having a 10 millisecond duration. A preemphasis circuit <b>130</b> operates on the digital speech to equalize its power spectrum to make its frequency spectrum more flat. A digital signal processing emphasis of 1-0.9 Z<sup>−1 </sup>is preferred to equalize the input signal and derive a preemphasized output signal.</p>
<p id="p-0015" num="0014">Low band bandpass filter <b>141</b>, mid band bandpass filter <b>143</b> and high band bandpass filter <b>145</b> split the preemphasized digital speech signal into a bank of preferably three sub-bands. Although a bank of three sub-bands is preferred, two or more sub-bands will work depending on the level of processing power and degree of detection accuracy needed for a noisy environment. It is preferred that the bandpass filters <b>141</b>,<b>143</b> and <b>145</b> divide the speech signal into somewhat equal sub-bands between 100 Hz and 3,000 Hz as follows. The low band bandpass filter <b>141</b> preferably has a band between 100 Hz and 1267 Hz, the mid and bandpass filter <b>143</b> preferably has a bandpass between 1267 Hz and 2433 Hz. The high band bandpass filter <b>145</b> preferably has a bandpass between 2433 Hz and 3600 Hz. Different band widths can be used for each sub-band.</p>
<p id="p-0016" num="0015">A matrix of shift registers <b>150</b> receives the three sub-bands from the bandpass filters <b>141</b>, <b>143</b> and <b>145</b>. The shift registers <b>150</b> store each of the sub-bands and shifted to a next register location for each frame. In the preferred embodiment a total of three frames are stored in the shift registers, thus creating a three-by-three matrix Y<sub>ij </sub>consisting of matrix elements Y<sub>11</sub>, Y<sub>12</sub>, Y<sub>13</sub>, Y<sub>21</sub>, Y<sub>22</sub>, Y<sub>23</sub>, Y<sub>31</sub>, Y<sub>32 </sub>and Y<sub>33</sub>. This matrix stores the speech information by way of both frequency and temporal considerations. Each of the three-by-three matrix elements contains sub-registers <b>250</b> for storing multiple samples k within a frame. For each of the register memories of the shift registers <b>150</b>, a power measurement X<sub>ij </sub>is derived from the contents of the sub-registers. The calculation of the power measurements X<sub>ij </sub>for each sub-band over a frame i within a preferred 10 ms frame duration is performed by</p>
<p id="p-0017" num="0016">
<maths id="MATH-US-00001" num="00001">
<math overflow="scroll">
<mtable>
  <mtr>
    <mtd>
      <mrow>
        <msub>
          <mi>X</mi>
          <mi>ij</mi>
        </msub>
        <mo>=</mo>
        <mrow>
          <munder>
            <mo>∑</mo>
            <mi>k</mi>
          </munder>
          <mo>⁢</mo>
          <msubsup>
            <mi>s</mi>
            <mi>ijk</mi>
            <mn>2</mn>
          </msubsup>
        </mrow>
      </mrow>
    </mtd>
    <mtd>
      <mrow>
        <mo>(</mo>
        <mn>1</mn>
        <mo>)</mo>
      </mrow>
    </mtd>
  </mtr>
</mtable>
</math>
</maths>
<ul id="ul0001" list-style="none">
    <li id="ul0001-0001" num="0000">
    <ul id="ul0002" list-style="none">
        <li id="ul0002-0001" num="0017">wherein i is the frame index;</li>
        <li id="ul0002-0002" num="0018">wherein j is a frequency sub-band index;</li>
        <li id="ul0002-0003" num="0019">wherein k is the sample index within a frame; and</li>
        <li id="ul0002-0004" num="0020">wherein S<sub>ijk </sub>is the speech samples for a given frame index i, a given frequency sub-band j and a given sample index k.</li>
    </ul>
    </li>
</ul>
</p>
<p id="p-0018" num="0021">The calculations of the power measurements X<sub>ij </sub>are preferably calculated within each of the matrix elements Y<sub>ij </sub>of the shift register <b>150</b>. The power measurement calculation sums the squares of each of the power samples for a particular sub-band over time. More detail for the preferred calculation of the power measurement for a sub-band across a number of samples in the shift register elements will later be described with reference to <figref idref="DRAWINGS">FIG. 2</figref> in more detail. Alternatively, a variance combining circuit <b>160</b> can be performed calculations of the power measurements.</p>
<p id="p-0019" num="0022">The inventors of the present invention have discovered there is a high variance associated with voiced speech such as vowels and the low variance associated with silences and wide-band noise. A variance is a mathematical relationship known in digital speech processing as defined in elementary digital signal processing textbooks as such as <i>Digital Communications</i>, equations 1.1.65 or 1.1.66, by Proakis on page 17, published in 1989. The present invention applies a variance to a time-frequency power measurement to detect speech presence.</p>
<p id="p-0020" num="0023">A variance combining circuit <b>160</b> calculates the variance of the plurality of power measurements for each sub-band and each frame. Calculating the variance VAR of the plurality of power measurements X<sub>ij </sub>for each sub-band j for each frame index i is calculated by</p>
<p id="p-0021" num="0024">
<maths id="MATH-US-00002" num="00002">
<math overflow="scroll">
<mtable>
  <mtr>
    <mtd>
      <mrow>
        <mi>VAR</mi>
        <mo>=</mo>
        <mrow>
          <mfrac>
            <mrow>
              <mo>∑</mo>
              <msubsup>
                <mi>X</mi>
                <mi>ij</mi>
                <mn>2</mn>
              </msubsup>
            </mrow>
            <mi>n</mi>
          </mfrac>
          <mo>-</mo>
          <msup>
            <mrow>
              <mo>(</mo>
              <mfrac>
                <mrow>
                  <mo>∑</mo>
                  <msub>
                    <mi>X</mi>
                    <mi>ij</mi>
                  </msub>
                </mrow>
                <mi>n</mi>
              </mfrac>
              <mo>)</mo>
            </mrow>
            <mn>2</mn>
          </msup>
        </mrow>
      </mrow>
    </mtd>
    <mtd>
      <mrow>
        <mo>(</mo>
        <mn>2</mn>
        <mo>)</mo>
      </mrow>
    </mtd>
  </mtr>
</mtable>
</math>
</maths>
<ul id="ul0003" list-style="none">
    <li id="ul0003-0001" num="0000">
    <ul id="ul0004" list-style="none">
        <li id="ul0004-0001" num="0025">wherein i is the frame index;</li>
        <li id="ul0004-0002" num="0026">wherein j is a frequency sub-band index;</li>
        <li id="ul0004-0003" num="0027">wherein X<sub>ij </sub>is the power for a given time sample index i and a given frequency sub-band j.</li>
    </ul>
    </li>
</ul>
</p>
<p id="p-0022" num="0028">A comparator <b>170</b> compares the variance VAR with a threshold to determine whether or not the presence of speech is detected. When the variance is above the threshold, the presence of speech is detected, and a speech detection indication signal <b>180</b> is output. The threshold is preferably a fixed level however a variable threshold under certain conditions will yield more favorable results. A variable threshold can depend on determined by using an average of the past history of non-speech frames. Further, multiple thresholds can be implemented, one for clearly speech, one for clearly unspeech. A decision is made upon a transition over either of these thresholds.</p>
<p id="p-0023" num="0029">The presence of speech indicated by the speech detection indication signal <b>180</b> can be used to gate on and off a speech recognition unit. The detection of the presence of speech is useful to gate and off a speech recognition unit so that the speech recognition unit does not need to operate continuously. This saves processing time that can be used for other purposes and/or conserves power, which reduces battery consumption in a portable electronic device. When a speech recognition circuit is present in a portable electronic device such as a cellular telephone, battery savings are achieved by freeing up the processor for other functions when speech presence is accurately determined. Also, the speech presence detection circuit does not require full activation of a recognition code so its more efficient. Reduction of miss-recognition is also achieved when using better speech presence accuracy. The speech detection indications are also useful for other devices such as speaker phones.</p>
<p id="p-0024" num="0030"><figref idref="DRAWINGS">FIG. 2</figref> illustrates a detailed schematic block diagram of the preferred construction of a plurality of sub-registers <b>250</b> and a power calculation circuit <b>259</b> for determining power measurements used in the speech detection according to the present invention. The preferred calculation of the power measurement for a sub-band, across a number of samples in one matrix element, is illustrated. The a plurality of sub-registers <b>250</b> and a power calculation circuit <b>259</b> are within one of the nine three-by-three matrix elements Y<sub>ij </sub>illustrated in <figref idref="DRAWINGS">FIG. 1</figref>. A plurality <b>250</b> of sub-register elements <b>251</b>, <b>252</b>, <b>253</b> through <b>255</b> receive the filtered sub-band speech from a bandpass filter of <figref idref="DRAWINGS">FIG. 1</figref>. Each sub-register element contains a speech sample S<sub>ijk </sub>for a given time and frequency sub-band. Sub-register element <b>251</b> corresponds to a first sample index k=1 within a frame for a given frame i and sub-band j. Sub-register element <b>252</b> corresponds to a second sample index and sub-register element <b>253</b> corresponds to a third sample index. A total of up to n sample indexes k are possible.</p>
<p id="p-0025" num="0031">A power calculation circuit <b>259</b> calculates the average power among the sub-register elements for the given frame i and sub-band j. The average power X<sub>ij </sub>is calculated using the above equation (1). Each power calculation circuit <b>259</b> corresponds to one of the shift register elements in the matrix of <figref idref="DRAWINGS">FIG. 1</figref>. The output of the power calculation circuit <b>259</b> connects to the variance combining circuit <b>160</b> of <figref idref="DRAWINGS">FIG. 1</figref>.</p>
<p id="p-0026" num="0032"><figref idref="DRAWINGS">FIG. 3</figref> illustrates a flow chart diagram for performing time-frequency matrix to detect speech according to the present invention. In step <b>310</b>, speech is received, often in a noisy environment. In step <b>320</b> the received speech is preemphasized to improve recognition accuracy by equalizing the power spectrum of the speech signal to flatten its frequency spectrum. In step <b>330</b> to the speech is bandpass filtered into sub-bands. A power calculation is made in step <b>340</b> for the various samples over the various sub-bands. A power calculation is made in step <b>342</b> over the samples for the various sub-bands after delaying one frame in step <b>341</b>. A power calculation is made in step <b>344</b> over the samples for the various sub-bands after delaying to frames in step <b>343</b>. In step <b>350</b>, a variance is calculated using the power calculations derived above over frequency and over time. This variance is compared in step <b>360</b> with at least one threshold <b>370</b> to indicate that speech presence is detected at output <b>380</b> when the variance is above the threshold.</p>
<p id="p-0027" num="0033">The signal processing techniques of the present invention disclosed herein with reference to the accompanying drawings are preferably implemented on one or more digital signal processors (DSPs) or other microprocessors. Nevertheless, such techniques could instead be implemented wholly or partially as discrete components. Further, it is appreciated by those of skill in the art that certain well known digital processing techniques are mathematically equivalent to one another and can be represented in different ways depending on the choice of implementation. For example the square of the terms in the variance calculation and/or power calculation can be substituted for absolute values without affecting the results.</p>
<p id="p-0028" num="0034">Although the invention has been described and illustrated in the above description and drawings, it is understood that this description is by example only, and that numerous changes and modifications can be made by those skilled in the art without departing from the true spirit and scope of the invention. Although the examples in the drawings depict only example constructions and embodiments, alternate embodiments are available given the teachings of the present patent disclosure.</p>
<?DETDESC description="Detailed Description" end="tail"?>
</description>
<us-math idrefs="MATH-US-00001" nb-file="US07299173-20071120-M00001.NB">
<img id="EMI-M00001" he="7.03mm" wi="76.20mm" file="US07299173-20071120-M00001.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-math idrefs="MATH-US-00002" nb-file="US07299173-20071120-M00002.NB">
<img id="EMI-M00002" he="8.13mm" wi="76.20mm" file="US07299173-20071120-M00002.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-math idrefs="MATH-US-00003" nb-file="US07299173-20071120-M00003.NB">
<img id="EMI-M00003" he="7.03mm" wi="76.20mm" file="US07299173-20071120-M00003.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-math idrefs="MATH-US-00004" nb-file="US07299173-20071120-M00004.NB">
<img id="EMI-M00004" he="7.79mm" wi="76.20mm" file="US07299173-20071120-M00004.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-math idrefs="MATH-US-00005" nb-file="US07299173-20071120-M00005.NB">
<img id="EMI-M00005" he="7.03mm" wi="76.20mm" file="US07299173-20071120-M00005.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-math idrefs="MATH-US-00006" nb-file="US07299173-20071120-M00006.NB">
<img id="EMI-M00006" he="7.79mm" wi="76.20mm" file="US07299173-20071120-M00006.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-claim-statement>What is claimed is:</us-claim-statement>
<claims id="claims">
<claim id="CLM-00001" num="00001">
<claim-text>1. A speech presence detection apparatus, comprising:
<claim-text>a plurality of bandpass filters for splitting speech into a bank of sub-bands;</claim-text>
<claim-text>a plurality of shift registers each connected to and associated with one of the bandpass filters for storing the speech of a corresponding sub-band in register elements;</claim-text>
<claim-text>a power determining circuit for determining individual power measurements of the speech stored in each register element;</claim-text>
<claim-text>a variance combining circuit for combining the individual power measurements to provide a time-frequency variance for the individual registers; and</claim-text>
<claim-text>a comparator circuit for comparing the variance with a threshold to indicate whether speech is detected.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00002" num="00002">
<claim-text>2. A method of detecting the presence of speech, comprising the steps of:
<claim-text>(a) calculating a plurality of power samples of speech, each power sample corresponding to a frequency sub-band and time frame of the speech; and</claim-text>
<claim-text>(b) calculating a time-frequency variance of the plurality of power samples; and</claim-text>
<claim-text>(c) comparing the time-frequency variance with at least one threshold to indicate whether speech is detected.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00003" num="00003">
<claim-text>3. A method according to <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein the calculation in step (a) of the plurality of power samples of the speech over time and frequency comprises calculating a power corresponding to different audible bands and different sampling periods.</claim-text>
</claim>
<claim id="CLM-00004" num="00004">
<claim-text>4. A method according to <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein the calculation in step (a) of the plurality of power samples of the speech over time and frequency comprises the substeps of (a<b>1</b>) bandpass filtering the speech into banks of sub-bands; (a<b>2</b>) storing the speech of a corresponding sub-band; and (a<b>3</b>) calculating a power of the sub-band over a frame.</claim-text>
</claim>
<claim id="CLM-00005" num="00005">
<claim-text>5. A method according to <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein step (a) of calculating a plurality of power samples of speech comprises</claim-text>
<claim-text>
<maths id="MATH-US-00003" num="00003">
<math overflow="scroll">
<mrow>
  <msub>
    <mi>X</mi>
    <mi>ij</mi>
  </msub>
  <mo>=</mo>
  <mrow>
    <munder>
      <mo>∑</mo>
      <mi>k</mi>
    </munder>
    <mo>⁢</mo>
    <msubsup>
      <mi>s</mi>
      <mi>ijk</mi>
      <mn>2</mn>
    </msubsup>
  </mrow>
</mrow>
</math>
</maths>
<claim-text>wherein i is the frame index;</claim-text>
<claim-text>wherein j is a frequency sub-band index;</claim-text>
<claim-text>wherein k is the sample index within a frame; and</claim-text>
<claim-text>wherein S<sub>ijk </sub>is the speech samples for a given frame index i, a given frequency sub-band j and a given sample index k.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00006" num="00006">
<claim-text>6. A method according to <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein step (b) of calculating a time-frequency variance of the plurality of power measurements comprises</claim-text>
<claim-text>
<maths id="MATH-US-00004" num="00004">
<math overflow="scroll">
<mrow>
  <mi>VAR</mi>
  <mo>=</mo>
  <mrow>
    <mfrac>
      <mrow>
        <mo>∑</mo>
        <msubsup>
          <mi>X</mi>
          <mi>ij</mi>
          <mn>2</mn>
        </msubsup>
      </mrow>
      <mi>n</mi>
    </mfrac>
    <mo>-</mo>
    <msup>
      <mrow>
        <mo>(</mo>
        <mfrac>
          <mrow>
            <mo>∑</mo>
            <msub>
              <mi>X</mi>
              <mi>ij</mi>
            </msub>
          </mrow>
          <mi>n</mi>
        </mfrac>
        <mo>)</mo>
      </mrow>
      <mn>2</mn>
    </msup>
  </mrow>
</mrow>
</math>
</maths>
<claim-text>wherein i is a frame index;</claim-text>
<claim-text>wherein j is a frequency sub-band index;</claim-text>
<claim-text>wherein X<sub>ij </sub>is the power measurement for a given time sample index i and a given frequency sub-band j.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00007" num="00007">
<claim-text>7. A method according to <claim-ref idref="CLM-00006">claim 6</claim-ref>, wherein the step (a) of calculating each power measurement comprises</claim-text>
<claim-text>
<maths id="MATH-US-00005" num="00005">
<math overflow="scroll">
<mrow>
  <msub>
    <mi>X</mi>
    <mi>ij</mi>
  </msub>
  <mo>=</mo>
  <mrow>
    <munder>
      <mo>∑</mo>
      <mi>k</mi>
    </munder>
    <mo>⁢</mo>
    <msubsup>
      <mi>s</mi>
      <mi>ijk</mi>
      <mn>2</mn>
    </msubsup>
  </mrow>
</mrow>
</math>
</maths>
<claim-text>wherein i is the frame index;</claim-text>
<claim-text>wherein j is a frequency sub-band index;</claim-text>
<claim-text>wherein k is a sample index within a frame; and</claim-text>
<claim-text>wherein S<sub>ijk </sub>is the speech samples for a given frame index i, a given frequency sub-band j and a given sample index k.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00008" num="00008">
<claim-text>8. A method according to <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein the calculation in step (c) of comparing the time-frequency variance with at least one threshold indicates that speech is detected when the time-frequency variance is above a threshold.</claim-text>
</claim>
<claim id="CLM-00009" num="00009">
<claim-text>9. An apparatus for detecting the presence of speech, comprising:
<claim-text>means for calculating a plurality of power samples of speech, each power sample corresponding to a frequency sub-band and time frame of the speech;</claim-text>
<claim-text>means for calculating a time-frequency variance of the plurality of power samples; and</claim-text>
<claim-text>means for comparing the time-frequency variance with at least one threshold to indicate whether speech is detected.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00010" num="00010">
<claim-text>10. An apparatus according to <claim-ref idref="CLM-00009">claim 9</claim-ref>, wherein the means for calculating a time-frequency variance of the plurality of power samples comprises</claim-text>
<claim-text>
<maths id="MATH-US-00006" num="00006">
<math overflow="scroll">
<mrow>
  <mi>VAR</mi>
  <mo>=</mo>
  <mrow>
    <mfrac>
      <mrow>
        <mo>∑</mo>
        <msubsup>
          <mi>X</mi>
          <mi>ij</mi>
          <mn>2</mn>
        </msubsup>
      </mrow>
      <mi>n</mi>
    </mfrac>
    <mo>-</mo>
    <msup>
      <mrow>
        <mo>(</mo>
        <mfrac>
          <mrow>
            <mo>∑</mo>
            <msub>
              <mi>X</mi>
              <mi>ij</mi>
            </msub>
          </mrow>
          <mi>n</mi>
        </mfrac>
        <mo>)</mo>
      </mrow>
      <mn>2</mn>
    </msup>
  </mrow>
</mrow>
</math>
</maths>
<claim-text>wherein i is a frame index;</claim-text>
<claim-text>wherein j is a frequency sub-band index;</claim-text>
<claim-text>wherein X<sub>ij </sub>is the power for a given time sample index i and a given frequency sub-band j.</claim-text>
</claim-text>
</claim>
</claims>
</us-patent-grant>
