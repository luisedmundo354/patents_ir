<us-patent-grant lang="EN" dtd-version="v4.2 2006-08-23" file="US07299188-20071120.XML" status="PRODUCTION" id="us-patent-grant" country="US" date-produced="20071106" date-publ="20071120">
<us-bibliographic-data-grant>
<publication-reference>
<document-id>
<country>US</country>
<doc-number>07299188</doc-number>
<kind>B2</kind>
<date>20071120</date>
</document-id>
</publication-reference>
<application-reference appl-type="utility">
<document-id>
<country>US</country>
<doc-number>10361256</doc-number>
<date>20030210</date>
</document-id>
</application-reference>
<us-application-series-code>10</us-application-series-code>
<us-term-of-grant>
<us-term-extension>770</us-term-extension>
</us-term-of-grant>
<classifications-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>G</section>
<class>10</class>
<subclass>L</subclass>
<main-group>11</main-group>
<subgroup>00</subgroup>
<symbol-position>F</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20071120</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>G</section>
<class>10</class>
<subclass>L</subclass>
<main-group>21</main-group>
<subgroup>06</subgroup>
<symbol-position>L</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20071120</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
</classifications-ipcr>
<classification-national>
<country>US</country>
<main-classification>704276</main-classification>
<further-classification>704  1</further-classification>
</classification-national>
<invention-title id="d0e53">Method and apparatus for providing an interactive language tutor</invention-title>
<references-cited>
<citation>
<patcit num="00001">
<document-id>
<country>US</country>
<doc-number>4749353</doc-number>
<kind>A</kind>
<name>Breedlove</name>
<date>19880600</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>434169</main-classification></classification-national>
</citation>
<citation>
<patcit num="00002">
<document-id>
<country>US</country>
<doc-number>5393236</doc-number>
<kind>A</kind>
<name>Blackmer et al.</name>
<date>19950200</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>434169</main-classification></classification-national>
</citation>
<citation>
<patcit num="00003">
<document-id>
<country>US</country>
<doc-number>5778256</doc-number>
<kind>A</kind>
<name>Darbee</name>
<date>19980700</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>710 72</main-classification></classification-national>
</citation>
<citation>
<patcit num="00004">
<document-id>
<country>US</country>
<doc-number>5799276</doc-number>
<kind>A</kind>
<name>Komissarchik et al.</name>
<date>19980800</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>704251</main-classification></classification-national>
</citation>
<citation>
<patcit num="00005">
<document-id>
<country>US</country>
<doc-number>5870709</doc-number>
<kind>A</kind>
<name>Bernstein</name>
<date>19990200</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>704275</main-classification></classification-national>
</citation>
<citation>
<patcit num="00006">
<document-id>
<country>US</country>
<doc-number>6055498</doc-number>
<kind>A</kind>
<name>Neumeyer et al.</name>
<date>20000400</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>704246</main-classification></classification-national>
</citation>
<citation>
<patcit num="00007">
<document-id>
<country>US</country>
<doc-number>6336089</doc-number>
<kind>B1</kind>
<name>Everding</name>
<date>20020100</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>704  1</main-classification></classification-national>
</citation>
<citation>
<patcit num="00008">
<document-id>
<country>US</country>
<doc-number>6728680</doc-number>
<kind>B1</kind>
<name>Aaron et al.</name>
<date>20040400</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>704271</main-classification></classification-national>
</citation>
<citation>
<patcit num="00009">
<document-id>
<country>US</country>
<doc-number>6741833</doc-number>
<kind>B2</kind>
<name>McCormick et al.</name>
<date>20040500</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>434350</main-classification></classification-national>
</citation>
<citation>
<patcit num="00010">
<document-id>
<country>US</country>
<doc-number>2003/0028378</doc-number>
<kind>A1</kind>
<name>August et al.</name>
<date>20030200</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>704260</main-classification></classification-national>
</citation>
<citation>
<nplcit num="00011">
<othercit>Speech Communciation 30 (2000) 83-93, “Automatic scoring of pronunciation quality,” Leonardo Neumeyer, Horacio Franco, Vassilios Digalakis, Mitchel Weintraub.</othercit>
</nplcit>
<category>cited by other</category>
</citation>
<citation>
<nplcit num="00012">
<othercit>Speech Communication 30 (2000) 95-108, “Phone-level pronunciation scoring and assessment for interactive language learning,” S.M. Witt, S.J. Young.</othercit>
</nplcit>
<category>cited by other</category>
</citation>
<citation>
<nplcit num="00013">
<othercit>Speech Communication 30 (2000) 131-143, “Teaching the pronunciation of Japanese double-mora phonemes using speech recognition technology,” Goh Kawai, Keikichi Hirose.</othercit>
</nplcit>
<category>cited by other</category>
</citation>
<citation>
<nplcit num="00014">
<othercit>Speech Communication 30 (2000) 146-166, “SLIM prosodic automatic tools for self-learning instruction,” Rodolfo Delmonte.</othercit>
</nplcit>
<category>cited by other</category>
</citation>
<citation>
<nplcit num="00015">
<othercit>Speech Technology and Research Laboratory, SRI International, “Automatic Detection of Mispronunciation for Language Instruction,” Orith Ronen, Leonardo Neumeyer, and Horacio Franco.</othercit>
</nplcit>
<category>cited by other</category>
</citation>
<citation>
<nplcit num="00016">
<othercit>Speech Technology and Research Laboratory, SRI International, “Automatic Pronunciation Scoring for Language Instruction,” Horacio Franco, Leonardo Neumeyer, Yoon Kim and Orith Ronen.</othercit>
</nplcit>
<category>cited by other</category>
</citation>
</references-cited>
<number-of-claims>19</number-of-claims>
<us-exemplary-claim>1</us-exemplary-claim>
<us-field-of-classification-search>
<classification-national>
<country>US</country>
<main-classification>704  1</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>704276</main-classification>
</classification-national>
</us-field-of-classification-search>
<figures>
<number-of-drawing-sheets>14</number-of-drawing-sheets>
<number-of-figures>20</number-of-figures>
</figures>
<us-related-documents>
<continuation-in-part>
<relation>
<parent-doc>
<document-id>
<country>US</country>
<doc-number>10188539</doc-number>
<kind>00</kind>
<date>20020703</date>
</document-id>
<parent-status>PENDING</parent-status>
</parent-doc>
<child-doc>
<document-id>
<country>US</country>
<doc-number>10361256</doc-number>
</document-id>
</child-doc>
</relation>
</continuation-in-part>
<us-provisional-application>
<document-id>
<country>US</country>
<doc-number>60397512</doc-number>
<kind>00</kind>
<date>20020719</date>
</document-id>
</us-provisional-application>
<related-publication>
<document-id>
<country>US</country>
<doc-number>20040006461</doc-number>
<kind>A1</kind>
<date>20040108</date>
</document-id>
</related-publication>
</us-related-documents>
<parties>
<applicants>
<applicant sequence="001" app-type="applicant-inventor" designation="us-only">
<addressbook>
<last-name>Gupta</last-name>
<first-name>Sunil K.</first-name>
<address>
<city>Edison</city>
<state>NJ</state>
<country>US</country>
</address>
</addressbook>
<nationality>
<country>omitted</country>
</nationality>
<residence>
<country>US</country>
</residence>
</applicant>
<applicant sequence="002" app-type="applicant-inventor" designation="us-only">
<addressbook>
<last-name>Lu</last-name>
<first-name>ZiYi</first-name>
<address>
<city>Shanghai</city>
<country>CN</country>
</address>
</addressbook>
<nationality>
<country>CN</country>
</nationality>
<residence>
<country>CN</country>
</residence>
</applicant>
<applicant sequence="003" app-type="applicant-inventor" designation="us-only">
<addressbook>
<last-name>Raghavan</last-name>
<first-name>Prabhu</first-name>
<address>
<city>Edison</city>
<state>NJ</state>
<country>US</country>
</address>
</addressbook>
<nationality>
<country>omitted</country>
</nationality>
<residence>
<country>US</country>
</residence>
</applicant>
<applicant sequence="004" app-type="applicant-inventor" designation="us-only">
<addressbook>
<last-name>Sayeed</last-name>
<first-name>Zulfiquar</first-name>
<address>
<city>Hightstown</city>
<state>NJ</state>
<country>US</country>
</address>
</addressbook>
<nationality>
<country>omitted</country>
</nationality>
<residence>
<country>US</country>
</residence>
</applicant>
<applicant sequence="005" app-type="applicant-inventor" designation="us-only">
<addressbook>
<last-name>Sethuraman</last-name>
<first-name>Aravind</first-name>
<address>
<city>Piscataway</city>
<state>NJ</state>
<country>US</country>
</address>
</addressbook>
<nationality>
<country>omitted</country>
</nationality>
<residence>
<country>US</country>
</residence>
</applicant>
<applicant sequence="006" app-type="applicant-inventor" designation="us-only">
<addressbook>
<last-name>Vinchhi</last-name>
<first-name>Chetan</first-name>
<address>
<city>Marlboro</city>
<state>NJ</state>
<country>US</country>
</address>
</addressbook>
<nationality>
<country>omitted</country>
</nationality>
<residence>
<country>US</country>
</residence>
</applicant>
</applicants>
</parties>
<assignees>
<assignee>
<addressbook>
<orgname>Lucent Technologies Inc.</orgname>
<role>02</role>
<address>
<city>Murray Hill</city>
<state>NJ</state>
<country>US</country>
</address>
</addressbook>
</assignee>
</assignees>
<examiners>
<primary-examiner>
<last-name>Smits</last-name>
<first-name>Talivaldis Ivars</first-name>
<department>2626</department>
</primary-examiner>
<assistant-examiner>
<last-name>Rider</last-name>
<first-name>Justin W</first-name>
</assistant-examiner>
</examiners>
</us-bibliographic-data-grant>
<abstract id="abstract">
<p id="p-0001" num="0000">A method and apparatus for generating a pronunciation score by receiving a user phrase intended to conform to a reference phrase and processing the user phrase in accordance with at least one of an articulation-scoring engine, a duration scoring engine and an intonation-scoring engine to derive thereby the pronunciation score. The scores provided by the various scoring engines are adapted to provide a visual and/or numerical feedback that provides information pertaining to correctness or incorrectness in one or more speech-features such as intonation, articulation, voicing, phoneme error and relative word duration. Such useful interactive feedback will allow a user to quickly identify the problem area and take remedial action in reciting “tutor” sentences or phrases.</p>
</abstract>
<drawings id="DRAWINGS">
<figure id="Fig-EMI-D00000" num="00000">
<img id="EMI-D00000" he="78.57mm" wi="121.67mm" file="US07299188-20071120-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00001" num="00001">
<img id="EMI-D00001" he="192.28mm" wi="152.06mm" orientation="landscape" file="US07299188-20071120-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00002" num="00002">
<img id="EMI-D00002" he="186.86mm" wi="118.19mm" file="US07299188-20071120-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00003" num="00003">
<img id="EMI-D00003" he="211.75mm" wi="135.64mm" file="US07299188-20071120-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00004" num="00004">
<img id="EMI-D00004" he="192.11mm" wi="114.89mm" file="US07299188-20071120-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00005" num="00005">
<img id="EMI-D00005" he="133.69mm" wi="141.31mm" file="US07299188-20071120-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00006" num="00006">
<img id="EMI-D00006" he="134.70mm" wi="149.01mm" file="US07299188-20071120-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00007" num="00007">
<img id="EMI-D00007" he="136.31mm" wi="150.62mm" file="US07299188-20071120-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00008" num="00008">
<img id="EMI-D00008" he="177.04mm" wi="151.21mm" file="US07299188-20071120-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00009" num="00009">
<img id="EMI-D00009" he="188.04mm" wi="125.05mm" file="US07299188-20071120-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00010" num="00010">
<img id="EMI-D00010" he="195.07mm" wi="130.89mm" file="US07299188-20071120-D00010.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00011" num="00011">
<img id="EMI-D00011" he="200.32mm" wi="135.13mm" file="US07299188-20071120-D00011.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00012" num="00012">
<img id="EMI-D00012" he="164.68mm" wi="154.35mm" file="US07299188-20071120-D00012.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00013" num="00013">
<img id="EMI-D00013" he="223.44mm" wi="165.69mm" orientation="landscape" file="US07299188-20071120-D00013.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00014" num="00014">
<img id="EMI-D00014" he="205.57mm" wi="155.79mm" orientation="landscape" file="US07299188-20071120-D00014.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
</drawings>
<description id="description">
<?RELAPP description="Other Patent Relations" end="lead"?>
<p id="p-0002" num="0001">This application is a continuation-in-part of U.S. application Ser. No. 10/188,539, filed Jul. 3, 2002, which is herein incorporated by reference. This application claims the benefit of U.S. Provisional Application No. 60/397,512 filed on Jul. 19, 2002, which is herein incorporated by reference.</p>
<?RELAPP description="Other Patent Relations" end="tail"?>
<?BRFSUM description="Brief Summary" end="lead"?>
<heading id="h-0001" level="1">TECHNICAL FIELD</heading>
<p id="p-0003" num="0002">The invention relates generally to signal analysis devices and, more specifically, to a method and apparatus for improving the language skills of a user.</p>
<heading id="h-0002" level="1">BACKGROUND OF THE INVENTION</heading>
<p id="p-0004" num="0003">During the past few years, there has been significant interest in developing new computer based techniques in the area of language learning. An area of significant growth has been the use of multimedia (audio, image, and video) for language learning. These approaches have mainly focused on the language comprehension aspects. In these approaches, proficiency in pronunciation is achieved through practice and self-evaluation.</p>
<p id="p-0005" num="0004">Typical pronunciation scoring algorithms are based upon the phonetic segmentation of a user's speech that identifies the begin and end time of each phoneme as determined by an automatic speech recognition system.</p>
<p id="p-0006" num="0005">Unfortunately, present computer based techniques do not provide sufficiently accurate scoring of several parameters useful or necessary in determining student progress. Additionally, techniques that might provide more accurate results tend to be computationally expensive in terms of processing power and cost. Other existing scoring techniques require the construction of large non-native speakers databases such that non-native students are scored in a manner that compensates for accents.</p>
<p id="p-0007" num="0006">Additionally, present computer based techniques do not provide feedback in a manner that allows a user to improve his or her speech. Namely, it is often difficult for a student to identify specific speech problems, e.g., improper relative word duration and intonation, that must be addressed to improve his or her speech. More specifically, once specific speech problems are identified, it will be beneficial to provide feedback in a manner that allows a user to visualize a comparison between his pronunciation against a reference pronunciation.</p>
<heading id="h-0003" level="1">SUMMARY OF THE INVENTION</heading>
<p id="p-0008" num="0007">These and other deficiencies of the prior art are addressed by the present invention of a method and apparatus for pronunciation scoring that can provide meaningful feedback to identify and correct pronunciation problems quickly. Advantageously, the scoring techniques of the invention enable students to acquire new language skills faster by providing real-time feedback on pronunciation errors. Such a feedback helps the student focus on the key areas that need improvement, such as phoneme pronunciation, intonation, duration, overall speaking rate, and voicing.</p>
<p id="p-0009" num="0008">More specifically, in one embodiment of the present invention, the scores provided by the various scoring engines are adapted to provide a visual and/or numerical feedback that provides information pertaining to correctness or incorrectness in one or more speech-features such as intonation, articulation, voicing, phoneme error and relative word duration. In one embodiment, speech-features associated with a reference utterance and a user's attempt to repeat the same reference utterance are displayed graphically to the user. In essence, the user is provided with visual feedback. Such useful interactive feedback will allow a user to quickly identify the problem area and take remedial action in reciting “reference” sentences or phrases.</p>
<?BRFSUM description="Brief Summary" end="tail"?>
<?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?>
<description-of-drawings>
<heading id="h-0004" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading>
<p id="p-0010" num="0009">In the drawing:</p>
<p id="p-0011" num="0010"><figref idref="DRAWINGS">FIG. 1</figref> depicts a high-level block diagram of a system according to an embodiment of the invention;</p>
<p id="p-0012" num="0011"><figref idref="DRAWINGS">FIG. 2</figref> depicts a flow diagram of a pronunciation scoring method according to an embodiment of the invention;</p>
<p id="p-0013" num="0012"><figref idref="DRAWINGS">FIG. 3A</figref> depicts a flow diagram of a training method useful in deriving to a scoring table for an articulation scoring engine method;</p>
<p id="p-0014" num="0013"><figref idref="DRAWINGS">FIG. 3B</figref> depicts a flow diagram of an articulation scoring engine method suitable for use in the pronunciation scoring method of <figref idref="DRAWINGS">FIG. 2</figref>;</p>
<p id="p-0015" num="0014"><figref idref="DRAWINGS">FIG. 4</figref> depicts a flow diagram of a duration scoring engine method suitable for use in the pronunciation scoring method of <figref idref="DRAWINGS">FIG. 2</figref>;</p>
<p id="p-0016" num="0015"><figref idref="DRAWINGS">FIG. 5</figref> depicts a flow diagram of an intonation scoring engine method suitable for use in the pronunciation scoring method of <figref idref="DRAWINGS">FIG. 2</figref>;</p>
<p id="p-0017" num="0016"><figref idref="DRAWINGS">FIG. 6</figref> graphically depicts probability density functions (pdfs) associated with a particular phoneme;</p>
<p id="p-0018" num="0017"><figref idref="DRAWINGS">FIG. 7</figref> graphically depicts a pitch contour comparison that benefits from time normalization in accordance with an embodiment of the invention;</p>
<p id="p-0019" num="0018"><figref idref="DRAWINGS">FIG. 8</figref> graphically depicts a pitch contour comparison that benefits from constrained dynamic programming in accordance with an embodiment of the invention;</p>
<p id="p-0020" num="0019"><figref idref="DRAWINGS">FIGS. 9A-9C</figref> graphically depict respective pitch contours of different pronunciations of a common phrase;</p>
<p id="p-0021" num="0020"><figref idref="DRAWINGS">FIG. 10</figref> depicts a flow diagram of a method for providing feedback graphically to a user;</p>
<p id="p-0022" num="0021"><figref idref="DRAWINGS">FIG. 11</figref> depicts an evaluation screen of the present invention;</p>
<p id="p-0023" num="0022"><figref idref="DRAWINGS">FIG. 12</figref> depicts an alternative evaluation screen of the present invention;</p>
<p id="p-0024" num="0023"><figref idref="DRAWINGS">FIG. 13</figref> depicts an alternative evaluation screen of the present invention;</p>
<p id="p-0025" num="0024"><figref idref="DRAWINGS">FIG. 14</figref> depicts an alternative evaluation screen of the present invention;</p>
<p id="p-0026" num="0025"><figref idref="DRAWINGS">FIG. 15</figref> depicts a portable apparatus that deploys the scoring and graphical feedback display methods of the present invention;</p>
<p id="p-0027" num="0026"><figref idref="DRAWINGS">FIG. 16</figref> depicts a score evaluation screen of the present invention; and</p>
<p id="p-0028" num="0027"><figref idref="DRAWINGS">FIG. 17</figref> depicts a history evaluation screen of the present invention.</p>
</description-of-drawings>
<?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?>
<?DETDESC description="Detailed Description" end="lead"?>
<p id="p-0029" num="0028">To facilitate understanding, identical reference numerals have been used, where possible, to designate identical elements that are common to the figures.</p>
<heading id="h-0005" level="1">DETAILED DESCRIPTION OF THE INVENTION</heading>
<p id="p-0030" num="0029">The subject invention will be primarily described within the context of methods and apparatus for assisting a user learning a new language. However, it will be appreciated by those skilled in the art that the present invention is also applicable within the context of the elimination or reduction of an accent, the learning of an accent (e.g., by an actor or tourist), speech therapy and the like.</p>
<p id="p-0031" num="0030">The various scoring methods and algorithms described herein are primarily directed to the following three main aspects; namely, an articulation scoring aspect, a duration scoring aspect and an intonation and voicing scoring aspect. Each of the three aspects is associated with a respective scoring engine.</p>
<p id="p-0032" num="0031">The articulation score is tutor-independent and is adapted to detect mispronunciation of phonemes. The articulation score indicates how close the user's pronunciation is to a reference or native speaker pronunciation. The articulation score is relatively insensitive to normal variability in pronunciation from one utterance to another utterance for the same speaker, as well as for different speakers. The articulation score is computed at the phoneme level and aggregated to produce scores at the word level and the complete user phrase.</p>
<p id="p-0033" num="0032">The duration score provides feedback on the relative duration differences between the user and the reference speaker for different sounds or words in a phrase. The overall speaking rate in relation to the reference speaker also provides important information to a user.</p>
<p id="p-0034" num="0033">The intonation score computes perceptually relevant differences in the intonation of the user and the reference speaker. The intonation score is tutor-dependent and provides feedback at the word and phrase level. The voicing score is also computed in a manner similar to the intonation score. The voicing score is a measure of the differences in voicing level of periodic and unvoiced components in the user's speech and the reference speaker's speech. For instance, the fricatives such as /s/ and /f/ are mainly unvoiced, vowel sounds (/a/, /e/, etc.) are mainly voiced, and voiced fricatives such as /z/, have both voiced and unvoiced components. Someone with speech disabilities may have difficulty with reproducing correct voicing for different sounds, thereby, making communication with others more difficult.</p>
<p id="p-0035" num="0034"><figref idref="DRAWINGS">FIG. 1</figref> depicts a high-level block diagram of a system according to an embodiment of the invention. Specifically, the system <b>100</b> of <figref idref="DRAWINGS">FIG. 1</figref> comprises a reference speaker source <b>110</b>, a controller <b>120</b>, a user prompting device <b>130</b> and a user voice input device <b>140</b>. It is noted that the system <b>100</b> of <figref idref="DRAWINGS">FIG. 1</figref> may comprise hardware typically associated with a standard personal computer (PC) or other computing devices. It is noted that the various databases and scoring engines described below may be stored locally in a user's PC, or stored remotely at a server location accessible via, for example, the Internet or other computer networks.</p>
<p id="p-0036" num="0035">The reference speaker source <b>110</b> comprises a live or recorded source of reference audio information. The reference audio information is subsequently stored within a reference database <b>128</b>-<b>1</b> within (or accessible by) the controller <b>120</b>. The user-prompting device <b>130</b> comprises a device suitable for prompting a user to respond and, generally, perform tasks in accordance with the subject invention and related apparatus and methods. The user-prompting device <b>130</b> may comprise a display device having associated with it an audio presentation device (e.g., speakers). The user-prompting device is suitable for providing audio and, optionally, video or image feedback to a user. The user voice input device <b>140</b> comprises, illustratively, a microphone or other audio input devices that responsively couple audio or voice input to the controller <b>120</b>.</p>
<p id="p-0037" num="0036">The controller <b>120</b> of <figref idref="DRAWINGS">FIG. 1</figref> comprises a processor <b>124</b> as well as memory <b>128</b> for storing various control programs <b>128</b>-<b>3</b>. The processor <b>124</b> cooperates with conventional support circuitry <b>126</b> such as power supplies, clock circuits, cache memory and the like as well as circuits that assist in executing the software routines stored in the memory <b>128</b>. As such, it is contemplated that some of the process steps discussed herein as software processes may be implemented within hardware, for example as circuitry that cooperates with the processor <b>124</b> to perform various steps. The controller <b>120</b> also contains input/output (I/O) circuitry <b>122</b> that forms an interface between the various functional elements communicating with the controller <b>120</b>. For example, in the embodiment of <figref idref="DRAWINGS">FIG. 1</figref>, the controller <b>120</b> communicates with the reference speaker source <b>110</b>, user prompting device <b>130</b> and user voice input device <b>140</b>.</p>
<p id="p-0038" num="0037">Although the controller <b>120</b> of <figref idref="DRAWINGS">FIG. 1</figref> is depicted as a general-purpose computer that is programmed to perform various control functions in accordance with the present invention, the invention can be implemented in hardware as, for example, an application specific integrated circuit (ASIC). As such, the process steps described herein are intended to be broadly interpreted as being equivalently performed by software, hardware or a combination thereof.</p>
<p id="p-0039" num="0038">The memory <b>128</b> is used to store a reference database <b>128</b>-<b>1</b>, scoring engines <b>128</b>-<b>2</b>, control programs and other programs <b>128</b>-<b>3</b> and a user database <b>128</b>-<b>4</b>. The reference database <b>128</b>-<b>1</b> stores audio information received from, for example, the reference speaker source <b>110</b>. The audio information stored within the reference database <b>128</b>-<b>1</b> may also be supplied via alternate means such as a computer network (not shown) or storage device (not shown) cooperating with the controller <b>120</b>. The audio information stored within the reference database <b>128</b>-<b>1</b> may be provided to the user-prompting device <b>130</b>, which responsively presents the stored audio information to a user.</p>
<p id="p-0040" num="0039">The scoring engines <b>128</b>-<b>2</b> comprise a plurality of scoring engines or algorithms suitable for use in the present invention. Briefly, the scoring engines <b>128</b>-<b>2</b> include one or more of an articulation-scoring engine, a duration scoring engine and an intonation and voicing-scoring engine. Each of these scoring engines is used to process voice or audio information provided via, for example, the user voice input device <b>140</b>. Each of these scoring engines is used to correlate the audio information provided by the user to the audio information provided by a reference source to determine thereby a score indicative of such correlation. The scoring engines will be discussed in more detail below with respect to <figref idref="DRAWINGS">FIGS. 3-5</figref>.</p>
<p id="p-0041" num="0040">The programs <b>128</b>-<b>3</b> stored within the memory <b>128</b> comprise various programs used to implement the functions described herein pertaining to the present invention. Such programs include those programs useful in receiving data from the reference speaker source <b>110</b> (and optionally encoding that data prior to storage), those programs useful in providing stored audio data to the user-prompting device <b>130</b>, those programs useful in receiving and encoding voice information received via the user voice input device <b>140</b>, those programs useful in applying input data to the scoring engines, operating the scoring engines and deriving results from the scoring engines. The user database <b>128</b>-<b>4</b> is useful in storing scores associated with a user, as well as voice samples provided by the user such that a historical record may be generated to show user progress in achieving a desired language skill level.</p>
<p id="p-0042" num="0041"><figref idref="DRAWINGS">FIG. 2</figref> depicts a flow diagram of a pronunciation scoring method according to an embodiment of the invention. Specifically, the method <b>200</b> of <figref idref="DRAWINGS">FIG. 2</figref> is entered at step <b>205</b> when a phrase or word pronounced by a reference speaker is presented to a user. That is, at step <b>205</b> a phrase or word stored within the reference database <b>128</b>-<b>1</b> is presented to a user via the user-prompting device <b>130</b> or other presentation device.</p>
<p id="p-0043" num="0042">At step <b>210</b>, the user is prompted to pronounce the word or phrase previously presented either in text form or in text and voice. At step <b>215</b>, the word or phrase spoken by the user in response to the prompt is recorded and, if necessary, encoded in a manner compatible with the user database <b>128</b>-<b>4</b> and scoring engines <b>128</b>-<b>2</b>. For example, the recorded user pronunciation of the word or phrase may be stored as a digitized voice stream or signal or as an encoded digitized voice stream or signal.</p>
<p id="p-0044" num="0043">At step <b>220</b>, the stored encoded or unencoded voice stream or signal is processed using an articulation-scoring engine. At step <b>225</b>, the stored encoded or unencoded voice stream or signal is processed using a duration scoring engine. At step <b>230</b>, the stored encoded or unencoded voice stream or signal is processed using an intonation and voicing scoring engine. It will be appreciated by those skilled in the art that the articulation, duration and intonation/voicing scoring engines may be used individually or in any combination to achieve a respective score. Moreover, the user's voice may be processed in real-time (i.e., without storing in the user database), after storing in an unencoded fashion, after encoding, or in any combination thereof.</p>
<p id="p-0045" num="0044">At step <b>235</b>, feedback is provided to the user based on one or more of the articulation, duration and/or intonation and voicing engine scores. At step <b>240</b>, a new phrase or word is selected, and steps <b>205</b>-<b>235</b> are repeated. After a predefined period of time, iterations through the loop or achieved level of scoring for one or more of the scoring engines, the method <b>200</b> is exited.</p>
<p id="h-0006" num="0000">Articulation Scoring Algorithm</p>
<p id="p-0046" num="0045">The articulation score is tutor-independent and is adapted to detect phoneme level and word-level mispronunciations. The articulation score indicates how close the user's pronunciation is to a reference or native speaker pronunciation. The articulation score is relatively insensitive to normal variability in pronunciation from one utterance to another utterance for the same speaker, as well as for different speakers.</p>
<p id="p-0047" num="0046">The articulation-storing algorithm computes an articulation score based upon speech templates that are derived from a speech database of native speakers only. A method to obtain speech templates is known in the art. In this approach, after obtaining segmentations by Viterbi decoding, an observation vector assigned to a particular phoneme is applied on a garbage model g (trained using, e.g., all the phonemes of the speech data combined). Thus, for each phoneme q<sub>i</sub>, two scores are obtained; one is the log-likelihood score l<sub>q </sub>for q<sub>i</sub>, the other is the log-likelihood score l<sub>g </sub>for garbage model g. The garbage model, also referred to as the general speech model, is a single model derived from all the speech data. By examining the difference between l<sub>q </sub>and l<sub>g</sub>, a score for the current phoneme is determined. A score table indexed by the log-likelihood difference is discussed below with respect to Table 1.</p>
<p id="p-0048" num="0047"><figref idref="DRAWINGS">FIG. 3</figref> depicts a flow diagram of an articulation scoring engine method suitable for use as, for example, step <b>220</b> in the method <b>200</b> of <figref idref="DRAWINGS">FIG. 2</figref>. <figref idref="DRAWINGS">FIG. 3A</figref> depicts a flow diagram of a training method useful in deriving a scoring table for an articulation scoring engine, while <figref idref="DRAWINGS">FIG. 3B</figref> depicts a flow diagram of an articulation scoring engine method.</p>
<p id="p-0049" num="0048">The method <b>300</b>A of <figref idref="DRAWINGS">FIG. 3A</figref> generates a score table indexed by the log-likelihood difference between l<sub>q </sub>and l<sub>g</sub>.</p>
<p id="p-0050" num="0049">At step <b>305</b>, a training database is determined. The training database is derived from a speech database of native speakers only (i.e., American English speakers in the case of American reference speakers).</p>
<p id="p-0051" num="0050">At step <b>310</b>, for each utterance, an “in-grammar” and “out-grammar” is constructed, where in-grammar is defined as conforming to the target phrase (i.e., the same as the speech) and where out-grammar is a some other randomly selected phrase in the training database randomly selected (i.e., non-conforming to the target phrase).</p>
<p id="p-0052" num="0051">At step <b>315</b>, l<sub>q </sub>and l<sub>g </sub>are calculated on the in-grammar and out-grammar for each utterance over the whole database. The in-grammar log-likelihood score for a phoneme q and a garbage model g are denoted as l<sub>q</sub><sup>i </sup>and l<sub>g</sub><sup>i</sup>, respectively. The out-grammar log-likelihood score for q and g are denoted as l<sub>q</sub><sup>o </sup>and l<sub>g</sub><sup>o</sup>, respectively.</p>
<p id="p-0053" num="0052">At step <b>320</b>, the l<sub>q </sub>and l<sub>g </sub>difference (d<sup>i</sup>) is calculated. That is, collect the score l<sub>q </sub>and l<sub>g </sub>for individual phonemes and compute the difference. The difference between l<sub>q </sub>and l<sub>g </sub>is d<sup>i</sup>=l<sub>q</sub><sup>i</sup>−l<sub>g</sub><sup>i</sup>, for in-grammar and d<sup>o</sup>=l<sub>q</sub><sup>o</sup>−l<sub>g</sub><sup>o </sup>for out-grammar log-likelihood scores. It is noted that there may be some phonemes that have the same position in the in-grammar and out-of-grammar phrases. These phonemes are removed from consideration by examining the amount of overlap, in time, of the phonemes in the in-grammar and out-of-grammar utterance.</p>
<p id="p-0054" num="0053">At step <b>325</b>, the probability density value for d<sup>i </sup>and d<sup>o </sup>is calculated. A Gaussian probability density function (pdf) is used to approximate the real pdf, then the two pdfs (in-grammar and out-grammar) can be expressed as f<sup>i</sup>=N(μ<sup>i</sup>, σ<sup>i</sup>) and f<sup>o</sup>=N(μ<sup>o</sup>, σ<sup>o</sup>), respectively.</p>
<p id="p-0055" num="0054"><figref idref="DRAWINGS">FIG. 6</figref> graphically depicts probability density functions (pdfs) associated with a particular phoneme as a function of the difference between l<sub>q </sub>and l<sub>g</sub>. Specifically, <figref idref="DRAWINGS">FIG. 6</figref> shows the in-grammar and out-grammar pdfs for a phoneme /C/. It is noted that the Gaussian pdf successfully approximates the actual pdf.</p>
<p id="p-0056" num="0055">At step <b>330</b>, a score table is constructed, such as depicted below as Table 1. The entry of the table is the difference d, the output is the score normalized in the range [0, 100]. The log-likelihood difference between the two pdfs f<sup>i </sup>and f<sup>o </sup>is defined as h(x)=log f<sup>i</sup>(x)−log f<sup>o</sup>(x).</p>
<p id="p-0057" num="0056">For example, assume the score at μ<sup>i </sup>as 100, such that at this point the log-likelihood difference between f<sup>i </sup>and f<sup>o </sup>is h(μ<sup>i</sup>). Also assume the score at μ<sup>o </sup>as 0, such that at this point the log-likelihood difference is h(μ<sup>o</sup>). Defining the two pdfs' cross point as C, the difference of two pdfs at this point is h(x=C)=0. From value μ<sup>i </sup>to C, an acoustic scoring table is provided as:</p>
<p id="p-0058" num="0057">
<tables id="TABLE-US-00001" num="00001">
<table frame="none" colsep="0" rowsep="0">
<tgroup align="left" colsep="0" rowsep="0" cols="1">
<colspec colname="1" colwidth="217pt" align="center"/>
<thead>
<row>
<entry namest="1" nameend="1" rowsep="1">TABLE 1</entry>
</row>
</thead>
<tbody valign="top">
<row>
<entry namest="1" nameend="1" align="center" rowsep="1"/>
</row>
<row>
<entry>The score table for acoustic scoring.</entry>
</row>
</tbody>
</tgroup>
<tgroup align="left" colsep="0" rowsep="0" cols="3">
<colspec colname="offset" colwidth="35pt" align="left"/>
<colspec colname="1" colwidth="84pt" align="left"/>
<colspec colname="2" colwidth="98pt" align="center"/>
<tbody valign="top">
<row>
<entry/>
<entry>D</entry>
<entry>score</entry>
</row>
<row>
<entry/>
<entry namest="offset" nameend="2" align="center" rowsep="1"/>
</row>
</tbody>
</tgroup>
<tgroup align="left" colsep="0" rowsep="0" cols="3">
<colspec colname="offset" colwidth="35pt" align="left"/>
<colspec colname="1" colwidth="84pt" align="left"/>
<colspec colname="2" colwidth="98pt" align="char" char="."/>
<tbody valign="top">
<row>
<entry/>
<entry>μ<sup>i</sup></entry>
<entry>100</entry>
</row>
<row>
<entry/>
<entry> </entry>
</row>
<row>
<entry/>
<entry>
<maths id="MATH-US-00001" num="00001">
<math overflow="scroll">
<mrow>
  <mi>x</mi>
  <mo>,</mo>
  <mrow>
    <mrow>
      <mi>sub</mi>
      <mo>⁢</mo>
      <mstyle>
        <mspace width="0.8em" height="0.8ex"/>
      </mstyle>
      <mo>⁢</mo>
      <mrow>
        <mi>h</mi>
        <mo>⁡</mo>
        <mrow>
          <mo>(</mo>
          <mi>x</mi>
          <mo>)</mo>
        </mrow>
      </mrow>
    </mrow>
    <mo>=</mo>
    <mrow>
      <mfrac>
        <mrow>
          <mi>log</mi>
          <mo>⁢</mo>
          <mfrac>
            <mn>90</mn>
            <mn>10</mn>
          </mfrac>
        </mrow>
        <mi>log10</mi>
      </mfrac>
      <mo>⁢</mo>
      <mrow>
        <mi>h</mi>
        <mo>⁡</mo>
        <mrow>
          <mo>(</mo>
          <msup>
            <mi>μ</mi>
            <mi>i</mi>
          </msup>
          <mo>)</mo>
        </mrow>
      </mrow>
    </mrow>
  </mrow>
</mrow>
</math>
</maths>
</entry>
<entry>90</entry>
</row>
<row>
<entry/>
<entry> </entry>
</row>
<row>
<entry/>
<entry>
<maths id="MATH-US-00002" num="00002">
<math overflow="scroll">
<mrow>
  <mi>x</mi>
  <mo>,</mo>
  <mrow>
    <mrow>
      <mi>sub</mi>
      <mo>⁢</mo>
      <mstyle>
        <mspace width="0.8em" height="0.8ex"/>
      </mstyle>
      <mo>⁢</mo>
      <mrow>
        <mi>h</mi>
        <mo>⁡</mo>
        <mrow>
          <mo>(</mo>
          <mi>x</mi>
          <mo>)</mo>
        </mrow>
      </mrow>
    </mrow>
    <mo>=</mo>
    <mrow>
      <mfrac>
        <mrow>
          <mi>log</mi>
          <mo>⁢</mo>
          <mfrac>
            <mn>80</mn>
            <mn>20</mn>
          </mfrac>
        </mrow>
        <mi>log10</mi>
      </mfrac>
      <mo>⁢</mo>
      <mrow>
        <mi>h</mi>
        <mo>⁡</mo>
        <mrow>
          <mo>(</mo>
          <msup>
            <mi>μ</mi>
            <mi>i</mi>
          </msup>
          <mo>)</mo>
        </mrow>
      </mrow>
    </mrow>
  </mrow>
</mrow>
</math>
</maths>
</entry>
<entry>80</entry>
</row>
<row>
<entry/>
<entry> </entry>
</row>
<row>
<entry/>
<entry>
<maths id="MATH-US-00003" num="00003">
<math overflow="scroll">
<mrow>
  <mi>x</mi>
  <mo>,</mo>
  <mrow>
    <mrow>
      <mi>sub</mi>
      <mo>⁢</mo>
      <mstyle>
        <mspace width="0.8em" height="0.8ex"/>
      </mstyle>
      <mo>⁢</mo>
      <mrow>
        <mi>h</mi>
        <mo>⁡</mo>
        <mrow>
          <mo>(</mo>
          <mi>x</mi>
          <mo>)</mo>
        </mrow>
      </mrow>
    </mrow>
    <mo>=</mo>
    <mrow>
      <mfrac>
        <mrow>
          <mi>log</mi>
          <mo>⁢</mo>
          <mfrac>
            <mn>70</mn>
            <mn>30</mn>
          </mfrac>
        </mrow>
        <mi>log10</mi>
      </mfrac>
      <mo>⁢</mo>
      <mrow>
        <mi>h</mi>
        <mo>⁡</mo>
        <mrow>
          <mo>(</mo>
          <msup>
            <mi>μ</mi>
            <mi>i</mi>
          </msup>
          <mo>)</mo>
        </mrow>
      </mrow>
    </mrow>
  </mrow>
</mrow>
</math>
</maths>
</entry>
<entry>70</entry>
</row>
<row>
<entry/>
<entry> </entry>
</row>
<row>
<entry/>
<entry>
<maths id="MATH-US-00004" num="00004">
<math overflow="scroll">
<mrow>
  <mi>x</mi>
  <mo>,</mo>
  <mrow>
    <mrow>
      <mi>sub</mi>
      <mo>⁢</mo>
      <mstyle>
        <mspace width="0.8em" height="0.8ex"/>
      </mstyle>
      <mo>⁢</mo>
      <mrow>
        <mi>h</mi>
        <mo>⁡</mo>
        <mrow>
          <mo>(</mo>
          <mi>x</mi>
          <mo>)</mo>
        </mrow>
      </mrow>
    </mrow>
    <mo>=</mo>
    <mrow>
      <mfrac>
        <mrow>
          <mi>log</mi>
          <mo>⁢</mo>
          <mfrac>
            <mn>60</mn>
            <mn>40</mn>
          </mfrac>
        </mrow>
        <mi>log10</mi>
      </mfrac>
      <mo>⁢</mo>
      <mrow>
        <mi>h</mi>
        <mo>⁡</mo>
        <mrow>
          <mo>(</mo>
          <msup>
            <mi>μ</mi>
            <mi>i</mi>
          </msup>
          <mo>)</mo>
        </mrow>
      </mrow>
    </mrow>
  </mrow>
</mrow>
</math>
</maths>
</entry>
<entry>60</entry>
</row>
<row>
<entry/>
<entry> </entry>
</row>
<row>
<entry/>
<entry>x, sub h(x) = 0</entry>
<entry>50</entry>
</row>
<row>
<entry/>
<entry> </entry>
</row>
<row>
<entry/>
<entry>
<maths id="MATH-US-00005" num="00005">
<math overflow="scroll">
<mrow>
  <mi>x</mi>
  <mo>,</mo>
  <mrow>
    <mrow>
      <mi>sub</mi>
      <mo>⁢</mo>
      <mstyle>
        <mspace width="0.8em" height="0.8ex"/>
      </mstyle>
      <mo>⁢</mo>
      <mrow>
        <mi>h</mi>
        <mo>⁡</mo>
        <mrow>
          <mo>(</mo>
          <mi>x</mi>
          <mo>)</mo>
        </mrow>
      </mrow>
    </mrow>
    <mo>=</mo>
    <mrow>
      <mfrac>
        <mrow>
          <mi>log</mi>
          <mo>⁢</mo>
          <mfrac>
            <mn>60</mn>
            <mn>40</mn>
          </mfrac>
        </mrow>
        <mi>log10</mi>
      </mfrac>
      <mo>⁢</mo>
      <mrow>
        <mi>h</mi>
        <mo>⁡</mo>
        <mrow>
          <mo>(</mo>
          <msup>
            <mi>μ</mi>
            <mi>o</mi>
          </msup>
          <mo>)</mo>
        </mrow>
      </mrow>
    </mrow>
  </mrow>
</mrow>
</math>
</maths>
</entry>
<entry>40</entry>
</row>
<row>
<entry/>
<entry> </entry>
</row>
<row>
<entry/>
<entry>
<maths id="MATH-US-00006" num="00006">
<math overflow="scroll">
<mrow>
  <mi>x</mi>
  <mo>,</mo>
  <mrow>
    <mrow>
      <mi>sub</mi>
      <mo>⁢</mo>
      <mstyle>
        <mspace width="0.8em" height="0.8ex"/>
      </mstyle>
      <mo>⁢</mo>
      <mrow>
        <mi>h</mi>
        <mo>⁡</mo>
        <mrow>
          <mo>(</mo>
          <mi>x</mi>
          <mo>)</mo>
        </mrow>
      </mrow>
    </mrow>
    <mo>=</mo>
    <mrow>
      <mfrac>
        <mrow>
          <mi>log</mi>
          <mo>⁢</mo>
          <mfrac>
            <mn>70</mn>
            <mn>30</mn>
          </mfrac>
        </mrow>
        <mi>log10</mi>
      </mfrac>
      <mo>⁢</mo>
      <mrow>
        <mi>h</mi>
        <mo>⁡</mo>
        <mrow>
          <mo>(</mo>
          <msup>
            <mi>μ</mi>
            <mi>o</mi>
          </msup>
          <mo>)</mo>
        </mrow>
      </mrow>
    </mrow>
  </mrow>
</mrow>
</math>
</maths>
</entry>
<entry>30</entry>
</row>
<row>
<entry/>
<entry> </entry>
</row>
<row>
<entry/>
<entry>
<maths id="MATH-US-00007" num="00007">
<math overflow="scroll">
<mrow>
  <mi>x</mi>
  <mo>,</mo>
  <mrow>
    <mrow>
      <mi>sub</mi>
      <mo>⁢</mo>
      <mstyle>
        <mspace width="0.8em" height="0.8ex"/>
      </mstyle>
      <mo>⁢</mo>
      <mrow>
        <mi>h</mi>
        <mo>⁡</mo>
        <mrow>
          <mo>(</mo>
          <mi>x</mi>
          <mo>)</mo>
        </mrow>
      </mrow>
    </mrow>
    <mo>=</mo>
    <mrow>
      <mfrac>
        <mrow>
          <mi>log</mi>
          <mo>⁢</mo>
          <mfrac>
            <mn>80</mn>
            <mn>20</mn>
          </mfrac>
        </mrow>
        <mi>log10</mi>
      </mfrac>
      <mo>⁢</mo>
      <mrow>
        <mi>h</mi>
        <mo>⁡</mo>
        <mrow>
          <mo>(</mo>
          <msup>
            <mi>μ</mi>
            <mi>o</mi>
          </msup>
          <mo>)</mo>
        </mrow>
      </mrow>
    </mrow>
  </mrow>
</mrow>
</math>
</maths>
</entry>
<entry>20</entry>
</row>
<row>
<entry/>
<entry> </entry>
</row>
<row>
<entry/>
<entry>
<maths id="MATH-US-00008" num="00008">
<math overflow="scroll">
<mrow>
  <mi>x</mi>
  <mo>,</mo>
  <mrow>
    <mrow>
      <mi>sub</mi>
      <mo>⁢</mo>
      <mstyle>
        <mspace width="0.8em" height="0.8ex"/>
      </mstyle>
      <mo>⁢</mo>
      <mrow>
        <mi>h</mi>
        <mo>⁡</mo>
        <mrow>
          <mo>(</mo>
          <mi>x</mi>
          <mo>)</mo>
        </mrow>
      </mrow>
    </mrow>
    <mo>=</mo>
    <mrow>
      <mfrac>
        <mrow>
          <mi>log</mi>
          <mo>⁢</mo>
          <mfrac>
            <mn>90</mn>
            <mn>10</mn>
          </mfrac>
        </mrow>
        <mi>log10</mi>
      </mfrac>
      <mo>⁢</mo>
      <mrow>
        <mi>h</mi>
        <mo>⁡</mo>
        <mrow>
          <mo>(</mo>
          <msup>
            <mi>μ</mi>
            <mi>o</mi>
          </msup>
          <mo>)</mo>
        </mrow>
      </mrow>
    </mrow>
  </mrow>
</mrow>
</math>
</maths>
</entry>
<entry>10</entry>
</row>
<row>
<entry/>
<entry> </entry>
</row>
<row>
<entry/>
<entry>μ<sup>o</sup></entry>
<entry>0</entry>
</row>
<row>
<entry/>
<entry namest="offset" nameend="2" align="center" rowsep="1"/>
</row>
</tbody>
</tgroup>
</table>
</tables>
</p>
<p id="p-0059" num="0058"><figref idref="DRAWINGS">FIG. 3B</figref> depicts a flow diagram of an articulation scoring engine method suitable for use in the pronunciation scoring method of <figref idref="DRAWINGS">FIG. 2</figref>. Specifically, the method <b>300</b>B of <figref idref="DRAWINGS">FIG. 3</figref> is entered at step <b>350</b>, where a forced alignment to obtain segmentation for the user utterance is performed. At step <b>355</b>, the l<sub>q </sub>and l<sub>g </sub>difference (d<sup>i</sup>) is calculated for each segment. At step <b>360</b>, a scoring table (e.g., such as constructed using the method <b>300</b>A of <figref idref="DRAWINGS">FIG. 3A</figref>) is used as a lookup table to obtain an articulation score for each segment.</p>
<p id="p-0060" num="0059">For the example of phoneme /C/, whose pdfs are shown in <figref idref="DRAWINGS">FIG. 6</figref>, the score table is constructed as shown in Table 2, as follows:</p>
<p id="p-0061" num="0060">
<tables id="TABLE-US-00002" num="00002">
<table frame="none" colsep="0" rowsep="0">
<tgroup align="left" colsep="0" rowsep="0" cols="1">
<colspec colname="1" colwidth="217pt" align="center"/>
<thead>
<row>
<entry namest="1" nameend="1" rowsep="1">TABLE 2</entry>
</row>
</thead>
<tbody valign="top">
<row>
<entry namest="1" nameend="1" align="center" rowsep="1"/>
</row>
<row>
<entry>The score table for phoneme C</entry>
</row>
</tbody>
</tgroup>
<tgroup align="left" colsep="0" rowsep="0" cols="3">
<colspec colname="offset" colwidth="56pt" align="left"/>
<colspec colname="1" colwidth="21pt" align="center"/>
<colspec colname="2" colwidth="140pt" align="center"/>
<tbody valign="top">
<row>
<entry/>
<entry>d</entry>
<entry>score</entry>
</row>
<row>
<entry/>
<entry namest="offset" nameend="2" align="center" rowsep="1"/>
</row>
</tbody>
</tgroup>
<tgroup align="left" colsep="0" rowsep="0" cols="3">
<colspec colname="offset" colwidth="56pt" align="left"/>
<colspec colname="1" colwidth="21pt" align="char" char="."/>
<colspec colname="2" colwidth="140pt" align="char" char="."/>
<tbody valign="top">
<row>
<entry/>
<entry>−6.10</entry>
<entry>100</entry>
</row>
<row>
<entry/>
<entry>−5.63</entry>
<entry>90</entry>
</row>
<row>
<entry/>
<entry>−4.00</entry>
<entry>80</entry>
</row>
<row>
<entry/>
<entry>−3.32</entry>
<entry>70</entry>
</row>
<row>
<entry/>
<entry>−2.86</entry>
<entry>60</entry>
</row>
<row>
<entry/>
<entry>−2.47</entry>
<entry>50</entry>
</row>
<row>
<entry/>
<entry>0.43</entry>
<entry>40</entry>
</row>
<row>
<entry/>
<entry>2.61</entry>
<entry>30</entry>
</row>
<row>
<entry/>
<entry>4.72</entry>
<entry>20</entry>
</row>
<row>
<entry/>
<entry>7.32</entry>
<entry>10</entry>
</row>
<row>
<entry/>
<entry>7.62</entry>
<entry>0</entry>
</row>
<row>
<entry/>
<entry namest="offset" nameend="2" align="center" rowsep="1"/>
</row>
</tbody>
</tgroup>
</table>
</tables>
</p>
<p id="p-0062" num="0061">Thus, illustratively, if the grammar is . . . C . . . , the forced Viterbi decoding gives the log-likelihood difference between the in-grammar and out-grammar log-likelihood score for phoneme C as −3.00, by searching the table, we find the d lies in [−2.86, −3.32], therefore the acoustic score for this phoneme is 60.</p>
<p id="p-0063" num="0062">Note that in the above example, anti-phone models that are individually constructed for each phoneme model could also replace the garbage model. The anti-phone model for a particular target phoneme may be constructed from those training speech data segments that correspond to phonemes that are most likely to be confused with the target phoneme or using methods known in the art. It should be noted that other techniques for decoding the speech utterance to obtain segmentation may be employed by a person skilled in the art.</p>
<p id="h-0007" num="0000">Duration Scoring Algorithm</p>
<p id="p-0064" num="0063">The duration score provides feedback on the relative duration differences between various sounds and words in the user and the reference speaker's utterance. The overall speaking rate in relation to the reference speaker also provides important information to a user.</p>
<p id="p-0065" num="0064">The phoneme-level segmentation information of user's speech L and tutor's speech T from a Viterbi decoder may be denoted as L=(L<sub>1</sub>,L<sub>2</sub>, . . . ,L<sub>N</sub>) and T=(T<sub>1</sub>,T<sub>2</sub>, . . . ,T<sub>N</sub>); where N is the total number of phonemes in the sentences, L<sub>i </sub>and T<sub>i </sub>are the user's and tutor's durations corresponding to the phoneme q<sub>i</sub>.</p>
<p id="p-0066" num="0065"><figref idref="DRAWINGS">FIG. 4</figref> depicts a flow diagram of a duration scoring engine method suitable for use as, for example, step <b>225</b> in the method <b>200</b> of <figref idref="DRAWINGS">FIG. 2</figref>. The method <b>400</b> of <figref idref="DRAWINGS">FIG. 4</figref> determines the difference in the relative duration of different phonemes between the user and the reference speech, thereby enabling a determination as to whether the user has unusually elongated certain sounds in the utterance in relation to other sounds.</p>
<p id="p-0067" num="0066">At step <b>405</b>, the duration series is normalized using the following equation:</p>
<p id="p-0068" num="0067">
<maths id="MATH-US-00009" num="00009">
<math overflow="scroll">
<mtable>
  <mtr>
    <mtd>
      <mrow>
        <msub>
          <mover>
            <mi>L</mi>
            <mo>^</mo>
          </mover>
          <mi>i</mi>
        </msub>
        <mo>=</mo>
        <mfrac>
          <msub>
            <mi>L</mi>
            <mi>i</mi>
          </msub>
          <mrow>
            <munderover>
              <mo>∑</mo>
              <mrow>
                <mi>i</mi>
                <mo>=</mo>
                <mn>1</mn>
              </mrow>
              <mi>N</mi>
            </munderover>
            <mo>⁢</mo>
            <msub>
              <mi>L</mi>
              <mi>i</mi>
            </msub>
          </mrow>
        </mfrac>
      </mrow>
    </mtd>
  </mtr>
  <mtr>
    <mtd>
      <mrow>
        <msub>
          <mover>
            <mi>T</mi>
            <mo>^</mo>
          </mover>
          <mi>i</mi>
        </msub>
        <mo>=</mo>
        <mfrac>
          <msub>
            <mi>T</mi>
            <mi>i</mi>
          </msub>
          <mrow>
            <munderover>
              <mo>∑</mo>
              <mrow>
                <mi>i</mi>
                <mo>=</mo>
                <mn>1</mn>
              </mrow>
              <mi>N</mi>
            </munderover>
            <mo>⁢</mo>
            <msub>
              <mi>T</mi>
              <mi>i</mi>
            </msub>
          </mrow>
        </mfrac>
      </mrow>
    </mtd>
  </mtr>
</mtable>
</math>
</maths>
</p>
<p id="p-0069" num="0068">At step <b>410</b>, the overall duration score is calculated based on the normalized duration values, as follows:</p>
<p id="p-0070" num="0069">
<maths id="MATH-US-00010" num="00010">
<math overflow="scroll">
<mrow>
  <mi>D</mi>
  <mo>=</mo>
  <mrow>
    <mi>max</mi>
    <mo>⁢</mo>
    <mrow>
      <mo>{</mo>
      <mrow>
        <mrow>
          <mn>0</mn>
          <mo>,</mo>
          <mn>1</mn>
        </mrow>
        <mo>-</mo>
        <mrow>
          <munderover>
            <mo>∑</mo>
            <mrow>
              <mi>i</mi>
              <mo>=</mo>
              <mn>1</mn>
            </mrow>
            <mi>N</mi>
          </munderover>
          <mo>⁢</mo>
          <mrow>
            <mo></mo>
            <mrow>
              <msub>
                <mi>L</mi>
                <mi>i</mi>
              </msub>
              <mo>-</mo>
              <msub>
                <mi>T</mi>
                <mi>i</mi>
              </msub>
            </mrow>
            <mo></mo>
          </mrow>
        </mrow>
      </mrow>
      <mo>}</mo>
    </mrow>
  </mrow>
</mrow>
</math>
</maths>
<br/>
Intonation Scoring Algorithm
</p>
<p id="p-0071" num="0070">The intonation (and voicing) score computes perceptually relevant differences in the intonation of the user and the reference speaker. The intonation score is tutor-dependent and provides feedback at the word and phrase level. The intonation scoring method operates to compare pitch contours of reference and user speech to derive therefrom a score. The intonation score reflects stress at syllable level, word level and sentence level, intonation pattern for each utterance, and rhythm.</p>
<p id="p-0072" num="0071">The smoothed pitch contours are then compared according to some perceptually relevant distance measures. Note that the details of the pitch-tracking algorithm are not important for this discussion. However, briefly, the pitch-tracking algorithm is a time domain algorithm that uses autocorrelation analysis. It first computes coarse pitch estimate in the decimated LPC residual domain. The final pitch estimate is obtained by refining the coarse estimate on the original speech signal. The pitch detection algorithm also produces an estimate of the voicing in the signal.</p>
<p id="p-0073" num="0072">The algorithm is applied on both the tutor's speech and the user's speech, to obtain two pitch series P(T) and P(L), respectively.</p>
<p id="p-0074" num="0073">The voicing score is computed in a manner similar to the intonation score. The voicing score is a measure of the differences in level of periodic and unvoiced components in the user's speech and the reference speaker's speech. For instance, the fricatives such as /s/ and /f/ are mainly unvoiced, vowel sounds (/a/, /e/, etc.) are mainly voiced, and voiced fricatives such as /z/, have both voiced and unvoiced components. Someone with speech disabilities may have difficulty with reproducing correct voicing for different sounds, thereby, making communication with others more difficult.</p>
<p id="p-0075" num="0074"><figref idref="DRAWINGS">FIG. 5</figref> depicts a flow diagram of an intonation scoring engine method suitable for use as, for example, step <b>230</b> in the method <b>200</b> of <figref idref="DRAWINGS">FIG. 2</figref>.</p>
<p id="p-0076" num="0075">At step <b>505</b>, the word-level segmentations of the user and reference phrases are obtained by, for example, a forced alignment technique. Word segmentation is used because the inventors consider pitch relatively meaningless in terms of phonemes.</p>
<p id="p-0077" num="0076">At step <b>510</b>, a pitch contour is mapped on a word-by-word basis using normalized pitch values. That is, the pitch contours of the user's and the tutor's speech are determined.</p>
<p id="p-0078" num="0077">At step <b>520</b>, constrained dynamic programming is applied as appropriate. Since the length of the tutor's pitch series is normally different from the user's pitch series, it may be necessary to normalize them. Even if the lengths of tutor's pitch series and user's are the same, there is likely to be a need for time normalizations.</p>
<p id="p-0079" num="0078">At step <b>525</b>, the pitch contours are compared to derive therefrom an intonation score.</p>
<p id="p-0080" num="0079">For example, assume that the pitch series for a speech utterance is P=(P<sub>1</sub>,P<sub>2</sub>, . . . ,P<sub>M</sub>) where M is the length of pitch series, P<sub>i </sub>is the pitch period corresponding to frame i. In the exemplary embodiments, a frame is a block of speech (typically 20-30 ms) for which a pitch value is computed. After mapping onto the word, the pitch series is obtained on a word-by-word basis, as follows: P=(P<sup>1</sup>,P<sup>2</sup>, . . . ,P<sup>N</sup>); and P<sup>i</sup>=(P<sub>1</sub><sup>i</sup>,P<sub>2</sub><sup>i</sup>, . . . ,P<sub>M</sub><sub><sub2>i</sub2></sub><sup>i</sup>), 1≦i≦N; where P<sup>i </sup>is the pitch series corresponding to i<sup>th </sup>word, M<sub>i </sub>is the length of pitch series of i<sup>th </sup>word, N is the number of words within the sentence.</p>
<p id="p-0081" num="0080">Optionally, a pitch-racking algorithm is applied on both the tutor and learner's speech to obtain two pitch series as follows: P<sub>T</sub>=(P<sub>T</sub><sup>1</sup>, P<sub>T</sub><sup>2</sup>, . . . ,P<sub>T</sub><sup>N</sup>) and P<sub>L</sub>=(P<sub>L</sub><sup>1</sup>,P<sub>L</sub><sup>2</sup>,P<sub>L</sub><sup>N</sup>). It should be noted that even for the same i<sup>th </sup>word, the tutor and the learner's duration may be different such that the length of P<sub>T</sub><sup>i </sup>is not necessarily equal to the length of P<sub>L</sub><sup>i</sup>. The most perceptually relevant part within the intonation is the word based pitch movements. Therefore, the word-level intonation score is determined first. That is, given the i<sup>th </sup>word level pitch series P<sub>T</sub><sup>i </sup>and P<sub>L</sub><sup>i</sup>, the “distance” between them is measured.</p>
<p id="p-0082" num="0081">For example, assume that two pitch series for a word are denoted as follows: P<sub>T</sub>=(T<sub>1</sub>,T<sub>2</sub>, . . . ,T<sub>D</sub>) and P<sub>L</sub>=(L<sub>1</sub>,L<sub>2</sub>, . . . ,L<sub>E</sub>), where D, E are the length of pitch series for the particular word. Since P<sub>T </sub>and P<sub>L </sub>are the pitch values corresponding to a single word, there may exist two cases; namely, (1) The pitch contours are continuous within a word such that no intra-word gap exists (i.e., no gap in the pitch contour of a word); and (2) There may be some parts of speech without pitch values within a word, thus leading gaps within pitch contour (i.e., the parts may be unvoiced phonemes like unvoiced fricatives and unvoiced stop consonants, or the parts may be voiced phonemes with little energy and/or low signal-to-Noise ratio which cannot be detected by pitch tracking algorithm).</p>
<p id="p-0083" num="0082">For the second case, the method operates to remove the gaps within pitch contour and thereby make the pitch contour appear to be continuous. It is noted that such operation may produce some discontinuity at the points where the pitch contours are bridged. As such, the smoothing algorithm is preferably applied in such cases to remove these discontinuities before computing any distance.</p>
<p id="p-0084" num="0083">In one embodiment of the invention, only the relative movement or comparison of pitch is considered, rather than changes or differences in absolute pitch value. In this embodiment, pitch normalizations are applied to remove those pitch values equal to zero; then the mean pitch value within the word is subtracted; then a scaling is applied to the pitch contour that normalizes for the difference in the nominal pitch between the tutor and the user's speech. For instance, the nominal pitch for a male speaker is quite different from the nominal pitch for a female speaker, or a child. The scaling accounts for these differences. The average of the pitch values over the whole utterance is used to compute the scale value. Note that the scale value may also be computed by maintaining average pitch values over multiple utterances to obtain more reliable estimate of the tutor's and user's nominal pitch values. The resulting pitch contours for a word are then represented as: {tilde over (P)}<sub>T</sub>=({tilde over (T)}<sub>1</sub>,{tilde over (T)}<sub>2</sub>, . . . ,{tilde over (T)}<sub>{tilde over (D)}</sub>) and {tilde over (P)}<sub>L</sub>=({tilde over (L)}<sub>1</sub>,{tilde over (L)}<sub>2</sub>, . . . ,{tilde over (L)}<sub>{tilde over (E)}</sub>), where {tilde over (D)}, {tilde over (E)}are the length of normalized pitch series.</p>
<p id="p-0085" num="0084"><figref idref="DRAWINGS">FIG. 7</figref> graphically depicts a pitch contour comparison that benefits from time normalization in accordance with an embodiment of the invention. Specifically, <figref idref="DRAWINGS">FIG. 7</figref> depicts a tutor's pitch contour <b>710</b> and a learner's pitch contour <b>720</b> that are misaligned in time by a temporal amount t<sub>LAG</sub>. It can be seen that the user or learner's intonation is quite similar to the tutor's intonation. However, due to the duration difference of phonemes within the word(s) spoken, the learner's pitch contour is not aligned with the tutor's. In this case, if the distance measure is applied directly, incorrect score will be obtained. Thus, in the case of such non-alignment, a constrained dynamic programming method is used to find the best match path between the tutor and learner's pitch series.</p>
<p id="p-0086" num="0085"><figref idref="DRAWINGS">FIG. 8</figref> graphically depicts a pitch contour comparison that benefits from constrained dynamic programming in accordance with an embodiment of the invention. Specifically, <figref idref="DRAWINGS">FIG. 8</figref> depicts a tutor's pitch contour <b>810</b> and a learner's pitch contour <b>820</b> that are quite different yet still yield a relatively good score since three parts (<b>812</b>, <b>814</b>, <b>816</b>) of the tutor's pitch contour <b>810</b> are very well matched to three corresponding parts (<b>822</b>, <b>824</b>, <b>826</b>) of the learner's pitch contour <b>820</b>.</p>
<p id="p-0087" num="0086">The dynamic programming described herein attempts to provide a “best” match to two pitch contours. However, this may result in an unreliable score unless some constraints are applied to the dynamic programming. The constraints limit the match area of dynamic programming. In one experiment, the inventors determined that the mapping path's slope should lie within [0.5,2]. Within the context of the dynamic programming, two warping functions are found; namely Φ<sub>T </sub>and Φ<sub>L</sub>, which related the indices of two pitch series, i<sub>T </sub>and i<sub>L</sub>, respectively. Specifically, i<sub>T</sub>=Φ<sub>T</sub>(k), k=1,2, . . . ,K and i<sub>L</sub>=Φ<sub>L</sub>(k), k=1,2, . . . ,K; where k is the normalized index.</p>
<p id="p-0088" num="0087">For example, assume that {tilde over (P)}<sub>T</sub>, {tilde over (P)}<sub>L </sub>are the normalized pitch series of tutor and learner, and that Δ{tilde over (P)}<sub>T</sub>, Δ{tilde over (P)}<sub>L </sub>are the first order temporal derivative of pitch series of the tutor and learner, respectively. Then the following equations are derived:</p>
<p id="p-0089" num="0088">
<maths id="MATH-US-00011" num="00011">
<math overflow="scroll">
<mtable>
  <mtr>
    <mtd>
      <mrow>
        <msub>
          <mover>
            <mi>P</mi>
            <mo>~</mo>
          </mover>
          <mi>T</mi>
        </msub>
        <mo>=</mo>
        <mrow>
          <mo>(</mo>
          <mrow>
            <msub>
              <mover>
                <mi>T</mi>
                <mo>~</mo>
              </mover>
              <mn>1</mn>
            </msub>
            <mo>,</mo>
            <msub>
              <mover>
                <mi>T</mi>
                <mo>~</mo>
              </mover>
              <mn>2</mn>
            </msub>
            <mo>,</mo>
            <mi>…</mi>
            <mo>⁢</mo>
            <mstyle>
              <mspace width="0.3em" height="0.3ex"/>
            </mstyle>
            <mo>,</mo>
            <msub>
              <mover>
                <mi>T</mi>
                <mo>~</mo>
              </mover>
              <mover>
                <mi>D</mi>
                <mo>~</mo>
              </mover>
            </msub>
          </mrow>
          <mo>)</mo>
        </mrow>
      </mrow>
    </mtd>
  </mtr>
  <mtr>
    <mtd>
      <mrow>
        <msub>
          <mover>
            <mi>P</mi>
            <mo>~</mo>
          </mover>
          <mi>L</mi>
        </msub>
        <mo>=</mo>
        <mrow>
          <mo>(</mo>
          <mrow>
            <msub>
              <mover>
                <mi>L</mi>
                <mo>~</mo>
              </mover>
              <mn>1</mn>
            </msub>
            <mo>,</mo>
            <msub>
              <mover>
                <mi>L</mi>
                <mo>~</mo>
              </mover>
              <mn>2</mn>
            </msub>
            <mo>,</mo>
            <mi>…</mi>
            <mo>⁢</mo>
            <mstyle>
              <mspace width="0.3em" height="0.3ex"/>
            </mstyle>
            <mo>,</mo>
            <msub>
              <mover>
                <mi>L</mi>
                <mo>~</mo>
              </mover>
              <mover>
                <mi>E</mi>
                <mo>~</mo>
              </mover>
            </msub>
          </mrow>
          <mo>)</mo>
        </mrow>
      </mrow>
    </mtd>
  </mtr>
  <mtr>
    <mtd>
      <mrow>
        <mrow>
          <mi>Δ</mi>
          <mo>⁢</mo>
          <mstyle>
            <mspace width="0.3em" height="0.3ex"/>
          </mstyle>
          <mo>⁢</mo>
          <msub>
            <mover>
              <mi>P</mi>
              <mo>~</mo>
            </mover>
            <mi>T</mi>
          </msub>
        </mrow>
        <mo>=</mo>
        <mrow>
          <mo>(</mo>
          <mrow>
            <mrow>
              <mi>Δ</mi>
              <mo>⁢</mo>
              <mstyle>
                <mspace width="0.3em" height="0.3ex"/>
              </mstyle>
              <mo>⁢</mo>
              <msub>
                <mover>
                  <mi>T</mi>
                  <mo>~</mo>
                </mover>
                <mn>1</mn>
              </msub>
            </mrow>
            <mo>,</mo>
            <mrow>
              <mi>Δ</mi>
              <mo>⁢</mo>
              <msub>
                <mover>
                  <mi>T</mi>
                  <mo>~</mo>
                </mover>
                <mn>2</mn>
              </msub>
            </mrow>
            <mo>,</mo>
            <mi>…</mi>
            <mo>⁢</mo>
            <mstyle>
              <mspace width="0.3em" height="0.3ex"/>
            </mstyle>
            <mo>,</mo>
            <mrow>
              <mi>Δ</mi>
              <mo>⁢</mo>
              <mstyle>
                <mspace width="0.3em" height="0.3ex"/>
              </mstyle>
              <mo>⁢</mo>
              <msub>
                <mover>
                  <mi>T</mi>
                  <mo>~</mo>
                </mover>
                <mover>
                  <mi>D</mi>
                  <mo>~</mo>
                </mover>
              </msub>
            </mrow>
          </mrow>
          <mo>)</mo>
        </mrow>
      </mrow>
    </mtd>
  </mtr>
  <mtr>
    <mtd>
      <mrow>
        <mrow>
          <mrow>
            <mi>Δ</mi>
            <mo>⁢</mo>
            <mstyle>
              <mspace width="0.3em" height="0.3ex"/>
            </mstyle>
            <mo>⁢</mo>
            <msub>
              <mover>
                <mi>P</mi>
                <mo>~</mo>
              </mover>
              <mi>L</mi>
            </msub>
          </mrow>
          <mo>=</mo>
          <mrow>
            <mo>(</mo>
            <mrow>
              <mrow>
                <mi>Δ</mi>
                <mo>⁢</mo>
                <msub>
                  <mover>
                    <mi>L</mi>
                    <mo>~</mo>
                  </mover>
                  <mn>1</mn>
                </msub>
              </mrow>
              <mo>,</mo>
              <mrow>
                <mi>Δ</mi>
                <mo>⁢</mo>
                <mstyle>
                  <mspace width="0.3em" height="0.3ex"/>
                </mstyle>
                <mo>⁢</mo>
                <msub>
                  <mover>
                    <mi>L</mi>
                    <mo>~</mo>
                  </mover>
                  <mn>2</mn>
                </msub>
              </mrow>
              <mo>,</mo>
              <mi>…</mi>
              <mo>⁢</mo>
              <mstyle>
                <mspace width="0.3em" height="0.3ex"/>
              </mstyle>
              <mo>,</mo>
              <mrow>
                <mi>Δ</mi>
                <mo>⁢</mo>
                <mstyle>
                  <mspace width="0.3em" height="0.3ex"/>
                </mstyle>
                <mo>⁢</mo>
                <msub>
                  <mover>
                    <mi>L</mi>
                    <mo>~</mo>
                  </mover>
                  <mover>
                    <mi>E</mi>
                    <mo>~</mo>
                  </mover>
                </msub>
              </mrow>
            </mrow>
            <mo>)</mo>
          </mrow>
        </mrow>
        <mo>;</mo>
        <mi>and</mi>
      </mrow>
    </mtd>
  </mtr>
  <mtr>
    <mtd>
      <mrow>
        <mrow>
          <mrow>
            <mi>Δ</mi>
            <mo>⁢</mo>
            <mstyle>
              <mspace width="0.3em" height="0.3ex"/>
            </mstyle>
            <mo>⁢</mo>
            <msub>
              <mover>
                <mi>T</mi>
                <mo>~</mo>
              </mover>
              <mi>i</mi>
            </msub>
          </mrow>
          <mo>=</mo>
          <mrow>
            <mi>μ</mi>
            <mo>⁢</mo>
            <mrow>
              <munderover>
                <mo>∑</mo>
                <mrow>
                  <mi>k</mi>
                  <mo>=</mo>
                  <mrow>
                    <mo>-</mo>
                    <mi>K</mi>
                  </mrow>
                </mrow>
                <mi>K</mi>
              </munderover>
              <mo>⁢</mo>
              <mrow>
                <mi>k</mi>
                <mo>⁢</mo>
                <mstyle>
                  <mspace width="0.3em" height="0.3ex"/>
                </mstyle>
                <mo>⁢</mo>
                <msub>
                  <mover>
                    <mi>T</mi>
                    <mo>~</mo>
                  </mover>
                  <mrow>
                    <mi>i</mi>
                    <mo>+</mo>
                    <mi>k</mi>
                  </mrow>
                </msub>
              </mrow>
            </mrow>
          </mrow>
        </mrow>
        <mo>,</mo>
        <mrow>
          <mn>1</mn>
          <mo>≤</mo>
          <mi>i</mi>
          <mo>≤</mo>
          <mover>
            <mi>D</mi>
            <mo>~</mo>
          </mover>
        </mrow>
      </mrow>
    </mtd>
  </mtr>
  <mtr>
    <mtd>
      <mrow>
        <mrow>
          <mrow>
            <mi>Δ</mi>
            <mo>⁢</mo>
            <mstyle>
              <mspace width="0.3em" height="0.3ex"/>
            </mstyle>
            <mo>⁢</mo>
            <msub>
              <mover>
                <mi>L</mi>
                <mo>~</mo>
              </mover>
              <mi>i</mi>
            </msub>
          </mrow>
          <mo>=</mo>
          <mrow>
            <mi>μ</mi>
            <mo>⁢</mo>
            <mrow>
              <munderover>
                <mo>∑</mo>
                <mrow>
                  <mi>k</mi>
                  <mo>=</mo>
                  <mrow>
                    <mo>-</mo>
                    <mi>K</mi>
                  </mrow>
                </mrow>
                <mi>K</mi>
              </munderover>
              <mo>⁢</mo>
              <mrow>
                <mi>k</mi>
                <mo>⁢</mo>
                <mstyle>
                  <mspace width="0.3em" height="0.3ex"/>
                </mstyle>
                <mo>⁢</mo>
                <msub>
                  <mover>
                    <mi>L</mi>
                    <mo>~</mo>
                  </mover>
                  <mrow>
                    <mi>i</mi>
                    <mo>+</mo>
                    <mi>k</mi>
                  </mrow>
                </msub>
              </mrow>
            </mrow>
          </mrow>
        </mrow>
        <mo>,</mo>
        <mrow>
          <mn>1</mn>
          <mo>≤</mo>
          <mi>i</mi>
          <mo>≤</mo>
          <mover>
            <mi>E</mi>
            <mo>~</mo>
          </mover>
        </mrow>
      </mrow>
    </mtd>
  </mtr>
</mtable>
</math>
</maths>
</p>
<p id="p-0090" num="0089">The constant μ is used for the normalization and controls the weight of the delta-pitch series. While K is normally selected as 4 to compute the derivatives, other values may be selected. Dynamic programming is used to minimize the distance between the series ({tilde over (P)}<sub>T</sub>,Δ{tilde over (P)}<sub>T</sub>) and ({tilde over (P)}<sub>L</sub>,Δ{tilde over (P)}<sub>L</sub>), as follows:</p>
<p id="p-0091" num="0090">
<maths id="MATH-US-00012" num="00012">
<math overflow="scroll">
<mrow>
  <msub>
    <mi>d</mi>
    <mi>Φ</mi>
  </msub>
  <mo>=</mo>
  <mrow>
    <munderover>
      <mo>∑</mo>
      <mrow>
        <mi>i</mi>
        <mo>=</mo>
        <mn>1</mn>
      </mrow>
      <mi>K</mi>
    </munderover>
    <mo>⁢</mo>
    <mrow>
      <mi>d</mi>
      <mo>⁡</mo>
      <mrow>
        <mo>(</mo>
        <mrow>
          <mrow>
            <msub>
              <mi>Φ</mi>
              <mi>T</mi>
            </msub>
            <mo>⁡</mo>
            <mrow>
              <mo>(</mo>
              <mi>i</mi>
              <mo>)</mo>
            </mrow>
          </mrow>
          <mo>,</mo>
          <mrow>
            <msub>
              <mi>Φ</mi>
              <mi>L</mi>
            </msub>
            <mo>⁡</mo>
            <mrow>
              <mo>(</mo>
              <mi>i</mi>
              <mo>)</mo>
            </mrow>
          </mrow>
        </mrow>
        <mo>)</mo>
      </mrow>
    </mrow>
  </mrow>
</mrow>
</math>
</maths>
</p>
<p id="p-0092" num="0091">d=({tilde over (T)}<sub>i</sub>−{tilde over (L)}<sub>i</sub>)<sup>2</sup>+(Δ{tilde over (T)}<sub>i</sub>−Δ{tilde over (L)}<sub>i</sub>)<sup>2 </sup>is the Euclidean distance between two normalized vectors ({tilde over (T)}<sub>i</sub>,Δ{tilde over (T)}<sub>i</sub>) and ({tilde over (L)}<sub>i</sub>,Δ{tilde over (L)}<sub>i</sub>) at time i. It is contemplated by the inventors that other distance measurements may also be used to practice various embodiments of the invention.</p>
<p id="p-0093" num="0092">The last intonation score is determined as:</p>
<p id="p-0094" num="0093">
<maths id="MATH-US-00013" num="00013">
<math overflow="scroll">
<mrow>
  <mrow>
    <mi>S</mi>
    <mo>=</mo>
    <mrow>
      <mn>1</mn>
      <mo>-</mo>
      <mrow>
        <mn>0.5</mn>
        <mo>×</mo>
        <mfrac>
          <msub>
            <mo>ⅆ</mo>
            <mi>Φ</mi>
          </msub>
          <mrow>
            <msub>
              <mo>ⅆ</mo>
              <mi>T</mi>
            </msub>
            <mo>⁢</mo>
            <mrow>
              <mo>+</mo>
              <msub>
                <mo>ⅆ</mo>
                <mi>L</mi>
              </msub>
            </mrow>
          </mrow>
        </mfrac>
      </mrow>
    </mrow>
  </mrow>
  <mo>;</mo>
  <mrow>
    <mrow>
      <mi>where</mi>
      <mo>⁢</mo>
      <mstyle>
        <mspace width="0.8em" height="0.8ex"/>
      </mstyle>
      <mo>⁢</mo>
      <msub>
        <mi>d</mi>
        <mi>T</mi>
      </msub>
    </mrow>
    <mo>=</mo>
    <mrow>
      <mrow>
        <mrow>
          <munderover>
            <mo>∑</mo>
            <mrow>
              <mi>i</mi>
              <mo>=</mo>
              <mn>1</mn>
            </mrow>
            <mi>K</mi>
          </munderover>
          <mo>⁢</mo>
          <msup>
            <mrow>
              <mo>(</mo>
              <msub>
                <mover>
                  <mi>T</mi>
                  <mo>~</mo>
                </mover>
                <mi>i</mi>
              </msub>
              <mo>)</mo>
            </mrow>
            <mn>2</mn>
          </msup>
        </mrow>
        <mo>+</mo>
        <mrow>
          <msup>
            <mrow>
              <mo>(</mo>
              <mrow>
                <mi>Δ</mi>
                <mo>⁢</mo>
                <mstyle>
                  <mspace width="0.3em" height="0.3ex"/>
                </mstyle>
                <mo>⁢</mo>
                <msub>
                  <mover>
                    <mi>T</mi>
                    <mo>~</mo>
                  </mover>
                  <mi>i</mi>
                </msub>
              </mrow>
              <mo>)</mo>
            </mrow>
            <mn>2</mn>
          </msup>
          <mo>⁢</mo>
          <mstyle>
            <mspace width="0.8em" height="0.8ex"/>
          </mstyle>
          <mo>⁢</mo>
          <mi>and</mi>
          <mo>⁢</mo>
          <mstyle>
            <mspace width="0.8em" height="0.8ex"/>
          </mstyle>
          <mo>⁢</mo>
          <msub>
            <mi>d</mi>
            <mi>L</mi>
          </msub>
        </mrow>
      </mrow>
      <mo>=</mo>
      <mrow>
        <mrow>
          <munderover>
            <mo>∑</mo>
            <mrow>
              <mi>i</mi>
              <mo>=</mo>
              <mn>1</mn>
            </mrow>
            <mi>K</mi>
          </munderover>
          <mo>⁢</mo>
          <msup>
            <mrow>
              <mo>(</mo>
              <msub>
                <mover>
                  <mi>L</mi>
                  <mo>~</mo>
                </mover>
                <mi>i</mi>
              </msub>
              <mo>)</mo>
            </mrow>
            <mn>2</mn>
          </msup>
        </mrow>
        <mo>+</mo>
        <msup>
          <mrow>
            <mo>(</mo>
            <mrow>
              <mi>Δ</mi>
              <mo>⁢</mo>
              <mstyle>
                <mspace width="0.3em" height="0.3ex"/>
              </mstyle>
              <mo>⁢</mo>
              <msub>
                <mover>
                  <mi>L</mi>
                  <mo>~</mo>
                </mover>
                <mi>i</mi>
              </msub>
            </mrow>
            <mo>)</mo>
          </mrow>
          <mn>2</mn>
        </msup>
      </mrow>
    </mrow>
  </mrow>
</mrow>
</math>
</maths>
</p>
<heading id="h-0008" level="1">EXAMPLE</heading>
<p id="p-0095" num="0094"><figref idref="DRAWINGS">FIGS. 9A-9C</figref> graphically depicts respective pitch contours of different pronunciations of a common phrase. Specifically, the same sentence (“my name is steve”) with different intonation was repeated by a reference speaker and by three different users to provide respective pitch contour data.</p>
<p id="p-0096" num="0095">A first pitch contour is depicted in <figref idref="DRAWINGS">FIG. 9A</figref> for the sentence ‘my name IS steve’ in which emphasis is placed on the word “is” by the speaker.</p>
<p id="p-0097" num="0096">A second pitch contour is depicted in <figref idref="DRAWINGS">FIG. 9B</figref> for the sentence ‘my NAME is steve’ in which emphasis is placed on the word “name” by the speaker.</p>
<p id="p-0098" num="0097">A third pitch contour is depicted in <figref idref="DRAWINGS">FIG. 9C</figref> for the sentence ‘MY name is steve’ in which emphasis is placed on the word “my” by the speaker.</p>
<p id="p-0099" num="0098">The tutor's pitch series for these three different intonations are denoted as T<sup>i</sup>,i=1,2,3. Three different non-native speakers were asked to read the sentences following tutor's different intonations. Each reader is requested to repeat 7 times for each phrase, giving 21 utterances per speaker, denoted as U<sub>k</sub><sup>i</sup>, i=1,2,3 represents different intonations, k=1,2, . . . ,7 is the utterance index. There are four words in sentence ‘my name is steve’, thus one reader actually produces 21 different intonations for each word. Taking T<sup>1</sup>,T<sup>2</sup>,T<sup>3 </sup>as tutor, we can get four word-level intonation scores and one overall score on these 21 sentences. The experiment is executed as follows:</p>
<p id="p-0100" num="0099">To compare the automatic score and the human-being expert score, the pitch contour of these 21 sentences is compared with the tutor's pitch contour T<sup>1</sup>,T<sup>2</sup>,T<sup>3</sup>. The reader's intonation is labeled as ‘good’, ‘reasonable’ and ‘bad’, where ‘good’ means perfect match between user's and tutor's intonations, ‘bad’ is poor match between them, and ‘reasonable’ lies within ‘good’ and ‘bad’. Perform the scoring algorithm discussed above on these 21 sentences, obtaining the scores for class ‘good’, ‘not bad’ and ‘bad’.</p>
<p id="p-0101" num="0100">The scoring algorithms and method described herein produce reliable and consistent scores that have perceptual relevance. They each focus on different aspects of pronunciation in a relatively orthogonal manner. This allows a user to focus on improving either the articulation problems, or the intonation problems, or the duration issues in isolation or together. These algorithms are computationally very simple and therefore can be implemented on very low-power processors. The methods and algorithms have been implemented on an ARM7 (74 MHz) series processor to run in real-time.</p>
<p id="p-0102" num="0101"><figref idref="DRAWINGS">FIG. 10</figref> depicts a flow diagram of a method <b>100</b> for providing feedback graphically to a user. Specifically, one embodiment of the present invention provides a user with a fast interactive approach to identify and track key areas for improvement of speech. The present invention accomplishes this fast identification and tracking by providing graphical and/or numerical feedback for speech-features. This provides the user with a comparison of his or her speech features to that of the ideal or reference speech and indicates how accurate that feature is to the feature of the ideal or reference speech. The speech features may include voicing, articulation, relative word duration, intonation and phoneme error indication. Optionally, a numeral rating, e.g., percentage accuracy, can also be provided to the user regarding the correctness of each speech-feature.</p>
<p id="p-0103" num="0102">By providing interactive feedback graphically, the present invention will assist the user in addressing con-genital speech problems and non-native language learners to improve their pronunciation and listening comprehension capabilities. For example, the present invention can be used to complement the capabilities of a trained speech pathologist in treating patients. The present invention can log a patient's progress over time and can be used for training when the patient is at home. With remote access capability, e.g., web access, as disclosed above, the present invention will permit a patient to have access to exercises prepared by his or her speech therapist, and the therapist, in turn, will have access to the patient's log files showing his or her performance both numerically and graphically.</p>
<p id="p-0104" num="0103">Returning to <figref idref="DRAWINGS">FIG. 10</figref>, method <b>1000</b> starts in step <b>1005</b> and proceeds to step <b>1010</b>, where a previously recorded tutor or reference voice of a sentence, phrase, or word is played. In practice, a set of previously recorded utterances, e.g., paragraphs, sentences, phrases and/or words, is stored for retrieval by the user. When a selected utterance is played, the user will hear the proper pronunciation of the sentence, phrase, or word that the user is attempting to learn.</p>
<p id="p-0105" num="0104">In step <b>1020</b>, method <b>1000</b> displays graphically a speech-feature of the tutor's speech of the utterance. The displayed speech-feature can be selected by the user and is further described below.</p>
<p id="p-0106" num="0105">In step <b>1030</b>, the user is prompted to repeat the tutor's voicing of the utterance and the user's utterance is recorded.</p>
<p id="p-0107" num="0106">In step <b>1040</b>, method <b>1000</b> displays graphically a speech-feature of the user's speech of the utterance. Again, the displayed speech-feature is selected by the user and the displayed speech-feature is presented in a manner that will allow the user to visually compare the speech-feature of the tutor's voicing of the utterance with the speech-feature of the user's voicing of the utterance.</p>
<p id="p-0108" num="0107">In step <b>1050</b>, method <b>1000</b> displays one or more numerical scores related to the user's voicing of the reference utterance. The numerical scores may comprise various scores as disclosed above or any other numerical measures, e.g., percentage accuracy and so on. It should be noted that displaying numerical scores is optionally deployed.</p>
<p id="p-0109" num="0108">In step <b>1060</b>, method <b>1000</b> queries whether the user is interested in repeating the previous reference utterance or selecting another reference utterance. If the query is positively answered, then steps <b>1010</b> to <b>1050</b> are repeated. If the query is negatively answered, then method <b>1000</b> ends in step <b>1065</b>.</p>
<p id="p-0110" num="0109">It should be noted that the present invention is not limited by the order of the steps of <figref idref="DRAWINGS">FIG. 10</figref>. Namely, the display steps of <b>1020</b>, <b>1040</b> and <b>1050</b> can be employed in different orders. For example, the tutor's utterance can be displayed at the same time frame along with the user's utterance and so on.</p>
<p id="p-0111" num="0110"><figref idref="DRAWINGS">FIG. 11</figref> depicts an evaluation screen <b>1100</b> of the present invention. The evaluation screen comprises several display areas: a sub-page selection area <b>1110</b>, a text area <b>1120</b>, a tutor speech-feature display area <b>1130</b>, a user speech-feature display area <b>1140</b>, and a control area <b>1150</b>.</p>
<p id="p-0112" num="0111">In operation, a user is presented with several sub-pages to view an utterance, e.g., a “view” sub-page <b>1102</b>, a “word” sub-page <b>1104</b> and a “phoneme” sub-page <b>1106</b>. The “view” sub-page is designed to display graphically the speech intonation of the tutor's utterance and the speech intonation of the user's utterance. This sub-page is intended to focus on the intonation speech-feature. The user is presented with a tutor or reference phrase in the text area <b>1120</b>. In this instance, an English sentence is presented with its corresponding Chinese translation. Below the text area <b>1120</b>, one or more intonation curves are provided in the tutor speech-feature display area <b>1130</b> to illustrate graphically the proper intonation of the utterance as displayed in the current text area <b>1120</b>. Once the intonation curve of the tutor's utterance is displayed, the user is prompted to repeat the same utterance by a message in the user speech-feature display area <b>1140</b>. The user's utterance is then recorded and, an intonation curve of the user's utterance (shown in <figref idref="DRAWINGS">FIG. 12</figref>) is presented in the user speech-feature display area <b>1140</b> for easy comparison with the intonation curve of the tutor's utterance. Finally, control area <b>1150</b> presents the user with a set of controls, e.g., escape, play tutor utterance, volume up, volume down, record, play user utterance, page down, display key for volume/speed and so on.</p>
<p id="p-0113" num="0112"><figref idref="DRAWINGS">FIG. 12</figref> depicts an alternative evaluation screen <b>1200</b> of the present invention. Specifically, <figref idref="DRAWINGS">FIG. 12</figref> is very similar to <figref idref="DRAWINGS">FIG. 11</figref> with the exception of presenting the intonation curve of the user's utterance in the user speech-feature display area <b>1140</b> for easy comparison with the intonation curve of the tutor's utterance. Additionally, <figref idref="DRAWINGS">FIG. 12</figref> illustrates a volume bar <b>1210</b> and a speed bar <b>1220</b> that indicate the current volume setting and the current speed setting (i.e., the speed of voicing the utterance), respectively.</p>
<p id="p-0114" num="0113"><figref idref="DRAWINGS">FIG. 13</figref> depicts an alternative evaluation screen <b>1300</b> of the present invention. More specifically, <figref idref="DRAWINGS">FIG. 13</figref> depicts a screen display where the “word” sub-page <b>1304</b> is selected. Using this sub-page, the user is able to check and play the recorded utterance word by word with intonation score feedback for each word.</p>
<p id="p-0115" num="0114">The “word” evaluation screen comprises several display areas: a sub-page selection area <b>1310</b>, a text area <b>1320</b>, a tutor speech-feature display area <b>1330</b>, a user speech-feature display area <b>1340</b>, a scoring area <b>1345</b> and a control area <b>1350</b>. The operation of this evaluation screen is similar to that of <figref idref="DRAWINGS">FIG. 11</figref>. As such, the present discussion will only focus on the differences.</p>
<p id="p-0116" num="0115">More specifically, the text area <b>1320</b> now presents the reference utterance in terms of its components, i.e., words. Each word can be highlighted and played. Additionally, the tutor speech-feature display area <b>1330</b> and the user speech-feature display area <b>1340</b> now display the intonation curves in a format that reflects word duration. Namely, the tutor's intonation curves are shown with word indication immediately below the intonation curves, where the length of the word maps with the duration of the tutor pronunciation of the current word.</p>
<p id="p-0117" num="0116">Furthermore, in addition to the user's intonation curves as shown in the user speech-feature display area <b>1340</b>, the present word sub-page also provides score feedback in the scoring area <b>1345</b>. Specifically, the length of each score field maps the duration of the user pronunciation of the current word. Thus, the user can quickly determine the mistakes based on the intonation curves and the feedback scores.</p>
<p id="p-0118" num="0117"><figref idref="DRAWINGS">FIG. 14</figref> depicts an alternative evaluation screen <b>1400</b> of the present invention. More specifically, <figref idref="DRAWINGS">FIG. 14</figref> depicts a screen display where the “phoneme” sub-page <b>1406</b> is selected. Using this sub-page, the user is able to check and play the recorded utterance, word by word with the phonemes of a currently selected word illustrated in International Phonetic Symbols.</p>
<p id="p-0119" num="0118">The “phoneme” evaluation screen comprises several display areas: a sub-page selection area <b>1410</b>, a text area <b>1420</b>, a tutor speech-feature display area <b>1430</b>, a user speech-feature display area <b>1440</b> and a control area <b>1450</b>. The operation of this evaluation screen is similar to that of <figref idref="DRAWINGS">FIG. 11</figref>. As such, the present discussion will only focus on the differences.</p>
<p id="p-0120" num="0119">In operation, the text, e.g., English text, of the reference utterance is shown with highlighted words that indicate words within the reference utterance that have been wrongly pronounced by the user. It should be noted that what constitute wrongly pronounced words is premised on thresholds that can be selectively set for different speech features. For example, a threshold of 80 can be set for the speech-feature acoustic such that any user uttered words having a score below 80 will cause such user uttered words to be deemed wrongly pronounced and so on.</p>
<p id="p-0121" num="0120">The user can easily select a highlighted word to view the International Phonetic Symbols associated with that word. More specifically, symbols that are incorrectly pronounced are highlighted. Alternatively, the highlighted symbols may correspond to incorrect or false symbols that were pronounced by the user.</p>
<p id="p-0122" num="0121">Finally, the user speech-feature display area <b>1440</b> displays three speech feature scores and one speed setting. The speed setting indicates the compare ratio between the tutor speech length and the user speech length. The three speech feature scores correspond to duration, acoustic and intonation.</p>
<p id="p-0123" num="0122">Although the present invention discloses several exemplary evaluation screens above, it should be noted that the present invention is not so limited. Specifically, the order and type of information presented in these evaluation screens can be adapted to meet particular implementation requirements, e.g., presenting different types of speech features, and using different types of curves, e.g., bar curves and so on.</p>
<p id="p-0124" num="0123"><figref idref="DRAWINGS">FIG. 16</figref> depicts a score evaluation screen <b>1600</b> of the present invention. Specifically, the various speech feature scores as described above can be gathered and viewed to evaluate the performance of a user. In one embodiment, <figref idref="DRAWINGS">FIG. 16</figref> illustrates the phoneme error viewing mode that displays the phoneme errors (e.g., a list of problematic phones <b>1610</b>) for a particular lesson. This allows a speech therapist to quick identify the problem areas for a particular user, thereby allowing a speech therapist to apply this performance data to tailor future lessons that will be most beneficial to a specific user. In fact, as performance data are accumulated for various lessons, the present invention provides a historical view to track the performance of a user. Additionally, scoring views can be deployed for other speech feature scores.</p>
<p id="p-0125" num="0124"><figref idref="DRAWINGS">FIG. 17</figref> depicts a historical evaluation screen <b>1700</b> of the present invention. Specifically, intonation, articulation and duration scores can be viewed graphically in a history mode. The history mode presents a graphical view <b>1710</b> that shows the performance trend as a function of time and shows the line charts for each snapshot date by showing the mean and standard deviation for that date. To illustrate, <figref idref="DRAWINGS">FIG. 17</figref> shows the history view for “lesson 1”. The horizontal axis displays the date on which the score snapshot was saved. The vertical axis represents the scores. The trend plot traces the mean scores over the snapshots. A color, e.g., blue, vertical bar around a mean score represents the standard deviation for that score.</p>
<p id="p-0126" num="0125">Alternatively, intonation, articulation and duration scores can be viewed in a histogram mode. In one embodiment, the histogram is a bar chart of the frequency distribution of a number of score intervals (e.g., 20), where the height of a bar indicates the number of times a user or student obtained a score in that interval.</p>
<p id="p-0127" num="0126">It should be noted that different evaluation and historical screens can be adapted to display the intonation, articulation and duration scores. Although the present specification provides several examples, the present invention is not limited to these examples.</p>
<p id="p-0128" num="0127"><figref idref="DRAWINGS">FIG. 15</figref> depicts a portable apparatus <b>1500</b> that deploys the scoring and graphical feedback display methods of the present invention. The apparatus <b>1500</b> is an illustrative embodiment of the apparatus as illustrated in <figref idref="DRAWINGS">FIG. 1</figref> above.</p>
<p id="p-0129" num="0128">Specifically, apparatus <b>1500</b> is a wireless handheld apparatus comprising a display <b>1510</b>, a power port <b>1520</b>, a microphone <b>1530</b>, a speaker or an earphone port <b>1540</b>, navigating keys <b>1550</b>, a brightness control key <b>1560</b>, a plurality of function keys <b>1570</b> and a communication port <b>1580</b>. In operation, the apparatus <b>1500</b> can be powered by a power port <b>1520</b>, where DC power is received to operate the apparatus and/or charge a battery (not shown) within the apparatus. During normal operation, a screen <b>1510</b> displays a number of icons representative of functions that can be performed, e.g., start a tutoring session, change settings, download a set of reference utterances or a quiz, request help and exit the program. The various icons illustrated on the display <b>1510</b> can be selected via navigation keys <b>1550</b>. In addition to the navigation keys, various function keys are provided to perform various functions, e.g., record, play, shift and menu, On/Off and so on.</p>
<p id="p-0130" num="0129">The apparatus <b>1500</b> is also provided with various input/output devices or ports, e.g., a microphone or a port for an external microphone <b>1530</b> and a speaker or a port for an earphone <b>1540</b>. Additionally, a communication port, e.g., a serial port <b>1580</b> is provided to download a set of reference utterance. Alternatively, the communication port <b>1580</b> may encompass a transmitter and a receiver to communicate with a wireless network. Finally, a brightness control key <b>1560</b> is provided to adjust the brightness of the display <b>1510</b>.</p>
<p id="p-0131" num="0130">Although various embodiments that incorporate the teachings of the present invention have been shown and described in detail herein, those skilled in the art can readily devise many other varied embodiments that still incorporate these teachings.</p>
<?DETDESC description="Detailed Description" end="tail"?>
</description>
<us-math idrefs="MATH-US-00001" nb-file="US07299188-20071120-M00001.NB">
<img id="EMI-M00001" he="9.57mm" wi="26.84mm" file="US07299188-20071120-M00001.TIF" alt="embedded image " img-content="table" img-format="tif"/>
</us-math>
<us-math idrefs="MATH-US-00002" nb-file="US07299188-20071120-M00002.NB">
<img id="EMI-M00002" he="9.57mm" wi="26.84mm" file="US07299188-20071120-M00002.TIF" alt="embedded image " img-content="table" img-format="tif"/>
</us-math>
<us-math idrefs="MATH-US-00003" nb-file="US07299188-20071120-M00003.NB">
<img id="EMI-M00003" he="9.57mm" wi="26.84mm" file="US07299188-20071120-M00003.TIF" alt="embedded image " img-content="table" img-format="tif"/>
</us-math>
<us-math idrefs="MATH-US-00004" nb-file="US07299188-20071120-M00004.NB">
<img id="EMI-M00004" he="9.57mm" wi="26.84mm" file="US07299188-20071120-M00004.TIF" alt="embedded image " img-content="table" img-format="tif"/>
</us-math>
<us-math idrefs="MATH-US-00005" nb-file="US07299188-20071120-M00005.NB">
<img id="EMI-M00005" he="9.57mm" wi="27.18mm" file="US07299188-20071120-M00005.TIF" alt="embedded image " img-content="table" img-format="tif"/>
</us-math>
<us-math idrefs="MATH-US-00006" nb-file="US07299188-20071120-M00006.NB">
<img id="EMI-M00006" he="9.57mm" wi="27.18mm" file="US07299188-20071120-M00006.TIF" alt="embedded image " img-content="table" img-format="tif"/>
</us-math>
<us-math idrefs="MATH-US-00007" nb-file="US07299188-20071120-M00007.NB">
<img id="EMI-M00007" he="9.57mm" wi="27.18mm" file="US07299188-20071120-M00007.TIF" alt="embedded image " img-content="table" img-format="tif"/>
</us-math>
<us-math idrefs="MATH-US-00008" nb-file="US07299188-20071120-M00008.NB">
<img id="EMI-M00008" he="9.57mm" wi="27.18mm" file="US07299188-20071120-M00008.TIF" alt="embedded image " img-content="table" img-format="tif"/>
</us-math>
<us-math idrefs="MATH-US-00009" nb-file="US07299188-20071120-M00009.NB">
<img id="EMI-M00009" he="21.51mm" wi="76.20mm" file="US07299188-20071120-M00009.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-math idrefs="MATH-US-00010" nb-file="US07299188-20071120-M00010.NB">
<img id="EMI-M00010" he="8.81mm" wi="76.20mm" file="US07299188-20071120-M00010.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-math idrefs="MATH-US-00011" nb-file="US07299188-20071120-M00011.NB">
<img id="EMI-M00011" he="38.44mm" wi="76.20mm" file="US07299188-20071120-M00011.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-math idrefs="MATH-US-00012" nb-file="US07299188-20071120-M00012.NB">
<img id="EMI-M00012" he="8.81mm" wi="76.20mm" file="US07299188-20071120-M00012.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-math idrefs="MATH-US-00013" nb-file="US07299188-20071120-M00013.NB">
<img id="EMI-M00013" he="16.26mm" wi="76.20mm" file="US07299188-20071120-M00013.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-claim-statement>What is claimed is:</us-claim-statement>
<claims id="claims">
<claim id="CLM-00001" num="00001">
<claim-text>1. A method for providing speech-feature feedback, said method comprising the steps of:
<claim-text>presenting a reference utterance;</claim-text>
<claim-text>displaying graphically each word of said reference utterance;</claim-text>
<claim-text>displaying graphically a speech-feature associated with speech of said reference utterance;</claim-text>
<claim-text>receiving a user utterance that represents speech of said reference utterance by a user;</claim-text>
<claim-text>displaying graphically a speech-feature associated with speech of said user utterance; and</claim-text>
<claim-text>displaying graphically a score field for each graphically displayed word of said reference utterance, wherein each score field has a length indicative of a duration of said corresponding word of said user utterance, wherein each score field includes at least one numerical score for at least one speech-feature of said word of said user utterance.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00002" num="00002">
<claim-text>2. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein said speech-feature represents intonation.</claim-text>
</claim>
<claim id="CLM-00003" num="00003">
<claim-text>3. The method of <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein said speech-feature associated with speech of said reference utterance and said speech-feature associated with speech of said user utterance are graphically displayed as intonation curves.</claim-text>
</claim>
<claim id="CLM-00004" num="00004">
<claim-text>4. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein said speech-feature represents acoustic quality.</claim-text>
</claim>
<claim id="CLM-00005" num="00005">
<claim-text>5. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein said speech-feature represents word duration.</claim-text>
</claim>
<claim id="CLM-00006" num="00006">
<claim-text>6. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein said at least one numerical score comprises an intonation score.</claim-text>
</claim>
<claim id="CLM-00007" num="00007">
<claim-text>7. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein said at least one numerical score comprises an acoustic quality score.</claim-text>
</claim>
<claim id="CLM-00008" num="00008">
<claim-text>8. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein said at least one numerical score comprises a word duration score.</claim-text>
</claim>
<claim id="CLM-00009" num="00009">
<claim-text>9. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising the step of:
<claim-text>playing said reference utterance.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00010" num="00010">
<claim-text>10. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein said speech-feature associated with speech of said reference utterance is presented graphically as a plurality of intonation curves corresponding to said words of said reference utterance.</claim-text>
</claim>
<claim id="CLM-00011" num="00011">
<claim-text>11. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein said speech-feature associated with speech of said user utterance is presented graphically as a plurality of international phonetic symbols corresponding to a selected graphically displayed word of said reference utterance.</claim-text>
</claim>
<claim id="CLM-00012" num="00012">
<claim-text>12. The method of <claim-ref idref="CLM-00011">claim 11</claim-ref>, wherein a subset of said plurality of international phonetic symbols are highlighted to indicate one or more phonemes of said user utterance that are deemed mispronounced.</claim-text>
</claim>
<claim id="CLM-00013" num="00013">
<claim-text>13. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein one or more of said graphically displayed words of said reference utterance are highlighted to indicated one or more words of said user utterance that are deemed mispronounced.</claim-text>
</claim>
<claim id="CLM-00014" num="00014">
<claim-text>14. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein said method is implemented by a handheld apparatus.</claim-text>
</claim>
<claim id="CLM-00015" num="00015">
<claim-text>15. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising the step of:
<claim-text>downloading a set of reference utterances from a remote source.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00016" num="00016">
<claim-text>16. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein numerical scores associated with said at least one speech-feature are graphically displayed for illustrating historical performance.</claim-text>
</claim>
<claim id="CLM-00017" num="00017">
<claim-text>17. A computer readable medium for storing instructions that, when executed by a processor, perform a method for providing speech-feature feedback, said method comprising the steps of:
<claim-text>presenting a reference utterance;</claim-text>
<claim-text>displaying graphically each word of said reference utterance;</claim-text>
<claim-text>displaying graphically a speech-feature associated with speech of said reference utterance;</claim-text>
<claim-text>receiving a user utterance that represents speech of said reference utterance by a user;</claim-text>
<claim-text>displaying graphically a speech-feature associated with speech of said user utterance; and</claim-text>
<claim-text>displaying graphically a score field for each graphically displayed word of said reference utterance, wherein each score field has a length indicative of a duration of said corresponding word of said user utterance, wherein each score field includes at least one numerical score for at least one speech-feature of said word of said user utterance.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00018" num="00018">
<claim-text>18. Apparatus for providing speech-feature feedback, said apparatus comprising:
<claim-text>means for presenting a reference utterance;</claim-text>
<claim-text>means for displaying graphically each word of said reference utterance;</claim-text>
<claim-text>means for displaying graphically a speech-feature associated with speech of said reference utterance;</claim-text>
<claim-text>means for receiving a user utterance that represents speech of said reference utterance by a user; and</claim-text>
<claim-text>means for displaying graphically a speech-feature associated with speech of said user utterance; and</claim-text>
<claim-text>means for displaying graphically a score field for each graphically displayed word of said reference utterance, wherein each score field has a length indicative of a duration of said corresponding word of said user utterance, wherein each score field includes at least one numerical score for at least one speech-feature of said word of said user utterance.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00019" num="00019">
<claim-text>19. A handheld device, comprising:
<claim-text>a voice input port for receiving a voiced user utterance that represents speech of a reference utterance;</claim-text>
<claim-text>a display device for displaying at least pronunciation scoring information;</claim-text>
<claim-text>a memory for storing software instructions representing an articulation scoring engine, a duration scoring engine, an intonation scoring engine, and a plurality of reference utterances; and</claim-text>
<claim-text>a processor for executing said software instructions representing said scoring engines to derive thereby a pronunciation score for said received utterance;</claim-text>
<claim-text>said display device for displaying graphically a score field for each of a plurality of graphically displayed words of said reference utterance, wherein each score field has a length indicative of a duration of a corresponding one of a respective plurality of words of said user utterance, wherein each score field includes at least one numerical score for at least one speech-feature of said word of said user utterance.</claim-text>
</claim-text>
</claim>
</claims>
</us-patent-grant>
