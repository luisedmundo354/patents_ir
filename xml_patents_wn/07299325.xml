<us-patent-grant lang="EN" dtd-version="v4.2 2006-08-23" file="US07299325-20071120.XML" status="PRODUCTION" id="us-patent-grant" country="US" date-produced="20071106" date-publ="20071120">
<us-bibliographic-data-grant>
<publication-reference>
<document-id>
<country>US</country>
<doc-number>07299325</doc-number>
<kind>B1</kind>
<date>20071120</date>
</document-id>
</publication-reference>
<application-reference appl-type="utility">
<document-id>
<country>US</country>
<doc-number>11212093</doc-number>
<date>20050824</date>
</document-id>
</application-reference>
<us-application-series-code>11</us-application-series-code>
<us-term-of-grant>
<us-term-extension>284</us-term-extension>
</us-term-of-grant>
<classifications-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>G</section>
<class>06</class>
<subclass>F</subclass>
<main-group>12</main-group>
<subgroup>00</subgroup>
<symbol-position>F</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20071120</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
</classifications-ipcr>
<classification-national>
<country>US</country>
<main-classification>711159</main-classification>
<further-classification>711133</further-classification>
<further-classification>711205</further-classification>
<further-classification>711221</further-classification>
<further-classification>707206</further-classification>
</classification-national>
<invention-title id="d0e53">Method of garbage collection on a data storage system</invention-title>
<references-cited>
<citation>
<patcit num="00001">
<document-id>
<country>US</country>
<doc-number>5640286</doc-number>
<kind>A</kind>
<name>Acosta et al.</name>
<date>19970600</date>
</document-id>
</patcit>
<category>cited by other</category>
<classification-national><country>US</country><main-classification>360 48</main-classification></classification-national>
</citation>
<citation>
<patcit num="00002">
<document-id>
<country>US</country>
<doc-number>5689727</doc-number>
<kind>A</kind>
<name>Bonke et al.</name>
<date>19971100</date>
</document-id>
</patcit>
<category>cited by other</category>
<classification-national><country>US</country><main-classification>710 20</main-classification></classification-national>
</citation>
<citation>
<patcit num="00003">
<document-id>
<country>US</country>
<doc-number>6108703</doc-number>
<kind>A</kind>
<name>Leighton et al.</name>
<date>20000800</date>
</document-id>
</patcit>
<category>cited by other</category>
<classification-national><country>US</country><main-classification>709226</main-classification></classification-national>
</citation>
<citation>
<patcit num="00004">
<document-id>
<country>US</country>
<doc-number>6182121</doc-number>
<kind>B1</kind>
<name>Wlaschin</name>
<date>20010100</date>
</document-id>
</patcit>
<category>cited by other</category>
<classification-national><country>US</country><main-classification>709215</main-classification></classification-national>
</citation>
<citation>
<patcit num="00005">
<document-id>
<country>US</country>
<doc-number>6295564</doc-number>
<kind>B1</kind>
<name>Shigetomi et al.</name>
<date>20010900</date>
</document-id>
</patcit>
<category>cited by other</category>
<classification-national><country>US</country><main-classification>710 74</main-classification></classification-national>
</citation>
<citation>
<patcit num="00006">
<document-id>
<country>US</country>
<doc-number>6496883</doc-number>
<kind>B2</kind>
<name>Shigetomi et al.</name>
<date>20021200</date>
</document-id>
</patcit>
<category>cited by other</category>
<classification-national><country>US</country><main-classification>710 74</main-classification></classification-national>
</citation>
<citation>
<patcit num="00007">
<document-id>
<country>US</country>
<doc-number>6557076</doc-number>
<kind>B1</kind>
<name>Copeland et al.</name>
<date>20030400</date>
</document-id>
</patcit>
<category>cited by other</category>
<classification-national><country>US</country><main-classification>711744</main-classification></classification-national>
</citation>
<citation>
<patcit num="00008">
<document-id>
<country>US</country>
<doc-number>6567905</doc-number>
<kind>B2</kind>
<name>Otis</name>
<date>20030500</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>711170</main-classification></classification-national>
</citation>
<citation>
<patcit num="00009">
<document-id>
<country>US</country>
<doc-number>6725392</doc-number>
<kind>B1</kind>
<name>Frey et al.</name>
<date>20040400</date>
</document-id>
</patcit>
<category>cited by other</category>
<classification-national><country>US</country><main-classification>714  6</main-classification></classification-national>
</citation>
<citation>
<patcit num="00010">
<document-id>
<country>US</country>
<doc-number>6996501</doc-number>
<kind>B1</kind>
<name>Rothberg</name>
<date>20060200</date>
</document-id>
</patcit>
<category>cited by other</category>
<classification-national><country>US</country><main-classification>702186</main-classification></classification-national>
</citation>
<citation>
<patcit num="00011">
<document-id>
<country>US</country>
<doc-number>7024582</doc-number>
<kind>B2</kind>
<name>Loy et al.</name>
<date>20060400</date>
</document-id>
</patcit>
<category>cited by other</category>
<classification-national><country>US</country><main-classification>714  4</main-classification></classification-national>
</citation>
<citation>
<patcit num="00012">
<document-id>
<country>US</country>
<doc-number>7117201</doc-number>
<kind>B2</kind>
<name>Kuno et al.</name>
<date>20061000</date>
</document-id>
</patcit>
<category>cited by other</category>
<classification-national><country>US</country><main-classification>707  3</main-classification></classification-national>
</citation>
<citation>
<patcit num="00013">
<document-id>
<country>US</country>
<doc-number>2003/0009563</doc-number>
<kind>A1</kind>
<name>Douglis et al.</name>
<date>20030100</date>
</document-id>
</patcit>
<category>cited by other</category>
<classification-national><country>US</country><main-classification>709227</main-classification></classification-national>
</citation>
<citation>
<patcit num="00014">
<document-id>
<country>US</country>
<doc-number>2003/0188097</doc-number>
<kind>A1</kind>
<name>Holland et al.</name>
<date>20031000</date>
</document-id>
</patcit>
<category>cited by other</category>
<classification-national><country>US</country><main-classification>711114</main-classification></classification-national>
</citation>
<citation>
<patcit num="00015">
<document-id>
<country>US</country>
<doc-number>2004/0003055</doc-number>
<kind>A1</kind>
<name>Holland et al.</name>
<date>20040100</date>
</document-id>
</patcit>
<category>cited by other</category>
<classification-national><country>US</country><main-classification>709219</main-classification></classification-national>
</citation>
<citation>
<patcit num="00016">
<document-id>
<country>US</country>
<doc-number>2004/0073582</doc-number>
<kind>A1</kind>
<name>Spiegel</name>
<date>20040400</date>
</document-id>
</patcit>
<category>cited by other</category>
<classification-national><country>US</country><main-classification>707204</main-classification></classification-national>
</citation>
<citation>
<patcit num="00017">
<document-id>
<country>US</country>
<doc-number>2004/0205110</doc-number>
<kind>A1</kind>
<name>Hinshaw</name>
<date>20041000</date>
</document-id>
</patcit>
<category>cited by other</category>
<classification-national><country>US</country><main-classification>709201</main-classification></classification-national>
</citation>
<citation>
<patcit num="00018">
<document-id>
<country>US</country>
<doc-number>2005/0165662</doc-number>
<kind>A1</kind>
<name>Shigetomi et al.</name>
<date>20050700</date>
</document-id>
</patcit>
<category>cited by other</category>
<classification-national><country>US</country><main-classification>705 27</main-classification></classification-national>
</citation>
<citation>
<patcit num="00019">
<document-id>
<country>US</country>
<doc-number>2005/0273686</doc-number>
<kind>A1</kind>
<name>Turner et al.</name>
<date>20051200</date>
</document-id>
</patcit>
<category>cited by other</category>
<classification-national><country>US</country><main-classification>714752</main-classification></classification-national>
</citation>
<citation>
<patcit num="00020">
<document-id>
<country>US</country>
<doc-number>2005/0283645</doc-number>
<kind>A1</kind>
<name>Turner et al.</name>
<date>20051200</date>
</document-id>
</patcit>
<category>cited by other</category>
<classification-national><country>US</country><main-classification>714  4</main-classification></classification-national>
</citation>
</references-cited>
<number-of-claims>17</number-of-claims>
<us-exemplary-claim>1</us-exemplary-claim>
<us-field-of-classification-search>
<classification-national>
<country>US</country>
<main-classification>711159</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>711133</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>711205</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>711221</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>707206</main-classification>
</classification-national>
</us-field-of-classification-search>
<figures>
<number-of-drawing-sheets>14</number-of-drawing-sheets>
<number-of-figures>14</number-of-figures>
</figures>
<us-related-documents>
<continuation-in-part>
<relation>
<parent-doc>
<document-id>
<country>US</country>
<doc-number>10883299</doc-number>
<kind>00</kind>
<date>20040630</date>
</document-id>
<parent-status>PENDING</parent-status>
</parent-doc>
<child-doc>
<document-id>
<country>US</country>
<doc-number>11212093</doc-number>
</document-id>
</child-doc>
</relation>
</continuation-in-part>
</us-related-documents>
<parties>
<applicants>
<applicant sequence="001" app-type="applicant-inventor" designation="us-only">
<addressbook>
<last-name>Waterhouse</last-name>
<first-name>Steven Richard</first-name>
<address>
<city>San Francisco</city>
<state>CA</state>
<country>US</country>
</address>
</addressbook>
<nationality>
<country>omitted</country>
</nationality>
<residence>
<country>US</country>
</residence>
</applicant>
<applicant sequence="002" app-type="applicant-inventor" designation="us-only">
<addressbook>
<last-name>Botros</last-name>
<first-name>Sherif M.</first-name>
<address>
<city>Redwood Shores</city>
<state>CA</state>
<country>US</country>
</address>
</addressbook>
<nationality>
<country>omitted</country>
</nationality>
<residence>
<country>US</country>
</residence>
</applicant>
<applicant sequence="003" app-type="applicant-inventor" designation="us-only">
<addressbook>
<last-name>Manczak</last-name>
<first-name>Olaf</first-name>
<address>
<city>Hayward</city>
<state>CA</state>
<country>US</country>
</address>
</addressbook>
<nationality>
<country>omitted</country>
</nationality>
<residence>
<country>US</country>
</residence>
</applicant>
<applicant sequence="004" app-type="applicant-inventor" designation="us-only">
<addressbook>
<last-name>Gates</last-name>
<first-name>Patrick</first-name>
<address>
<city>San Francisco</city>
<state>CA</state>
<country>US</country>
</address>
</addressbook>
<nationality>
<country>omitted</country>
</nationality>
<residence>
<country>US</country>
</residence>
</applicant>
<applicant sequence="005" app-type="applicant-inventor" designation="us-only">
<addressbook>
<last-name>Werner</last-name>
<first-name>Jeremy</first-name>
<address>
<city>San Francisco</city>
<state>CA</state>
<country>US</country>
</address>
</addressbook>
<nationality>
<country>omitted</country>
</nationality>
<residence>
<country>US</country>
</residence>
</applicant>
<applicant sequence="006" app-type="applicant-inventor" designation="us-only">
<addressbook>
<last-name>Arnoud</last-name>
<first-name>Sacha</first-name>
<address>
<city>San Francisco</city>
<state>CA</state>
<country>US</country>
</address>
</addressbook>
<nationality>
<country>omitted</country>
</nationality>
<residence>
<country>US</country>
</residence>
</applicant>
</applicants>
<agents>
<agent sequence="01" rep-type="attorney">
<addressbook>
<orgname>Gunnison, McKay &amp; Hodgson, L.L.P.</orgname>
<address>
<country>unknown</country>
</address>
</addressbook>
</agent>
<agent sequence="02" rep-type="attorney">
<addressbook>
<last-name>Norris</last-name>
<first-name>Lisa A.</first-name>
<address>
<country>unknown</country>
</address>
</addressbook>
</agent>
</agents>
</parties>
<assignees>
<assignee>
<addressbook>
<orgname>Sun Microsystems, Inc.</orgname>
<role>02</role>
<address>
<city>Santa Clara</city>
<state>CA</state>
<country>US</country>
</address>
</addressbook>
</assignee>
</assignees>
<examiners>
<primary-examiner>
<last-name>Elmore</last-name>
<first-name>Stephen C.</first-name>
<department>2185</department>
</primary-examiner>
</examiners>
</us-bibliographic-data-grant>
<abstract id="abstract">
<p id="p-0001" num="0000">A method includes determining a garbage collection list for a data storage structure of a node in a data storage system, the garbage collection list including one or more layout map identifiers (IDs) for garbage collection. A data fragment stored on the data storage structure is located at a first location and a layout map ID associated with the data fragment is determined. A determination is made whether the layout map ID associated with the data fragment matches a layout map ID for garbage collection in the garbage collection list. If the layout map ID associated with the data fragment matches a layout map ID for garbage collection in the garbage collection list, a determination is made whether the data fragment is present at a second location on the data storage system. If the data fragment is present at a second location on the data storage system, the data fragment at the first location is determined to be a garbage fragment and deleted from the data storage system.</p>
</abstract>
<drawings id="DRAWINGS">
<figure id="Fig-EMI-D00000" num="00000">
<img id="EMI-D00000" he="259.50mm" wi="180.42mm" file="US07299325-20071120-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00001" num="00001">
<img id="EMI-D00001" he="197.44mm" wi="167.64mm" file="US07299325-20071120-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00002" num="00002">
<img id="EMI-D00002" he="263.48mm" wi="194.06mm" orientation="landscape" file="US07299325-20071120-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00003" num="00003">
<img id="EMI-D00003" he="244.52mm" wi="190.67mm" orientation="landscape" file="US07299325-20071120-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00004" num="00004">
<img id="EMI-D00004" he="229.62mm" wi="173.74mm" orientation="landscape" file="US07299325-20071120-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00005" num="00005">
<img id="EMI-D00005" he="193.38mm" wi="168.32mm" orientation="landscape" file="US07299325-20071120-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00006" num="00006">
<img id="EMI-D00006" he="297.43mm" wi="212.68mm" orientation="landscape" file="US07299325-20071120-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00007" num="00007">
<img id="EMI-D00007" he="222.84mm" wi="163.24mm" orientation="landscape" file="US07299325-20071120-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00008" num="00008">
<img id="EMI-D00008" he="219.46mm" wi="138.51mm" orientation="landscape" file="US07299325-20071120-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00009" num="00009">
<img id="EMI-D00009" he="245.87mm" wi="183.90mm" orientation="landscape" file="US07299325-20071120-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00010" num="00010">
<img id="EMI-D00010" he="238.08mm" wi="200.07mm" file="US07299325-20071120-D00010.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00011" num="00011">
<img id="EMI-D00011" he="248.92mm" wi="173.74mm" orientation="landscape" file="US07299325-20071120-D00011.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00012" num="00012">
<img id="EMI-D00012" he="173.74mm" wi="162.56mm" file="US07299325-20071120-D00012.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00013" num="00013">
<img id="EMI-D00013" he="284.73mm" wi="197.19mm" file="US07299325-20071120-D00013.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00014" num="00014">
<img id="EMI-D00014" he="272.63mm" wi="198.12mm" file="US07299325-20071120-D00014.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
</drawings>
<description id="description">
<?RELAPP description="Other Patent Relations" end="lead"?>
<p id="p-0002" num="0001">This application is a continuation-in-part of U.S. patent application Ser. No. 10/883,299, filed Jun. 30, 2004, and entitled “Method For Recovery of Data”, which is incorporated herein by reference in its entirety.</p>
<?RELAPP description="Other Patent Relations" end="tail"?>
<?BRFSUM description="Brief Summary" end="lead"?>
<heading id="h-0001" level="1">BACKGROUND OF THE INVENTION</heading>
<p id="p-0003" num="0002">1. Field of the Invention</p>
<p id="p-0004" num="0003">The present invention is directed to the field of data storage. In particular, the present invention is directed to collection of garbage data on a data storage system.</p>
<p id="p-0005" num="0004">2. Description of Related Art</p>
<p id="p-0006" num="0005">The quantity of fixed data content, such as text files and image files, is rising rapidly. For example, the Internet Archive WayBack Machine (www.archive.org) currently archives 30 billion web pages.</p>
<p id="p-0007" num="0006">Further, compliance with stricter government regulations is requiring the storage of large amounts of selected data, such as securities and medical data, together with procedures for timely and verifiable retrieval of this data from the data storage system.</p>
<p id="p-0008" num="0007">Due to rapidly increasing processor performance and disk storage size, data is increasingly stored on computer-based data storage systems, and, particularly, disk drives. However, while the storage capacity on disk drives has progressed rapidly, the ability to locate, access, and retrieve selected data has not progressed at the same rate. In particular, once selected data is located in a data storage system, the retrieval of the data is still limited by the performance of the disk head to write or read the data to or from the disk, as well as the bandwidth of the communication channels used to transfer the data into or out of the data storage system.</p>
<p id="p-0009" num="0008">Prior art data storage systems primarily based the reliability of the data storage system on the hardware utilized by the system. Thus, many prior art storage systems often used highly configured data storage systems with costly hardware and inflexible architectures to attempt to manage the storage and retrieval of data in large data storage systems. If a component failed, a system administrator was often immediately notified to repair or replace the component to prevent failure of the system. Consequently, one or more system administrators were sometimes needed to maintain the hardware, and thus the reliability of the data storage system.</p>
<p id="p-0010" num="0009">Additionally, most prior art data storage systems permitted modification of data stored on the data storage system. Thus, to maintain coherent data, these prior art data storage systems often utilized lock managers that prevented concurrent modification of stored data. Disadvantageously, the lock managers often became a bottleneck in the data storage system.</p>
<p id="p-0011" num="0010">Further, if a user desired to execute an application using data stored on a prior art data storage system, the data had to be located on the data storage system, transferred from the data storage system to the user's system, and then the application could be executed using the transferred data on the user's system. When large amounts of data were requested, data transfer was often a lengthy process due to bandwidth limitations of the communications channels used to transfer the data. Additionally, once the user received the data, the user was limited to the processing capabilities of their computer system.</p>
<heading id="h-0002" level="1">SUMMARY OF THE INVENTION</heading>
<p id="p-0012" num="0011">In accordance with one embodiment of the invention, a method includes determining a garbage collection list for a data storage structure of a node in a data storage system, the garbage collection list including one or more layout map identifiers (IDs) for garbage collection. A data fragment stored on the data storage structure is located at a first location and a layout map ID associated with the data fragment is determined. A determination is made whether the layout map ID associated with the data fragment matches a layout map ID for garbage collection in the garbage collection list. If the layout map ID associated with the data fragment matches a layout map ID for garbage collection in the garbage collection list, a determination is made whether the data fragment is present at a second location on the data storage system. In one embodiment, the second location is a disk mask location. If the data fragment is present at a second location on the data storage system, the data fragment at the first location is determined to be a garbage fragment and deleted from the data storage system.</p>
<?BRFSUM description="Brief Summary" end="tail"?>
<?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?>
<description-of-drawings>
<heading id="h-0003" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading>
<p id="p-0013" num="0012">The accompanying drawings, which are incorporated in, and constitute a part of this specification, illustrate embodiments of the invention, and together with the description, serve to explain the invention. In the drawings, the same reference numbers are used to denote similar components in the various embodiments.</p>
<p id="p-0014" num="0013">In the drawings:</p>
<p id="p-0015" num="0014"><figref idref="DRAWINGS">FIG. 1</figref> illustrates one example of a method for distributively storing data objects on a data storage system in accordance with one embodiment of the invention;</p>
<p id="p-0016" num="0015"><figref idref="DRAWINGS">FIG. 2</figref> illustrates a diagram of a distributed data storage system on which the method of <figref idref="DRAWINGS">FIG. 1</figref> is implemented for distributively storing data in accordance with one embodiment of the invention;</p>
<p id="p-0017" num="0016"><figref idref="DRAWINGS">FIG. 3</figref> illustrates one example of a peer node computer system in accordance with one embodiment of the invention;</p>
<p id="p-0018" num="0017"><figref idref="DRAWINGS">FIG. 4</figref> illustrates a functional block diagram of a symmetric storage system application of a peer node computer system in accordance with one embodiment of the invention;</p>
<p id="p-0019" num="0018"><figref idref="DRAWINGS">FIG. 5</figref> illustrates a process flow diagram of a method for dividing a data object into data fragments with the generation of parity fragments in accordance with one embodiment of the invention;</p>
<p id="p-0020" num="0019"><figref idref="DRAWINGS">FIG. 6</figref> illustrates the method of <figref idref="DRAWINGS">FIG. 5</figref> in accordance with one embodiment of the invention;</p>
<p id="p-0021" num="0020"><figref idref="DRAWINGS">FIG. 7</figref> illustrates a block diagram of an extensible metadata cache in accordance with one embodiment of the invention;</p>
<p id="p-0022" num="0021"><figref idref="DRAWINGS">FIG. 8</figref> illustrates a functional diagram of modules utilized by the object archive module of <figref idref="DRAWINGS">FIG. 4</figref> in the storage, retrieval, and recovery of data objects in accordance with one embodiment of the invention;</p>
<p id="p-0023" num="0022"><figref idref="DRAWINGS">FIG. 9</figref> illustrates an example of a layout map for a data object on a data storage system in which the number of peer node computer systems is less than the value of 2(N+M) in accordance with one embodiment of the invention;</p>
<p id="p-0024" num="0023"><figref idref="DRAWINGS">FIG. 10</figref> illustrates a process flow diagram of a method for generating a layout map for a data object in accordance with one embodiment of the invention;</p>
<p id="p-0025" num="0024"><figref idref="DRAWINGS">FIG. 11</figref> illustrates an example of a layout map for a data object on a data storage system in which the number of nodes is not less than the value of 2(N+M) in accordance with one embodiment of the invention;</p>
<p id="p-0026" num="0025"><figref idref="DRAWINGS">FIG. 12</figref> illustrates a process flow diagram of method for retrieving a data object stored on a data storage system in accordance with one embodiment of the present invention;</p>
<p id="p-0027" num="0026"><figref idref="DRAWINGS">FIG. 13</figref> illustrates a process flow diagram of a method for recovery of data objects on a data storage system in accordance with one embodiment of the invention; and</p>
<p id="p-0028" num="0027"><figref idref="DRAWINGS">FIG. 14</figref> illustrates a process flow diagram of a method for garbage collection of garbage fragments on a data storage system in accordance with one embodiment of the invention.</p>
</description-of-drawings>
<?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?>
<?DETDESC description="Detailed Description" end="lead"?>
<heading id="h-0004" level="1">DETAILED DESCRIPTION</heading>
<p id="p-0029" num="0028">The invention will now be described in reference to the accompanying drawings. The same reference numbers may be used throughout the drawings and the following description to refer to the same or like parts.</p>
<p id="p-0030" num="0029">In accordance with one embodiment, referring to <figref idref="DRAWINGS">FIG. 14</figref>, a method <b>1400</b> includes determining a garbage collection list for a data storage structure of a node in a data storage system, the garbage collection list including one or more layout map identifiers (IDs) for garbage collection (operation <b>1404</b>). The data storage structure is scanned (operation <b>1408</b>), and a data fragment stored on the data storage structure is located at a first location (operation <b>1410</b>. A layout map ID associated with the data fragment is determined (operation <b>1412</b>), and a determination is made whether the layout map ID associated with the data fragment matches a layout map ID for garbage collection in the garbage collection list (operation <b>1414</b>).</p>
<p id="p-0031" num="0030">If the layout map ID associated with the data fragment matches a layout map ID for garbage collection in the garbage collection list (“YES”), a determination is made whether the data fragment is present at a second location on the data storage system (operation <b>1416</b>). In one embodiment, the second location is a disk mask location. If the data fragment is present at a second location on the data storage system (“YES”), the data fragment at the first location is determined to be a garbage fragment and deleted from the data storage system (operation <b>1418</b>).</p>
<p id="p-0032" num="0031">Alternatively, if the data fragment does not match one of the layout map IDs in the garbage collection list, or is not present at the second location, the data fragment is not deleted. A determination is made whether the scan of the data storage structure is complete (operation <b>1422</b>). If the scan is not complete, the method returns to operation <b>1408</b>, otherwise the method is exited (operation <b>1420</b>).</p>
<p id="p-0033" num="0032">Method <b>1400</b> is further described herein as implemented on a distributed data storage system <b>200</b> with reference to <figref idref="DRAWINGS">FIG. 2</figref>. Those of skill in the art can recognize that method <b>1400</b> can be implemented on other distributed data storage systems that can support the operations of method <b>1400</b> as further described herein.</p>
<p id="p-0034" num="0033"><figref idref="DRAWINGS">FIG. 2</figref> illustrates a diagram of a distributed data storage system <b>200</b> on which method <b>1400</b> is utilized for removing garbage data object fragments in accordance with one embodiment of the invention. As illustrated, in one embodiment, intelligent data storage system <b>200</b>, hereinafter referred to as storage system <b>200</b>, includes a plurality of peer node computer systems <b>202</b>A-<b>202</b><i>n</i>, hereinafter referred to as nodes <b>202</b>A-<b>202</b><i>n</i>, interconnected by a storage system network <b>208</b>. In one embodiment, each node <b>202</b>A-<b>202</b><i>n </i>includes a network protocol interface that allows data to be stored, retrieved, and operated on in storage system <b>200</b> through any of nodes <b>202</b>A-<b>202</b><i>n</i>. Further, in one embodiment, each node <b>202</b>A-<b>202</b><i>n </i>is assigned a different IP address.</p>
<p id="p-0035" num="0034">In the present embodiment, each node <b>202</b>A-<b>202</b><i>n </i>includes a symmetric storage system application <b>204</b> including method <b>1400</b> connected to locally attached storage <b>206</b>A-<b>206</b><i>n</i>, respectively. For example, symmetric storage system application <b>204</b> is connected to locally attached storage <b>206</b>A in node <b>202</b>A. In storage system <b>200</b>, symmetric storage system application <b>204</b> is symmetric, i.e., the same, across all of nodes <b>202</b>A-<b>202</b><i>n</i>, however, the hardware components that make up each node <b>202</b>A-<b>202</b><i>n</i>, for example, locally attached storage <b>206</b>A, need not be symmetric.</p>
<p id="p-0036" num="0035">Each node <b>202</b>A-<b>202</b><i>n </i>is a complete compute and storage unit and includes all the software required to run storage system <b>200</b>. In one embodiment, the group of one or more nodes <b>202</b>A-<b>202</b><i>n</i>, for example, sixteen nodes, are termed a cell, and management of the cell is by any of nodes <b>202</b>A-<b>202</b><i>n </i>(at any one time), herein termed a master node. In one embodiment, selection of a master node is determined using an election process. Election processes are well known to those of skill in the art and are not further described herein for clarity of description of the present invention.</p>
<p id="p-0037" num="0036">In one embodiment, each node <b>202</b>A-<b>202</b><i>n </i>provides status information over storage system network <b>208</b> that is received by the other nodes and used to build and maintain individual views of the cell, herein termed a cell view. In one embodiment, a cell view includes information about whether a node <b>202</b>A-<b>202</b><i>n </i>is active and available for storage of data and about which node <b>202</b>A-<b>202</b><i>n </i>is the master node. In other embodiments, the status information provided by each node <b>202</b>A-<b>202</b><i>n </i>and the cell view includes additional information, such as the availability of individual storage structures in each locally attached storage <b>206</b>A-<b>206</b><i>n</i>, and data storage levels of each locally attached storage <b>206</b>A-<b>206</b><i>n. </i></p>
<p id="p-0038" num="0037">When a node failure is detected in the cell view, each affected node <b>202</b>A-<b>202</b><i>n </i>in storage system <b>200</b> independently starts a recovery process to reconstruct data stored on the failed node or on a locally attached storage allowing distributed and balanced recovery of data in which the storage reliability of a data object is maintained.</p>
<p id="p-0039" num="0038">In <figref idref="DRAWINGS">FIG. 2</figref>, some of nodes <b>202</b>A-<b>202</b><i>n </i>can be active and available, herein termed on-line, while some others of nodes <b>202</b>A-<b>202</b><i>n </i>may not be actively available, herein termed off-line. Any of nodes <b>202</b>A-<b>202</b><i>n </i>can be selectively brought on-line or taken off-line in storage system <b>200</b> as needed. In the present description, a node <b>202</b>A-<b>202</b><i>n </i>is assumed to be on-line and available unless otherwise specified. Those of skill in the art can recognize that in other embodiments the status of nodes <b>202</b>A-<b>202</b><i>n </i>can be differently defined, for example as available, unavailable, on, off, waiting, updating, or error.</p>
<p id="p-0040" num="0039">In one embodiment, each of nodes <b>202</b>A-<b>202</b><i>n </i>is a field replaceable unit (FRU). In one embodiment, when a new node is added to storage system <b>200</b>, the new node is automatically recognized by storage system <b>200</b> and any needed applications, such as symmetric storage system application <b>204</b>, is automatically loaded on to the new node from one of nodes <b>202</b>A-<b>202</b><i>n</i>. The new node is then registered with storage system <b>200</b>, any needed formatting is performed, and any data transfer to the new node occurs.</p>
<p id="p-0041" num="0040">Data is stored on storage system <b>200</b> as data objects. Herein a data object is data of a finite length that is separately identifiable from other data objects and is transferable to storage system <b>200</b>, for example, a text file, an image file, or a program file, among others.</p>
<p id="p-0042" num="0041">In one embodiment, a layout map ID is generated and associated with a data object. The layout map ID is used to determine a layout map for distributed placement of the data object on storage system <b>200</b>.</p>
<p id="p-0043" num="0042">In one embodiment, an object identifier (ID) is generated and assigned to each data object. In one embodiment, the object ID is a unique identifier based on an intrinsic property of the data object, such as the content of the data object, which identifies the data object and provides content addressability for the data object. In another embodiment, the object ID is randomly generated and not based on content. In a further embodiment, the object ID is based on a randomly generated value and includes the layout map ID used in placement of the data object on storage system <b>200</b>. In one embodiment, the data object is divided into data fragments to permit distributed placement of the data object on storage system <b>200</b>.</p>
<p id="p-0044" num="0043">During fragmentation of a data object, parity fragments are also generated in accordance with external criteria, such as a desired reliability of storage. The data fragments and the parity fragments are distributively stored on storage system <b>200</b> in accordance with the layout map and associated with the object ID.</p>
<p id="p-0045" num="0044">System metadata is also generated during fragmentation of a data object, and at least a portion of the system metadata is stored with each data fragment and parity fragment and is used to reconstruct the data object. Metadata generated from other sources, termed extended metadata, can also be associated with a data object. Thus, although a data object has a single object ID, several metadata can be associated with the object ID.</p>
<p id="p-0046" num="0045">In one embodiment, a unique metadata object identifier (ID) is generated for each metadata associated with a data object. In one embodiment, selected metadata are indexed and stored in one or more metadata caches to provide enhanced searching and retrieval of data objects on storage system <b>200</b>.</p>
<p id="p-0047" num="0046">In one embodiment, storage system <b>200</b> includes mechanisms for uploading an application, herein termed a disklet, onto storage system <b>200</b> and executing the disklet on storage system <b>200</b> using data objects that are being written to, being read from, or are stored on storage system <b>200</b>.</p>
<p id="p-0048" num="0047">In the present embodiment, access to storage system <b>200</b>, and, in particular, to nodes <b>202</b>A-<b>202</b><i>n</i>, is via a switch <b>210</b>. In some embodiments, one or more switches <b>210</b> are utilized, for example, to provide redundancy or back-up in the event of failure of a switch <b>210</b>.</p>
<p id="p-0049" num="0048">In one embodiment, switch <b>210</b> is configured to support communications on two networks: an external network for external traffic between storage system <b>200</b> and external clients, such as computer servers <b>212</b> and <b>220</b>, and computer system <b>216</b> on external network <b>218</b>; and an internal network, i.e., storage system network <b>208</b>, for internal traffic between nodes <b>202</b>A-<b>202</b><i>n. </i></p>
<p id="p-0050" num="0049">In one embodiment, switch <b>210</b> is configured to present two IP addresses to the external network: a virtual IP address for client access to storage system <b>200</b>; and, an administrative IP address used to access both switch <b>210</b> and a node <b>202</b>A-<b>202</b><i>n </i>designated as the master node. The administrative IP address is further used to permit administration of storage system <b>200</b>, such as by a system administrator, for example on computer system <b>214</b>. In this embodiment, although each of nodes <b>202</b>A-<b>202</b><i>n </i>has an associated IP address, nodes <b>202</b>A-<b>202</b><i>n </i>are abstracted from clients on the external network, allowing nodes <b>202</b>A-<b>202</b><i>n</i>, to be viewed as a single entity.</p>
<p id="p-0051" num="0050">Requests, such as store and retrieve requests, received by switch <b>210</b> are sent to a node <b>202</b>A-<b>202</b><i>n </i>in storage system <b>200</b> using a specified schema, such as according to a switching table utilized by switch <b>210</b>. In one embodiment, the switching table is periodically updated, e.g., reconfigured, by one or more of nodes <b>202</b>A-<b>202</b><i>n </i>to dynamically effect a desired load spreading on storage system <b>200</b>.</p>
<p id="p-0052" num="0051">In an alternative embodiment, rather than switch <b>210</b> presenting a single IP address to external clients, each node <b>202</b>A-<b>202</b><i>n </i>presents its assigned IP address to the external network via switch <b>210</b>, and, rather than load spreading occurring at switch <b>210</b>, a basic load spreading mechanism is included with a client API (resident at the client) in order to spread the load among nodes <b>202</b>A-<b>202</b><i>n</i>. In one embodiment, a listing of nodes <b>202</b>A-<b>202</b><i>n </i>is passed to the client API in order to effect the load spreading.</p>
<p id="p-0053" num="0052"><figref idref="DRAWINGS">FIG. 3</figref> illustrates one example of peer node computer system <b>202</b>A in accordance with one embodiment of the invention. Although the present embodiment is described with reference to peer node computer system <b>202</b>A, herein node <b>202</b>A, the description is applicable to any of nodes <b>202</b>A-<b>202</b><i>n</i>. It is to be understood that the present example is chosen for illustrative purposes only, and that other configurations and hardware are possible, in particular, locally attached storage <b>106</b>A can be formed of fewer or greater numbers of storage structures, e.g., disk <b>1</b>-disk n, and with individually different storage capacities.</p>
<p id="p-0054" num="0053">In <figref idref="DRAWINGS">FIG. 3</figref>, in one embodiment, node <b>202</b>A includes one or more network interface(s) <b>312</b> which provide connectivity to network <b>208</b> (including switch(es) <b>210</b>). In embodiments in which one or more switch(es) <b>210</b> are present in storage system <b>200</b>, one or more corresponding network interface(s) <b>312</b> can be utilized on node <b>202</b>A.</p>
<p id="p-0055" num="0054">In the present embodiment, node <b>202</b>A further includes: a processor <b>302</b>; a memory <b>304</b>; an operating system <b>306</b>; a virtual machine platform <b>308</b>; a file management system <b>310</b>; symmetric storage system application <b>204</b>; and locally attached storage <b>206</b>A. In one embodiment, locally attached storage <b>206</b>A includes one or more storage structures <b>314</b>, such as a finite number of disks <b>1</b>-<i>n</i>, for example, four disks.</p>
<p id="p-0056" num="0055">In the present embodiment, node <b>202</b>A is organized to be an ergonomically acceptable field replaceable unit (FRU), for example, in one embodiment, a 1U form factor FRU.</p>
<p id="p-0057" num="0056"><figref idref="DRAWINGS">FIG. 4</figref> illustrates a functional block diagram of symmetric storage system application <b>204</b> of node <b>202</b>A in accordance with one embodiment of the invention. As illustrated in <figref idref="DRAWINGS">FIG. 4</figref>, in one embodiment, symmetric storage system application <b>204</b> includes: a core module <b>402</b>; an interface module <b>404</b>; a disklet module <b>406</b>; a metadata module <b>408</b>; an object archive module <b>410</b>; and, a node management module <b>412</b>.</p>
<p id="p-0058" num="0057">In one embodiment, node management module <b>412</b> controls modules <b>402</b>, <b>404</b>, <b>406</b>, <b>408</b>, and <b>410</b> of symmetric storage system application <b>204</b>, and controls storage system <b>200</b> in instances when node <b>202</b>A is designated, e.g., elected, as the master node. In one embodiment, functions utilized in controlling storage system <b>200</b> can be embodied as a separate cell management module (not shown) in symmetric storage application <b>204</b>, and the cell management module is inactive until activated by the designation, e.g., election of the node as the master node.</p>
<p id="p-0059" num="0058">Node management module <b>412</b> generates and maintains an overall view of storage system <b>200</b>, herein termed a cell view, that includes information about nodes <b>202</b>A-<b>202</b><i>n </i>present in the cell and about which node is the currently acting master node. Based on changes to the cell view, node management module <b>412</b> can independently start a recovery process to reconstruct data stored on a failed node <b>202</b>B-<b>202</b><i>n </i>or stored on a failed storage structure of locally attached storage device <b>206</b>A. Node management module <b>412</b> can further independently start a garbage collection process to remove data from the node (node <b>202</b>A), such as duplicate data fragments.</p>
<p id="p-0060" num="0059">In the present embodiment, node management module <b>412</b> includes a monitoring function and a management function. Node management module <b>412</b> monitors modules <b>402</b>, <b>404</b>, <b>406</b>, <b>408</b>, and <b>410</b> to gather information about the operational status of node <b>202</b>A and storage system <b>200</b>.</p>
<p id="p-0061" num="0060">Node management module <b>412</b> has hooks, i.e., communication links, into interface module <b>404</b>, disklet module <b>406</b>, metadata module <b>408</b>, and object archive module <b>410</b> to monitor each module in node <b>202</b>A. Node management module <b>412</b> also communicates with core module <b>402</b> and monitors core module <b>402</b> for any events, such as error codes generated during the monitoring of the hardware. In the event core module <b>402</b> does not have the capability to perform hardware monitoring and event generation, node management module <b>412</b> includes mechanisms that permit it to assume these functions.</p>
<p id="p-0062" num="0061">The monitoring function of node management module <b>412</b> generates events, as needed, that the management function acts on. The management function of node management module <b>412</b> includes response strategies for responding to different events, such as error codes, and executes the response strategies based on the event, for example, harvesting data, and rebooting a node, among others.</p>
<p id="p-0063" num="0062">In one embodiment, the monitoring of modules <b>402</b>, <b>404</b>, <b>406</b>, <b>408</b> and <b>410</b> is implemented using a near stateless communication process termed a shared mailbox, also conventionally termed a distributed mailbox, hints, or heartbeats. In shared mailbox communications each module <b>402</b>, <b>404</b>, <b>406</b>, <b>408</b>, <b>410</b>, and <b>412</b> sends a “heartbeat” that indicates its status at a designated interval or time. Each module <b>402</b>, <b>404</b>, <b>406</b>, <b>408</b>, <b>410</b>, and <b>412</b> does not have to be individually queried as to its status, but rather the “heartbeats” are monitored for any changes that require a response to be generated. Thus, each module <b>402</b>, <b>404</b>, <b>406</b>, <b>408</b>, and <b>410</b> is generally able to maintain its status without having to lock on to any particular process to provide its status. In one embodiment, the “heartbeat” of node management module <b>412</b> is monitored by a node management module of another node. For example, the “heartbeat” of node management module <b>412</b> in node <b>202</b>A is monitored by a node management module in node <b>202</b>B.</p>
<p id="p-0064" num="0063">As earlier described, node management module <b>412</b> communicates with core module <b>402</b> and monitors core module <b>402</b> for any events, such as error codes generated during the monitoring of the hardware. In one embodiment, core module <b>402</b> includes information about the hardware of node <b>202</b>A, such as the number, types, and layout of disks, the number and types of communication channels, processor <b>302</b>, and network interface(s) <b>312</b>.</p>
<p id="p-0065" num="0064">Core module <b>402</b> also includes information about the operating system and other applications utilized on storage system <b>200</b> on node <b>202</b>A. For example, referring to node <b>202</b>A (<figref idref="DRAWINGS">FIG. 2</figref>), core module <b>402</b> includes information about operating system <b>306</b>, virtual machine platform <b>308</b>, and file management system <b>310</b>. In some embodiments, core module <b>402</b> monitors operating system <b>306</b>, virtual machine platform <b>308</b>, and file management system <b>310</b>.</p>
<p id="p-0066" num="0065">Additionally, core module <b>402</b> includes a series of drivers that allow instructions to be passed between symmetric storage system application <b>204</b> and the hardware of node <b>202</b>A.</p>
<p id="p-0067" num="0066">In one embodiment, interface module <b>404</b> provides a transfer protocol and overarching application program interface (API) to access storage system <b>200</b>. In one embodiment, the transfer protocol is Hypertext Transfer Protocol (HTTP), however in other embodiments, other protocols can be used, such as the Network File System (NFS) protocol and the Web-based Distributed Authoring and Versioning (WebDAV) protocol. In one embodiment, the language binding for the API is Java@, however in other embodiments, other language bindings can be used, for example, a C language binding. Transfer protocol, application program interface, and language binding are terms well known to those of skill in the art and are not further described herein to avoid detracting from the description of the present invention.</p>
<p id="p-0068" num="0067">Interface module <b>404</b> receives requests to store, e.g., write, read, and operate on data on storage system <b>200</b>. Interface module <b>404</b> receives any metadata provided with data to be stored on storage system <b>200</b>. Interface module <b>404</b> also receives disklets for storage and execution on storage system <b>200</b>.</p>
<p id="p-0069" num="0068">In one embodiment, disklet module <b>406</b> manages disklets stored and executing on storage system <b>200</b>. A disklet is code, e.g., an application, written by a user against a disklet API, such as a Java® API, for storage and execution on storage system <b>200</b>. In one embodiment, the disklet is precompiled code, such as Java® byte code. In one embodiment, a disklet is developed on a user's system, such as computer systems <b>214</b> or <b>216</b>, and uploaded to storage system <b>200</b>.</p>
<p id="p-0070" num="0069">The disklet is stored on storage system <b>200</b> and a handle to the disklet is generated by disklet module <b>406</b> and returned to the user. The user uses the handle in a request to execute the disklet using data that is stored on, being read from, or being written to storage system <b>200</b>. Thus, a disklet is executed on storage system <b>200</b> and the results returned to the user (if so desired). Thus, data does not have to first be retrieved and transferred to the user in order to execute the application.</p>
<p id="p-0071" num="0070">In one embodiment, disklet module <b>406</b> also implements disklet security to prevent disklets from inadvertently or maliciously damaging storage system <b>200</b>. In one embodiment, Java® sandboxing is used to set limits on what operations a disklet can perform and to externally limit the amount of processing and memory a disklet can consume.</p>
<p id="p-0072" num="0071">In one embodiment, a disklet is first read by a security application of disklet module <b>406</b> that determines whether the disklet is approved for use on storage system <b>200</b>. In one embodiment, an approved disklet is identified as approved, e.g., signed, and allowed to be used on storage system <b>200</b>. A non-approved disklet is not identified as approved, e.g., not signed, and can be deleted, quarantined, or processed for further security evaluation. In some embodiments, a notification is generated advising a user that a disklet is not approved.</p>
<p id="p-0073" num="0072">In one embodiment, object archive module <b>410</b> distributively stores, retrieves, and reconstructs data objects in storage system <b>200</b>. Retrieval and reconstruction of data objects in storage system <b>200</b> is based upon the mechanism of storage implemented by object archive module <b>410</b>.</p>
<p id="p-0074" num="0073">In storing a data object, object archive module <b>410</b> determines placement, fragmentation, and storage of a data object. During placement, object archive module <b>410</b> receives a data object and determines a set of storage structures, such as hard disk drives, to be used in distributively storing fragments of the data object, including data fragments and parity fragments.</p>
<p id="p-0075" num="0074">In one embodiment, object archive module <b>410</b> generates a random, and reproducible layout map identifier (ID) that is assigned to the data object. While non-random layout map IDs may be used, assignment of randomized layout map IDs allows data objects to be evenly distributed among storage structures of locally attached storage <b>206</b>A-<b>206</b><i>n </i>in storage system <b>200</b>, even if some of nodes <b>202</b>A-<b>202</b><i>n </i>are removed or added.</p>
<p id="p-0076" num="0075">The layout map ID is used to generate a layout map which represents possible distributed layouts for that data object in storage system <b>200</b> based on the current availability of storage structures, for example storage disks, in locally attached storage <b>206</b>A-<b>206</b><i>n</i>. In one embodiment, object archive <b>410</b> maintains a current view of the available storage structures in storage system <b>200</b>, herein termed a disk mask, for use in generating layout maps.</p>
<p id="p-0077" num="0076">In one embodiment, a layout map indicates an initial distribution, or initial layout, of fragments for a data object in selected storage structures of one or more of locally attached storage <b>206</b>A-<b>206</b><i>n</i>, as well as alternative locations for fragments, such as when a storage structure in the initial layout fails or otherwise becomes unavailable or when a previously unavailable storage structure becomes available.</p>
<p id="p-0078" num="0077">In fragmentation of a data object, in one embodiment, object archive module <b>410</b> divides the data object into data fragments and further generates parity fragments as further described herein with reference to <figref idref="DRAWINGS">FIGS. 5 and 6</figref>.</p>
<p id="p-0079" num="0078"><figref idref="DRAWINGS">FIG. 5</figref> illustrates a process diagram of a method <b>500</b> for dividing a data object into data fragments with the generation of parity fragments in accordance with to one embodiment of the invention. <figref idref="DRAWINGS">FIG. 6</figref> illustrates method <b>500</b> in accordance with one embodiment of the invention. In one embodiment, method <b>500</b> (<figref idref="DRAWINGS">FIG. 5</figref>) is implemented by object archive module <b>410</b> (<figref idref="DRAWINGS">FIG. 4</figref>) of symmetric storage system application <b>204</b> (<figref idref="DRAWINGS">FIG. 3</figref>). Referring initially to <figref idref="DRAWINGS">FIG. 5</figref>, in one embodiment, from an ENTER operation <b>502</b>, processing transitions to a RECEIVE DATA OBJECT operation <b>504</b>.</p>
<p id="p-0080" num="0079">In RECEIVE DATA OBJECT operation <b>504</b>, referring additionally to <figref idref="DRAWINGS">FIGS. 4</figref>, <b>5</b>, and <b>6</b>, object archive module <b>410</b> receives a data object <b>602</b> for storage in storage system <b>200</b> (<figref idref="DRAWINGS">FIG. 2</figref>). In one embodiment, object archive module <b>410</b> generates a unique object identifier (ID) for the data object. In one embodiment, the object ID is generated based upon an intrinsic property of the data object. In one embodiment, the object ID is generated based upon the content of the data object.</p>
<p id="p-0081" num="0080">In one embodiment, object archive module <b>410</b> generates the object ID for data object <b>602</b> using an SHA-1 hash of the data object contents. In other embodiments, the object ID can be generated based upon other intrinsic properties of the data object that result in a unique object ID. Uniquely identifying a data object based on its content or other intrinsic property of the data object enables the unique identifier, i.e., the object ID, to be used as an identifier of the data object in storage system <b>200</b> and provides content addressability. Upon receipt of data object <b>602</b> and generation of the object ID, processing transitions from RECEIVE DATA OBJECT operation <b>504</b> to a DIVIDE INTO DATA BLOCKS operation <b>506</b>.</p>
<p id="p-0082" num="0081">In DIVIDE INTO DATA BLOCKS operation <b>506</b>, object archive module <b>410</b> divides data object <b>602</b> into one or more data blocks <b>604</b>. Herein a data block is a portion of a data object, such as a logical data size that operating system <b>306</b> uses to read or write files, for example, 384 KB. In one embodiment, based on the object ID, any of data blocks <b>604</b> can be distributed to any of the other nodes <b>202</b>B-<b>202</b><i>n </i>in storage system <b>200</b> and the remainder of method <b>500</b> performed at those other nodes <b>202</b>B-<b>202</b><i>n</i>. Upon division of data object <b>602</b> into data blocks <b>604</b>, processing transitions from DIVIDE INTO DATA BLOCKS operation <b>506</b> to a DIVIDE INTO DATA FRAGMENTS operation <b>508</b>.</p>
<p id="p-0083" num="0082">In DIVIDE INTO DATA FRAGMENTS operation <b>508</b>, object archive module <b>410</b> divides each of data blocks <b>604</b> into one or more data fragments <b>606</b>. Herein a data fragment is a portion of a data block, such as 64 KB on disk. Upon division of each of data blocks <b>604</b> into data fragments <b>606</b>, processing transitions from DIVIDE INTO DATA FRAGMENTS operation <b>508</b> to a GENERATE PARITY FRAGMENTS operation <b>510</b>.</p>
<p id="p-0084" num="0083">In GENERATE PARITY FRAGMENTS operation <b>510</b>, object archive module <b>410</b> applies an erasure coding algorithm, such as a Reed-Solomon erasure coding algorithm, to data fragments <b>606</b> from each of data blocks <b>604</b> to generate one or more parity fragments <b>608</b>. In one embodiment, any parity fragment <b>608</b> can be used to generate any data fragment <b>606</b> of data block <b>604</b> of data object <b>602</b>. In one embodiment, the erasure coding algorithm utilizes an external criterion, such as a desired reliability of storage, in generating parity fragments <b>608</b>. Upon generation of parity fragments <b>608</b>, processing transitions from GENERATE PARITY FRAGMENTS operation <b>510</b> to an EXIT operation <b>512</b> with processing exiting method <b>500</b>.</p>
<p id="p-0085" num="0084">In an alternative embodiment, object archive module <b>410</b> divides data object <b>602</b> directly into data fragments <b>606</b> and the erasure coding algorithm is applied to data fragments <b>606</b> to generate parity fragments <b>608</b>, e.g., operation <b>506</b> is not performed. Although division of data object <b>602</b> into data blocks <b>604</b> is not a required intermediate operation to division of data object <b>602</b> into data fragments <b>606</b>, it enables parallelization of operations <b>508</b> and <b>510</b> by enabling data blocks <b>604</b> to be distributed to other nodes <b>202</b>B-<b>202</b><i>n </i>in which the generation of data fragments <b>606</b> and parity fragments <b>608</b> can occur.</p>
<p id="p-0086" num="0085">Following generation of the data fragments and parity fragments (method <b>500</b>), object archive module <b>410</b> stores the data fragments of the data object and the parity fragments associated with the data object to one or more storage structures, e.g., storage disks, in storage system <b>200</b>, in accordance with the layout map associated with the data object. In one embodiment, object archive <b>410</b> periodically evaluates storage structures in locally attached storage <b>206</b>A, for example by checking disk segments of disks, and reporting problems to node management module <b>412</b> (<figref idref="DRAWINGS">FIG. 4</figref>).</p>
<p id="p-0087" num="0086">During placement, fragmentation, and storage of a data object, object archive module <b>410</b> also generates metadata associated with the data object. Metadata is data that provides a description of a data object stored in storage system <b>200</b>, and is used to perform searches and retrieve data in storage system <b>200</b>. In particular, object archive module <b>410</b> generates system metadata which is metadata that is stored, e.g., encapsulated, as part of each of the data fragments and parity fragments of that data object.</p>
<p id="p-0088" num="0087">In one embodiment, system metadata includes information that is utilized by object archive module <b>410</b> to retrieve and reconstruct a data object once a data object fragment of that data object is located. Examples of system metadata include number of data fragments, number of parity fragments, the layout for a data object, the data fragment length, the size of a data object, and the create time for a data object. System metadata generated by object archive module <b>410</b> are also provided to metadata module <b>408</b>.</p>
<p id="p-0089" num="0088">In one embodiment, metadata module <b>408</b> receives metadata from metadata generators, such as an object archive, e.g., object archive module <b>410</b>, a client, e.g., a client application program interface (API), or a disklet, e.g., an executing disklet. In one embodiment, as earlier described with reference to object archive module <b>410</b>, metadata received from object archive module <b>410</b> is termed system metadata. Other metadata, such as metadata received from a client or generated in accordance with a disklet, is termed extended metadata.</p>
<p id="p-0090" num="0089">Extended metadata received by metadata module <b>408</b> is passed to object archive <b>410</b> for distributed storage on storage system <b>200</b>. In one embodiment, metadata module <b>408</b> locates the layout map ID and provides the layout map ID to object archive module <b>410</b>.</p>
<p id="p-0091" num="0090">Object archive module <b>410</b> determines the placement of the metadata based on the layout map ID. In one embodiment, object archive module <b>410</b> fragments the metadata similar to a data object as earlier described with reference to <figref idref="DRAWINGS">FIGS. 5 and 6</figref> with resultant metadata data fragments and metadata parity fragments. Following generation of the metadata data fragments and metadata parity fragments, object archive module <b>410</b> stores the metadata data fragments and the metadata parity fragments to one or more storage structures, e.g., disks, in storage system <b>200</b>.</p>
<p id="p-0092" num="0091">In one embodiment, metadata module <b>408</b> further indexes selected metadata, e.g., selected from the system metadata and/or extended metadata, into one or more metadata caches. In one embodiment, each metadata cache is an indexed data store of selected metadata. In one embodiment, each metadata cache is extensible.</p>
<p id="p-0093" num="0092">The metadata caches are distributively stored on storage system <b>200</b> and used to enhance searching and retrieval of data objects on storage system <b>200</b>, e.g., by searching the metadata caches rather than the object archive.</p>
<p id="p-0094" num="0093"><figref idref="DRAWINGS">FIG. 7</figref> illustrates a block diagram of an extensible metadata cache <b>700</b> in accordance with one embodiment of the invention. In <figref idref="DRAWINGS">FIG. 7</figref>, extensible metadata cache <b>700</b> includes system metadata fields <b>702</b>, such as system metadata fields <b>702</b>A-<b>702</b><i>n</i>, and extended metadata fields <b>704</b>, such as extended metadata fields <b>704</b>A-<b>704</b><i>n</i>. For example, system metadata fields <b>702</b>A-<b>702</b><i>n </i>can include: an object ID field <b>702</b>A (an object ID of a data object); an “N” field <b>702</b>B (number of data fragments); an “M” field <b>702</b>C (number of parity fragments); and a size field <b>702</b><i>n </i>(size of a data object). Extended metadata fields <b>704</b>A-<b>704</b><i>n</i>, such as for those metadata generated in response to execution of a disklet on storage system <b>200</b>, can include, for example: a patient name field <b>704</b>A; an image type field <b>704</b>B; and a doctor name field <b>704</b><i>n</i>. In one embodiment, the number of extended metadata fields is definable by a system administrator of storage system <b>200</b>.</p>
<p id="p-0095" num="0094">In one embodiment, the metadata caches generated by metadata module <b>408</b> can be replicated on any of nodes <b>202</b>A-<b>202</b><i>n </i>to provide a desired reliability. The metadata caches are scalable with the size of storage system <b>200</b> and can be distributed across nodes <b>202</b>A-<b>202</b><i>n</i>. In some embodiments, metadata module <b>408</b> can further generate metadata caches associated with selected parameters of a data object, such as a type of data object, or with a particular metadata generator.</p>
<p id="p-0096" num="0095"><figref idref="DRAWINGS">FIG. 1</figref> illustrates one example of a method <b>100</b> for distributively storing data objects on a data storage system in accordance with one embodiment of the invention. Referring now particularly to <figref idref="DRAWINGS">FIG. 1</figref> together with <figref idref="DRAWINGS">FIG. 2</figref>, in one embodiment, method <b>100</b> is implemented on storage system <b>200</b> and used for distributively storing data objects in one or more storage structures <b>314</b> (<figref idref="DRAWINGS">FIG. 3</figref>) of locally attached storage <b>206</b>A-<b>206</b><i>n</i>, e.g., disks. As earlier described, each node <b>202</b>A-<b>202</b><i>n </i>maintains a cell view which includes information about whether a node <b>202</b>A-<b>202</b><i>n </i>is active and available for storage of data and about which node <b>202</b>A-<b>202</b><i>n </i>is the master node. In the present embodiment, each cell view also includes information about the availability of individual storage structures <b>314</b>, e.g., individual disks. In the present embodiment, information about the availability of a node <b>202</b>A-<b>202</b><i>n </i>and the individual storage structures <b>314</b> (<figref idref="DRAWINGS">FIG. 3</figref>) of each locally attached storage <b>206</b>A-<b>206</b><i>n </i>in a cell view are changed when the master node approves, e.g., publishes, the changed information.</p>
<p id="p-0097" num="0096">In the present embodiment, each node <b>202</b>A-<b>202</b><i>n </i>also maintains values used in determining how a data object is stored on storage system <b>200</b> including: the cell size, i.e., the number of nodes <b>202</b>A-<b>202</b><i>n </i>in the cell; the node size, i.e., the number of individual storage structures <b>314</b> (<figref idref="DRAWINGS">FIG. 3</figref>), such as disks, in the locally attached storage <b>206</b>A-<b>206</b><i>n </i>for each node <b>202</b>A-<b>202</b><i>n</i>, respectively; the number of data fragments per data object (N); and, the number of parity fragments generated per data object (M). In particular, the values of N and M represent a desired reliability of storage of a data object on storage system <b>200</b>.</p>
<p id="p-0098" num="0097">In one embodiment, the values of N and M, as well as the cell size and the node size are configured when storage system <b>200</b> is initially installed, and do not change unless storage system <b>200</b> is reconfigured for different parameters, for example, by a system administrator. Consequently, changes in the number of operational individual storage structures, e.g., disks, or available nodes <b>202</b>A-<b>202</b><i>n </i>does not affect the cell size or the node size values.</p>
<p id="p-0099" num="0098">In one embodiment, method <b>100</b> is implemented on storage system <b>200</b> to distributively store data on one or more of nodes <b>202</b>A-<b>202</b><i>n </i>in accordance with a desired level of reliability. Method <b>100</b> is implemented by a node <b>202</b>A-<b>202</b><i>n </i>independent of the other nodes. Method <b>100</b> permits the reliable storage of data on storage system <b>200</b> as well as the efficient retrieval and recovery of data on storage system <b>200</b> in which reliability of data storage is maintained as further described herein.</p>
<p id="p-0100" num="0099">In the present embodiment, method <b>100</b> is described as implemented by symmetric storage system application <b>204</b> (<figref idref="DRAWINGS">FIG. 2</figref>), and thus in one embodiment, is part of symmetric storage system application <b>204</b> (<figref idref="DRAWINGS">FIG. 2</figref>), as further described herein with reference to <figref idref="DRAWINGS">FIG. 8</figref>.</p>
<p id="p-0101" num="0100"><figref idref="DRAWINGS">FIG. 8</figref> illustrates a functional diagram of modules utilized by object archive module <b>410</b> in the storage, retrieval, and recovery of data objects in accordance with one embodiment of the invention. As illustrated in <figref idref="DRAWINGS">FIG. 8</figref>, in one embodiment, object archive <b>410</b> utilizes a disk mask module <b>802</b>, a layout module <b>804</b>, and, a recovery module <b>806</b>. In one embodiment, disk mask module <b>802</b>, layout module <b>804</b>, and recovery module <b>806</b> are modules of symmetric storage system application <b>204</b> separate from object archive module <b>410</b> and accessible by object archive module <b>410</b>. However, in other embodiments, some or all of disk mask module <b>802</b>, layout module <b>804</b>, and recovery module <b>806</b> are part of object archive module <b>410</b>. The operations performed by each of these components in storage, retrieval, and recovery of data objects on storage system <b>200</b> are further described herein.</p>
<p id="p-0102" num="0101">Referring now particularly to <figref idref="DRAWINGS">FIG. 1</figref> and <figref idref="DRAWINGS">FIG. 8</figref> together, in one embodiment, a request to store a data object is received at a node <b>202</b>A-<b>202</b><i>n </i>(<figref idref="DRAWINGS">FIG. 2</figref>), for example, at node <b>202</b>A (<figref idref="DRAWINGS">FIG. 2</figref>). In the present embodiment, the request to store a data object is communicated to symmetric storage application <b>204</b> (<figref idref="DRAWINGS">FIG. 2</figref>), and in particular to object archive module <b>410</b>, for example, from interface module <b>404</b> (<figref idref="DRAWINGS">FIG. 4</figref>). Object archive module <b>410</b> initiates a store and enters method <b>100</b> from an ENTER operation <b>102</b>, and processing transitions to a DETERMINE LAYOUT MAP ID operation <b>104</b>.</p>
<p id="p-0103" num="0102">In DETERMINE LAYOUT MAP ID operation <b>104</b>, a layout map identifier (ID) is determined and assigned to the data object. In one embodiment, object archive module <b>410</b> requests a layout map ID and a corresponding layout from layout module <b>804</b> for a data object that is to be stored.</p>
<p id="p-0104" num="0103">A layout map ID is a randomly selected value that is assigned to the data object and is used by layout out module <b>804</b> as a seed in generating a layout map for storage of data object fragments of a data object. The layout map ID is independent of the data object content.</p>
<p id="p-0105" num="0104">In one embodiment, the number of valid layout map IDs is restricted to a finite number to limit the number of layout maps that are generated while still being large enough to provide an acceptable distribution of data object fragments across individual storage structures <b>314</b> (<figref idref="DRAWINGS">FIG. 3</figref>), e.g., disks, in storage system <b>200</b> (<figref idref="DRAWINGS">FIG. 2</figref>). For example, in one embodiment, the number of layout map IDs is approximately 1,000 for a cell size of sixteen (16) nodes <b>202</b>A-<b>202</b><i>n </i>with four (4) individual storage structures <b>314</b> per locally attached storage <b>206</b>A-<b>206</b><i>n. </i></p>
<p id="p-0106" num="0105">In general, random assignment of layout map IDs permits a substantially even storage distribution of data objects among individual storage structures <b>314</b>, e.g., disks, in storage system <b>200</b> even if nodes are removed or added. In one embodiment, the layout map ID assigned to a data object is stored as metadata indexed by metadata module <b>408</b> (<figref idref="DRAWINGS">FIG. 4</figref>) and associated with the data object. In one embodiment, the layout map ID is stored as system metadata with each data object fragment of the data object. From DETERMINE LAYOUT MAP ID operation <b>104</b>, processing transitions to a DETERMINE LAYOUT MAP operation <b>106</b>.</p>
<p id="p-0107" num="0106">In DETERMINE LAYOUT MAP operation <b>106</b>, layout module <b>804</b> utilizes the layout map ID as a seed in generating a layout map. The layout map represents defined layouts for the distributed placement of a data object across one or more storage structures <b>314</b>, e.g., disks, in storage system <b>200</b>.</p>
<p id="p-0108" num="0107"><figref idref="DRAWINGS">FIG. 9</figref> illustrates an example of a layout map <b>902</b> for a data object in accordance with one embodiment of the invention. In one embodiment, layout maps for a layout map ID in a particular cell are based upon the layout map ID, the cell size, the node size, and the maximum (N+M) data object fragments allowed in the cell (per data object).</p>
<p id="p-0109" num="0108">In one embodiment, the maximum value of (N+M) for a cell is not changed once it is established. However, in other embodiments, the maximum value of (N+M) for a cell is modifiable, for example, by a system administrator. In the present embodiment, the maximum value (N+M) does not exceed the number of storage structures <b>314</b>, e.g., disks, in the cell, and in some embodiments, is smaller than the number of nodes <b>202</b>A-<b>202</b><i>n. </i></p>
<p id="p-0110" num="0109">In <figref idref="DRAWINGS">FIG. 9</figref>, layout map <b>902</b> is represented by a matrix of columns and rows, in which the number of columns is equal to (N+M) columns, and the number of rows is equal to the total number of storage structures <b>314</b>, e.g., disks, in the cell, divided by (N+M) (rounded to the closest integer). In the present example, it is assumed that the number of nodes=6, e.g., nodes <b>202</b>A-<b>202</b>F, and (N+M)=2+2 (or 4), and each node has four storage structures <b>314</b>, e.g., disks. The nodes <b>202</b>A-<b>202</b>F are denoted using node identifiers (IDs), for example the capital letters A, B, C, D, E, and F, and storage structures <b>314</b>, e.g., disks, are denoted using storage structure identifiers (IDs), for example the numbers <b>1</b>, <b>2</b>, <b>3</b>, <b>4</b>, which also denote the positions of the storage structures <b>314</b>. Herein storage structure IDs are described as disk identifiers (IDs).</p>
<p id="p-0111" num="0110">Each element of the matrix is an identifier of a specific storage structure <b>314</b>, e.g., disk, in the cell. In the present embodiment, each storage structure <b>314</b>, e.g., disk, is identified by the pair (node ID, disk ID). Thus, a letter with a numeric subscript identifies a storage structure <b>314</b>, e.g., disk, on a particular node, for example A_<b>2</b> identifies disk <b>2</b> on a node A, such as node <b>202</b>A.</p>
<p id="p-0112" num="0111"><figref idref="DRAWINGS">FIG. 10</figref> illustrates a process flow diagram of a method <b>1000</b> for generating a layout map for a data object in accordance with one embodiment of the invention. Herein method <b>1000</b> is described with reference to generation of layout map <b>902</b>, however, method <b>1000</b> is not limited to this example and other layout maps can be generated using method <b>1000</b> as further described herein. From an ENTER operation <b>1002</b>, processing transitions to a GENERATE LIST OF NODE IDs operation <b>1004</b>.</p>
<p id="p-0113" num="0112">In GENERATE LIST OF NODE IDs operation <b>1004</b>, node IDs of nodes <b>202</b>A-<b>202</b><i>n </i>in the cell, e.g., A, B, C, D, E, and F, are randomized to generate a randomly ordered list of node IDs, herein termed List A. Node IDs can be any identifier of a node <b>202</b>A-<b>202</b><i>n </i>used in storage system <b>200</b> so long no two nodes have the same node ID, for example, a network address.</p>
<p id="p-0114" num="0113">In one embodiment, the layout map ID (determined in operation <b>104</b>, <figref idref="DRAWINGS">FIG. 1</figref>) is used as a seed to a random or pseudo-random number generator (so that the sequence is reproducible) to generate List A, for example:</p>
<p id="p-0115" num="0114">List A: D A F E C B.</p>
<p id="h-0005" num="0000">Thus, the size of List A is the total number of nodes <b>202</b>A-<b>202</b><i>n </i>in the cell, e.g., 6. From GENERATE LIST OF NODE IDs operation <b>1004</b>, processing transitions to a GENERATE LIST OF STORAGE STRUCTURE IDs operation <b>1006</b>.</p>
<p id="p-0116" num="0115">In GENERATE LIST OF STORAGE STRUCTURE IDs operation <b>1006</b>, disk IDs, e.g., positions of the disks per node, are randomized to generate a randomly ordered list of disk IDs, herein termed List B. In one embodiment, the layout map ID is used as a seed to a random or pseudo-random number generator (so that the sequence is reproducible) to generate List B, for example:</p>
<p id="p-0117" num="0116">List B: 2 3 1 4.</p>
<p id="h-0006" num="0000">Thus, the size of List B is the total number of storage structures <b>314</b>, e.g., disks, per node. From GENERATE LIST OF STORAGE STRUCTURE IDs operation <b>1006</b>, processing transitions to a LIST A LESS THAN (N+M) check operation <b>1008</b>.</p>
<p id="p-0118" num="0117">In LIST A LESS THAN (N+M) check operation <b>1008</b>, a determination is made whether the total entries in the randomly ordered list of node IDs (List A) are less than the value of (N+M), e.g., the number of data object fragments per data object. When the randomly ordered list of node IDs (List A) is not less than the value of (N+M) (“NO”), from LIST A LESS THAN (N+M) check operation <b>1008</b>, processing transitions to a GENERATE LIST OF FIRST (N+M) NODE IDs operation <b>1012</b>.</p>
<p id="p-0119" num="0118">As operation <b>1004</b> generated a List A of 6 nodes and the value of (N+M)=4, List A is not less than (N+M) (“NO”), and, processing transitions to GENERATE LIST OF FIRST (N+M) NODE IDs operation <b>1012</b>.</p>
<p id="p-0120" num="0119">Otherwise, when the total entries in List A are less than the value of (N+M) (“YES”), from LIST A LESS THAN (N+M) check operation <b>1008</b>, processing transitions to a REPLICATE LIST A operation <b>1010</b>.</p>
<p id="p-0121" num="0120">In REPLICATE LIST A operation <b>1010</b>, the randomly ordered list of node IDs (List A) is replicated (thus extending the entries of List A), and processing transitions to LIST A LESS THAN (N+M) check operation <b>1008</b> as described.</p>
<p id="p-0122" num="0121">When the total entries in List A are not less than the value of (N+M) (“NO”), in GENERATE LIST OF FIRST (N+M) NODE IDs operation <b>1012</b>, the first (N+M) node IDs are selected from List A in the order they appear in List A, to generate a List C. In the present example, selection of the first four (4) node IDs in order from List A produce List C, for example:</p>
<p id="p-0123" num="0122">List C=D A F E.</p>
<p id="h-0007" num="0000">From GENERATE LIST OF FIRST (N+M) NODE IDs operation <b>1012</b> processing transitions to a GENERATE NODE GROUPS operation <b>1014</b>.</p>
<p id="p-0124" num="0123">In GENERATE NODE GROUPS operation <b>1014</b>, node groups are generated for each column of layout map <b>902</b>.</p>
<p id="p-0125" num="0124">In one embodiment, for each column of layout map <b>902</b>, a ordered group of node IDs, herein termed a Node Group, are selected in the order of List C such that:</p>
<p id="p-0126" num="0125">(a) the node ID at an index i of List C is first in a Node Group i, and</p>
<p id="p-0127" num="0126">(b) no node ID is selected twice before all the other node IDs are selected.</p>
<p id="h-0008" num="0000">Further, the size of each Node Group i is limited to:</p>
<p id="h-0009" num="0000">a minimum of (#nodes/(N+M)) for (#nodes)&gt;=2(N+M), and</p>
<p id="h-0010" num="0000">a maximum of (#nodes/(N+M)) for (#nodes)&lt;2(N+M).</p>
<p id="p-0128" num="0127">Given (a) and (b) above, and the minimum and maximum node group size limits imposed above, no node ID appears more than twice when the number of nodes is less than 2(N+M), and no node groups share any nodes when the number of nodes is greater than or equal to 2(N+M).</p>
<p id="p-0129" num="0128">In the present example, the number of node IDs in each node group is limited to a maximum of (6/4)=2 (rounded up to the closest integer). The number of node groups is 4 (operation <b>1012</b>). The first 4 node IDs in List C are placed as the first node IDs in each of the node groups, D in Group 1, A in Group 2, F in Group 3, and E in Group 4.</p>
<p id="p-0130" num="0129">The next node ID in list A (the next node ID in list A following the first (N+M) node IDs, node C, is placed as the second node ID in Group 1, and B as the second node ID in Group 2. As this exhausts List A, the remaining positions of Node Groups 3 and 4 are filled beginning from the start of List A, in order, thus, D is placed as the second node ID in Group 3, and A as the second node ID in Group 4. Thus, one example of a node group set is:</p>
<p id="p-0131" num="0130">Node Group 1=DC</p>
<p id="p-0132" num="0131">Node Group 2=AB</p>
<p id="p-0133" num="0132">Node Group 3=FD</p>
<p id="p-0134" num="0133">Node Group 4=EA,</p>
<p id="h-0011" num="0000">and from GENERATE NODE GROUPS operation <b>1014</b>, processing transitions to an ENTER NODE IDs operation <b>1016</b>.</p>
<p id="p-0135" num="0134">In ENTER NODE IDs operation <b>1016</b>, for each row in the matrix enter the corresponding node ID for each column (in order) beginning at the first node ID listed in Node Group 1 and iterating through the first node ID in each Node Group, e.g., 2, 3, and 4, and wrapping around to the beginning of Node Group 1 and continuing with the second node ID in Node Group 1, and iterating through the second node ID in each Node Group, e.g., 2, 3, and 4. If a selected node has been used as a node ID d times, where d is the number of storage structures <b>314</b>, e.g., disks, per node (e.g., 4), the node is removed from all node groups. Thus, one example of the matrix with the node ID entered is:</p>
<p id="p-0136" num="0135">
<tables id="TABLE-US-00001" num="00001">
<table frame="none" colsep="0" rowsep="0">
<tgroup align="left" colsep="0" rowsep="0" cols="5">
<colspec colname="offset" colwidth="35pt" align="left"/>
<colspec colname="1" colwidth="14pt" align="center"/>
<colspec colname="2" colwidth="77pt" align="center"/>
<colspec colname="3" colwidth="14pt" align="center"/>
<colspec colname="4" colwidth="77pt" align="center"/>
<thead>
<row>
<entry/>
<entry namest="offset" nameend="4" align="center" rowsep="1"/>
</row>
</thead>
<tbody valign="top">
<row>
<entry/>
<entry>D</entry>
<entry>A</entry>
<entry>F</entry>
<entry>E</entry>
</row>
<row>
<entry/>
<entry>C</entry>
<entry>B</entry>
<entry>D</entry>
<entry>A</entry>
</row>
<row>
<entry/>
<entry>D</entry>
<entry>A</entry>
<entry>F</entry>
<entry>E</entry>
</row>
<row>
<entry/>
<entry>C</entry>
<entry>B</entry>
<entry>D</entry>
<entry>A</entry>
</row>
<row>
<entry/>
<entry>C</entry>
<entry>B</entry>
<entry>F</entry>
<entry>E</entry>
</row>
<row>
<entry/>
<entry>C</entry>
<entry>B</entry>
<entry>F</entry>
<entry> E.</entry>
</row>
<row>
<entry/>
<entry namest="offset" nameend="4" align="center" rowsep="1"/>
</row>
</tbody>
</tgroup>
</table>
</tables>
<br/>
Note that node D and node A were selected d times and thus were removed from the Node Groups 1 and 2, respectively, after selection d times. From ENTER NODE IDs operation <b>1016</b>, processing transitions to an ENTER STORAGE STRUCTURE IDs operation <b>1018</b>.
</p>
<p id="p-0137" num="0136">In ENTER STORAGE STRUCTURE IDs operation <b>1018</b>, for each node ID, all matrix entries containing a node ID, for example, A, B, C, D, E, and F, are iterated through in order of appearance in the matrix (for example, beginning at matrix entry identified with storage structure <b>906</b> and proceeding across each row before advancing to the next row and proceeding across that row), and the disk IDs (the storage structure ID) in order of List B are entered with each node ID. Thus, layout map <b>902</b> represents an example of the resulting layout map, and processing transitions from ENTER STORAGE STRUCTURE IDs operation <b>1018</b> to an EXIT operation <b>1020</b>, with processing exiting method <b>1000</b>.</p>
<p id="p-0138" num="0137"><figref idref="DRAWINGS">FIG. 9</figref> illustrates an example in which the number of nodes is less than 2(N+M). <figref idref="DRAWINGS">FIG. 11</figref> illustrates an example in which the number of nodes is not less than the value of 2(N+M), and the removal of nodes from an associated Node Group during operation <b>1016</b> as described with reference to <figref idref="DRAWINGS">FIG. 9</figref> (due to the number of nodes being less than 2(N+M)) is not exhibited.</p>
<p id="p-0139" num="0138"><figref idref="DRAWINGS">FIG. 11</figref> illustrates an example of a layout map <b>1102</b> for a data object in which the number of nodes <b>202</b>A-<b>202</b><i>n </i>is not less than the value of 2(N+M) in accordance with the invention. In this further example, it is assumed that the number of nodes=8, e.g., nodes <b>202</b>A-<b>202</b>H, and (N+M)=2+2 (or 4), i.e., the number of nodes=2(N+M), and each node has four storage structures <b>314</b>, e.g., disks. The node IDs are denoted by the capital letters A, B, C, D, E, F, G and H and the storage structures <b>314</b>, e.g., disks, on each node are denoted by the numbers <b>1</b>, <b>2</b>, <b>3</b>, and <b>4</b>, which also denote the positions of the disks. In this example, no nodes are removed from an associated Node Group.</p>
<p id="p-0140" num="0139">Thus, following earlier described operations of method <b>1000</b>, from ENTER operation <b>1002</b>, processing transitions to GENERATE LIST OF NODE IDs operation <b>1004</b>.</p>
<p id="p-0141" num="0140">In GENERATE LIST OF NODE IDs operation <b>1004</b>, a layout map ID (determined in operation <b>104</b>, <figref idref="DRAWINGS">FIG. 1</figref>) is used as a seed to a random or pseudo-random number generator to generate List A, for example:</p>
<p id="p-0142" num="0141">List A: A C E G B D F H,</p>
<p id="h-0012" num="0000">and processing transitions from GENERATE LIST OF NODE IDs operation <b>1004</b> to GENERATE LIST OF STORAGE STRUCTURE IDs operation <b>1006</b>.</p>
<p id="p-0143" num="0142">In GENERATE LIST OF STORAGE STRUCTURE IDs operation <b>1006</b>, the layout map ID is used as a seed to a random or pseudo-random number generator to generate List B, for example:</p>
<p id="p-0144" num="0143">List B: 4 2 3 1,</p>
<p id="h-0013" num="0000">and processing transitions from GENERATE LIST OF STORAGE STRUCTURE IDs operation <b>1006</b> to LIST A LESS THAN (N+M) check operation <b>1008</b>.</p>
<p id="p-0145" num="0144">In LIST A LESS THAN (N+M) check operation <b>1008</b>, a determination is made whether List A is less than (N+M). As operation <b>1004</b> generated a list of 8 nodes and (N+M)=4, List A is not less than (N+M) (“NO”), and, processing transitions from LIST A LESS THAN (N+M) check operation <b>1008</b> to GENERATE LIST OF FIRST (N+M) NODE IDs operation <b>1012</b>.</p>
<p id="p-0146" num="0145">In GENERATE LIST OF FIRST (N+M) NODE IDs operation <b>1012</b>, the first (N+M) nodes in List A are selected to generate List C. Thus, the first four (4) nodes are selected in order from List A to produce List C, for example:</p>
<p id="p-0147" num="0146">List C=A C E G,</p>
<p id="h-0014" num="0000">and processing transitions to GENERATE NODE GROUPS operation <b>1014</b>.</p>
<p id="p-0148" num="0147">In GENERATE NODE GROUPS operation <b>1014</b>, the number of nodes per group is limited to a minimum of (8/4)=2. Thus, one example of a node group set is:</p>
<p id="p-0149" num="0148">Node Group 1=AB</p>
<p id="p-0150" num="0149">Node Group 2=CD</p>
<p id="p-0151" num="0150">Node Group 3=EF</p>
<p id="p-0152" num="0151">Node Group 4=GH,</p>
<p id="h-0015" num="0000">and processing transitions from GENERATE NODE GROUPS operation <b>1014</b> to ENTER NODE IDs operation <b>1016</b>.</p>
<p id="p-0153" num="0152">In ENTER NODE IDs operation <b>1016</b>, the node IDs are entered as earlier described, thus, one example of the matrix with the node IDs entered is:</p>
<p id="p-0154" num="0153">
<tables id="TABLE-US-00002" num="00002">
<table frame="none" colsep="0" rowsep="0">
<tgroup align="left" colsep="0" rowsep="0" cols="5">
<colspec colname="offset" colwidth="35pt" align="left"/>
<colspec colname="1" colwidth="14pt" align="center"/>
<colspec colname="2" colwidth="77pt" align="center"/>
<colspec colname="3" colwidth="14pt" align="center"/>
<colspec colname="4" colwidth="77pt" align="center"/>
<thead>
<row>
<entry/>
<entry namest="offset" nameend="4" align="center" rowsep="1"/>
</row>
</thead>
<tbody valign="top">
<row>
<entry/>
<entry>A</entry>
<entry>C</entry>
<entry>E</entry>
<entry>G</entry>
</row>
<row>
<entry/>
<entry>B</entry>
<entry>D</entry>
<entry>F</entry>
<entry>H</entry>
</row>
<row>
<entry/>
<entry>A</entry>
<entry>C</entry>
<entry>E</entry>
<entry>G</entry>
</row>
<row>
<entry/>
<entry>B</entry>
<entry>D</entry>
<entry>F</entry>
<entry>H</entry>
</row>
<row>
<entry/>
<entry>A</entry>
<entry>C</entry>
<entry>E</entry>
<entry>G</entry>
</row>
<row>
<entry/>
<entry>B</entry>
<entry>D</entry>
<entry>F</entry>
<entry>H</entry>
</row>
<row>
<entry/>
<entry>A</entry>
<entry>C</entry>
<entry>E</entry>
<entry>G</entry>
</row>
<row>
<entry/>
<entry>B</entry>
<entry>D</entry>
<entry>F</entry>
<entry> H,</entry>
</row>
<row>
<entry/>
<entry namest="offset" nameend="4" align="center" rowsep="1"/>
</row>
</tbody>
</tgroup>
</table>
</tables>
<br/>
and processing transitions from ENTER NODE IDs operation <b>1016</b> to ENTER STORAGE STRUCTURE IDs operation <b>1018</b>.
</p>
<p id="p-0155" num="0154">In ENTER STORAGE STRUCTURE IDs operation <b>1018</b>, the disk IDs are entered as earlier described, resulting in layout map <b>1102</b>, and processing transitions from ENTER STORAGE STRUCTURE IDs operation <b>1018</b> to EXIT operation <b>1020</b> with processing exiting method <b>1000</b>.</p>
<p id="p-0156" num="0155">Utilizing method <b>1000</b> in the generation of layout maps, the same storage structure <b>314</b>, e.g., disk, should not appear twice in a layout map, and the maximum number of columns that share a given node should not exceed two (2) for cells with less than 2(N+M) nodes, but greater or equal to (N+M) nodes, and is one (1) for larger cells. This minimizes the placement of more than one (1) data object fragment of a data object on the same node.</p>
<p id="p-0157" num="0156">Further, method <b>1000</b> generates layout maps that tolerate at least F node failures without data corruptions, such that:</p>
<p id="p-0158" num="0157">F&lt;M, for (# nodes)&lt;(N+M), and</p>
<p id="p-0159" num="0158">F=xM, for (# nodes)&gt;=x(N+M), where x&gt;=1.</p>
<p id="h-0016" num="0000">Additionally, the number of disk failures tolerated before losing any data objects is larger than M.</p>
<p id="p-0160" num="0159">In the above descriptions of DETERMINE LAYOUT MAP operation <b>106</b>, and in particular method <b>1000</b> (<figref idref="DRAWINGS">FIG. 10</figref>), the determination of layout maps is described as performed at each store of a data object; however, in another embodiment, layout maps for each layout map ID are initially generated and stored in layout module <b>804</b> based on the current global cell view published by the master node, and updated layout maps are generated when the current global cell view is changed by the master node.</p>
<p id="p-0161" num="0160">Returning now to <figref idref="DRAWINGS">FIG. 1</figref>, upon determination of the layout map, in DETERMINE LAYOUT operation <b>108</b>, layout module <b>804</b> determines the layout. Herein a layout is a selected set of storage structures <b>314</b>, e.g., disks, in storage system <b>200</b> that is available for storage of data, e.g., operational. In the present embodiment, selection of a layout by a node <b>202</b>A-<b>202</b><i>n</i>, e.g., node <b>202</b>A, is independent from all other nodes.</p>
<p id="p-0162" num="0161">In one embodiment, layout module <b>804</b> reads the status, e.g., online or offline, of all storage structures <b>314</b>, e.g., disks, in the cell from a disk mask maintained by disk mask module <b>802</b> and determines the layout for the selected layout map ID from the layout map and the disk mask.</p>
<p id="p-0163" num="0162">In one embodiment, a disk mask is a set of Boolean values indicating which storage structures <b>314</b>, e.g., disks, are currently operational in the cell. A disk mask has an entry for each storage structure <b>314</b>, e.g., disk, that is potentially in the cell. Thus, the size of the disk mask is constant, although the values within the disk mask change when a storage structure <b>314</b>, e.g., a disk, fails or comes back online.</p>
<p id="p-0164" num="0163">In one embodiment, a storage structure <b>314</b>, e.g., a disk, is indicated as failed in a disk mask based on a disk status indicator received from a disk driver, for example, received by node management module <b>412</b> (<figref idref="DRAWINGS">FIG. 4</figref>), and published on network <b>208</b> (<figref idref="DRAWINGS">FIG. 2</figref>). However, if a node <b>202</b>A-<b>202</b><i>n </i>is unresponsive, even after a configurable timeout, all storage structures <b>314</b>, e.g., disks, on the node are treated as failed. In one embodiment, changes to the disk mask are made only after approval by the master node, for example, by update of the global cell view maintained by the master node.</p>
<p id="p-0165" num="0164">In the present embodiment, each node <b>202</b>A-<b>202</b><i>n </i>computes the disk mask independently, thus, it is possible for nodes <b>202</b>A-<b>202</b><i>n </i>to generate slightly different disk masks for a very brief period of time. Although, the use of the disk mask can result in excessive recovery of a data object, for example, extra data object fragments, the extra data object fragments can later be collected, for example by a garbage collection mechanism when the disk mask views become consistent. For example, in one embodiment, a garbage collection mechanism removes extra data object fragments after verifying that the data object fragments have been reconstructed elsewhere, so that different disk maps across the cell do not result in the erroneous deletion of data.</p>
<p id="p-0166" num="0165">In one embodiment, storage structures <b>314</b>, e.g., disks, within storage system <b>200</b> are associated as index entries to a disk mask based on the disk positions within the cell. For example, disk <b>1</b> on node <b>202</b>A can be at disk mask index <b>1</b>, disk <b>2</b> on node <b>202</b>A can be at disk index <b>2</b>, and so on. This embodiment creates an ordering of nodes <b>202</b>A-<b>202</b><i>n</i>, and of storage structures <b>314</b>, e.g., disks, on a node that persists across node reboots.</p>
<p id="p-0167" num="0166">In one embodiment, writing an identifying header to the first sector of each storage structure <b>314</b>, e.g., disk, can be used to accomplish storage structure ordering and also to detect if a physical storage structure <b>314</b> is manually relocated to a different position within the cell.</p>
<p id="p-0168" num="0167">Referring again to <figref idref="DRAWINGS">FIG. 9</figref> and layout map <b>902</b>, if disks D_<b>2</b>, A_<b>2</b>, F_<b>2</b> and E_<b>2</b> (the first row), are available, they are chosen as the initial layout for a data object assigned layout map <b>902</b>. Storage structures in the subsequent rows ordered in each column represent alternative placements for a data object fragment should a preceding storage structure become unavailable. Thus, for example, if storage structure <b>906</b>, e.g., D_<b>2</b>, fails, in initial layout <b>904</b>, the data object fragment, e.g., fragment <b>1</b>, is placed in the next storage structure in alternative placements <b>906</b>, e.g., C_<b>2</b>. If C_<b>2</b> is unavailable, the data object fragment, e.g., fragment <b>1</b>, is placed in D_<b>1</b>, and so forth.</p>
<p id="p-0169" num="0168">In one embodiment, if a disk fails, for example, storage structure <b>906</b>, e.g., disk D_<b>2</b>, node C automatically reconstructs the data object fragment, e.g., fragment <b>1</b>, for all data objects that were assigned this layout map ID, on disk C_<b>2</b>. Similarly, if disk F_<b>2</b> fails, node D will reconstruct the data object fragment, e.g., fragment <b>3</b>, for all data objects that were assigned this layout map ID, on disk D_<b>3</b>.</p>
<p id="p-0170" num="0169">Consequently, in the example of layout map <b>902</b>, if recovery capacity is available, up to one half of the disks in the cell can be lost, non-concurrently, without losing data. Further, up to three nodes out of the six can be lost non-concurrently. Without recovery, only two nodes and two disks can be lost concurrently. Recovery of data objects is further described herein.</p>
<p id="p-0171" num="0170">In one embodiment, if storage structure <b>906</b> in the first row comes back online, that data object fragment is reconstructed back onto storage structure <b>906</b>. Thus, each column of layout map <b>902</b> predetermines the placement trajectory of a data object fragment as individual storage structures in that column fail or come back online. Thus, by using the layout map, the system metadata remains unchanged, i.e., the layout map ID remains unchanged, even when layouts of data objects change.</p>
<p id="p-0172" num="0171">Similarly, referring to <figref idref="DRAWINGS">FIG. 11</figref> and layout map <b>1102</b>, if disks A_<b>4</b>, C_<b>4</b>, E_<b>4</b>, and G_<b>4</b> (the first row), are available, they are chosen as the initial layout <b>1104</b> for a data object assigned layout map <b>1102</b>. Storage structures in the subsequent rows ordered in each column represent alternative placements for a data object fragment should a preceding storage structure become unavailable.</p>
<p id="p-0173" num="0172">Upon determination of a layout, layout module <b>804</b> returns the layout map ID and layout to object archive module <b>410</b>, and processing transitions from DETERMINE LAYOUT operation <b>108</b> to a FRAGMENT DATA OBJECT operation <b>110</b>.</p>
<p id="p-0174" num="0173">In FRAGMENT DATA OBJECT <b>110</b>, the data object is received by object archive module <b>410</b>, for example, by streaming, and fragmented as earlier described with reference to method <b>500</b> and <figref idref="DRAWINGS">FIGS. 5 and 6</figref>. In particular, the data object is fragmented using erasure coding, such as Reed-Solomon erasure coding, to obtain (N+M) data object fragments that are associated with a unique object ID that is based upon an intrinsic parameter of the data object, such as the content.</p>
<p id="p-0175" num="0174">In one embodiment, the data object fragments are ordered so that each data object fragment is separately identifiable from other data object fragments of the data object, for example, by assigning an ordered number to each data object fragment. For example, if a data object is fragmented into (N+M) fragments of (2+2), the data object fragments are ordered <b>1</b>, <b>2</b>, <b>3</b>, and <b>4</b>. Further, as earlier described, system metadata associated with the data object is generated during the fragmentation of the data object by object archive module <b>410</b>. From FRAGMENT DATA OBJECT operation <b>110</b>, processing transitions to a STORE DATA OBJECT operation <b>112</b>.</p>
<p id="p-0176" num="0175">In STORE DATA OBJECT operation <b>112</b>, object archive <b>410</b> stores the data object fragments in accordance with the selected layout. In particular, object archive <b>410</b> stores the data object fragments in the storage structures <b>314</b> indicated in the layout determined in DETERMINE LAYOUT operation <b>108</b>. In one embodiment, object archive module <b>410</b> stores at least a portion of the system metadata for a data object with each data object fragment.</p>
<p id="p-0177" num="0176">In one embodiment, object archive module <b>410</b> also writes the layout map ID associated with the data object, e.g., the data object ID, to the data object's system metadata in metadata module <b>408</b>. In one embodiment, object archive module <b>410</b> returns a confirmation of the storage indicating the object ID, for example, via interface module <b>404</b> to switch <b>110</b>. In some embodiments, additional information, such as the layout map ID, are also included. From STORE DATA OBJECT operation <b>112</b>, processing transitions to an EXIT operation <b>114</b>, with processing exiting method <b>100</b>.</p>
<p id="p-0178" num="0177">As earlier described, utilization of method <b>100</b> permits efficient retrieval of data on storage system <b>200</b> as further described herein with reference to <figref idref="DRAWINGS">FIG. 12</figref>.</p>
<p id="p-0179" num="0178"><figref idref="DRAWINGS">FIG. 12</figref> illustrates a process flow diagram of method <b>1200</b> for retrieving data stored on a data storage system in accordance with one embodiment of the present invention. As illustrated in <figref idref="DRAWINGS">FIG. 12</figref>, in one embodiment, a request to retrieve, e.g., read, a data object is received at a node <b>202</b>A-<b>202</b><i>n </i>(<figref idref="DRAWINGS">FIG. 2</figref>), for example, at node <b>202</b>A. In one embodiment, the request to retrieve includes the object ID of the data object. In the present embodiment, the request to retrieve a data object is communicated to symmetric storage application <b>204</b>, and in particular to object archive module <b>410</b>, for example, from interface module <b>404</b>. Object archive module <b>410</b> initiates a retrieve and enters method <b>1200</b> from an ENTER operation <b>1202</b>, and processing transitions to a DETERMINE LAYOUT MAP ID operation <b>1204</b>.</p>
<p id="p-0180" num="0179">In DETERMINE LAYOUT MAP ID operation <b>1204</b>, object archive module <b>410</b> determines the layout map ID associated with the data object. In one embodiment, object archive module <b>410</b> requests the layout map ID from metadata module <b>408</b>. Metadata module <b>408</b> locates the layout map ID associated with the data object from the indexed metadata. Metadata module <b>408</b> returns the layout map ID to object archive module <b>410</b>, and from DETERMINE LAYOUT MAP ID operation <b>1204</b>, processing transitions to a DETERMINE LAYOUT MAP operation <b>1206</b>.</p>
<p id="p-0181" num="0180">In DETERMINE LAYOUT MAP operation <b>1206</b>, object archive module <b>410</b> requests the layout associated with the layout map ID from layout module <b>804</b>. Layout module <b>804</b> reads the current disk mask from disk mask module <b>802</b> and determines the layout map associated with the layout map ID, and from DETERMINE LAYOUT MAP operation <b>1206</b>, processing transitions to a DETERMINE LAYOUT operation <b>1208</b>.</p>
<p id="p-0182" num="0181">In DETERMINE LAYOUT operation <b>1208</b>, layout module <b>804</b> determines the layout for the layout map ID based on the current disk mask and the layout map. Layout module <b>804</b> returns the layout to object archive module <b>410</b> and from DETERMINE LAYOUT operation <b>1208</b>, processing transitions to a RETRIEVE DATA OBJECT operation <b>1210</b>.</p>
<p id="p-0183" num="0182">In RETRIEVE DATA OBJECT operation <b>1210</b>, object archive module <b>410</b> retrieves the data object based on the layout, e.g., reads the data fragments from the locations on storage structures <b>314</b>, e.g., disks, indicated in the layout, and assembles the data object. In one embodiment, any missing data fragments are reconstructed using other data and parity fragments and the erasure coding algorithm, e.g., the Reed-Solomon erasure coding algorithm. Object archive module <b>410</b> returns the data object, for example, via interface module <b>404</b> to switch <b>110</b>, and processing transitions from RETRIEVE DATA OBJECT operation <b>1210</b> to an EXIT operation <b>1212</b>, with processing exiting method <b>1200</b>.</p>
<p id="p-0184" num="0183">In some embodiments, the layout map ID is additionally included with the object ID in the request. In these embodiments, DETERMINE LAYOUT MAP ID operation <b>1204</b> is not performed.</p>
<p id="p-0185" num="0184">In addition to efficiently storing and retrieving data objects, in one embodiment, storage system <b>200</b> permits recovery of data objects, such as when a storage structure <b>314</b>, e.g., a disk, fails. In one embodiment, a recovery process is automatically initiated in storage system <b>200</b> each time a configured interval elapses or when the disk mask changes. In one embodiment, the initiation of the recovery process is selectable, for example, by a system administrator command, and the recovery process is automatically initiated unless the recovery process has not been selected, e.g., turned off.</p>
<p id="p-0186" num="0185">In one embodiment, recovery module <b>806</b> of node <b>202</b>A periodically polls disk mask module <b>802</b> to determine whether there has been a change to the disk mask maintained by disk mask module <b>802</b>. When there has been a change, disk mask module <b>802</b> returns the updated disk mask to recovery module <b>806</b>, and recovery module <b>806</b> automatically initiates a recovery process as further described with reference to <figref idref="DRAWINGS">FIG. 13</figref> and method <b>1300</b>.</p>
<p id="p-0187" num="0186">In one embodiment, the recovery process can be started manually, or stopped in-progress, such as by a system administrator command. In one embodiment, the on/off and start/stop commands apply on a cell-wide level. In the present embodiment, if the disk mask changes while recovery is in progress, the recovery procedure is restarted.</p>
<p id="p-0188" num="0187">In one embodiment, each node <b>202</b>A-<b>202</b><i>n </i>completes a recovery process independently from other nodes, thus, if a node <b>202</b>A-<b>202</b><i>n </i>fails during one of the operations of the recovery process, the node can simply start over from the beginning.</p>
<p id="p-0189" num="0188"><figref idref="DRAWINGS">FIG. 13</figref> illustrates a process flow diagram of a method <b>1300</b> for recovery of data objects on a data storage system in accordance with one embodiment of the invention. For purposes of explanation, method <b>1300</b> is described as implemented on node <b>202</b>A, however, in one embodiment, each of the nodes <b>202</b>A-<b>202</b><i>n </i>in the cell also performs method <b>1300</b>.</p>
<p id="p-0190" num="0189">As illustrated in <figref idref="DRAWINGS">FIG. 13</figref>, from an ENTER operation <b>1302</b>, processing transitions to a DETERMINE LAYOUT MAP IDs operation <b>1304</b>.</p>
<p id="p-0191" num="0190">In DETERMINE LAYOUT MAP IDs operation <b>1304</b>, recovery module <b>806</b> of node <b>202</b>A determines the layout map IDs for data objects stored on the node, e.g., node <b>202</b>A. In one embodiment, recovery module <b>806</b> requests the layout map IDs that place data object fragments on node <b>202</b>A (given the current disk mask) from layout module <b>804</b>.</p>
<p id="p-0192" num="0191">Layout module <b>804</b> determines the layout map IDs of layout maps that place data object fragments on node <b>202</b>A and returns a list of layout map IDs, if any, to recovery module <b>806</b>. From DETERMINE LAYOUT MAP IDs operation <b>1304</b> processing transitions to LAYOUT MAP IDs RETURNED check operation <b>1306</b>.</p>
<p id="p-0193" num="0192">In LAYOUT MAP IDs RETURNED check operation <b>1306</b>, recovery module <b>806</b> determines whether any layout map IDs are returned from layout module <b>804</b>, for example by determining whether the number of layout map IDs returned from layout module <b>804</b> is greater than zero (0). If no layout map IDs are returned, for example the number of layout map IDs returned is not greater than zero (0), there are no data objects placed on node <b>202</b>A to recover, and processing transitions from LAYOUT MAP IDs RETURNED check operation <b>1306</b> to an EXIT operation <b>1328</b> with processing exiting method <b>1300</b>.</p>
<p id="p-0194" num="0193">Otherwise, if recovery module <b>806</b> determines layout map IDs are returned from layout module <b>804</b>, for example, the number of layout map IDs returned is greater than zero (0), processing transitions from LAYOUT MAP IDs RETURNED check operation <b>1306</b> to a SELECT LAYOUT MAP ID operation <b>1308</b>.</p>
<p id="p-0195" num="0194">In SELECT LAYOUT MAP ID operation <b>1308</b>, recovery module <b>806</b> initially selects the first layout map ID in the returned list of layout map IDs, and processing transitions from SELECT LAYOUT MAP ID operation <b>1308</b>, to a DETERMINE OBJECT IDs operation <b>1310</b>.</p>
<p id="p-0196" num="0195">In DETERMINE OBJECT IDs operation <b>1310</b>, recovery module <b>806</b> queries metadata module <b>408</b> for object IDs of data objects which utilize the selected layout map ID. Metadata module <b>408</b> returns the list of object IDs with the associated layout map IDs, and from DETERMINE OBJECT IDs operation <b>1310</b>, processing transitions to a SELECT OBJECT ID operation <b>1312</b>.</p>
<p id="p-0197" num="0196">In SELECT OBJECT ID operation <b>1312</b>, recovery module <b>806</b> initially selects the first object ID in the list of object IDs returned for the selected layout map ID, and processing transitions to a DETERMINE FRAGMENTS operation <b>1314</b>.</p>
<p id="p-0198" num="0197">In DETERMINE FRAGMENTS operation <b>1314</b>, recovery module <b>806</b> determines a list of the data object fragments placed on the node for the selected object ID based on the layout derived from the layout map ID (using the current disk mask). From DETERMINE FRAGMENTS operation <b>1314</b>, processing transitions to a SELECT FRAGMENT operation <b>1316</b>.</p>
<p id="p-0199" num="0198">In SELECT FRAGMENT operation <b>1316</b>, recovery module <b>806</b> initially selects a first data object fragment associated with the selected object ID, and that should reside on a disk on the node, for example, node <b>202</b>A, and processing transitions from SELECT FRAGMENT operation <b>1316</b> to a FRAGMENT VERIFIED check operation <b>1318</b>.</p>
<p id="p-0200" num="0199">In FRAGMENT VERIFIED check operation <b>1318</b>, recovery module <b>806</b> verifies that the selected data object fragment identified as placed on node <b>202</b>A actually exists on the appropriate data storage structure <b>314</b>, e.g., disk, on node <b>202</b>A, for example, by scanning the appropriate data storage disk for the selected fragment. If the data object fragment is not verified (“NO”), processing transitions from FRAGMENT VERIFIED check operation <b>1318</b> to a RECONSTRUCT operation <b>1320</b>. Otherwise, if the data object fragment is verified (“YES”), processing transitions from FRAGMENT VERIFIED check operation <b>1318</b> to a LAST FRAGMENT check operation <b>1322</b>.</p>
<p id="p-0201" num="0200">In RECONSTRUCT operation <b>1320</b>, recovery module <b>806</b> requests object archive module <b>410</b> to reconstruct the missing data object fragment and to store the reconstructed data object fragment to the appropriate data storage location, e.g., disk, for example, in accordance with the current disk mask and the associated layout map ID. In one embodiment, object archive module <b>410</b> utilizes an inverse of the erasure coding algorithm used to store data objects, for example, Reed-Solomon erasure coding, together with at least some of the available data fragments and parity fragments located on storage system <b>200</b> to reconstruct the missing data object fragment. From RECONSTRUCT operation <b>1320</b>, processing transitions to a LAST FRAGMENT check operation <b>1322</b>.</p>
<p id="p-0202" num="0201">In LAST FRAGMENT check operation <b>1322</b>, recovery module <b>806</b> determines whether the last data object fragment that should be placed on the node, e.g., node <b>202</b>A, for the selected object ID has been verified or reconstructed, e.g., if all the listed data object fragments for the selected object ID have been verified or reconstructed. If not (“NO”), processing transitions from LAST FRAGMENT check operation <b>1322</b> and returns to SELECT FRAGMENT operation <b>1316</b> earlier described (with selection of the next data object fragment in the list). Otherwise, if the last data object fragment for the selected object ID has been verified or reconstructed (“YES”), processing transitions from LAST FRAGMENT check operation <b>1322</b> to a LAST OBJECT ID check operation <b>1324</b>.</p>
<p id="p-0203" num="0202">In LAST OBJECT ID check operation <b>1324</b>, recovery module <b>806</b> determines whether the last object ID on the list of object IDs for the selected layout map ID has been selected and processed in accordance with method <b>1300</b>. If not (“NO”), processing transitions from LAST OBJECT ID check operation <b>1324</b> and returns to SELECT OBJECT ID operation <b>1312</b> earlier described (with selection of the next object ID in the list). Otherwise, if the last object ID for the selected layout map ID has been selected and processed in accordance with method <b>1300</b> (“YES”), processing transitions from LAST OBJECT ID check operation <b>1324</b> to a LAST LAYOUT MAP ID check operation <b>1326</b>.</p>
<p id="p-0204" num="0203">In LAST LAYOUT MAP ID check operation <b>1326</b>, recovery module <b>806</b> determines whether the last layout map ID in the list of layout map IDs has been selected and processed in accordance with method <b>1300</b>. If not (“NO”), processing transitions from LAST LAYOUT MAP ID check operation <b>1326</b> and returns to SELECT LAYOUT MAP ID operation <b>1308</b> (with selection of the next layout map ID in the list).</p>
<p id="p-0205" num="0204">Otherwise, if the last layout map ID for node <b>202</b>A returned from layout module <b>804</b> has been selected and processed in accordance with method <b>1300</b> (“YES”), recovery on node <b>202</b>A is complete, and processing transitions from LAST LAYOUT MAP ID check operation <b>1326</b> to EXIT operation <b>1328</b>, with processing exiting method <b>1300</b>.</p>
<p id="p-0206" num="0205">In one embodiment, as each node <b>202</b>A-<b>202</b><i>n </i>proceeds through the above operations, each node <b>202</b>A-<b>202</b><i>n </i>reports its recovery status, for example, by reporting the percentage of the layout map IDs (returned in operation <b>1304</b>) that have been processed. When all nodes <b>202</b>A-<b>202</b><i>n </i>have completed method <b>1300</b> for the current disk mask, in one embodiment, a notification is provided, for example, to a system administrator, that recovery is complete, i.e., all (available) disks now contain the expected data object fragments based on the current disk mask and the layout map IDs used by each data object.</p>
<p id="p-0207" num="0206">As described above, in one embodiment, method <b>1300</b> determines which data object fragments should be on a given node and recovers any missing data object fragments by reconstructing the data object fragments on the data storage system. In another embodiment of the invention, if new nodes become operational, intact data object fragments, rather than missing data object fragments, are moved between storage structures <b>314</b> to maintain a uniform distribution of data object fragments across operational storage structures <b>314</b> in storage system <b>200</b>, herein termed rebalancing.</p>
<p id="p-0208" num="0207">For example, in one embodiment, a rebalancing of data object fragments on storage system <b>200</b> occurs in response to a previously failed storage structure <b>314</b>, e.g., a disk, resuming operation (coming back online), or in response to the addition of a new node to storage system <b>200</b>. In one embodiment, data object fragments selected for rebalancing are reconstructed, e.g., recovered onto another storage structure <b>314</b> similar to recovery and reconstruction of data object fragments lost due to disk failure (method <b>1300</b>). In an alternative embodiment, data object fragments selected for rebalancing are copied from an original location to a storage structure <b>314</b> to a new location on a storage structure <b>314</b> to avoid the need to utilize erasure coding in reconstructing the data object fragment as in method <b>1300</b>. For example, in one embodiment, a previous disk mask is used to locate a data object fragment that is to be copied, e.g., rebalanced, from one storage structure <b>314</b> (a first storage structure) to another storage structure <b>314</b> (a second storage structure).</p>
<p id="p-0209" num="0208">As described above, in one embodiment, a recovery process (method <b>1300</b>) determines data object fragments that should be on a given node and recovers any missing data object fragments by reconstructing the data fragments. In some instances, data object fragments are detected on a node that are not accounted for during the recovery process, herein termed garbage fragments. An example of a garbage fragment is a duplicate data object fragment. Garbage fragments utilize storage space in storage system <b>200</b> that could be more efficiently used, e.g., used for storage. Thus, in accordance with the invention, in one embodiment, these garbage fragments are removed through a method of garbage collection.</p>
<p id="p-0210" num="0209">In one embodiment, recovery module <b>806</b> notes any data object fragments on the node, e.g., node <b>202</b>A, that are not accounted for during a recovery process (method <b>1300</b>) and marks these unaccounted for data object fragments for garbage collection, e.g., as garbage fragments. Prior to deleting the garbage fragments, recovery module <b>806</b> determines whether the garbage fragments are reconstructed elsewhere on storage system <b>200</b>.</p>
<p id="p-0211" num="0210">In one embodiment, recovery module <b>806</b> obtains the current layout for a garbage fragment from layout module <b>804</b>, for example, utilizing the object ID or layout map ID present as part of the metadata data of the garbage fragment (part of the metadata stored as part of the data object fragment marked for garbage collection). Recovery module <b>806</b> determines the current placement of the data object associated with the object ID on storage system <b>200</b> in accordance with the layout and verifies that a duplicate data object fragment exists on the expected storage structure <b>314</b> (<figref idref="DRAWINGS">FIG. 3</figref>).</p>
<p id="p-0212" num="0211">If recovery module <b>806</b> verifies the existence of a duplicate data object fragment on the expected storage structure <b>314</b> (<figref idref="DRAWINGS">FIG. 3</figref>), the garbage fragment is removed, e.g., deleted. Otherwise, the garbage fragment is not removed, and is not marked for garbage collection. One embodiment of a method of garbage collection is further described herein with reference to <figref idref="DRAWINGS">FIG. 14</figref> and method <b>1400</b>.</p>
<p id="p-0213" num="0212"><figref idref="DRAWINGS">FIG. 14</figref> illustrates a process flow diagram of a method <b>1400</b> for garbage collection of garbage fragments on a data storage system in accordance with one embodiment of the invention. For purposes of explanation, method <b>1400</b> is described as implemented on node <b>202</b>A, however, in one embodiment, each of nodes <b>202</b>A-<b>202</b><i>n </i>independently performs method <b>1400</b>.</p>
<p id="p-0214" num="0213">In the present embodiment, method <b>1400</b> is described as implemented by symmetric storage system application <b>204</b>. In the present embodiment, method <b>1400</b> is described as implemented by recovery module <b>806</b> of symmetric storage system application <b>204</b>, however, in other embodiments, method <b>1400</b> can be implemented by a separate module, such as a garbage collection module of symmetric storage system application <b>204</b>. In still other embodiments, some or all of the operations of method <b>1400</b> can be implemented by different modules of symmetric storage system application <b>204</b>.</p>
<p id="p-0215" num="0214">In the present embodiment, method <b>1400</b> is performed on each active storage structure of a node, such as on each active disk <b>1</b>-disk n of node <b>202</b>A. In other embodiments, method <b>1400</b> is performed on one or more selected active storage structures of a node. Implementation of method <b>1400</b> can be on demand, scheduled, or upon occurrence of a specified event or events, such as following a recovery.</p>
<p id="p-0216" num="0215">As illustrated in <figref idref="DRAWINGS">FIG. 14</figref>, from an ENTER operation <b>1402</b>, processing transitions to a DETERMINE LAYOUT MAP IDs FOR GARBAGE COLLECTION operation <b>1404</b>.</p>
<p id="p-0217" num="0216">In DETERMINE LAYOUT MAP IDs FOR GARBAGE COLLECTION operation <b>1404</b>, recovery module <b>806</b> of node <b>202</b>A determines a listing of layout map IDs that do not place data fragments on a selected storage structure given the current disk mask, also termed the layout map IDs for garbage collection. Herein this listing of layout map IDS for garbage collection is termed a garbage collection list as data fragments associated with any of the listed layout map IDs should not be stored on the selected storage structure, i.e., are garbage fragments.</p>
<p id="p-0218" num="0217">For example, in one embodiment, recovery module <b>806</b> requests the garbage collection list (given the current disk mask) for a selected storage structure, such as disk <b>1</b> of node <b>202</b>A, from layout module <b>804</b>. Layout module <b>804</b> determines the layout map IDs of layout maps that do not place data fragments on the specified storage structure (e.g., disk <b>1</b> of node <b>202</b>A) given the current disk mask and returns the garbage collection list, if any, to recovery module <b>806</b>. From DETERMINE LAYOUT MAP IDs FOR GARBAGE COLLECTION operation <b>1404</b> processing transitions to a LAYOUT MAP IDs RETURNED check operation <b>1406</b>.</p>
<p id="p-0219" num="0218">In LAYOUT MAP IDs RETURNED check operation <b>1406</b>, recovery module <b>806</b> determines whether any layout map IDs for garbage collection are returned from layout module <b>804</b>. For example, in one embodiment, recovery module <b>806</b> determines whether the number of layout map IDs for garbage collection returned from layout module <b>804</b> is greater than zero (0). If no layout map IDs for garbage collection are returned (“NO”), for example the number of layout map IDs for garbage collection returned in the garbage collection list is not greater than zero (0), processing transitions from LAYOUT MAP IDs RETURNED check operation <b>1406</b> to an EXIT operation <b>1420</b> with processing exiting method <b>1400</b>.</p>
<p id="p-0220" num="0219">Alternatively, if recovery module <b>806</b> determines layout map IDs for garbage collection are returned from layout module <b>804</b>, for example, the number of layout map IDs for garbage collection in the garbage collection list is greater than zero (0) (“YES”), processing transitions from LAYOUT MAP IDs RETURNED check operation <b>1406</b> to a SCAN operation <b>1408</b>.</p>
<p id="p-0221" num="0220">In SCAN operation <b>1408</b>, recovery module <b>806</b> initially scans the selected storage structure <b>314</b> (e.g., disk <b>1</b> of node <b>202</b>A) for data fragments. Scanning a disk to locate data is well-known to those of skill in the art and is not further described herein to avoid detracting from the description of the present invention. From SCAN operation <b>1408</b>, processing transitions to a LOCATE FRAGMENT check operation <b>1410</b>.</p>
<p id="p-0222" num="0221">In LOCATE FRAGMENT check operation <b>1412</b>, recovery module <b>806</b> determines whether a data fragment is located during the scan. If a data fragment is not located on the selected storage structure (e.g., not located on disk <b>1</b> of node <b>202</b>A), there is no data fragment to evaluate for garbage collection, and processing transitions from LOCATE FRAGMENT check operation <b>1410</b> to EXIT operation <b>1420</b> with processing exiting method <b>1400</b>.</p>
<p id="p-0223" num="0222">Alternatively, if a data fragment is located on the selected storage structure (e.g., is located at a first location on disk <b>1</b> of node <b>202</b>A), processing transitions from LOCATE FRAGMENT check operation <b>1410</b>, to a DETERMINE LAYOUT MAP ID OF FRAGMENT operation <b>1412</b>.</p>
<p id="p-0224" num="0223">In DETERMINE LAYOUT MAP ID OF FRAGMENT operation <b>1412</b>, recovery module <b>806</b> determines the layout map ID associated with the data fragment. In embodiments in which the layout map ID is part of the object ID of the data fragment, the layout map ID is obtained from the object ID stored with the data fragment. In other embodiments, in which the layout map ID is not part of the object ID of the data fragment, but can be associated with the object ID of the data fragment, recovery module <b>806</b> initiates queries to obtain the corresponding layout map ID, for example, to object archive module <b>402</b> or metadata module <b>408</b>, to determine the corresponding layout map ID. FROM DETERMINE LAYOUT MAP ID OF FRAGMENT operation <b>1412</b>, processing transitions to a MATCH check operation <b>1414</b>.</p>
<p id="p-0225" num="0224">In MATCH check operation <b>1414</b>, recovery module <b>806</b> determines whether the layout map ID of the data fragment matches one of the layout map IDs of the garbage collection list. If the layout map ID of the data fragment does not match one of the layout map IDs of the garbage collection list, the data fragment is not assumed to be a garbage fragment, and processing transitions from MATCH check operation <b>1414</b> to a SCAN COMPLETE check operation <b>1422</b>.</p>
<p id="p-0226" num="0225">In SCAN COMPLETE check operation <b>1422</b>, recovery module <b>806</b> determines whether the scan of the storage structure <b>314</b> is complete (e.g., whether the scanning of disk <b>1</b> of node <b>202</b>A is complete). If the scan is not complete, processing transitions from SCAN COMPLETE check operation <b>1422</b> and returns to SCAN operation <b>1408</b>, in which scanning is continued, for example, following the data fragment located at the first location in operation <b>1410</b>).</p>
<p id="p-0227" num="0226">Otherwise, if the scan is complete (e.g., the scan of disk <b>1</b> of node <b>202</b>A is complete), processing transitions from SCAN COMPLETE check operation <b>1422</b>, to EXIT operation <b>1420</b>, with processing exiting method <b>1400</b>.</p>
<p id="p-0228" num="0227">Referring again to MATCH check operation <b>1414</b>, alternatively, if the layout map ID of the data fragment matches one of the layout map IDs of the garbage collection list, the data fragment is tentatively assumed to be a garbage fragment (e.g., the data fragment is a candidate for deletion). From MATCH check operation <b>1414</b>, processing transitions to a FRAGMENT VERIFIED check operation <b>1416</b>.</p>
<p id="p-0229" num="0228">In FRAGMENT VERIFIED check operation <b>1416</b>, recovery module <b>806</b> initiates a query to determine whether the data fragment tentatively assumed to be a garbage fragment in operation <b>1414</b> is correctly stored at second location on system <b>200</b>. In one embodiment, recovery module <b>806</b> determines a location where the data fragment should be located on system <b>200</b> given the current disk mask and the layout map ID of the data fragment, herein termed the disk mask location (i.e., a second location).</p>
<p id="p-0230" num="0229">Recovery module <b>806</b> determines whether the data fragment is actually stored at the disk mask location on system <b>200</b>. In one embodiment, recovery module <b>806</b> initiates a query to the node containing the disk mask location, for example, node <b>202</b>B. In one embodiment, recovery module <b>806</b> queries node <b>202</b>B via network <b>208</b> to verify the presence of the data fragment at the disk mask location. The responsible node (e.g., node <b>202</b>B) returns a verification or a non-verification to recovery module <b>806</b> as to the presence of the data fragment at the disk mask location.</p>
<p id="p-0231" num="0230">If the fragment is not verified at the disk mask location (“NO”), processing transitions from FRAGMENT VERIFIED check operation <b>1416</b>, to SCAN complete check operation <b>1422</b>, as earlier described. In this instance, although the data fragment is present on system <b>200</b> at a location not in accordance with the present disk mask, i.e., the first location, neither is present at the disk mask location on system <b>200</b>, i.e., the second location. Thus, rather than risk the possibility of losing the data fragment from system <b>200</b>, the data fragment is maintained on system <b>200</b>.</p>
<p id="p-0232" num="0231">Alternatively, if the data fragment is verified at the disk mask location (“YES”), the data fragment is present on data storage system <b>200</b> in at least two locations, the first location and at the disk mask location (i.e., the second location). Thus, the data fragment located at the first location is determined to be a garbage fragment. From FRAGMENT VERIFIED check operation <b>1416</b>, processing transitions to a DELETE FRAGMENT operation <b>1418</b>.</p>
<p id="p-0233" num="0232">In DELETE FRAGMENT operation <b>1418</b>, the data fragment located at the first location is deleted from data storage system <b>200</b> (e.g., deleted from the first location on disk one of node <b>202</b>A). In one embodiment, the data fragment is deleted by overwriting the fragment and/or removing references to that data fragment on data storage system <b>200</b>, or by otherwise making available to system <b>200</b> the memory space allocated to the garbage fragment at the first location. From DELETE FRAGMENT operation <b>1418</b>, processing transitions to SCAN COMPLETE check operation <b>1422</b> as earlier described.</p>
<p id="p-0234" num="0233">Although the above embodiment has been described with reference to a single storage structure of a node, in other embodiments, method <b>1400</b> is implemented on all operational (e.g., active) storage structures of a node. In one embodiment, method <b>1400</b> is implemented, either serially (e.g., sequentially) or in parallel, on all the operational storage structures (e.g., disk <b>1</b>-disk n) of a node. In one embodiment, each node <b>202</b>A-<b>202</b><i>n </i>independently implements method <b>1400</b> on each operational storage structure of its locally attached storage structure.</p>
<p id="p-0235" num="0234">In one embodiment, each node <b>202</b>A-<b>202</b><i>n </i>reports its garbage collection status, for example, by reporting the percentage of the locally attached storage that has been scanned or processed in accordance with method <b>1400</b>. When all nodes <b>202</b>A-<b>202</b><i>n </i>have completed method <b>1400</b>, in one embodiment, a notification is provided, for example, to a system administrator, that garbage collection is complete.</p>
<p id="p-0236" num="0235">Thus, in accordance with the invention, there has been described a method for garbage collection of data fragments in a data storage system (method <b>1400</b>), such as data storage system <b>200</b>. A listing of layout map IDs that do not place data fragments on a selected storage structure given the current disk mask is determined (i.e., the garbage collection list). The selected storage structure is scanned for data fragments. When a data fragment is located, the layout map ID of the data fragment is determined. If the layout map ID of the data fragment matches a layout map ID in the garbage collection list, the data object fragment is tentatively assumed to be a garbage fragment pending a verification that the data object fragment exists on system <b>200</b> at a disk mask location. If the data fragment is verified as present on data storage system <b>200</b> at a disk mask location, the data fragment is determined to be a garbage fragment and is deleted. Data fragments having layout map IDs not matching a layout map ID in the garbage collection list or that cannot be verified are not assumed to be garbage fragments and are not deleted.</p>
<p id="p-0237" num="0236">In one embodiment, symmetric storage system application <b>204</b> of data storage system <b>200</b> can be configured as a computer program product. Herein a computer program product comprises a medium configured to store computer-readable instructions, such as program code for symmetric storage system application <b>204</b>, including all, any, or parts of processes described herein with reference to <figref idref="DRAWINGS">FIGS. 1-14</figref>, or in which computer-readable instructions for symmetric storage system application <b>204</b>, including all, any, or parts of processes described herein with reference to <figref idref="DRAWINGS">FIGS. 1-14</figref> are stored. Some examples of computer program products are CD-ROM discs, ROM cards, floppy discs, magnetic tapes, computer hard drives, servers on a network representing stored computer-readable instructions. Further herein, a means for performing a particular function is accomplished using the appropriate computer-readable instructions and the related hardware necessary to perform the function.</p>
<p id="p-0238" num="0237">The foregoing description of implementations of the invention have been presented for purposes of illustration and description only, and, therefore, are not exhaustive and do not limit the invention to the precise forms disclosed. Modifications and variations are possible in light of the above teachings or can be acquired from practicing the invention. Consequently, Applicants do not wish to be limited to the specific embodiments shown for illustrative purposes.</p>
<?DETDESC description="Detailed Description" end="tail"?>
</description>
<us-claim-statement>What is claimed is:</us-claim-statement>
<claims id="claims">
<claim id="CLM-00001" num="00001">
<claim-text>1. A method comprising:
<claim-text>determining a garbage collection list for a data storage structure of a node in a data storage system, said garbage collection list including one or more layout map identifiers (IDs) for garbage collection;</claim-text>
<claim-text>locating a data fragment stored on said data storage structure at a first location;</claim-text>
<claim-text>determining a layout map ID associated with said data fragment;</claim-text>
<claim-text>determining whether said layout map ID associated with said data fragment matches a layout map ID for garbage collection in said garbage collection list;</claim-text>
<claim-text>upon a determination that said layout map ID of said data fragment matches said layout map ID for garbage collection in said garbage collection list, determining whether said data fragment is present at a second location on said data storage system; and</claim-text>
<claim-text>upon a determination that said data fragment is present at said second location on said data storage system, removing said data fragment at said first location.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00002" num="00002">
<claim-text>2. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein said locating a data fragment stored on said data storage structure of said node at a first location comprises:
<claim-text>scanning said data storage structure; and</claim-text>
<claim-text>detecting said data fragment at said first location.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00003" num="00003">
<claim-text>3. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein said second location is a disk mask location.</claim-text>
</claim>
<claim id="CLM-00004" num="00004">
<claim-text>4. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein upon a upon a determination that said layout map ID associated with said data fragment does not match said layout map ID for garbage collection in said garbage collection list, not removing said data fragment at said first location.</claim-text>
</claim>
<claim id="CLM-00005" num="00005">
<claim-text>5. The method of <claim-ref idref="CLM-00004">claim 4</claim-ref>, further comprising:
<claim-text>determining whether scanning said data storage structure is complete; and</claim-text>
<claim-text>upon a determination that scanning said data storage structure is not complete, continuing said scanning.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00006" num="00006">
<claim-text>6. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein upon a determination that said data fragment is not present at said second location on said data storage system, not removing said data fragment at said first location.</claim-text>
</claim>
<claim id="CLM-00007" num="00007">
<claim-text>7. A data storage system including one or more nodes, each of said one or more nodes including one or more data storage structures, said data storage system comprising:
<claim-text>means for determining a garbage collection list for a data storage structure of a node in said data storage system, said garbage collection list including one or more layout map identifiers (IDs) for garbage collection;</claim-text>
<claim-text>means for locating a data fragment stored on said data storage structure at a first location;</claim-text>
<claim-text>means for determining a layout map ID associated with said data fragment;</claim-text>
<claim-text>means for determining whether said layout map ID associated with said data fragment matches a layout map ID for garbage collection in said garbage collection list;</claim-text>
<claim-text>means for determining whether said data fragment is present at a second location on said data storage system; and</claim-text>
<claim-text>means for removing said data fragment at said first location.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00008" num="00008">
<claim-text>8. The data storage system of <claim-ref idref="CLM-00007">claim 7</claim-ref>, further comprising:
<claim-text>means for scanning said data storage structure for said data fragment; and</claim-text>
<claim-text>means for detecting said data fragment at said first location.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00009" num="00009">
<claim-text>9. The data storage system of <claim-ref idref="CLM-00007">claim 7</claim-ref>, wherein said second location is a disk mask location.</claim-text>
</claim>
<claim id="CLM-00010" num="00010">
<claim-text>10. The data storage system of <claim-ref idref="CLM-00007">claim 7</claim-ref>, further comprising:
<claim-text>means for determining whether scanning said data storage structure is complete.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00011" num="00011">
<claim-text>11. The data storage system of <claim-ref idref="CLM-00007">claim 7</claim-ref>, further comprising:
<claim-text>means for determining a current disk mask;</claim-text>
<claim-text>means for determining at least one layout map ID for garbage collection.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00012" num="00012">
<claim-text>12. A computer program product comprising computer-readable instructions embodied as computer readable code for a method comprising:
<claim-text>determining a garbage collection list for a data storage structure of a node in a data storage system, said garbage collection list including one or more layout map identifiers (IDs) for garbage collection;</claim-text>
<claim-text>locating a data fragment stored on said data storage structure at a first location;</claim-text>
<claim-text>determining a layout map ID associated with said data fragment;</claim-text>
<claim-text>determining whether said layout map ID associated with said data fragment matches a layout map ID for garbage collection in said garbage collection list;</claim-text>
<claim-text>upon a determination that said layout map ID of said data fragment matches said layout map ID for garbage collection in said garbage collection list, determining whether said data fragment is present at a second location on said data storage system; and</claim-text>
<claim-text>upon a determination that said data fragment is present at said second location on said data storage system, removing said data fragment at said first location.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00013" num="00013">
<claim-text>13. The computer program product of <claim-ref idref="CLM-00012">claim 12</claim-ref>, wherein said locating a data fragment stored on said data storage structure of said node at a first location comprises:
<claim-text>scanning said data storage structure; and</claim-text>
<claim-text>detecting said data fragment at said first location.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00014" num="00014">
<claim-text>14. The computer program product of <claim-ref idref="CLM-00012">claim 12</claim-ref>, wherein said second location is a disk mask location.</claim-text>
</claim>
<claim id="CLM-00015" num="00015">
<claim-text>15. The computer program product of <claim-ref idref="CLM-00012">claim 12</claim-ref>, wherein upon a upon a determination that said layout map ID associated with said data fragment does not match said layout map ID for garbage collection in said garbage collection list, not removing said data fragment at said first location.</claim-text>
</claim>
<claim id="CLM-00016" num="00016">
<claim-text>16. The computer program product of <claim-ref idref="CLM-00015">claim 15</claim-ref>, said method further comprising:
<claim-text>determining whether scanning said data storage structure is complete; and</claim-text>
<claim-text>upon a determination that scanning said data storage structure is not complete, continuing said scanning.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00017" num="00017">
<claim-text>17. The computer program product of <claim-ref idref="CLM-00012">claim 12</claim-ref>, wherein upon a determination that said data fragment is not present at said second location on said data storage system, not removing said data fragment at said first location.</claim-text>
</claim>
</claims>
</us-patent-grant>
