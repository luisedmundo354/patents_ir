<us-patent-grant lang="EN" dtd-version="v4.2 2006-08-23" file="US07298867-20071120.XML" status="PRODUCTION" id="us-patent-grant" country="US" date-produced="20071106" date-publ="20071120">
<us-bibliographic-data-grant>
<publication-reference>
<document-id>
<country>US</country>
<doc-number>07298867</doc-number>
<kind>B2</kind>
<date>20071120</date>
</document-id>
</publication-reference>
<application-reference appl-type="utility">
<document-id>
<country>US</country>
<doc-number>10781641</doc-number>
<date>20040220</date>
</document-id>
</application-reference>
<us-application-series-code>10</us-application-series-code>
<us-term-of-grant>
<us-term-extension>504</us-term-extension>
</us-term-of-grant>
<classifications-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>G</section>
<class>06</class>
<subclass>K</subclass>
<main-group>9</main-group>
<subgroup>00</subgroup>
<symbol-position>F</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20071120</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
</classifications-ipcr>
<classification-national>
<country>US</country>
<main-classification>382103</main-classification>
</classification-national>
<invention-title id="d0e53">Component association tracker system and method</invention-title>
<references-cited>
<citation>
<patcit num="00001">
<document-id>
<country>US</country>
<doc-number>3586770</doc-number>
<kind>A</kind>
<name>Bonebreak</name>
<date>19710600</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<patcit num="00002">
<document-id>
<country>US</country>
<doc-number>4739401</doc-number>
<kind>A</kind>
<name>Sacks et al.</name>
<date>19880400</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<patcit num="00003">
<document-id>
<country>US</country>
<doc-number>4910786</doc-number>
<kind>A</kind>
<name>Eichel</name>
<date>19900300</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>382199</main-classification></classification-national>
</citation>
<citation>
<patcit num="00004">
<document-id>
<country>US</country>
<doc-number>4942533</doc-number>
<kind>A</kind>
<name>Kakinami et al.</name>
<date>19900700</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>701225</main-classification></classification-national>
</citation>
<citation>
<patcit num="00005">
<document-id>
<country>US</country>
<doc-number>5075875</doc-number>
<kind>A</kind>
<name>Love et al.</name>
<date>19911200</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<patcit num="00006">
<document-id>
<country>US</country>
<doc-number>5103484</doc-number>
<kind>A</kind>
<name>Stafford et al.</name>
<date>19920400</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<patcit num="00007">
<document-id>
<country>US</country>
<doc-number>5233541</doc-number>
<kind>A</kind>
<name>Corwin et al.</name>
<date>19930800</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<patcit num="00008">
<document-id>
<country>US</country>
<doc-number>5283839</doc-number>
<kind>A</kind>
<name>Edelman et al.</name>
<date>19940200</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<patcit num="00009">
<document-id>
<country>US</country>
<doc-number>5323987</doc-number>
<kind>A</kind>
<name>Pinson</name>
<date>19940600</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>244  316</main-classification></classification-national>
</citation>
<citation>
<patcit num="00010">
<document-id>
<country>US</country>
<doc-number>5335298</doc-number>
<kind>A</kind>
<name>Hevenor et al.</name>
<date>19940800</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>382199</main-classification></classification-national>
</citation>
<citation>
<patcit num="00011">
<document-id>
<country>US</country>
<doc-number>5341435</doc-number>
<kind>A</kind>
<name>Corbett et al.</name>
<date>19940800</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<patcit num="00012">
<document-id>
<country>US</country>
<doc-number>5390133</doc-number>
<kind>A</kind>
<name>Sohie</name>
<date>19950200</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<patcit num="00013">
<document-id>
<country>US</country>
<doc-number>5434927</doc-number>
<kind>A</kind>
<name>Brady et al.</name>
<date>19950700</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>382104</main-classification></classification-national>
</citation>
<citation>
<patcit num="00014">
<document-id>
<country>US</country>
<doc-number>5555312</doc-number>
<kind>A</kind>
<name>Shima et al.</name>
<date>19960900</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>382104</main-classification></classification-national>
</citation>
<citation>
<patcit num="00015">
<document-id>
<country>US</country>
<doc-number>5657251</doc-number>
<kind>A</kind>
<name>Fiala</name>
<date>19970800</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<patcit num="00016">
<document-id>
<country>US</country>
<doc-number>5751831</doc-number>
<kind>A</kind>
<name>Ono</name>
<date>19980500</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>382103</main-classification></classification-national>
</citation>
<citation>
<patcit num="00017">
<document-id>
<country>US</country>
<doc-number>5761326</doc-number>
<kind>A</kind>
<name>Brady et al.</name>
<date>19980600</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>382103</main-classification></classification-national>
</citation>
<citation>
<patcit num="00018">
<document-id>
<country>US</country>
<doc-number>5762292</doc-number>
<kind>A</kind>
<name>Schweyer et al.</name>
<date>19980600</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>244  317</main-classification></classification-national>
</citation>
<citation>
<patcit num="00019">
<document-id>
<country>US</country>
<doc-number>5872857</doc-number>
<kind>A</kind>
<name>Chodos et al.</name>
<date>19990200</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>382103</main-classification></classification-national>
</citation>
<citation>
<patcit num="00020">
<document-id>
<country>US</country>
<doc-number>5911004</doc-number>
<kind>A</kind>
<name>Ohuchi et al.</name>
<date>19990600</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<patcit num="00021">
<document-id>
<country>US</country>
<doc-number>5940538</doc-number>
<kind>A</kind>
<name>Spiegel et al.</name>
<date>19990800</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>382236</main-classification></classification-national>
</citation>
<citation>
<patcit num="00022">
<document-id>
<country>US</country>
<doc-number>6075875</doc-number>
<kind>A</kind>
<name>Gu</name>
<date>20000600</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<patcit num="00023">
<document-id>
<country>US</country>
<doc-number>6265704</doc-number>
<kind>B1</kind>
<name>Livingston</name>
<date>20010700</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>2502032</main-classification></classification-national>
</citation>
<citation>
<patcit num="00024">
<document-id>
<country>US</country>
<doc-number>6323987</doc-number>
<kind>B1</kind>
<name>Rinaudo et al.</name>
<date>20011100</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>359260</main-classification></classification-national>
</citation>
<citation>
<patcit num="00025">
<document-id>
<country>US</country>
<doc-number>6445832</doc-number>
<kind>B1</kind>
<name>Lee et al.</name>
<date>20020900</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>382266</main-classification></classification-national>
</citation>
<citation>
<patcit num="00026">
<document-id>
<country>US</country>
<doc-number>6480615</doc-number>
<kind>B1</kind>
<name>Sun et al.</name>
<date>20021100</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>382103</main-classification></classification-national>
</citation>
<citation>
<patcit num="00027">
<document-id>
<country>US</country>
<doc-number>6901110</doc-number>
<kind>B1</kind>
<name>Tsougarakis et al.</name>
<date>20050500</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>37524012</main-classification></classification-national>
</citation>
<citation>
<patcit num="00028">
<document-id>
<country>US</country>
<doc-number>6993158</doc-number>
<kind>B2</kind>
<name>Cho et al.</name>
<date>20060100</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>382103</main-classification></classification-national>
</citation>
<citation>
<patcit num="00029">
<document-id>
<country>US</country>
<doc-number>7110602</doc-number>
<kind>B2</kind>
<name>Krause</name>
<date>20060900</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>382199</main-classification></classification-national>
</citation>
<citation>
<patcit num="00030">
<document-id>
<country>US</country>
<doc-number>2005/0265582</doc-number>
<kind>A1</kind>
<name>Buehler et al.</name>
<date>20051200</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>382103</main-classification></classification-national>
</citation>
<citation>
<nplcit num="00031">
<othercit>Eu-Wei Huang, Wei-Guan Yau, and Li-Chen Fu, “An Edge Based Visual Tracking for Target within Complex Enviroment”, Jun. 2000, pp. 1993-1997, Proceedings of the American Control Conference.</othercit>
</nplcit>
<category>cited by examiner</category>
</citation>
<citation>
<nplcit num="00032">
<othercit>“An Edge Based Visual Tracking for Target within Complex Environment”, En-Wei Huang, Wei-Guan Yan, Li-Chen Fu, Proceedings of the American Control Conference, Jun. 2000.</othercit>
</nplcit>
<category>cited by other</category>
</citation>
</references-cited>
<number-of-claims>12</number-of-claims>
<us-exemplary-claim>1</us-exemplary-claim>
<us-field-of-classification-search>
<classification-national>
<country>US</country>
<main-classification>382103</main-classification>
</classification-national>
</us-field-of-classification-search>
<figures>
<number-of-drawing-sheets>4</number-of-drawing-sheets>
<number-of-figures>5</number-of-figures>
</figures>
<us-related-documents>
<related-publication>
<document-id>
<country>US</country>
<doc-number>20050185822</doc-number>
<kind>A1</kind>
<date>20050825</date>
</document-id>
</related-publication>
</us-related-documents>
<parties>
<applicants>
<applicant sequence="001" app-type="applicant-inventor" designation="us-only">
<addressbook>
<last-name>Slaski</last-name>
<first-name>James</first-name>
<address>
<city>Orlando</city>
<state>FL</state>
<country>US</country>
</address>
</addressbook>
<nationality>
<country>omitted</country>
</nationality>
<residence>
<country>US</country>
</residence>
</applicant>
</applicants>
<agents>
<agent sequence="01" rep-type="attorney">
<addressbook>
<orgname>Birch, Stewart, Kolasch &amp; Birch, LLP</orgname>
<address>
<country>unknown</country>
</address>
</addressbook>
</agent>
</agents>
</parties>
<assignees>
<assignee>
<addressbook>
<orgname>Lockheed Martin Corporation</orgname>
<role>02</role>
<address>
<city>Bethesda</city>
<state>MD</state>
<country>US</country>
</address>
</addressbook>
</assignee>
</assignees>
<examiners>
<primary-examiner>
<last-name>Bella</last-name>
<first-name>Matthew C.</first-name>
<department>2624</department>
</primary-examiner>
<assistant-examiner>
<last-name>Lu</last-name>
<first-name>Tom Y</first-name>
</assistant-examiner>
</examiners>
</us-bibliographic-data-grant>
<abstract id="abstract">
<p id="p-0001" num="0000">A component association tracker is disclosed. The tracker detects, identifies and tracks a target or targets using components extracted from an image. The tracker extracts the components from a window in the image according to edge directions of connected pixels. The tracker associates the components with existing tracks from a track file. The tracks are updated according to information from the associated components.</p>
</abstract>
<drawings id="DRAWINGS">
<figure id="Fig-EMI-D00000" num="00000">
<img id="EMI-D00000" he="171.37mm" wi="184.57mm" file="US07298867-20071120-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00001" num="00001">
<img id="EMI-D00001" he="222.93mm" wi="172.47mm" file="US07298867-20071120-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00002" num="00002">
<img id="EMI-D00002" he="256.46mm" wi="184.83mm" file="US07298867-20071120-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00003" num="00003">
<img id="EMI-D00003" he="217.09mm" wi="156.38mm" orientation="landscape" file="US07298867-20071120-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00004" num="00004">
<img id="EMI-D00004" he="183.81mm" wi="115.91mm" file="US07298867-20071120-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
</drawings>
<description id="description">
<?BRFSUM description="Brief Summary" end="lead"?>
<heading id="h-0001" level="1">FIELD OF THE INVENTION</heading>
<p id="p-0002" num="0001">The present invention relates to a system and method for tracking an object (target) in a sequence of images by tracking the individual components of the object and individual components of the surrounding imagery (clutter). A user may designate particular components as the target to be tracked while other components are considered clutter. For subsequent received images, the components are associated with existing target and clutter markers (tracks) in an efficient manner to provide the target position.</p>
<heading id="h-0002" level="1">BACKGROUND OF THE RELATED ART</heading>
<p id="p-0003" num="0002">Recognizing targets is a challenge to existing detection systems. A function of automatic target recognition is to find candidate targets and to separate them from clutter which commonly includes target detection, classification, and identification. Another related function is to track identified targets by updating target information (e.g., target position) over time. Tracking targets using image data is accomplished by using known processes to recognize data pixels associated with received images (broken down into individual components) of the tracked target. Conventional detection and tracking systems analyze images for pixel distortion to determine a difference to separate the background or clutter from an established or new target.</p>
<p id="p-0004" num="0003">For example, target tracking in fire control and missile seeker applications locate potential targets in high background clutter areas and track the potential targets. Several problems can occur in target tracking. One problem is accomplishing target acquisition and tracking in real time. Processing requirements, especially for image data, can be large so as to prevent timely updates of the target environment. Different environments, targets, and background also pose difficulties. A target tracking system should be flexible and sophisticated enough to accomplish real time image processing.</p>
<p id="p-0005" num="0004">Another problem involves eliminating potential loss-of-lock, or loss of track, of identified targets that occur in high background clutter areas. The background can be cluttered with objects of various shapes and sizes. The components derived from these objects interfere with tracked target components. Further, discrimination between true and false targets is problematic in high background clutter. Thus, target tracking systems strive to reduce the effect of background clutter and to improve target identification and tracking.</p>
<p id="p-0006" num="0005">Conventional target trackers use image correlation to update target position and direction. These trackers analyze image data to determine an updated position and direction of the track target. Conventional trackers, however, can suffer from input error and from problems with image processing that impede target tracking. For example, a global shift can occur in a received target image from a previous target image. An association algorithm for a conventional target tracker does not account for this global shift, and established targets are improperly tracked. Data derived from the image can be associated with the wrong target in a multi-target tracker. Global shift problems also can be common in missile and other airborne seeker applications that change directions, altitude, flight paths, speed and the like.</p>
<p id="p-0007" num="0006">Received images also are filtered and processed using conventional methods to detect components that are associated with tracked targets. Problems occur, however, when peaks of energy of the component pixels are not so readily identified compared with the background or clutter. Further, the derived components may not be readily associated with an established target, or a new target. These efforts waste processing resources. Thus, conventional target tracking systems suffer from identification and tracking errors or inefficient tracking that wastes resources. Moreover, the inability to identify targets against high background clutter impede the effectiveness of detection and tracking systems.</p>
<heading id="h-0003" level="1">SUMMARY OF THE INVENTION</heading>
<p id="p-0008" num="0007">Accordingly, the disclosed embodiments of the present invention are directed to a component association tracking system and method that reduce the problems associated with the conventional tracking systems described above. The disclosed embodiments of the present invention enhance tracking of established and new targets while efficiently utilizing processing resources.</p>
<p id="p-0009" num="0008">According to one exemplary embodiment of the present invention, a method for tracking a target in an image is disclosed. The target is associated with an image component marker (track) defined as a target track. The method includes extracting at least one component from a received image, within a search window established in the image, according to an edge direction of connected pixels within the component. The window may be a predetermined search window associated with an estimated target position. The method also includes selecting the component to update the track according to the association. The method also includes updating the track with the edge direction of the component.</p>
<p id="p-0010" num="0009">Additional features and advantages of the present invention are set forth in the description that follows and, in part, will be implied by the description or can be learned from practice of the invention. The objectives and other advantages of the invention are realized and attained by the structure and methods particularly pointed out in the written description and the claims, as well as the appended drawings.</p>
<?BRFSUM description="Brief Summary" end="tail"?>
<?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?>
<description-of-drawings>
<heading id="h-0004" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading>
<p id="p-0011" num="0010">The accompanying drawings, which are included to provide a further understanding of the disclosed embodiments of the present invention, illustrate the invention and, together with the description, serve to explain the principles of the invention. In the drawings:</p>
<p id="p-0012" num="0011"><figref idref="DRAWINGS">FIG. 1</figref> illustrates a detection and tracking system according to embodiments of the present invention.</p>
<p id="p-0013" num="0012"><figref idref="DRAWINGS">FIG. 2A</figref> illustrates a flowchart for associating components for target identification and tracking using a component association tracker according to the embodiments of the present invention.</p>
<p id="p-0014" num="0013"><figref idref="DRAWINGS">FIG. 2B</figref> illustrates plurality of edge directions that may be designated for image components for target identification and tracking using a component association tracker according to the embodiments of the present invention.</p>
<p id="p-0015" num="0014"><figref idref="DRAWINGS">FIG. 3</figref> illustrates tracks and components in a current frame of an image according to embodiments of the present invention.</p>
<p id="p-0016" num="0015"><figref idref="DRAWINGS">FIG. 4</figref> illustrates a flowchart for associating tracks and components according to embodiments of the present invention.</p>
</description-of-drawings>
<?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?>
<?DETDESC description="Detailed Description" end="lead"?>
<heading id="h-0005" level="1">DETAILED DESCRIPTION OF THE PREFERRED EMBODIMENTS</heading>
<p id="p-0017" num="0016"><figref idref="DRAWINGS">FIG. 1</figref> depicts a target detection, identification and tracking system <b>100</b> according to embodiments of the present invention. Detection system <b>100</b> detects and tracks candidate targets and any other objects of interest. Detection system <b>100</b> operates in any environment, and preferably operates on an airborne platform that seeks, detects, identifies and tracks targets in the air, on the ground, on water, under water, and the like. More preferably, detection system <b>100</b> is configured within a missile seeker that detects and tracks targets.</p>
<p id="p-0018" num="0017">Detection system <b>100</b> includes detectors <b>102</b> that receive information from the environment and detect candidate targets <b>188</b>. The information includes radiant flux for infrared detection, or any other energy detectable by detectors <b>102</b>. Alternatively, detectors <b>102</b> can receive electromagnetic or radio frequency energy. Using known processes, detectors <b>102</b> and any accompanying hardware and software generate an image <b>180</b>. Image <b>180</b> includes image data regarding targets <b>188</b>. Detectors <b>102</b> are any known detectors, such as infrared detectors, radar detectors, a camera or cameras, sensors, or any device capable of capturing data regarding targets <b>188</b> for a certain area. Detectors <b>102</b> are coupled directly or indirectly to tracker <b>104</b>.</p>
<p id="p-0019" num="0018">Tracker <b>104</b> receives image <b>180</b>. Tracker <b>104</b> can refine or perform processing operations on image <b>180</b>. Preferably, image <b>180</b> contains any information regarding potential candidate targets. More preferably, image <b>180</b> comprises pixels. Each pixel can have different characteristics, such as brightness, color, contrast, and the like. Image <b>180</b> includes one or more targets which may be tracked such as targets <b>188</b>. Targets which are being tracked, such as targets <b>188</b>, are received as a sequence of images <b>180</b>. The sequence of images <b>180</b> including the target object <b>188</b> may be broken down into a plurality of individual image components as well as the surrounding imagery (clutter). Tracker <b>104</b> tracks the locations of the individual components for these by associating the components with existing image component markers, referred to as tracks, that are stored in a track file <b>116</b>. Track file <b>116</b> can include three types of tracks (image component markers), discussed in greater detail below.</p>
<p id="p-0020" num="0019">Tracker <b>104</b> includes processor <b>106</b>, memory <b>110</b>, and instructions <b>108</b>. Processor <b>106</b> executes instructions <b>108</b> to perform functions for tracker <b>104</b>. Preferably, instructions <b>108</b> are software code used to execute a software program stored on tracker <b>104</b>. Instructions <b>108</b> can be stored in memory <b>110</b> to perform operations on tracker <b>104</b>. Alternatively, instructions <b>108</b> may reside in another source within or coupled to tracker <b>104</b>. Processor <b>106</b> performs computations, actions, and other functions to track targets identified in image <b>180</b> according to instructions <b>108</b>.</p>
<p id="p-0021" num="0020">Instructions <b>108</b> guide the operations of tracker <b>104</b>. Instructions <b>108</b> include instructions to identify components within image <b>180</b>, and to perform any of the processes disclosed below. Instructions <b>108</b> also include instructions for other functions executable on tracker <b>104</b>.</p>
<p id="p-0022" num="0021">Specifically, tracker <b>104</b> processes or analyzes image data within image <b>180</b>. The results help predict the position of a target, such as targets <b>188</b>. The positions of tracks, associated with targets and non-targets, also are predicted. Tracker <b>104</b> breaks up potentially interesting pieces of image <b>180</b> for further consideration according to the predicted positions. The potentially interesting pieces are referred to as “components”. A target, along with surrounding clutter, is broken up into multiple components within image <b>180</b>. A target/clutter detection method similar to the method disclosed in commonly assigned U.S. patent application Ser. No. 10/465,723, filed on Jun. 20, 2003, which is hereby incorporated by reference and entitled “TARGET DETECTION SYSTEM USING TRAINED AND UNTRAINED DETECTION AND METHODS THEREFOR”, may be used.</p>
<p id="p-0023" num="0022">All of the identified components are tracked individually through a track/component association process executed on processor <b>106</b> that attempts to find an optimal match between the existing tracks and the components identified from image <b>180</b>. The tracking of individual components within image <b>180</b>, rather than the entire image (or target image) allows for flexible constraints to be placed on the tracking operations of tracker <b>104</b>.</p>
<p id="p-0024" num="0023">The tracking by tracker <b>104</b> is based on segmenting image <b>180</b> into small edge components. A target, as well as one or more clutter objects, within image <b>180</b> that is being tracked may be segmented into a plurality of individual components. The locations of individual components may be tracked using existing tracks (image component markers) stored in track file <b>116</b> on tracker <b>104</b>. The image component markers (tracks) stored in track file <b>116</b> may have been generated using individual components previously segmented from received image <b>180</b>.</p>
<p id="p-0025" num="0024">Track file <b>116</b> preferably includes of three types of tracks. Non-established tracks are tracks that have been in existence for a short period of time and can be caused by spurious components not associated with a target, such as clutter or background. Established tracks have been in existence for at least a specified minimum amount of time and have been associated with received components consistently. Target tracks are a subset of established tracks and have position coordinates relative to a target being tracked. Additional types of tracks may be included in track file <b>116</b>.</p>
<p id="p-0026" num="0025">Preferably, to initiate a target track, on the first frame of an image sequence the user or other entity designates a rectangular region that contains the target or regions that contain targets. This region may be segmented into image components that are designated as the target and may be used to create “established”, target tracks. All other image components, outside of the rectangular region, may be designated as clutter and may be used to create “established”, non-target tracks. Preferably, this process is only used on the first frame of the received image <b>180</b>. Alternatively, the rectangular region may be automatically designated via an algorithm or preset conditions. Target initialization allows the track file to be initialized from a “start” position. If more than one target is applicable, then a plurality of rectangular regions are created.</p>
<p id="p-0027" num="0026">Further, processor <b>106</b> can determine when a non-established track should be converted into an established track, and when an established track should be converted into a target track. In addition, processor <b>106</b> facilitates the identification and generation of new tracks by using received components within image <b>180</b> that are not associated with existing tracks. Memory <b>110</b> stores information regarding target tracks identified by tracker <b>104</b>.</p>
<p id="p-0028" num="0027">Tracker <b>104</b> continually predicts the updated position of a target using individual image components segmented from received image <b>180</b>. Initially for the first received image frame <b>180</b>, tracker <b>104</b>, in response to user selection, establishes a target track and non-target tracks from segmented components from received image <b>180</b> which are stored in track file <b>116</b>. For future received image frames <b>180</b>, tracker <b>104</b> uses linear prediction to predict the position of stored, non-target tracks in the current received image frame <b>180</b> based on each track record, in the track file <b>116</b>, including the track's position in pixels relative to the target center, and the velocity of the track in pixels per frame. Also, the target track is fixedly positioned relative to the target center (normalized to the target size) allowing prediction of the target position from the prediction of the target track position or vice versa. Track file <b>116</b> can be stored in memory <b>110</b>. Alternatively, track file <b>116</b> is stored in another memory location, or even another storage medium, such as a disk, portable memory, mobile device, and the like.</p>
<p id="p-0029" num="0028">After predicting of target position and non-target track positions, image components may be extracted/identified from the received image <b>180</b> around the predicted target position. Preferably, the extracted components are associated with established tracks that have the same edge direction, and fall within a search window (e.g., rectangular) around the predicted track position in received image <b>180</b>. Preferably, the search window may be sized large to account for target movement and changes in the size of the field of view (FOV) from the motion (frame-by-frame) of sensors being used by detectors <b>102</b>.</p>
<p id="p-0030" num="0029">From the received image <b>180</b>, tracker <b>104</b> extracts and produces a plurality of components that may be potentially associated with an established track including determining the best one-to-one association, or global optimization, between tracks and components identified from image <b>180</b>. Tracker <b>104</b> uses final track-to-component pairings to update the established tracks and to predict current position of target <b>180</b>.</p>
<p id="p-0031" num="0030">The updated established tracks and target positions are communicated to a user or any other entity via display/output <b>130</b>. Display/output <b>130</b> can be any device, medium, means, system, component or the like that is able to display or output target identification, detection, or track information. Display/output <b>130</b> may be a display monitor coupled to tracker <b>104</b>. For an exemplary missile application where the tracker <b>104</b> is used to detect, track, and destroy opposition missiles, tracker <b>104</b> may feed the output to a missile autopilot to control the flight path of the missile without requiring use of a display.</p>
<p id="p-0032" num="0031">Any components that have not been associated with established tracks are associated with non-established tracks by tracker <b>104</b>. Preferably, tracker <b>104</b> uses a simple nearest-neighbor association that searches over a smaller area than the previous established track correlation within image <b>180</b>. If any components are unused after the second association step, new non-established tracks are created from these non-associated components identified in image <b>180</b>. Tracker <b>104</b> then performs track file maintenance including deleting old tracks, and promoting tracks to established or target status in accordance with specified criteria. These features are disclosed in greater detail below.</p>
<p id="p-0033" num="0032">Input <b>120</b> also is coupled to tracker <b>104</b> to allow data input or interface with a user or other entity. Tracker <b>104</b> can be modified or constrained according to system requirements/specifications or user-defined requirements/specifications. Input <b>120</b> can be any device, medium, means, or system that allows the input of data or requirements/specifications to tracker <b>104</b>. For example, a user inputs instructions to further define operations on processor <b>106</b> using input <b>120</b>. In this example, input <b>120</b> is a keyboard attached to a computer executing instructions <b>108</b> to detect, identify and track targets using tracker <b>104</b>. For an exemplary missile application, data input may come from a missile launcher interface where data is transmitted from pilot controls.</p>
<p id="p-0034" num="0033">Thus, tracker <b>104</b> is a component association tracker according to the disclosed embodiments of the present invention that receives image <b>180</b> for detecting and tracking targets <b>188</b>. Tracker <b>104</b> segments image <b>180</b> into components based on the edge direction of groups of connected pixels within image <b>180</b>.</p>
<p id="p-0035" num="0034">Tracker <b>104</b> also includes logic that controls how the target and clutter appearance is allowed to change. This logic is controlled by processor <b>106</b> and instructions <b>108</b>. Further, instructions <b>108</b> may include an algorithm that can be adjusted to balance performance of target tracker <b>104</b> versus processor load on processor <b>106</b>. Thus, tracker <b>104</b> enables dynamic target detection, classification, identification and tracking according to specified requirements and parameters.</p>
<p id="p-0036" num="0035"><figref idref="DRAWINGS">FIG. 2A</figref> is a flowchart for associating components for target identification and tracking using a component association tracker according to the embodiments of the present invention. Preferably, the flowchart of <figref idref="DRAWINGS">FIG. 2A</figref> is implemented by the hardware and software configuration disclosed by <figref idref="DRAWINGS">FIG. 1</figref>. Neither <figref idref="DRAWINGS">FIG. 1</figref> nor <figref idref="DRAWINGS">FIG. 2A</figref>, however, limits each other in their applicability to the embodiments of the present invention.</p>
<p id="p-0037" num="0036">Step <b>202</b> executes by receiving an image at the component association tracker which may be received from any source. Preferably, the image is received from detectors and/or sensors. Referring back to <figref idref="DRAWINGS">FIG. 1</figref>, image <b>180</b> is received at tracker <b>104</b> after being generated by detectors <b>102</b>. Advantageously, detectors <b>102</b> may include a plurality of sensors including millimeter-wave (MMW) and/or infrared (IR) sensors. Also, during this step, targets (and clutter) may be designated and target tracks (and established, non-target tracks) may be created as described above.</p>
<p id="p-0038" num="0037">Step <b>204</b> executes by calculating threshold values for the input image received in step <b>202</b> using a Sobel edge determination algorithm. Step <b>204</b> analyzes groups of connected pixels with the same edge direction that potentially forms each image component within the input image. Using a predetermined threshold, component association tracker <b>104</b> determines whether connected pixels have the same edge direction and should be grouped together as a potential image component. Advantageously, the threshold may be set as a percentage of pixels. The edge magnitude may be histogrammed and the threshold may be set such that a certain, predetermined percentage of the pixels will satisfy and pass. For example, the percentage may be initially set to 10%, and can be varied according to processing load and performance considerations. In alternative embodiments, the threshold may be dynamically adjusted on a frame-to-frame basis to account for changing conditions (e.g., weather). Advantageously, the threshold may be dynamically adjusted to keep the number of components between predetermined upper and lower bounds.</p>
<p id="p-0039" num="0038">Step <b>206</b> executes by extracting image components from the received image. Step <b>206</b> extracts groups of pixels with the same edge direction to form each component. For example, image <b>180</b> may be segmented into a plurality of discrete components, using discrete search windows for each component, after being input to tracker <b>104</b>. Each component includes a portion of the input image <b>180</b>. The size of the image portion (subimage) preferably is three times the estimated target size in width and height of image <b>180</b>, but should also be constrained to a maximum size to limit the processing load on processor <b>106</b>.</p>
<p id="p-0040" num="0039">The extracted image components may be smoothed using a 3×3 mean filter and then a gradient operator may be applied to the components to produce gradient magnitude and direction. The functions of both smoothing and gradient calculation may be achieved by using a Sobel filter. At each pixel, x and y direction gradient values are produced, dX and dY. The gradient magnitude is calculated as sqrt (dX<sup>2</sup>+dY<sup>2</sup>) and the direction as arctan(dX/dY). The gradient direction may be then quantized to n discrete edge directions that are sequentially numbered 1 to n. In an exemplary embodiment, the gradient direction may be quantized to eight directions, labeled <b>1</b> to <b>8</b>, illustrated by octagon <b>250</b> as shown in <figref idref="DRAWINGS">FIG. 2B</figref>. Further, the eight directions (<b>1</b> to <b>8</b>) may be distinguished by color coding (e.g., 1-blue, 8-yellow) during input image <b>180</b> processing by tracker <b>104</b>. The threshold described herein may be applied to the gradient magnitude, and any pixels that are below this threshold have both the gradient magnitude and the direction set to zero.</p>
<p id="p-0041" num="0040">After the gradient magnitude threshold is applied, image <b>180</b> is broken down into the extracted components. As defined above, a component is a group of pixels that has the same non-zero gradient direction and are connected horizontally or vertically within image <b>180</b>. Components that contain pixels at the edges of the window encompassing the subimage are ignored, because it is likely that the entire component is not being detected. The ignored component preferably is being tracked in another window. The number of pixels in a component also should exceed a threshold, initialized to 1 pixel, that can be adjusted to control processing load on processor <b>106</b>. Components not meeting this threshold are discarded, thereby preventing processor <b>106</b> from performing unnecessary operations and associations. Thus, step <b>206</b> extracts components by extracting the essential data required for each component by tracker <b>104</b>. This data includes, at the least, the edge direction and the x- and y-centroids of the component pixels within image <b>180</b>.</p>
<p id="p-0042" num="0041">Step <b>208</b> executes by associating the extracted components with established tracks (image component markers). Preferably, step <b>208</b> matches the tracks with all nearby components that have the same edge direction. For each established track, tracker <b>104</b> finds all the components in a current received frame of image <b>180</b> that fall within a search window around the predicted target position having the same edge direction as the established track. Preferably, the position coordinates of every track may be relative to the target center. A search for components within a search window, relative to an established track, may uncover no components or a plurality of components depending on finding a match between the edge direction for the component and the established track within a predetermined distance. These features are disclosed in greater detail with reference to <figref idref="DRAWINGS">FIGS. 3</figref>, <b>4</b> and Table 3.</p>
<p id="p-0043" num="0042">Step <b>210</b> executes by determining the best set of associations between established tracks and extracted components from received image <b>180</b>. In step <b>210</b>, possible associations are searched to find the “best” global match between tracks and components. The list of possible track/component associations are processed to determine the best set of one-to-one track/component associations. Each track/component association is assigned a value, or “weight,” and a set of associations that produces the minimum total weight (the “best” association) is determined. The association processes of the present invention are disclosed in greater detail below with reference to <figref idref="DRAWINGS">FIGS. 3 and 4</figref>.</p>
<p id="p-0044" num="0043">Step <b>212</b> executes by estimating target position for a target or targets, such as targets <b>188</b> within image <b>180</b>. Tracks, whose positions are relative to the target center, are designated as target tracks and are used to update the current target position according to the extracted image components from the input <b>180</b>. The position and velocity of the target being tracked is updated using the mean position for extracted components, on the current frame of image <b>180</b>, associated with target tracks. Step <b>212</b> also sends the determined (calculated) target position to step <b>226</b> to update the target state of that target associated with the track.</p>
<p id="p-0045" num="0044">Step <b>214</b> executes by updating established tracks from track file <b>116</b> of tracker <b>104</b>. Established tracks are updated based on the previously determined track/component associations, as disclosed above. Established tracks are those tracks that have been in existence for a predetermined number of frames of image <b>180</b>. Step <b>216</b> executes by updating non-established tracks within track file <b>116</b>. Non-established tracks are updated with components that did not associate with established tracks by testing for association with these non-established tracks.</p>
<p id="p-0046" num="0045">Starting with the oldest tracks, the extracted image components from image <b>180</b> are searched for components that fall within a search window outside of previous search windows used for extracting components associated with the target track or established tracks. If any of the components fall within this window, the component closest to a track's predicted position is assigned to that track, and the track data is updated with the non-established track/component association.</p>
<p id="p-0047" num="0046">Step <b>218</b> executes by forming new tracks. New tracks are formed from any unused components left over from the previous steps which did not associated with target, established, or non-established tracks. The component position relative to a target and the component edge direction are copied to the new track, the velocity is set to zero, and all counters and flags are set to initial values. The component edge direction that is copied to the new track becomes the edge direction for the new track and remains for the life of the track. Preferably, the new track may only be updated by associating components having the same edge direction. Thus, all extracted components are accounted for by association with either established or non-established tracks, or the extracted components are used to form new tracks. All of these actions result in an update to track file <b>116</b> on tracker <b>104</b>.</p>
<p id="p-0048" num="0047">Step <b>220</b> executes by deleting and promoting tracks. Tracks that have been in existence for a predetermined number of frames will be either set to “established” (promoted) or deleted from the track file <b>116</b>. In step <b>220</b>, newly established tracks are promoted and marked as target or clutter. Step <b>220</b> also may be known as “track file maintenance” that updates the state of the tracks in track file <b>116</b>. After track-component association initially occurs, a count is maintained of the total number of times a track is associated with a component. If this count exceeds a threshold, the track is set to “established” as disclosed above. Otherwise, the track is deleted from track file <b>116</b>. Newly established tracks that fall within the target region and do not have significant motion relative to the target center are designated as “target tracks”.</p>
<p id="p-0049" num="0048">Step <b>220</b> may also delete tracks that no longer correlate to components within image <b>180</b>. A track is deleted if it “coasts” which is defined as not associating with a component for a predetermined amount of time after initial track creation/association during receiving of updated images <b>180</b>. For determining whether a track should be deleted, each time a track associates to an extracted component, a counter is incremented until a predetermined maximum value is reached, as disclosed above. Each time the track does not associate to an extracted component (“coasting”), the counter is decremented. When the counter reaches zero, after being decremented due to non-associating (“coasting”), the track is deleted from track file <b>116</b>.</p>
<p id="p-0050" num="0049">Step <b>222</b> executes by updating track file <b>116</b> in accordance with the results of steps <b>214</b>, <b>216</b>, <b>218</b>, and <b>220</b>. Updating track file <b>116</b> includes promoting tracks to established tracks and accounting for new tracks formed by unassociated, extracted components. In addition, deleted tracks are removed from track file <b>116</b>. Updated track file <b>116</b> is available for further processing when step <b>208</b> is repeated in response to an updated image <b>180</b> being received by tracker <b>104</b>. This feature allows for newly established tracks to be accounted for in associating extracted components in subsequently received images.</p>
<p id="p-0051" num="0050">Step <b>224</b> executes by estimating target growth. The comparison of the initial, predicted position of the target from step <b>202</b> and the updated target position from step <b>212</b> generates target growth. Step <b>224</b> receives the updated target track, as well as the existing target state. From steps <b>212</b>, <b>214</b>, the target position may be determined and provided to a user or other entity via display/output <b>130</b>. Promoted tracks identified as target or clutter (non-target, established tracks initially designated in step <b>202</b>) may also be considered in updating target position.</p>
<p id="p-0052" num="0051"><figref idref="DRAWINGS">FIG. 3</figref> depicts tracks and components in a current frame of an image according to the present invention. <figref idref="DRAWINGS">FIG. 3</figref> shows image frame <b>300</b> of a received image, such as image <b>180</b> of <figref idref="DRAWINGS">FIG. 1</figref>. Search window <b>302</b> is identified in frame <b>300</b>. Frame <b>300</b> can include additional search windows and is not limited to search window <b>302</b>. Search window <b>302</b> is shown for the sake of brevity and ease of explanation. Search window <b>302</b> corresponds to a target identified within frame <b>300</b>.</p>
<p id="p-0053" num="0052">Search window <b>302</b> surrounds tracks and components that may, or may not, correspond to that target. Preferably, window <b>302</b> has a rectangular shape. Search window <b>302</b> is a portion of the input image that is extracted around a predicted target location. Preferably, the size of window <b>302</b> is three times the estimated target size. The size, however, of window <b>302</b> can be constrained due to processing requirements and limits to any specified parameter, such as width, height, radius, and the like.</p>
<p id="p-0054" num="0053">Referring to <figref idref="DRAWINGS">FIG. 3</figref>, tracks <b>310</b>, <b>312</b>, and <b>314</b> are within window <b>302</b>. Components <b>320</b>, <b>322</b>, and <b>324</b> also are within window <b>302</b>. Tracks <b>310</b>, <b>312</b>, and <b>314</b> are known as tracks <b>1</b>, <b>2</b>, and <b>3</b>, respectively. Components <b>320</b>, <b>322</b>, and <b>324</b> are known as components <b>1</b>, <b>2</b>, and <b>3</b>, respectively. Window <b>302</b>, in alternative embodiments, can have additional tracks and components, less tracks and components, or even no tracks and components. Window <b>302</b> of the present invention is not limited by the tracks and components depicted in <figref idref="DRAWINGS">FIG. 3</figref>.</p>
<p id="p-0055" num="0054">Tracks <b>310</b>, <b>312</b>, and <b>314</b> can be established or non-established tracks. For example, tracks <b>310</b> and <b>312</b> can be established tracks, while track <b>314</b> is a non-established track. Components <b>320</b>, <b>322</b>, and <b>324</b> are then associated with the tracks, as disclosed above. For example, components <b>320</b>, <b>322</b>, and <b>324</b> are within the search window of existing tracks <b>310</b>, <b>312</b>, and <b>314</b>. A list is compiled of possible track-to-component associations via processing to determine the best set of one-to-one track-to-component associations. Further, the possible directions of components <b>320</b>, <b>322</b>, and <b>324</b> are determined when they are extracted. A possible direction is determined by analyzing the pixels within the component. Tracks <b>310</b>, <b>312</b>, and <b>314</b> also have directions. The associating process is disclosed in greater detail with reference to <figref idref="DRAWINGS">FIG. 4</figref> below which takes into account the edge direction and position for each track and component to determine potential association pairs.</p>
<p id="p-0056" num="0055"><figref idref="DRAWINGS">FIG. 4</figref> depicts a flowchart for associating tracks and components according to embodiments of the present invention. <figref idref="DRAWINGS">FIG. 4</figref> describes exemplary steps <b>206</b>, <b>208</b>, <b>210</b>, <b>212</b>, <b>214</b>, <b>216</b>, and <b>218</b> of <figref idref="DRAWINGS">FIG. 2A</figref> in more detail but these analogous steps of <figref idref="DRAWINGS">FIG. 2A</figref> are not limited by the disclosure of <figref idref="DRAWINGS">FIG. 4</figref>.</p>
<p id="p-0057" num="0056">After receiving the input image <b>180</b> at step <b>202</b> of <figref idref="DRAWINGS">FIG. 2A</figref>, step <b>402</b> may execute by determining current track positions and edge directions based on prior track history as previously described using either linear prediction for non-target tracks or calculation of target track position relative to current target center position. Alternatively, if the input image is received at target and track initialization, as previously described image components are extracted from the input image <b>180</b> in response to user selection to create initial target and non-target tracks.</p>
<p id="p-0058" num="0057">Referring to <figref idref="DRAWINGS">FIG. 3</figref>, the track positions and edge directions of tracks <b>310</b>, <b>312</b>, and <b>314</b> are determined within window <b>302</b> using an exemplary coordinate system <b>350</b>. The track position may be determined by predicting the current position of the track based on its previous position and velocity where the positions are relative to the center of the target. The edge direction of the track may be a fixed value determined by the edge direction of the component which initially formed the track as previously described during track initialization or as performed in step <b>218</b>. Tracks <b>310</b>, <b>312</b>, and <b>314</b> are within a specified distance of the predicted target location as initially designated in step <b>202</b> or updated in step <b>212</b>. The track file, such as track file <b>116</b>, stores the tracks. Advantageously, every track has a position and edge direction that are determined by the components which formed the track in prior frames. Preferably, the edge direction is given as one of eight directions, as shown in <figref idref="DRAWINGS">FIG. 2B</figref>, and may be relative to an x-y coordinate axis as shown in <figref idref="DRAWINGS">FIG. 3</figref> for each track and component illustrated. The directions shown in <figref idref="DRAWINGS">FIG. 2B</figref> are solely exemplary as in an exemplary embodiment, Direction <b>1</b> may be “up” or when x=0 and y&lt;0 in reference to an x-y coordinate system as shown in <figref idref="DRAWINGS">FIG. 3</figref>. For example, Table 1 below lists the tracks with their positions and edge directions:</p>
<p id="p-0059" num="0058">
<tables id="TABLE-US-00001" num="00001">
<table frame="none" colsep="0" rowsep="0">
<tgroup align="left" colsep="0" rowsep="0" cols="3">
<colspec colname="1" colwidth="91pt" align="center"/>
<colspec colname="2" colwidth="35pt" align="left"/>
<colspec colname="3" colwidth="91pt" align="center"/>
<thead>
<row>
<entry namest="1" nameend="3" rowsep="1">TABLE 1</entry>
</row>
<row>
<entry namest="1" nameend="3" align="center" rowsep="1"/>
</row>
<row>
<entry>TRK</entry>
<entry>POS</entry>
<entry>DIR</entry>
</row>
<row>
<entry namest="1" nameend="3" align="center" rowsep="1"/>
</row>
</thead>
<tbody valign="top">
<row>
<entry>1</entry>
<entry>  0.5, −3</entry>
<entry>1</entry>
</row>
<row>
<entry>2</entry>
<entry>  −3, 1</entry>
<entry>7</entry>
</row>
<row>
<entry>3</entry>
<entry>    3, 1</entry>
<entry>7</entry>
</row>
<row>
<entry namest="1" nameend="3" align="center" rowsep="1"/>
</row>
</tbody>
</tgroup>
</table>
</tables>
</p>
<p id="p-0060" num="0059">Referring to track <b>1</b>, which corresponds to track <b>310</b> in <figref idref="DRAWINGS">FIG. 3</figref>, it is determined that it has a direction of <b>1</b> (up) and a position of 0.5, −3 on the x-y-coordinate axis.</p>
<p id="p-0061" num="0060">Step <b>404</b>, analogous to step <b>206</b>, executes by determining the component positions and edge directions, such as components <b>320</b>, <b>322</b>, and <b>324</b> within window <b>302</b> of <figref idref="DRAWINGS">FIG. 3</figref> for extracted image components from received image <b>180</b>. The component positions may be in a sensor coordinate frame using sensors in detectors <b>102</b> which are preferably x and y pixel locations within an image. The component edge directions are the quantized gradient directions calculated in step <b>206</b>. The components are extracted from the image of current frame <b>300</b>, for example, of <figref idref="DRAWINGS">FIG. 3</figref>. Table 2 below shows the component positions and edge directions:</p>
<p id="p-0062" num="0061">
<tables id="TABLE-US-00002" num="00002">
<table frame="none" colsep="0" rowsep="0">
<tgroup align="left" colsep="0" rowsep="0" cols="3">
<colspec colname="1" colwidth="91pt" align="center"/>
<colspec colname="2" colwidth="35pt" align="left"/>
<colspec colname="3" colwidth="91pt" align="center"/>
<thead>
<row>
<entry namest="1" nameend="3" rowsep="1">TABLE 2</entry>
</row>
<row>
<entry namest="1" nameend="3" align="center" rowsep="1"/>
</row>
<row>
<entry>COMP</entry>
<entry>POS</entry>
<entry>DIR</entry>
</row>
<row>
<entry namest="1" nameend="3" align="center" rowsep="1"/>
</row>
</thead>
<tbody valign="top">
<row>
<entry>1</entry>
<entry>  9, 8.5</entry>
<entry>7</entry>
</row>
<row>
<entry>2</entry>
<entry>11.5, 5</entry>
<entry>1</entry>
</row>
<row>
<entry>3</entry>
<entry>  21, 10</entry>
<entry>7</entry>
</row>
<row>
<entry namest="1" nameend="3" align="center" rowsep="1"/>
</row>
</tbody>
</tgroup>
</table>
</tables>
</p>
<p id="p-0063" num="0062">Steps <b>406</b>, <b>408</b>, <b>410</b> may be preferably executed within steps <b>208</b>, <b>210</b> of <figref idref="DRAWINGS">FIG. 2A</figref>. For an exemplary embodiment, step <b>406</b> executes by determining possible track-component associations and the target position that would be derived from each association. Possible track-component associations are determined from the track and component list which list the positions and directions for each track and component as shown in Tables 1 and 2. For example, all the possible associations of tracks <b>310</b>, <b>312</b>, and <b>314</b> and components <b>320</b>, <b>322</b>, and <b>324</b> shown in <figref idref="DRAWINGS">FIG. 3</figref> may be determined by associating tracks and components with the same edge direction and within a predetermined distance of each other. Further, the target position is calculated that is implied by each track-component association. Since each track position is relative to the target center, and the component position is in received image coordinates, the track-component combination gives a potential location for the target in image coordinates. In accordance with user criteria, the target may be within a rectangular region including the component associated with a track. Based on the position and velocity of the track and component, an estimation may be made for the target position as shown in Table 3. For example, as shown in Table 3, the potential target position (center) may be 11,8 for track <b>1</b>/component <b>2</b> association (component <b>2</b> located at 11.5, 5) since the target is predicted to fall within a rectangular region encompassing component <b>2</b> and is moving in an upward direction.</p>
<p id="p-0064" num="0063">The potential target position listed in Table 3 is measured in absolute value (x, y) coordinate distance away from the track position as the target track has a fixed position relative to the target center, normalized to the target size. For example for the track <b>1</b>/component <b>2</b> pair, track <b>1</b> is located at (0.5, −3) as indicated in Table 1 and component <b>2</b> is located at (11.5, 5) as indicated in Table 2. Therefore, the potential target position lies an absolute value (x, y) distance away of (|11.5−0.5|, |5−(−3)|) equaling (11, 8) distance away. Table 3 below shows the possible track-component combinations and target positions of the tracks listed in Table 1 and the components listed in Table 2:</p>
<p id="p-0065" num="0064">
<tables id="TABLE-US-00003" num="00003">
<table frame="none" colsep="0" rowsep="0">
<tgroup align="left" colsep="0" rowsep="0" cols="4">
<colspec colname="offset" colwidth="28pt" align="left"/>
<colspec colname="1" colwidth="21pt" align="center"/>
<colspec colname="2" colwidth="70pt" align="center"/>
<colspec colname="3" colwidth="98pt" align="left"/>
<thead>
<row>
<entry/>
<entry namest="offset" nameend="3" rowsep="1">TABLE 3</entry>
</row>
<row>
<entry/>
<entry namest="offset" nameend="3" align="center" rowsep="1"/>
</row>
<row>
<entry/>
<entry/>
<entry/>
<entry>POTENTIAL</entry>
</row>
<row>
<entry/>
<entry/>
<entry/>
<entry>TGT POS DIST</entry>
</row>
<row>
<entry/>
<entry>TRK</entry>
<entry>COMP</entry>
<entry>(in abs [X, Y] coord)</entry>
</row>
<row>
<entry/>
<entry namest="offset" nameend="3" align="center" rowsep="1"/>
</row>
</thead>
<tbody valign="top">
<row>
<entry/>
<entry>1</entry>
<entry>2</entry>
<entry>11, 8</entry>
</row>
<row>
<entry/>
<entry>2</entry>
<entry>1</entry>
<entry>12, 7.5</entry>
</row>
<row>
<entry/>
<entry>2</entry>
<entry>3</entry>
<entry>24, 9</entry>
</row>
<row>
<entry/>
<entry>3</entry>
<entry>1</entry>
<entry> 6, 7.5</entry>
</row>
<row>
<entry/>
<entry>3</entry>
<entry>3</entry>
<entry>18, 9</entry>
</row>
<row>
<entry/>
<entry namest="offset" nameend="3" align="center" rowsep="1"/>
</row>
</tbody>
</tgroup>
</table>
</tables>
</p>
<p id="p-0066" num="0065">Step <b>408</b> may execute by determining track-to-component associations based on the possible associations determined above. Possible track-to-component associations are processed to determine one-to-one track-to-component associations. Each track-to-component association is assigned a value, or weight, and the set of associations that produces the minimum total weight is determined, as described below and identified in Table 4. The total weight calculated represents the total degree of closeness to the determined target position in search window <b>302</b> for each set of track/component associations.</p>
<p id="p-0067" num="0066">Step <b>410</b> may execute by determining the weight of these associations. For each set of associations, each track can only associate with one distinct component within a predetermined distance and having the same edge direction, or else a track may not associate with a component (indicated by a “-” in Table 4) indicating that the track is the furthest distance away from the mean target position determination in search window <b>302</b>. For this example, track <b>1</b> may only associate with component <b>2</b> since they have the same edge direction (<b>1</b>) or else not associate with a component giving track <b>1</b> only two association possibilities. Alternatively, both track <b>2</b> and track <b>3</b> have three association possibilities since both of these tracks may associate with either component <b>1</b> or component <b>3</b> since they have the same edge direction (<b>7</b>). However, the total number of track <b>2</b>/track <b>3</b> association pairs becomes (3^2−3) equaling 6 since you want to subtract the association pairs (<b>2</b>) with the same component association since this cannot occur (track <b>2</b> and track <b>3</b> cannot both associate with either component <b>1</b> or <b>3</b>) since a component may only associate with only one track and you also subtract the association pair where both tracks do not associate (-, -) since this is non-useful data for the target tracking. Therefore, the number of track <b>2</b>/track <b>3</b> association pairs (<b>6</b>) is multiplied by 2 (the number of track <b>1</b> association possibilities) providing a total number of twelve (12) sets of track <b>1</b>/track <b>2</b>/track <b>3</b>-to-component associations.</p>
<p id="p-0068" num="0067">The weight for each track-to-component pair is the squared Euclidian distance between the estimated target position for that pair (as identified in Table 3) and the mean target position estimate corresponding to the predicted target location given above. If a track does not have an associated component, indicated by a dash (-) in Table 4, a fixed maximum weight is used to indicate that this track is the furthest distance away from the mean target position determination in search window <b>302</b>. Preferably, the fixed maximum weight is the lowest weight (indicating the track the furthest distance away from the target position in window <b>302</b>) for a component that would fall outside the track-component association window <b>302</b> (e.g., target window) used for step <b>406</b>.</p>
<p id="p-0069" num="0068">For example, a track within window <b>302</b> of <figref idref="DRAWINGS">FIG. 3</figref> may not have a component associated to that track. The track may not be near an extracted component, or the track may be along the edge of window <b>302</b>. In this instance, a fixed maximum weight (indicating a distant component—outside of window <b>302</b>) can be used in order to provide a track-to-component association weight. Table 4 below shows some of the possible combinations and the total weight associated with those combinations according to the example given above. These combinations and weights correspond to the track and component combinations shown in <figref idref="DRAWINGS">FIG. 3</figref> and Table 3.</p>
<p id="p-0070" num="0069">In the first row of the table there is only one track-component pair, track <b>3</b>/component <b>1</b>, so the target position of this pair is equal to the mean target position for that row, and the contribution to the weight for this pair is therefore 0. There is no distance away for this pair since the target coordinates for this track-component pair are the exact coordinates for the mean target position estimate. Tracks <b>1</b> and <b>2</b> have no component association so they each receive the unassociated weight (fixed maximum weight), which is 8 in this example. This gives a total weight of 16 to row <b>1</b>. In row <b>4</b> both tracks <b>2</b> and <b>3</b> have associations and referencing table 3, the potential target positions for these pairs are (12, 7.5) and (18, 9). The mean target position is calculated as (15, 8.25). Taking the squared distance of the potential target positions from the mean gives (3<sup>2</sup>+0.75<sup>2</sup>)+(3<sup>2</sup>+0.75<sup>2</sup>), or 19.125. Adding the unassociated weight for track <b>1</b> gives a total weight of 27.125. All the weights must be added to come up with a total degree of closeness to the target for the row of track/component associations.</p>
<p id="p-0071" num="0070">
<tables id="TABLE-US-00004" num="00004">
<table frame="none" colsep="0" rowsep="0">
<tgroup align="left" colsep="0" rowsep="0" cols="5">
<colspec colname="offset" colwidth="14pt" align="left"/>
<colspec colname="1" colwidth="49pt" align="center"/>
<colspec colname="2" colwidth="63pt" align="center"/>
<colspec colname="3" colwidth="49pt" align="center"/>
<colspec colname="4" colwidth="42pt" align="center"/>
<thead>
<row>
<entry/>
<entry namest="offset" nameend="4" rowsep="1">TABLE 4</entry>
</row>
<row>
<entry/>
<entry namest="offset" nameend="4" align="center" rowsep="1"/>
</row>
<row>
<entry/>
<entry>TRK 1 COMP</entry>
<entry>TRK 2 COMP</entry>
<entry>TRK 3 COMP</entry>
<entry>WGT</entry>
</row>
<row>
<entry/>
<entry namest="offset" nameend="4" align="center" rowsep="1"/>
</row>
</thead>
<tbody valign="top">
<row>
<entry/>
</row>
</tbody>
</tgroup>
<tgroup align="left" colsep="0" rowsep="0" cols="5">
<colspec colname="offset" colwidth="14pt" align="left"/>
<colspec colname="1" colwidth="49pt" align="center"/>
<colspec colname="2" colwidth="63pt" align="center"/>
<colspec colname="3" colwidth="49pt" align="center"/>
<colspec colname="4" colwidth="42pt" align="char" char="."/>
<tbody valign="top">
<row>
<entry/>
<entry>—</entry>
<entry>—</entry>
<entry>1</entry>
<entry>16</entry>
</row>
<row>
<entry/>
<entry>—</entry>
<entry>—</entry>
<entry>3</entry>
<entry>16</entry>
</row>
<row>
<entry/>
<entry>—</entry>
<entry>1</entry>
<entry>—</entry>
<entry>16</entry>
</row>
<row>
<entry/>
<entry>—</entry>
<entry>1</entry>
<entry>3</entry>
<entry>27.125</entry>
</row>
<row>
<entry/>
<entry>—</entry>
<entry>3</entry>
<entry>—</entry>
<entry>16</entry>
</row>
<row>
<entry/>
<entry>—</entry>
<entry>3</entry>
<entry>1</entry>
<entry>171.125</entry>
</row>
<row>
<entry/>
<entry>2</entry>
<entry>—</entry>
<entry>—</entry>
<entry>16</entry>
</row>
<row>
<entry/>
<entry>2</entry>
<entry>—</entry>
<entry>1</entry>
<entry>20.625</entry>
</row>
<row>
<entry/>
<entry>2</entry>
<entry>—</entry>
<entry>3</entry>
<entry>33</entry>
</row>
<row>
<entry/>
<entry>2</entry>
<entry>1</entry>
<entry>—</entry>
<entry>8.625</entry>
</row>
<row>
<entry/>
<entry>2</entry>
<entry>1</entry>
<entry>3</entry>
<entry>29.832</entry>
</row>
<row>
<entry/>
<entry>2</entry>
<entry>3</entry>
<entry>—</entry>
<entry>93</entry>
</row>
<row>
<entry/>
<entry namest="offset" nameend="4" align="center" rowsep="1"/>
</row>
</tbody>
</tgroup>
</table>
</tables>
</p>
<p id="p-0072" num="0071">Step <b>412</b> may execute by determining the best set of associations for the track-to-component associations. Preferably, the set of associations that produces the minimum total weight is the best set. For example, referring back to Table 4, the minimum total weight within Table 4 is the weight value of 8.625. This row contains the best set of associations indicating the closest track/component associations to the target position in search window <b>302</b>. The target position estimate for this row comes to 11.5, 7.75 by taking the mean average of the potential target positions for track <b>1</b>/component <b>2</b> association (11, 8) and track <b>2</b>/component <b>1</b> association (12, 7.5). In accordance with the target position calculation, the target is assumed to cover the rectangular region of search window <b>302</b> where the size of this region is determined by the initial target range and size as selected by the user, and the current estimate of the target range as determined by the track/component association and related absolute value (x, y) distance away as shown in Table 3.</p>
<p id="p-0073" num="0072">Preferably, a suboptimal search algorithm is used in determining the weights and the best set of associations. The suboptimal search algorithm is used in order to reduce computation time, although an exhaustive search of all combinations yields optimal results. The suboptimal search algorithm reduces processing requirements from tracker <b>104</b>, for example. Referring again to Table 4, according to the best set, track <b>1</b> is associated with component <b>2</b>, track <b>2</b> is associated with component <b>1</b>, and track <b>3</b> is not associated with any component. Further, track <b>3</b> can be indicated as coasting as no component is associated with track <b>3</b>. Also, when a component is not associated with a track, this component may be used to establish a new track in accordance with step <b>218</b>.</p>
<p id="p-0074" num="0073">Step <b>414</b>, analogous to steps <b>214</b>, <b>216</b>, may execute by updating tracks in track file <b>116</b> with new component-to-track associations including indicating the updated track position and velocity from the completed target position determination from step <b>412</b>. The track position and velocity may be updated since all tracks are positioned relative to the target center as previously described. Tracks not associated with a component are “coasted” for that frame as described above. Also, the position and velocity of the target being tracked is updated using the best track/component association(s) from step <b>412</b> to determine the target position (which may be reported for a user).</p>
<p id="p-0075" num="0074">In accordance with the disclosed embodiments described herein, tracker <b>104</b> provides a plurality of advantages including increased clutter suppression within image <b>180</b>. Tracking a plurality of objects within the received image <b>180</b> in addition to the target object <b>188</b> reduces target detection error of confusing the target <b>188</b> with surrounding clutter or other adjacent targets. Also, tracking multiple points on the target object (via the extracted components) enables accurate target tracking during partial obscuring of the target.</p>
<p id="p-0076" num="0075">Additionally, tracker <b>104</b>, according to embodiments of the present invention, includes the ability to measure target growth and eliminates the need for range input because of dynamic target tracking. Tracker <b>104</b> also establishes the tolerance of moderate roll distortions generated by motion between targets <b>188</b> and detectors <b>102</b> in tracking targets <b>188</b> using the components identified from image <b>180</b>.</p>
<p id="p-0077" num="0076">Further advantages include referencing the distance calculation to the mean target position estimate allowing the disclosed embodiments of the present invention to account for a global shift in the received image, such as image <b>180</b>. Also, because of motion within missiles and other airborne devices, the disclosed embodiments of the present invention proceed as if there is an inaccurate inertial reference.</p>
<p id="p-0078" num="0077">It will be apparent to those skilled in the art that various modifications and variations that can be made without departing from the scope or spirit of the invention. Thus, it is intended that the present invention covers the modifications and variations of the disclosed embodiments provided that they come within the scope of the following claims and their equivalents.</p>
<?DETDESC description="Detailed Description" end="tail"?>
</description>
<us-claim-statement>What is claimed:</us-claim-statement>
<claims id="claims">
<claim id="CLM-00001" num="00001">
<claim-text>1. A method for tracking a target, comprising:
<claim-text>receiving an input image including a target having a target position;</claim-text>
<claim-text>determining components in a window of the image according to an edge direction of connected pixels, the window being associated with the target position;</claim-text>
<claim-text>for all determined components in the window, determining possible track-to-component associations and the target position that would be derived from each association using tracks in a track file;</claim-text>
<claim-text>assigning a weight to each track-to-component association in the window;</claim-text>
<claim-text>determining a best set of track-to-component associations in the window and using this to determine current target position; and</claim-text>
<claim-text>updating the tracks in the track file based on the determined current target position.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00002" num="00002">
<claim-text>2. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein said determining components in the window is based on said connected pixels satisfying a predetermined threshold.</claim-text>
</claim>
<claim id="CLM-00003" num="00003">
<claim-text>3. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein said updating the tracks in the track file includes determining velocity of the target.</claim-text>
</claim>
<claim id="CLM-00004" num="00004">
<claim-text>4. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein
<claim-text>updating the tracks in the track file includes</claim-text>
<claim-text>setting any track to an established track when a predetermined threshold is satisfied, and</claim-text>
<claim-text>deleting said any track when said predetermined threshold is not satisfied.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00005" num="00005">
<claim-text>5. The method of <claim-ref idref="CLM-00004">claim 4</claim-ref>, wherein setting said any track to an established track occurs when a number of times said any track is associated with a component exceeds a first threshold in a time period, and deleting said any track when the number of times said any track is associated with a components does not exceed a second threshold in the time period.</claim-text>
</claim>
<claim id="CLM-00006" num="00006">
<claim-text>6. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein said updating the tracks includes deleting a track if there is failure to associate the track with a component for a predetermined amount of time in determining possible track-to-component associations.</claim-text>
</claim>
<claim id="CLM-00007" num="00007">
<claim-text>7. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising:
<claim-text>generating a track to associate with a component when there is failure to associate the component with a track in the track file in determining possible track-to-component associations.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00008" num="00008">
<claim-text>8. A device for tracking a target, comprising:
<claim-text>a processor for receiving an input image including the target having a target position; and</claim-text>
<claim-text>a track file including a plurality of predetermined tracks, wherein said processor</claim-text>
<claim-text>determines components in a window of the image according to an edge direction of connected pixels, the window being associated with the target position;</claim-text>
<claim-text>for all determined components in the window, determines possible track-to-component associations and the target position that would be derived from each association using tracks in a track file;</claim-text>
<claim-text>assigns a weight to each track-to-component association in the window;</claim-text>
<claim-text>determines a best set of track-to-component associations in the window and using this to determine current target position; and</claim-text>
</claim-text>
<claim-text>updates the tracks in the track file based on the determined current target position.</claim-text>
</claim>
<claim id="CLM-00009" num="00009">
<claim-text>9. The device of <claim-ref idref="CLM-00008">claim 8</claim-ref>, further comprising a memory to store instructions accessible by the processor.</claim-text>
</claim>
<claim id="CLM-00010" num="00010">
<claim-text>10. A computer program product comprising a recording medium having stored thereon a computer readable program executable by a computer for tracking a target by carrying out:
<claim-text>receiving an input image including a target having a target position;</claim-text>
<claim-text>determining components in a window of the image according to an edge direction of connected pixels, the window being associated with the target position;</claim-text>
<claim-text>for all determined components in the window, determining possible track-to-component associations and the target position that would be derived from each association using tracks in a track file;</claim-text>
<claim-text>assigning a weight to each track-to-component association in the window;</claim-text>
<claim-text>determining a best set of track-to-component associations in the window and using this to determine current target position; and</claim-text>
<claim-text>updating the tracks in the track file based on the determined current target position.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00011" num="00011">
<claim-text>11. A method for tracking a target, comprising:
<claim-text>receiving an input image including a target having a target position;</claim-text>
<claim-text>determining a plurality of components in the image according to an edge direction of connected pixels within the component;</claim-text>
<claim-text>associating the plurality of components with a plurality of predetermined tracks based on the edge direction of said component, to generate a plurality of sets of track-to-component associations, wherein at least one track is associated with the target position and each component is associated with no more than one track in a set;</claim-text>
<claim-text>assigning a weight to each track-to-component association in a set based on the distance between each track and associated component as related to the target position;</claim-text>
<claim-text>determining the best set of track-to-component associations based on the total weight, calculated by adding the assigned weight for each track-to-component association in the set, for one of the sets summing up to a minimum value, wherein the best set determines the current target position.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00012" num="00012">
<claim-text>12. The method of <claim-ref idref="CLM-00011">claim 11</claim-ref>, wherein said associating includes generating at least one set of track-to-component associations wherein at least one track in the set fails to associate with a component.</claim-text>
</claim>
</claims>
</us-patent-grant>
