<us-patent-grant lang="EN" dtd-version="v4.2 2006-08-23" file="US07298907-20071120.XML" status="PRODUCTION" id="us-patent-grant" country="US" date-produced="20071106" date-publ="20071120">
<us-bibliographic-data-grant>
<publication-reference>
<document-id>
<country>US</country>
<doc-number>07298907</doc-number>
<kind>B2</kind>
<date>20071120</date>
</document-id>
</publication-reference>
<application-reference appl-type="utility">
<document-id>
<country>US</country>
<doc-number>10468229</doc-number>
<date>20020213</date>
</document-id>
</application-reference>
<us-application-series-code>10</us-application-series-code>
<priority-claims>
<priority-claim sequence="01" kind="national">
<country>JP</country>
<doc-number>2001-042299</doc-number>
<date>20010219</date>
</priority-claim>
</priority-claims>
<us-term-of-grant>
<us-term-extension>744</us-term-extension>
</us-term-of-grant>
<classifications-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>G</section>
<class>06</class>
<subclass>K</subclass>
<main-group>9</main-group>
<subgroup>62</subgroup>
<symbol-position>F</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20071120</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>G</section>
<class>06</class>
<subclass>K</subclass>
<main-group>9</main-group>
<subgroup>00</subgroup>
<symbol-position>L</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20071120</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>G</section>
<class>06</class>
<subclass>K</subclass>
<main-group>9</main-group>
<subgroup>68</subgroup>
<symbol-position>L</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20071120</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
</classifications-ipcr>
<classification-national>
<country>US</country>
<main-classification>382209</main-classification>
<further-classification>382103</further-classification>
<further-classification>382107</further-classification>
<further-classification>382165</further-classification>
<further-classification>382215</further-classification>
<further-classification>382216</further-classification>
<further-classification>382217</further-classification>
<further-classification>382218</further-classification>
<further-classification>382219</further-classification>
<further-classification>382220</further-classification>
<further-classification>382221</further-classification>
</classification-national>
<invention-title id="d0e71">Target recognizing device and target recognizing method</invention-title>
<references-cited>
<citation>
<patcit num="00001">
<document-id>
<country>US</country>
<doc-number>5067014</doc-number>
<kind>A</kind>
<name>Bergen et al.</name>
<date>19911100</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>382107</main-classification></classification-national>
</citation>
<citation>
<patcit num="00002">
<document-id>
<country>US</country>
<doc-number>5173865</doc-number>
<kind>A</kind>
<name>Koike et al.</name>
<date>19921200</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>702155</main-classification></classification-national>
</citation>
<citation>
<patcit num="00003">
<document-id>
<country>US</country>
<doc-number>5521633</doc-number>
<kind>A</kind>
<name>Nakajima et al.</name>
<date>19960500</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>348118</main-classification></classification-national>
</citation>
<citation>
<patcit num="00004">
<document-id>
<country>US</country>
<doc-number>5657087</doc-number>
<kind>A</kind>
<name>Jeong et al.</name>
<date>19970800</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>37524016</main-classification></classification-national>
</citation>
<citation>
<patcit num="00005">
<document-id>
<country>US</country>
<doc-number>5796435</doc-number>
<kind>A</kind>
<name>Nonomura et al.</name>
<date>19980800</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>37524003</main-classification></classification-national>
</citation>
<citation>
<patcit num="00006">
<document-id>
<country>US</country>
<doc-number>6130707</doc-number>
<kind>A</kind>
<name>Koller et al.</name>
<date>20001000</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>348155</main-classification></classification-national>
</citation>
<citation>
<patcit num="00007">
<document-id>
<country>US</country>
<doc-number>6130957</doc-number>
<kind>A</kind>
<name>Horikoshi et al.</name>
<date>20001000</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>382107</main-classification></classification-national>
</citation>
<citation>
<patcit num="00008">
<document-id>
<country>US</country>
<doc-number>6404455</doc-number>
<kind>B1</kind>
<name>Ito et al.</name>
<date>20020600</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>348169</main-classification></classification-national>
</citation>
<citation>
<patcit num="00009">
<document-id>
<country>US</country>
<doc-number>6546115</doc-number>
<kind>B1</kind>
<name>Ito et al.</name>
<date>20030400</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>382100</main-classification></classification-national>
</citation>
<citation>
<patcit num="00010">
<document-id>
<country>US</country>
<doc-number>6621929</doc-number>
<kind>B1</kind>
<name>Lai et al.</name>
<date>20030900</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>382217</main-classification></classification-national>
</citation>
<citation>
<patcit num="00011">
<document-id>
<country>US</country>
<doc-number>6819778</doc-number>
<kind>B2</kind>
<name>Kamei</name>
<date>20041100</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>382103</main-classification></classification-national>
</citation>
<citation>
<patcit num="00012">
<document-id>
<country>JP</country>
<doc-number>6-30417</doc-number>
<kind>A</kind>
<date>19940200</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<patcit num="00013">
<document-id>
<country>JP</country>
<doc-number>6-231252</doc-number>
<kind>A</kind>
<date>19940800</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<nplcit num="00014">
<othercit>Claims 1-3 and 7-10 are rejected under 35 U.S.C. 102(b) as being anticipated by Burt et al. “Object tracking with a moving camera”, I.E.E.E. An Application of Dynamic Motion Analysis. 1989 pp. 2-12.</othercit>
</nplcit>
<category>cited by examiner</category>
</citation>
</references-cited>
<number-of-claims>7</number-of-claims>
<us-exemplary-claim>1</us-exemplary-claim>
<us-field-of-classification-search>
<classification-national>
<country>US</country>
<main-classification>382103</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>382107</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>382165</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>382170</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>382209</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>382215</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>382216</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>382217</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>382218</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>382219</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>382220-223</main-classification>
<additional-info>unstructured</additional-info>
</classification-national>
</us-field-of-classification-search>
<figures>
<number-of-drawing-sheets>7</number-of-drawing-sheets>
<number-of-figures>10</number-of-figures>
</figures>
<us-related-documents>
<related-publication>
<document-id>
<country>US</country>
<doc-number>20040066952</doc-number>
<kind>A1</kind>
<date>20040408</date>
</document-id>
</related-publication>
</us-related-documents>
<parties>
<applicants>
<applicant sequence="001" app-type="applicant-inventor" designation="us-only">
<addressbook>
<last-name>Hasegawa</last-name>
<first-name>Yuji</first-name>
<address>
<city>Fujimi</city>
<country>JP</country>
</address>
</addressbook>
<nationality>
<country>JP</country>
</nationality>
<residence>
<country>JP</country>
</residence>
</applicant>
</applicants>
<agents>
<agent sequence="01" rep-type="attorney">
<addressbook>
<orgname>Birch, Stewart, Kolasch &amp; Birch, LLP</orgname>
<address>
<country>unknown</country>
</address>
</addressbook>
</agent>
</agents>
</parties>
<assignees>
<assignee>
<addressbook>
<orgname>Honda Giken Kogyo Kabushiki Kaisha</orgname>
<role>03</role>
<address>
<city>Tokyo</city>
<country>JP</country>
</address>
</addressbook>
</assignee>
</assignees>
<examiners>
<primary-examiner>
<last-name>Le</last-name>
<first-name>Brian</first-name>
<department>2624</department>
</primary-examiner>
</examiners>
<pct-or-regional-filing-data>
<document-id>
<country>WO</country>
<doc-number>PCT/JP02/01199</doc-number>
<kind>00</kind>
<date>20020213</date>
</document-id>
<us-371c124-date>
<date>20030818</date>
</us-371c124-date>
</pct-or-regional-filing-data>
<pct-or-regional-publishing-data>
<document-id>
<country>WO</country>
<doc-number>WO02/067199</doc-number>
<kind>A </kind>
<date>20020829</date>
</document-id>
</pct-or-regional-publishing-data>
</us-bibliographic-data-grant>
<abstract id="abstract">
<p id="p-0001" num="0000">A target recognizing device that creates a template concerning a target by itself is provided. The device comprises a robot camera for creating continuous images showing a target and an arithmetic operation unit for controlling the robot camera. A robot camera acquires continuous image information by means of a camera rotatable by a motor. The arithmetic operation unit comprises a template management section for allowing an update part to update a template stored in a template storage part and a target recognition section having an image storage part where the acquired image information is stored, a recognizing part for recognizing a target by using the template, and a motion command part for rotating a camera according to the result of recognition.</p>
</abstract>
<drawings id="DRAWINGS">
<figure id="Fig-EMI-D00000" num="00000">
<img id="EMI-D00000" he="217.25mm" wi="123.44mm" file="US07298907-20071120-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00001" num="00001">
<img id="EMI-D00001" he="193.38mm" wi="159.51mm" orientation="landscape" file="US07298907-20071120-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00002" num="00002">
<img id="EMI-D00002" he="217.93mm" wi="123.61mm" file="US07298907-20071120-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00003" num="00003">
<img id="EMI-D00003" he="185.93mm" wi="159.09mm" orientation="landscape" file="US07298907-20071120-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00004" num="00004">
<img id="EMI-D00004" he="192.96mm" wi="145.88mm" orientation="landscape" file="US07298907-20071120-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00005" num="00005">
<img id="EMI-D00005" he="205.74mm" wi="151.72mm" file="US07298907-20071120-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00006" num="00006">
<img id="EMI-D00006" he="226.91mm" wi="155.96mm" orientation="landscape" file="US07298907-20071120-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00007" num="00007">
<img id="EMI-D00007" he="214.71mm" wi="146.22mm" file="US07298907-20071120-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
</drawings>
<description id="description">
<?BRFSUM description="Brief Summary" end="lead"?>
<p id="p-0002" num="0001">This application is the national phase under 35 U.S.C. §371 of PCT International Application No. PCT/JP02/01199 which has an International filing date of Feb. 13, 2002, which designated the United States of America.</p>
<heading id="h-0001" level="1">TECHNICAL FIELD</heading>
<p id="p-0003" num="0002">The present invention relates to a target recognizing device and a target recognizing method for recognizing a target by using templates.</p>
<p id="p-0004" num="0003">This application is based on a patent application in Japan (Japanese Patent Application No. 2001-042299), and the contents in the application in Japan are hereby incorporated as a part of this specification.</p>
<heading id="h-0002" level="1">BACKGROUND ART</heading>
<p id="p-0005" num="0004">In conventional image recognizing devices, various sensors such as a camera are used to obtain a scene including a target, being an object to be recognized, as image information, and target recognition is conducted based on the image information and fragmentary knowledge with respect to the target. The main object of the target recognition is to grasp the scene including the target from the image information as accurately as possible, and rebuild the target. At this time, as a method for specifying the target, a template matching method is frequently used, which obtains a consistency between the target in the image information, and many templates prepared beforehand with respect to the target.</p>
<p id="p-0006" num="0005">In such a conventional target recognizing device however, it is necessary to prepare many templates so that consistency with various targets can be realized, in order to correspond to various conditions. In other words, there is a problem in that before conducting the recognition processing, it is necessary to store the templates for the number of types of target, being an object to be recognized, beforehand in the target recognizing device.</p>
<p id="p-0007" num="0006">When it is attempted to realize the development of an autonomously movable robot having a device which recognizes a scene including a target, and an autonomously shift device, with a conventional technique, the following method can be considered. That is to say, at first, a plurality of image information relating to a target point is stored beforehand as templates. Image information including the target point is then obtained by a camera or the like mounted therein, to extract an area having the best consistency with the template stored in advance, from the image information. The robot then moves toward the direction corresponding to the extracted area, by the autonomously shift device.</p>
<p id="p-0008" num="0007">In dynamic scene analysis for analyzing the movement of a target object, by using a plurality of images continuous timewise, an optical flow is frequently used. The optical flow is a vector representation of the direction of movement at one point on the image and the magnitude of the velocity thereof. By measuring the optical flow of the target object, the movement of the target object can be recognized.</p>
<p id="p-0009" num="0008">In the target recognizing device in the autonomously movable robot however, the scene photographed by a camera or the like changes moment by moment due to the movement of the robot, and even in the case of the same target, the size of the target occupying the obtained image information changes. Therefore, even by simply using the optical flow, it is difficult to perform accurate target recognition, and many templates corresponding to the respective situations become necessary.</p>
<p id="p-0010" num="0009">As described above, in the conventional technique, there is a problem in that it may be difficult to perform accurate recognition processing of a target in a scene, which is changing timewise.</p>
<heading id="h-0003" level="1">DISCLOSURE OF INVENTION</heading>
<p id="p-0011" num="0010">It is an object of the present invention to provide a target recognizing device and a target recognizing method, which can self-generate a template relating to the newest target, without requiring storage of a large amount of templates, in which the types, direction and size of the target are changed.</p>
<p id="p-0012" num="0011">The first aspect of the present invention provides a device comprising a template storing device which stores a template for recognizing a target, an image acquiring device which acquires continuous images including the target, a recognition processing device which detects an optical flow between at least two images of the continuous images to obtain an evaluation function value based on the optical flow; and an update device which updates the template stored in the template storing device, based on the image including the target acquired by the image acquiring device, until the evaluation function value exceeds a predetermined value.</p>
<p id="p-0013" num="0012">In the above configuration, continuous images including the target are acquired, an optical flow between at least two images of the acquired continuous images is detected, and the template is updated based on the image including the target, until the evaluation function value based on the optical flow exceeds the predetermined value. As a result, it is not necessary to store beforehand a large amount of templates in which the types, direction and size of the target are change, and a template relating to the newest target can be self-generated. Moreover, since the template is updated based on the optical flow obtained from between the respective images, a noise component other than the target can be removed. As a result, the situation can be prevented where a noise component is included in the template.</p>
<p id="p-0014" num="0013">According to the second aspect of the present invention, the device further comprises a driving device which rotates the image acquiring device, and an operation instruction device which outputs an operation instruction to the driving device based on the instruction from the recognition processing device, and the operation instruction device outputs an operation instruction for stopping the rotational motion when the evaluation function value exceeds the predetermined value.</p>
<p id="p-0015" num="0014">In the above configuration, the image acquiring device is rotated based on the instruction from the recognition processing device, and when a predetermined condition is satisfied, the rotational motion is stopped. As a result, the target recognizing device can perform predetermined operations such as rotating the image acquiring device in a direction of a target recognized by using templates, and shift the image acquiring device, and hence can be preferably used for controlling a robot.</p>
<p id="p-0016" num="0015">According to the third aspect of the present invention, the device further comprises: a control signal receiving device which receives a control signal indicating a timing at which the template is updated; and an identification device which identifies the content of the control signal received by the control signal receiving device and informs the update device of the content, and the update device updates the template corresponding to an identification result of the control signal.</p>
<p id="p-0017" num="0016">In the above configuration, since the control signal indicating the timing at which the template is updated is received, and the template is updated at the timing when the control signal is received, update of the template for recognizing the target can be performed according to an instruction from outside, and when the image acquiring device turns to an optional direction, this direction can be designated as a target direction. Moreover, since the update processing of the template for recognizing the target can be performed from the image obtained only when the image acquiring device turns to a predetermined target direction, more reliable template generation can be performed.</p>
<p id="p-0018" num="0017">According to the fourth aspect of the present invention, the evaluation function value is obtained by comparing the degree of approximation of the template and the image including the target, setting a shift quantity with respect to the approximating pixel to the position in the template, and summing up shift quantities for all pixels in the whole templates.</p>
<p id="p-0019" num="0018">In the above configuration, the degree of approximation of the image including the target and the template is compared, the shift quantity of the approximating pixel to the target pixel in the template is set, and the shift quantities for all pixels in the whole templates are summed up, to obtain the evaluation function value. As a result, only a characteristic point of the target is emphasized (extracted), and in the target direction, a portion, which is not stably detected, such as a characteristic point of an object existing irregularly, can be removed.</p>
<p id="p-0020" num="0019">According to the fifth aspect of the present invention, there are provided at least two-types of templates for use at the time of updating while rotating in one direction, one of which is added with a positive-polarity weighting value, and another one of which is added with a negative-polarity weighting value.</p>
<p id="p-0021" num="0020">In the above configuration, as the template updated at the time of rotating in the same direction, at least two templates respectively added with a positive-polarity and a negative-polarity weighting value are provided. As a result, the robustness with respect to the optical flow detected in the actual environment can be improved.</p>
<p id="p-0022" num="0021">According to the sixth aspect of the present invention, a plurality of template groups is provided depending on the rotation direction of the image acquiring device.</p>
<p id="p-0023" num="0022">In the above configuration, since the template group to be updated is different, depending on the rotation direction of the image acquiring device, imbalance of movement in the target direction with respect to the rotation direction of the image acquiring device can be eliminated, and the detection accuracy of the target can be improved.</p>
<p id="p-0024" num="0023">The seventh aspect of the present invention provides a target identification method comprises a template storing step in which a template for recognizing a target is stored, an image acquiring step in which continuous images including the target are acquired by self-rotational motion; a recognition processing step in which an optical flow between at least two images of the continuous images is detected, to obtain an evaluation function value based on the optical flow, and an update step in which the template stored by the template storing step is updated based on an image including the target acquired by the image acquiring step, by template update timing instructed from outside, until the evaluation function value exceeds a predetermined value.</p>
<p id="p-0025" num="0024">The eighth aspect of the present invention provides a computer program for target identification method to be executed by a computer, the program comprises the steps of template storage processing for storing a template for recognizing a target, image acquisition processing for acquiring continuous images including the target, recognition processing for detecting an optical flow between at least two images of the continuous images, to obtain an evaluation function value based on the optical flow, and update processing for updating the template stored by the template storage processing, based on an image including the target acquired by the image acquisition processing, until the evaluation function value exceeds a predetermined value.</p>
<p id="p-0026" num="0025">According to the ninth aspect of the present invention, the image acquisition program to be executed in a computer comprises the steps of driving processing for performing self-rotational motion, and operation instruction processing for outputting operation instructions with respect to the driving processing, based on instructions from the recognition processing, are further executed by the computer, and the operation instruction processing outputs an operation instruction for stopping rotational motion at a point in time when the evaluation function value exceeds a predetermined value.</p>
<p id="p-0027" num="0026">According to the tenth aspect of the present invention, the computer program for executing the target identification method in a computer comprises the steps of control signal reception processing for receiving a control signal indicating a timing at which the template is updated, and identification processing for identifying the content of the control signal received by the control signal reception processing and informing the content with respect to the update processing are further executed by the computer, wherein the update processing updates the template corresponding to an identification result of the control signal.</p>
<p id="p-0028" num="0027">According to the eleventh aspect of the present invention, the target identification program comprises the steps of comparing the degree of approximation of the template and the image including the target, setting a shift quantity of the approximating pixel to the target position, and summing up the shift quantities for all pixels corresponding to all pixels of the templates.</p>
<p id="p-0029" num="0028">According to the twelfth aspect the present invention, a plurality of templates are provided, which differ depending on the rotation direction of the image acquiring device.</p>
<p id="p-0030" num="0029">According to the thirteenth aspect the present invention, among the plurality of templates to be updated at the time of rotating in the same direction, there are at least two types of templates, one of which is added with a positive-polarity weighting value and another one of which is added with a negative-polarity weighting value.</p>
<?BRFSUM description="Brief Summary" end="tail"?>
<?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?>
<description-of-drawings>
<heading id="h-0004" level="1">BRIEF DESCRIPTION OF DRAWINGS</heading>
<p id="p-0031" num="0030"><figref idref="DRAWINGS">FIG. 1</figref> is a block diagram showing the configuration of one embodiment of the present invention.</p>
<p id="p-0032" num="0031"><figref idref="DRAWINGS">FIG. 2</figref> is a flowchart showing the operation of a target recognizing device shown in <figref idref="DRAWINGS">FIG. 1</figref>.</p>
<p id="p-0033" num="0032"><figref idref="DRAWINGS">FIG. 3</figref> is a flowchart showing the operation of update processing for templates and weighting shown in <figref idref="DRAWINGS">FIG. 2</figref>.</p>
<p id="p-0034" num="0033"><figref idref="DRAWINGS">FIG. 4</figref> is a diagram showing the operation of a camera <b>11</b>.</p>
<p id="p-0035" num="0034"><figref idref="DRAWINGS">FIG. 5</figref> is a diagram showing constitution of a template.</p>
<p id="p-0036" num="0035"><figref idref="DRAWINGS">FIG. 6</figref> is a diagram showing the constitution of the degree of approximation</p>
<p id="p-0037" num="0036"><figref idref="DRAWINGS">FIG. 7</figref> is a diagram showing the constitution of a local shift quantity.</p>
<p id="p-0038" num="0037"><figref idref="DRAWINGS">FIG. 8</figref> is a diagram showing the update operation for templates and weighting values.</p>
<p id="p-0039" num="0038"><figref idref="DRAWINGS">FIG. 9</figref> is a diagram showing tracks of a camera <b>11</b> in an embodiment.</p>
<p id="p-0040" num="0039"><figref idref="DRAWINGS">FIG. 10</figref> is a diagram showing a track of the direction of the camera <b>11</b>, when the target recognition processing is executed by using sufficient templates for recognizing the target in the above embodiment.</p>
</description-of-drawings>
<?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?>
<?DETDESC description="Detailed Description" end="lead"?>
<heading id="h-0005" level="1">BEST MODE FOR CARRYING OUT THE INVENTION</heading>
<p id="p-0041" num="0040">The target recognizing device according to one embodiment of the present invention will be described, with reference to the drawings. <figref idref="DRAWINGS">FIG. 1</figref> is a block diagram showing the configuration of this embodiment. Reference symbol <b>1</b> denotes a robot camera comprising a camera <b>11</b> (for example, a CCD camera) for acquiring peripheral images, and a motor <b>12</b> for rotating the camera <b>11</b> in the horizontal direction to direct the optical axis of the camera <b>11</b> to an optional direction. The motor <b>12</b> is provided with an encoder (not shown), so that the direction (angle of rotation) of the camera <b>11</b> at the time of rotational motion can be detected by a value of the encoder. Reference symbol <b>2</b> denotes an arithmetic operation section, which comprises a target recognition section <b>21</b> and a template control section <b>22</b>. Reference symbol <b>23</b> denotes an image storage section for storing images taken by the camera <b>11</b>, and images stored herein are sampled and quantized images. Reference symbol <b>24</b> denotes a recognition processing section for recognizing a target based on the image stored in the image storage section <b>23</b>. Reference symbol <b>25</b> denotes an operation instruction section for outputting operation instructions to the robot camera <b>1</b>, based on the target recognition result in the recognition processing section <b>24</b>. Reference symbol <b>26</b> denotes a control signal reception section for receiving a control signal Rn transmitted from outside or from the robot camera <b>1</b>. Reference symbol <b>27</b> denotes an identification section for identifying the control signal Rn and issuing an instruction based on the identification result. Reference symbol <b>28</b> denotes an update section for updating a template stored in a template storage section <b>29</b>.</p>
<p id="p-0042" num="0041">The outline of the operation of the target recognizing device will be described, with reference to <figref idref="DRAWINGS">FIG. 1</figref>. n pieces (n is a natural number) of templates Mn are stored in advance in the template storage section <b>29</b> in the template control section <b>22</b>.</p>
<p id="p-0043" num="0042">The camera <b>11</b> provided in the robot camera <b>1</b> takes images of the external environment as seen from the robot camera <b>1</b> continuously, and the image information is sequentially stored in the image storage section <b>23</b> in the target recognition section <b>21</b>. The recognition processing section <b>24</b> in the target recognition section <b>21</b> calculates the optical flow between the continuous images taken out from the image storage section <b>23</b>, and changes in the operation are obtained for each local portion in the image to extract a characteristic portion in the image. The recognition processing section <b>24</b> searches the template Mn corresponding to the input image in the template storage section <b>29</b>, to perform recognition processing of a target in the external environment. The processing result in the recognition processing section <b>24</b> is provided to the operation instruction section <b>25</b>. Upon reception of the processing result, the operation instruction section <b>25</b> provides an operation instruction corresponding to the processing result in the recognition processing section <b>24</b>, to the motor <b>12</b> in the robot camera <b>1</b>. The motor <b>12</b> generates a driving force corresponding to the operation instruction, to thereby cause spontaneous rotational motion of the camera <b>11</b>.</p>
<p id="p-0044" num="0043">On the other hand, the templates Mn stored in the template storage section <b>29</b> are updated as required. Update of the templates Mn is performed corresponding to a control signal Rn provided from outside of the target recognizing device, or a control signal Rn transmitted from the robot camera <b>1</b> at the timing when the camera turns to a direction including the target. The control signal Rn is received by the control signal reception section <b>26</b>, and the content thereof is identified by the identification section <b>27</b>. The identification section <b>27</b> provides an update instruction corresponding to the identification result, to the update section <b>28</b>. The update section <b>28</b> updates the corresponding template Mn based on the received update instruction.</p>
<p id="p-0045" num="0044">The operation of the target recognizing device will be described in detail, with reference to <figref idref="DRAWINGS">FIGS. 2 and 3</figref>. <figref idref="DRAWINGS">FIG. 2</figref> is a flowchart showing the operation for creating the template Mn corresponding to the image including a target. <figref idref="DRAWINGS">FIG. 3</figref> is a flowchart showing the operation in step S<b>22</b> shown in <figref idref="DRAWINGS">FIG. 2</figref>. At first, the operation instruction section <b>25</b> issues an instruction for performing rotational motion, to the motor <b>12</b>. Upon reception of the instruction, the motor <b>12</b> drives, to rotate the camera <b>11</b> about an axis perpendicular to the ground.</p>
<p id="p-0046" num="0045">The rotation operation of the camera <b>11</b> will be described here. <figref idref="DRAWINGS">FIG. 4</figref> is a diagram of the robot camera <b>1</b> as seen from the above. Though the computation section <b>2</b> is not shown in <figref idref="DRAWINGS">FIG. 4</figref>, the computation section <b>2</b> is installed in the vicinity of the robot camera <b>1</b>. As shown in this figure, the camera <b>11</b> performs reciprocating rotational motion within the range of rotational motion of from −50° to 50°, due to driving of the motor <b>12</b>. At this time, before starting the rotational motion, the direction to which the optical axis of the camera <b>11</b> is directed is assumed to be 0°. This direction of 0° is the direction where the target exists (target direction). In this processing, the image in the target direction is acquired, and a template corresponding to the acquired image is created. In this embodiment, the camera <b>11</b> is rotated to obtain the optical flow from, the image in the vicinity of the target direction and the image in the target direction, which is then used for creating and updating the template.</p>
<p id="p-0047" num="0046">When the image in the target direction and the image in the vicinity of the target direction are acquired, it can also be considered to simply rotate the camera in the vicinity of the target direction, but in order to create a better template relating to the image in the vicinity of the target direction, it is necessary to take pictures of the image repetitively. Therefore, in order to obtain stable images at the time of rotational motion of the camera in the vicinity of the target direction, the range of rotational motion is designated as up to 50° right and left. The encoder provided in the motor <b>12</b> outputs “1” as the control signal Rn, when the angle of rotation is 0°. This control signal Rn is received by the control signal reception section <b>26</b>. By receiving this control signal Rn, the computation section <b>2</b> can detect that the camera <b>11</b> has turned to the target direction, and creates and updates the template Mn, by using the image at this timing.</p>
<p id="p-0048" num="0047">In <figref idref="DRAWINGS">FIG. 4</figref>, the camera <b>11</b> acquires an image in the direction of 0° (target direction) indicated by reference symbol S. The camera <b>11</b> then rotates to the direction of −50° and returns to the direction of 0°. At this time, the camera <b>11</b> acquires images at a certain sampling interval. Further, the camera <b>11</b> rotates to the direction of +50° and returns to the direction of 0°. Also at this time, the camera <b>11</b> acquires images at a certain sampling interval. The optical flow is calculated from a series of continuous images obtained here.</p>
<p id="p-0049" num="0048">The images acquired by the camera <b>11</b> are sequentially stored in the image storage section in the computation section <b>2</b> (step S<b>21</b> in <figref idref="DRAWINGS">FIG. 2</figref>). The stored image is described here as an image It(x, y). Each pixel in the image It(x, y) is expressed by a gray value of 8 bits (256 gradations).</p>
<p id="p-0050" num="0049">In parallel with the operation for storing the images, the update section <b>28</b> updates the template Mn and the weighting value Wn (step S<b>22</b>). Details of the processing in step S<b>22</b> will be described later.</p>
<p id="p-0051" num="0050">The recognition processing section <b>24</b> then calculates the degree of approximation Sn(x, y) between the newest image It(x, y) stored in the image storage section <b>23</b> and each pixel in the n templates Mn held in the recognition processing section <b>24</b> beforehand (step S<b>23</b>). The operation for calculating the degree of approximation Sn(x, y) will be described here, with reference to <figref idref="DRAWINGS">FIGS. 5 and 6</figref>. <figref idref="DRAWINGS">FIG. 5</figref> is a diagram showing one template Mn representing the templates Mn, which are previously stored. <figref idref="DRAWINGS">FIG. 6</figref> is a diagram showing the degree of approximation Sn for each pixel obtained by the processing in step S<b>23</b>.</p>
<p id="p-0052" num="0051">As shown in <figref idref="DRAWINGS">FIG. 5</figref>, each template Mn is constituted of 20×20 pieces of pixels, and the position of each pixel is expressed by the coordinates (x, y). The gray value of each pixel in all templates Mn in the initial state is set to any value of 256 gradations (0 to 255) at random. The construction may be such that the template storage section <b>29</b> stores a transform value for each of the 20×20 pixels, by using a Fourier transform, a Gabor transform or a Wavelet transform, instead of the gray value.</p>
<p id="p-0053" num="0052">The template Mn can correspond to an optional position on the input image It acquired by the camera <b>11</b>. For example, if the pixel It(0, 0) at the upper left of the input image It is made to agree with the pixel Mn(0, 0) at the upper left of the template Mn, the image recognition for 20×20 pixels at the upper left of the input image It is carried out, by using the template Mn.</p>
<p id="p-0054" num="0053">In the embodiment described later, an experiment was carried out for setting such that the center of the input image It agrees with the center of the template Mn, but the template Mn may be used for the other areas of the input image It.</p>
<p id="p-0055" num="0054">The degree of approximation Sn can be obtained by performing for each pixel, processing where the newest image It(x, y) stored in the image storage section <b>23</b> is compared with the template Mn(x, y) stored in the template storage section <b>29</b> for each pixel to obtain a difference in the gray value, and if the difference is smaller than a predetermined value, it is assumed that the image It(x, y) approximates to the template Mn(x, y), and if the difference is larger than the predetermined value, it is assumed that the image It(x, y) does not approximate to the template Mn(x, y). In other words, a difference Dn(x, y) between the gray value Mn(x, y) of the template and the gray value It(x, y) of the input image is calculated as follows:
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>Dn</i>(<i>x, y</i>)=|<i>Mn</i>(<i>x, y</i>)−<i>It</i>(<i>x, y</i>)|  (1).<?in-line-formulae description="In-line Formulae" end="tail"?>
<br/>
If Dn(x, y) is smaller than the predetermined threshold Gn, it is assumed that the degree of approximation Sn=1, and if Dn(x, y) is not smaller than Gn, it is assumed that the degree of approximation Sn=0. By this processing, as a result of comparison of the input image It with the template Mn, “1” is set to only the approximating pixels.
</p>
<p id="p-0056" num="0055">The recognition processing section <b>24</b> calculates the optical flow by using an input image It−1(x, y) obtained immediately before the newest input image It(x, y), and the input image It(x, y), to obtain the optical flow Vx (step S<b>24</b>). The optical flow is calculated by the following equation (2):
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?>∂<i>xVx</i>(<i>x, y</i>)+∂<i>yVy</i>(<i>x, y</i>)+∂<i>t=</i>0  (2).<?in-line-formulae description="In-line Formulae" end="tail"?>
</p>
<p id="p-0057" num="0056">Since the camera <b>11</b> performs only the right and left rotational motion, then in the optical flow, only the x component is used, and Vx(x, y) obtained by the equation (2) is designated as the optical flow.</p>
<p id="p-0058" num="0057">The recognition processing section <b>24</b> calculates a local shift quantity Vn from the obtained optical flow Vx(x, y), using the following equation (3) (step S<b>25</b>):
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>Vn</i>(<i>x, y</i>)=<i>Sn</i>(<i>x, y</i>)·<i>Vx</i>(<i>x, y</i>)  (3).<?in-line-formulae description="In-line Formulae" end="tail"?>
</p>
<p id="p-0059" num="0058">This reflects the optical flow value, with respect to only the pixel having the degree of approximation Sn=1. The result of the obtained local shift quantity Vn is shown in <figref idref="DRAWINGS">FIG. 7</figref>.</p>
<p id="p-0060" num="0059">The recognition processing section <b>24</b> calculates the evaluation function value B by the following equation (4) (step S<b>26</b>):</p>
<p id="p-0061" num="0060">
<maths id="MATH-US-00001" num="00001">
<math overflow="scroll">
<mtable>
  <mtr>
    <mtd>
      <mrow>
        <mi>B</mi>
        <mo>=</mo>
        <mrow>
          <munderover>
            <mo>∑</mo>
            <mi>n</mi>
            <mstyle>
              <mspace width="0.3em" height="0.3ex"/>
            </mstyle>
          </munderover>
          <mo>⁢</mo>
          <mrow>
            <mi>α</mi>
            <mo>⁢</mo>
            <mstyle>
              <mspace width="0.3em" height="0.3ex"/>
            </mstyle>
            <mo>⁢</mo>
            <mi>n</mi>
            <mo>⁢</mo>
            <mrow>
              <munderover>
                <mo>∑</mo>
                <mi>x</mi>
                <mstyle>
                  <mspace width="0.3em" height="0.3ex"/>
                </mstyle>
              </munderover>
              <mo>⁢</mo>
              <mrow>
                <munderover>
                  <mo>∑</mo>
                  <mi>y</mi>
                  <mstyle>
                    <mspace width="0.3em" height="0.3ex"/>
                  </mstyle>
                </munderover>
                <mo>⁢</mo>
                <mrow>
                  <mrow>
                    <mi>Wn</mi>
                    <mo>⁡</mo>
                    <mrow>
                      <mo>(</mo>
                      <mrow>
                        <mi>x</mi>
                        <mo>,</mo>
                        <mi>y</mi>
                      </mrow>
                      <mo>)</mo>
                    </mrow>
                  </mrow>
                  <mo>⁢</mo>
                  <mrow>
                    <mi>Vn</mi>
                    <mo>⁡</mo>
                    <mrow>
                      <mo>(</mo>
                      <mrow>
                        <mi>x</mi>
                        <mo>,</mo>
                        <mi>y</mi>
                      </mrow>
                      <mo>)</mo>
                    </mrow>
                  </mrow>
                </mrow>
              </mrow>
            </mrow>
          </mrow>
        </mrow>
      </mrow>
    </mtd>
    <mtd>
      <mrow>
        <mo>(</mo>
        <mn>4</mn>
        <mo>)</mo>
      </mrow>
    </mtd>
  </mtr>
</mtable>
</math>
</maths>
<br/>
where, Wn(x, y) is a weighting value with respect to the pixel, which can take a positive or negative value, and αn is a weighting value for each template in a range of 0&lt;αn≦1. This Wn(x, y) is a value obtained in step S<b>22</b>. The processing in step S<b>26</b> means to sum up values including the respective weighting portions for all pixels of all templates. In other words, with an increase in the number of pixels having a large weighting value Wn and a degree of approximation Sn=1, the evaluation function value B increases.
</p>
<p id="p-0062" num="0061">The recognition processing section <b>24</b> determines whether the absolute value of the obtained evaluation function value B is larger than a predetermined threshold k (step S<b>27</b>). As a result of this determination, if the evaluation function value B is not larger than the threshold k, control returns to step S<b>21</b> and step S<b>22</b>, to repeat the processing. On the other hand, if the evaluation function value B is larger than the threshold k, the recognition processing section <b>24</b> informs the operation instruction section <b>25</b> that the template creation processing has finished. In response to this, the operation instruction section <b>25</b> issues an instruction for stopping the drive of the motor <b>12</b> to the motor <b>12</b>. As a result, the rotational motion of the camera <b>11</b> stops, with the camera <b>11</b> turning to the target direction (step S<b>28</b>).</p>
<p id="p-0063" num="0062">As a result, the template Mn corresponding to the image in the target direction is complete, thereby enabling the target recognition processing using this template Mn.</p>
<p id="p-0064" num="0063">If the threshold k is set to a very low value, the evaluation function value B is likely to exceed the threshold k. Therefore, the camera <b>11</b> can be quickly located at a position, which is considered to be the target direction. However, if the threshold k is set to a value too low, there is the possibility that the camera <b>11</b> is located erroneously in a direction where image information similar to the target direction is obtained. On the contrary, if the threshold k is set to a value too high, the camera <b>11</b> is reliably located in the true target direction, but it takes too much time. Moreover, if the rotational speed of the camera <b>11</b> is too fast as compared with the number of pixels in the template Mn, since the change in the evaluation function value B corresponding to the rotation of the camera <b>11</b> becomes large, there is the possibility that the camera <b>11</b> is stopped with the camera <b>11</b> turning to an inadequate direction.</p>
<p id="p-0065" num="0064">Therefore, it is desired to appropriately set the threshold k corresponding to various parameters, such as the complexity of the environment where the robot camera <b>1</b> is placed, the recognition accuracy required for the target recognizing device and the time required for recognition, the processing capability of the computation section <b>2</b>, the number of pixels in the template Mn and the number of templates Mn, and the rotational speed of the camera <b>11</b>.</p>
<p id="p-0066" num="0065">Moreover, when |B|&gt;k is not satisfied, even if the processing of from steps S<b>21</b> to S<b>27</b> is repeated, after the processing is performed for a predetermined number of times, or after a predetermined period of time, the largest value of |B| may be detected, so that the camera <b>11</b> is stopped, directed in that direction.</p>
<p id="p-0067" num="0066">The update processing for the template Mn and the weighting value Wn shown in step S<b>22</b> in <figref idref="DRAWINGS">FIG. 2</figref> will be described, with reference to <figref idref="DRAWINGS">FIGS. 3 and 8</figref>. <figref idref="DRAWINGS">FIG. 3</figref> is a flowchart showing the operation of the update processing for the template Mn and the weighting value Wn. At first, the identification section <b>27</b> determines whether the control signal Rn is “1” (ON) (step S<b>31</b>). This control signal Rn is such a signal that when the camera <b>11</b> turns to the target direction (the direction of 0° shown in <figref idref="DRAWINGS">FIG. 4</figref>), “1” is output. As a result of the determination, if the control signal Rn is not “1”, it is determined whether |Vn|&gt;d (step S<b>34</b>), wherein d denotes a preset threshold. As a result of the determination, if |Vn| is not larger than d, the control signal Rn “1” has not been input, and the camera <b>11</b> should not turn to the target direction. Therefore, the update processing for the template and the like is not executed, and the processing finishes. If |Vn|&gt;d, the update section <b>28</b> sets the weighting value Wn to “0” (step S<b>35</b>). This processing means that a pixel having the size of Vn larger than the predetermined value, though the control signal Rn “1” has not been input, is regarded as noise as a result of having caught an object other than the target, and the record of this portion is cancelled (deleted) by designating the weighting value Wn as “0”, so that the update processing for the template and the like is not executed.</p>
<p id="p-0068" num="0067">On the other hand, in step S<b>31</b>, when the control signal Rn is “1”, the update section <b>28</b> determines whether |Vn|&gt;d, and Wn&gt;0 (step S<b>32</b>). As a result of the determination, if |Vn|&gt;d, and Wn&gt;0, the update section <b>28</b> updates the template Mn and the weighting value Wn (step S<b>33</b>), to finish the processing. The respective update is performed according to the following equations (5) and (6).
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>Wn</i>(<i>x, y</i>)=<i>Wn</i>(<i>x, y</i>)+<i>w</i>  (5)<?in-line-formulae description="In-line Formulae" end="tail"?>
<br/>
where w denotes an initial value of the weighting value.
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>Mn</i>(<i>x, y</i>)=<i>Mn−q</i>(<i>Mn−It</i>)  (6)<?in-line-formulae description="In-line Formulae" end="tail"?>
<br/>
where q denotes a fixed value set to any figure in the range of 0&lt;q≦1
</p>
<p id="p-0069" num="0068">In other words, as the pixel in the template Mn becomes closer to the input image It, the initial value w is added to the weighting value Wn to increase the weighting, so that the template Mn is brought close to the gray value of the pixel in the input image It. As a result, while the camera <b>11</b> performs the rotational motion repetitively, the characteristic point of the target is emphasized (extracted) in the input image It, and the portion which is not detected stably in the target direction, such as a characteristic point of an object existing irregularly, is cut out.</p>
<p id="p-0070" num="0069">On the other hand, in step S<b>32</b>, if |Vn| is not larger than d, or Wn is not larger than 0, the update section <b>28</b> determines whether the weighting value Wn is “0” (step S<b>36</b>). As a result of the determination, if Wn is not <b>0</b>, the processing finishes. On the other hand, when Wn=0, the update section <b>28</b> sets the template Mn and the weighting value Wn (step S<b>37</b>). This processing is performed even if the target may have been caught, but since the weighting value Wn is still 0, it is determined that the image of this portion is obtained for the first time, and Wn is set to w (initial value), and the pixel is set to a gray value of a corresponding pixel in the input image It.</p>
<p id="p-0071" num="0070"><figref idref="DRAWINGS">FIG. 8</figref> is a diagram for explaining the update processing for the template Mn and the weighting value Wn. In <figref idref="DRAWINGS">FIG. 8</figref>, there is shown a case where the templates Mn are four (M<b>1</b> to M<b>4</b>). The control signal Rn is provided for updating the template Mn of a corresponding sign. For example, when a control signal R<b>1</b>=1 is given, the template M<b>1</b> is updated.</p>
<p id="p-0072" num="0071">In <figref idref="DRAWINGS">FIG. 8</figref>, patterns <b>1</b> and <b>3</b> show the state where the camera <b>11</b> is not directed to the target direction (direction of 0°). At this time, the control signal Rn is R<b>1</b>=R<b>2</b>=R<b>3</b>=R<b>4</b>=0, and corresponds to a case of “NO” in step S<b>31</b> shown in <figref idref="DRAWINGS">FIG. 3</figref>. Patterns <b>2</b> and <b>4</b> show the state where the camera <b>11</b> is directed to the target direction (direction of 0°). At this time, the control signal Rn is R<b>1</b>=R<b>2</b>=1, or R<b>3</b>=R<b>4</b>=1, and corresponds to a case of “YES” in step S<b>31</b> shown in <figref idref="DRAWINGS">FIG. 3</figref>.</p>
<p id="p-0073" num="0072">In pattern <b>2</b> in <figref idref="DRAWINGS">FIG. 8</figref>, the templates M<b>1</b> and M<b>2</b> are updated, but weighting values W1 and W2 with polarities opposite to each other are provided. Similarly, in pattern <b>4</b>, the templates M<b>3</b> and M<b>4</b> are updated, but weighting values W3 and W4 with polarities opposite to each other are provided. This is for improving the robustness with respect to the optical flow by means of an object detected in the actual environment. For example, in the actual environment, the boundary portion of an object is likely to be detected as an optical flow in a direction opposite to that of the face portion, and the detection result is unstable. Therefore, in order to improve the detection accuracy as a whole, weighting values Wn having opposite polarities are used to clarify the boundary portion.</p>
<p id="p-0074" num="0073">As shown in <figref idref="DRAWINGS">FIG. 8</figref>, the updated templates Mn are grouped (into M<b>1</b> and M<b>2</b>, or M<b>3</b> and M<b>4</b>) according to a case where the camera <b>11</b> reaches the direction of 0° (target direction) counterclockwise (pattern <b>2</b>), and a case where the camera <b>11</b> reaches the direction of 0° (target direction) clockwise (pattern <b>4</b>). This is for coping with the imbalance of the movement in the target direction with respect to the movement of the camera <b>11</b>. Normally, it is rare that an object existing in the target direction has a completely symmetric shape or pattern, and the way of generating the optical flow is different in the counterclockwise rotation and the clockwise rotation. In the present invention, in order to correspond to the imbalance and to improve the detection accuracy, the templates Mn are grouped for each direction of rotation. In other words, the characteristic of the optical flow generated by an object existing in the target direction, when the camera <b>11</b> is rotated counterclockwise, is reflected in the templates M<b>1</b> and M<b>2</b>, and the characteristic of the optical flow generated by the object existing in the target direction, when the camera <b>11</b> is rotated clockwise, is reflected in the templates M<b>3</b> and M<b>4</b>.</p>
<p id="p-0075" num="0074">The camera <b>11</b> may be set so as to continue to rotate in either one direction. In this case, a target recognizing device needs only to record and hold the templates M<b>1</b> and M<b>2</b>, or the templates M<b>3</b> and M<b>4</b>.</p>
<p id="p-0076" num="0075">When the camera <b>11</b> is to be stopped, there is the possibility that the camera <b>11</b> is stopped in the state of being displaced too much and shifted from the target direction or the target position due to the influence of inertia. Therefore, the construction may be such that a stopping instruction is issued before the target direction (0°) by a predetermined angle, or the camera <b>11</b> is made to return by a predetermined angle and stop reliably, with the camera facing the target direction. Moreover, the angle of rotation of the camera <b>11</b> is not limited to ±50°, and other angles may be employed. The update processing for the templates Mn and the like may be performed at an angle other than 0°, for example, at a point in time when the camera <b>11</b> turns to a direction where a pixel or pattern having a particular density exists.</p>
<p id="p-0077" num="0076">By the above described processing operation, the template used for recognizing a predetermined target is complete, and stored in the template storage section <b>29</b>.</p>
<p id="p-0078" num="0077">The results of tests conducted for preparing the template by the above described operation will now be described. <figref idref="DRAWINGS">FIG. 9</figref> is a diagram showing tracks of the direction of the camera <b>11</b>. As shown in <figref idref="DRAWINGS">FIG. 9</figref>, in the first test, when in total four control signals Rn were provided, the above described at-end condition was satisfied and the test finished. In other words, it is shown that when the fourth control signal Rn is provided, the template Mn(x, y) having had a random value becomes a sufficient value for recognizing the target in the direction of 0°.</p>
<p id="p-0079" num="0078">The second test was conducted by using the template Mn(x,y) obtained as a result of the first test. In the second test, when the camera <b>11</b> was turned from the direction of +50° to the direction of 0°, as in the first test, the control signal Rn was provided. In this case, the test finished by providing two control signals Rn. In other words, the initial value was different from that of the first test, corresponding to the target. As a result, it is seen that the number of control signals Rn provided for generating the template used for the target recognition can be reduced.</p>
<p id="p-0080" num="0079">In the third test, the template obtained as a result of the second test was used, to start the test from the state where the camera <b>11</b> was directed in the direction of −50°. In this case, the at-end condition was satisfied by only providing a first control signal. In other words, it is seen that the template required for recognizing the target has been nearly completed from the initial stage.</p>
<p id="p-0081" num="0080">As shown in <figref idref="DRAWINGS">FIG. 9</figref>, it is seen that by repeating the template update processing, the template is updated to one sufficient for recognizing the target, and the camera is located in the vicinity of 0° in the target direction at an early stage.</p>
<p id="p-0082" num="0081">The test results of the target recognition processing using the complete template in the target recognizing device will be described. <figref idref="DRAWINGS">FIG. 10</figref> is a diagram showing a track of the direction of the camera <b>11</b>, when the target recognition processing is performed by using a template sufficient for recognizing the target, obtained by repeating the template update processing. As shown in <figref idref="DRAWINGS">FIG. 10</figref>, it is seen that even if the direction to which the camera <b>11</b> is made to turn at first is counterclockwise, or clockwise, the target is recognized quickly in substantially the same time, and the camera <b>11</b> is located in the target direction.</p>
<p id="p-0083" num="0082">The robot camera <b>1</b> and the computation section <b>2</b> may be connected by wire, but a radio communication apparatus may be provided, and the information exchanged by using radio communication. The robot camera <b>1</b> and the computation section <b>2</b> may be integrated.</p>
<p id="p-0084" num="0083">Furthermore, when a control signal Rn is transmitted from outside to the control signal reception section <b>26</b>, the output timing of the control signal Rn may be determined by an operator, every time the camera <b>11</b> turns to the target direction.</p>
<p id="p-0085" num="0084">Moreover, a program for realizing the function of each block shown in <figref idref="DRAWINGS">FIG. 1</figref> may be recorded in a computer readable recording medium, and the program recorded in the recording medium may be read into a computer system, and executed, to conduct the template update processing and the target recognition processing. The “computer system” referred to herein includes OS and hardware such as peripheral equipment. The “computer readable recording medium” stands for portable media such as flexible disks, magneto-optical disks, ROM and CD-ROM, and memories such as hard disks built into the computer system. The “computer readable recording medium” further includes one holding the program for a certain period of time, such as a volatile memory (RAM) within the computer system, which becomes a server or a client, when the program is transmitted via a network such as the Internet or a communication circuit such as telephone lines.</p>
<p id="p-0086" num="0085">The program may be transmitted from the computer system, which stores the program in a memory or the like, to other computer systems via a transmission medium, or by transmitted waves in the transmission medium. The “transmission medium” for transmitting the program stands for a medium having a function of transmitting the information like a network (communication network) such as the Internet, or a communication circuit (communication line) such as telephone lines. The program may be for realizing a part of the above described function. Moreover, the program may be a so-called differential file (differential program) which can realize the above described function by a combination with a program already recorded in the computer system.</p>
<p id="p-0087" num="0086">As described above, the target recognizing device of the present invention comprises: a template storing device which stores a template for recognizing a target; an image acquiring device which acquires continuous images including the target; a recognition processing device which detects an optical flow between at least two images of the continuous images, to obtain an evaluation function value based on the optical flow; and an update device which updates the template stored in the template storing device, based on the image including the target acquired by the image acquiring device, until the evaluation function value exceeds a predetermined value. As a result, it is not necessary to store a large amount of templates beforehand, in which the types, direction and size of targets are changed, and a template relating to the newest target can be self-generated.</p>
<p id="p-0088" num="0087">The target recognizing device of the present invention can perform predetermined operations, such as rotating the camera in a direction of a target recognized by using the template, and shift the camera. Hence, the target recognizing device of the present invention is suitable for controlling a robot.</p>
<?DETDESC description="Detailed Description" end="tail"?>
</description>
<us-math idrefs="MATH-US-00001" nb-file="US07298907-20071120-M00001.NB">
<img id="EMI-M00001" he="9.57mm" wi="76.20mm" file="US07298907-20071120-M00001.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-claim-statement>The invention claimed is:</us-claim-statement>
<claims id="claims">
<claim id="CLM-00001" num="00001">
<claim-text>1. A target recognizing device comprising:
<claim-text>an image acquiring device which acquires images including a target, while performing rotational movement;</claim-text>
<claim-text>a driving device which rotates the image acquiring device;</claim-text>
<claim-text>a template storing device which stores a template used for recognizing the target, the template including a plurality of pixels each having a pixel-gradation value and a weighting value;</claim-text>
<claim-text>a template updating device which updates the template by updating the pixel-gradation value and the weighting value, when the image acquiring device directs a predetermined direction, or in accordance with a control signal input from outside;</claim-text>
<claim-text>a recognition processing device which compares a newly obtained image and an updated template for each pixels, and calculates the pixel-gradation value that can be obtained for each pixels, and an evaluation function value based on an optical flow; and</claim-text>
<claim-text>an operation instruction device which outputs an operation instruction to the driving device in accordance with the direction of the image acquiring device and the evaluation function value, wherein</claim-text>
<claim-text>the recognition processing device calculates:</claim-text>
<claim-text>a degree of approximation of the each pixels by comparing the pixel-gradation value of the each pixels of the images of the pixel-gradation value of the corresponding pixels of the template;</claim-text>
<claim-text>the optical flow of the pixels based on the continuous images,</claim-text>
<claim-text>a local shift quantity of the each pixels by multiplying the degree of approximation by the optical flow; and</claim-text>
<claim-text>the evaluation function value by summing up all values obtained by multiplying the local shift quantity and the weighting value for the each pixels.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00002" num="00002">
<claim-text>2. The target recognizing device according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein
<claim-text>the target recognizing device comprises a group of templates for images acquired while rotating the image acquiring device; and</claim-text>
<claim-text>the group of template is updated according to the rotation direction of the image acquiring device.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00003" num="00003">
<claim-text>3. The target recognizing device according to <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein
<claim-text>the group of templates which are updated while rotating in the same direction comprises at least two types of templates; and</claim-text>
<claim-text>one of which is added with a positive-polarity weighting value, while the other one of which is added with a negative-polarity weighting value.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00004" num="00004">
<claim-text>4. A target recognition program stored in a computer-readable medium, for executing in a computer comprising:
<claim-text>a driving process of rotating an image acquiring device which acquires images including a target while performing rotational movement;</claim-text>
<claim-text>a template storing process of storing a template used for recognizing the target, the template including a plurality of pixels each having a pixel-gradation value and a weighting value;</claim-text>
<claim-text>a template updating process of updating the template by updating the pixel-gradation value and the weighting value, when the image acquiring device directs a predetermined direction, or in accordance with a control signal input from outside;</claim-text>
<claim-text>a recognition process of comparing a newly obtained image and an updated template for each pixels, and calculating the pixel-gradation value for the each pixels, and an evaluation function value based on an optical flow; and</claim-text>
<claim-text>an operation instruction process of outputting an operation instruction to the driving device in accordance with the direction of the image acquiring device and the evaluation function value, wherein</claim-text>
<claim-text>in the recognition process</claim-text>
<claim-text>a degree of approximation of the each pixels is calculated by comparing the pixel-gradation value of the each pixels of the images and the pixel-gradation value of the corresponding pixels of the template,</claim-text>
<claim-text>the optical flow of the pixels is calculated based on the continuous images,</claim-text>
<claim-text>a local shift quantity of the each pixels is calculated by multiplying the degree of approximation by the optical flow, and</claim-text>
<claim-text>the evaluation function value is calculated by summing up all values obtained by multiplying the local shift quantity and the weighting value of the each pixels.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00005" num="00005">
<claim-text>5. The target recognition program stored in a computer-readable medium, according to <claim-ref idref="CLM-00004">claim 4</claim-ref>, wherein
<claim-text>a plurality of template groups are provided for use in updating templates according to the rotation directions at the time of image acquiring processing.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00006" num="00006">
<claim-text>6. The target recognition program stored in a computer-readable medium, according to <claim-ref idref="CLM-00005">claim 5</claim-ref>, wherein
<claim-text>the plurality of templates to be updated at the time of rotating in the same direction include at least two templates, and</claim-text>
<claim-text>one of which is added with a positive-polarity weighting value while the other one of which is added with a negative-polarity weighting value.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00007" num="00007">
<claim-text>7. A target recognizing method comprising:
<claim-text>a driving step of rotating an image acquiring device which acquires images including a target while performing rotational movement;</claim-text>
<claim-text>a template storing step of storing a template used for recognizing the target, the template including a plurality of pixels each having a pixel-gradation value and a weighting value;</claim-text>
<claim-text>a template updating step of updating the template by updating the pixel-gradation value and the weighting value, when the image acquiring device directs a predetermined direction, or in accordance with a control signal input from outside;</claim-text>
<claim-text>a recognition processing step of comparing a newly obtained image and an updated template for each pixels, and calculates the pixel-gradation value for the each pixels, and an evaluation function value based on an optical flow; and</claim-text>
<claim-text>an operation instruction step of outputting an operation instruction to the driving device in accordance with the direction of the image acquiring device and the evaluation function value, wherein</claim-text>
<claim-text>in the recognition processing step,</claim-text>
<claim-text>a degree of approximation of the each pixels is calculated by comparing the pixel-gradation value of the each pixels of the images and the pixel-gradation value of the corresponding pixels of the template,</claim-text>
<claim-text>the optical flow of the pixels is calculated based on the continuous images,</claim-text>
<claim-text>a local shift quantity of the each pixels is calculated by multiplying the degree of approximation by the optical flow, and</claim-text>
<claim-text>the evaluation function value is calculated by summing up all values obtained by multiplying the local shift quantity and the weighting value of the each pixels.</claim-text>
</claim-text>
</claim>
</claims>
</us-patent-grant>
