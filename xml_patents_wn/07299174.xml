<us-patent-grant lang="EN" dtd-version="v4.2 2006-08-23" file="US07299174-20071120.XML" status="PRODUCTION" id="us-patent-grant" country="US" date-produced="20071106" date-publ="20071120">
<us-bibliographic-data-grant>
<publication-reference>
<document-id>
<country>US</country>
<doc-number>07299174</doc-number>
<kind>B2</kind>
<date>20071120</date>
</document-id>
</publication-reference>
<application-reference appl-type="utility">
<document-id>
<country>US</country>
<doc-number>10554619</doc-number>
<date>20040430</date>
</document-id>
</application-reference>
<us-application-series-code>10</us-application-series-code>
<priority-claims>
<priority-claim sequence="01" kind="national">
<country>JP</country>
<doc-number>2003-125665</doc-number>
<date>20030430</date>
</priority-claim>
</priority-claims>
<classifications-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>G</section>
<class>10</class>
<subclass>L</subclass>
<main-group>19</main-group>
<subgroup>04</subgroup>
<symbol-position>F</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20071120</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>G</section>
<class>10</class>
<subclass>L</subclass>
<main-group>19</main-group>
<subgroup>12</subgroup>
<symbol-position>L</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20071120</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
</classifications-ipcr>
<classification-national>
<country>US</country>
<main-classification>704219</main-classification>
<further-classification>704223</further-classification>
</classification-national>
<invention-title id="d0e61">Speech coding apparatus including enhancement layer performing long term prediction</invention-title>
<references-cited>
<citation>
<patcit num="00001">
<document-id>
<country>US</country>
<doc-number>5671327</doc-number>
<kind>A</kind>
<name>Akamine et al.</name>
<date>19970900</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<patcit num="00002">
<document-id>
<country>US</country>
<doc-number>5781880</doc-number>
<kind>A</kind>
<name>Su</name>
<date>19980700</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<patcit num="00003">
<document-id>
<country>US</country>
<doc-number>5797118</doc-number>
<kind>A</kind>
<name>Saito</name>
<date>19980800</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<patcit num="00004">
<document-id>
<country>US</country>
<doc-number>5864797</doc-number>
<kind>A</kind>
<name>Fujimoto</name>
<date>19990100</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<patcit num="00005">
<document-id>
<country>US</country>
<doc-number>6208957</doc-number>
<kind>B1</kind>
<name>Nomura</name>
<date>20010300</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>704207</main-classification></classification-national>
</citation>
<citation>
<patcit num="00006">
<document-id>
<country>US</country>
<doc-number>6735567</doc-number>
<kind>B2</kind>
<name>Gao et al.</name>
<date>20040500</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>704258</main-classification></classification-national>
</citation>
<citation>
<patcit num="00007">
<document-id>
<country>US</country>
<doc-number>6856961</doc-number>
<kind>B2</kind>
<name>Thyssen</name>
<date>20050200</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>704500</main-classification></classification-national>
</citation>
<citation>
<patcit num="00008">
<document-id>
<country>US</country>
<doc-number>7020605</doc-number>
<kind>B2</kind>
<name>Gao</name>
<date>20060300</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>704225</main-classification></classification-national>
</citation>
<citation>
<patcit num="00009">
<document-id>
<country>US</country>
<doc-number>2005/0171771</doc-number>
<kind>A1</kind>
<name>Yasunaga et al.</name>
<date>20050800</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<patcit num="00010">
<document-id>
<country>US</country>
<doc-number>2005/0197833</doc-number>
<kind>A1</kind>
<name>Yasunaga et al.</name>
<date>20050900</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<nplcit num="00011">
<othercit>English Language Abstract of JP 8-054900.</othercit>
</nplcit>
<category>cited by other</category>
</citation>
<citation>
<nplcit num="00012">
<othercit>English Language Abstract of JP 8-328595.</othercit>
</nplcit>
<category>cited by other</category>
</citation>
<citation>
<nplcit num="00013">
<othercit>English Language Abstract of JP 10-177399.</othercit>
</nplcit>
<category>cited by other</category>
</citation>
<citation>
<nplcit num="00014">
<othercit>English Language Abstract of JP 5-249999.</othercit>
</nplcit>
<category>cited by other</category>
</citation>
<citation>
<nplcit num="00015">
<othercit>English Language Abstract of JP 8-147000.</othercit>
</nplcit>
<category>cited by other</category>
</citation>
<citation>
<nplcit num="00016">
<othercit>English Language Abstract of JP 8-211895.</othercit>
</nplcit>
<category>cited by other</category>
</citation>
<citation>
<nplcit num="00017">
<othercit>English Language Abstract of JP 5-073099.</othercit>
</nplcit>
<category>cited by other</category>
</citation>
<citation>
<nplcit num="00018">
<othercit>English Language Abstract of JP 6-102900.</othercit>
</nplcit>
<category>cited by other</category>
</citation>
</references-cited>
<number-of-claims>6</number-of-claims>
<us-exemplary-claim>1</us-exemplary-claim>
<us-field-of-classification-search>
<classification-national>
<country>US</country>
<main-classification>704219</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>704201</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>704223</main-classification>
</classification-national>
</us-field-of-classification-search>
<figures>
<number-of-drawing-sheets>9</number-of-drawing-sheets>
<number-of-figures>9</number-of-figures>
</figures>
<us-related-documents>
<related-publication>
<document-id>
<country>US</country>
<doc-number>20060173677</doc-number>
<kind>A1</kind>
<date>20060803</date>
</document-id>
</related-publication>
</us-related-documents>
<parties>
<applicants>
<applicant sequence="001" app-type="applicant-inventor" designation="us-only">
<addressbook>
<last-name>Sato</last-name>
<first-name>Kaoru</first-name>
<address>
<city>Yokohama</city>
<country>JP</country>
</address>
</addressbook>
<nationality>
<country>JP</country>
</nationality>
<residence>
<country>JP</country>
</residence>
</applicant>
<applicant sequence="002" app-type="applicant-inventor" designation="us-only">
<addressbook>
<last-name>Morii</last-name>
<first-name>Toshiyuki</first-name>
<address>
<city>Kawasaki</city>
<country>JP</country>
</address>
</addressbook>
<nationality>
<country>JP</country>
</nationality>
<residence>
<country>JP</country>
</residence>
</applicant>
</applicants>
<agents>
<agent sequence="01" rep-type="attorney">
<addressbook>
<orgname>Greenblum &amp; Bernstein, P.L.C.</orgname>
<address>
<country>unknown</country>
</address>
</addressbook>
</agent>
</agents>
</parties>
<assignees>
<assignee>
<addressbook>
<orgname>Matsushita Electric Industrial Co., Ltd.</orgname>
<role>03</role>
<address>
<city>Osaka</city>
<country>JP</country>
</address>
</addressbook>
</assignee>
</assignees>
<examiners>
<primary-examiner>
<last-name>Smits</last-name>
<first-name>Talivaldis Ivars</first-name>
<department>2626</department>
</primary-examiner>
<assistant-examiner>
<last-name>Shortledge</last-name>
<first-name>Thomas E.</first-name>
</assistant-examiner>
</examiners>
<pct-or-regional-filing-data>
<document-id>
<country>WO</country>
<doc-number>PCT/JP2004/006294</doc-number>
<kind>00</kind>
<date>20040430</date>
</document-id>
<us-371c124-date>
<date>20060327</date>
</us-371c124-date>
</pct-or-regional-filing-data>
<pct-or-regional-publishing-data>
<document-id>
<country>WO</country>
<doc-number>WO2004/097796</doc-number>
<kind>A </kind>
<date>20041111</date>
</document-id>
</pct-or-regional-publishing-data>
</us-bibliographic-data-grant>
<abstract id="abstract">
<p id="p-0001" num="0000">To implement scalable coding, a base layer coding section encodes an input signal to obtain base layer coded information, which is decoded by a base layer decoding section to obtain a base layer decoded signal and long term prediction information (pitch lag). An adding section inverts the polarity of the base layer decoded signal to add to the input signal, and obtains a residual signal. An enhancement layer coding section encodes a long term prediction coefficient calculated using the long term prediction information and the residual signal to obtain enhancement layer coded information. Also using the long term prediction information, an enhancement layer decoding section decodes the enhancement layer coded information to obtain an enhancement layer decoded signal. An adding section adds the base layer decoded signal and enhancement layer decoded signal to obtain a speech/sound signal.</p>
</abstract>
<drawings id="DRAWINGS">
<figure id="Fig-EMI-D00000" num="00000">
<img id="EMI-D00000" he="132.76mm" wi="228.01mm" file="US07299174-20071120-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00001" num="00001">
<img id="EMI-D00001" he="234.27mm" wi="155.70mm" orientation="landscape" file="US07299174-20071120-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00002" num="00002">
<img id="EMI-D00002" he="212.17mm" wi="149.35mm" orientation="landscape" file="US07299174-20071120-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00003" num="00003">
<img id="EMI-D00003" he="157.31mm" wi="97.62mm" orientation="landscape" file="US07299174-20071120-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00004" num="00004">
<img id="EMI-D00004" he="223.60mm" wi="127.08mm" orientation="landscape" file="US07299174-20071120-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00005" num="00005">
<img id="EMI-D00005" he="239.52mm" wi="148.00mm" orientation="landscape" file="US07299174-20071120-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00006" num="00006">
<img id="EMI-D00006" he="210.82mm" wi="127.00mm" orientation="landscape" file="US07299174-20071120-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00007" num="00007">
<img id="EMI-D00007" he="235.46mm" wi="166.29mm" orientation="landscape" file="US07299174-20071120-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00008" num="00008">
<img id="EMI-D00008" he="222.50mm" wi="149.35mm" orientation="landscape" file="US07299174-20071120-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00009" num="00009">
<img id="EMI-D00009" he="217.42mm" wi="142.07mm" orientation="landscape" file="US07299174-20071120-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
</drawings>
<description id="description">
<?BRFSUM description="Brief Summary" end="lead"?>
<heading id="h-0001" level="1">TECHNICAL FIELD</heading>
<p id="p-0002" num="0001">The present invention relates to a speech coding apparatus, speech decoding apparatus and methods thereof used in communication systems for coding and transmitting speech and/or sound signals.</p>
<heading id="h-0002" level="1">BACKGROUND ART</heading>
<p id="p-0003" num="0002">In the fields of digital wireless communications, packet communications typified by Internet communications, and speech storage and so forth, techniques for coding/decoding speech signals are indispensable in order to efficiently use the transmission channel capacity of radio signal and storage medium, and many speech coding/decoding schemes have been developed. Among the systems, the CELP speech coding/decoding scheme has been put into practical use as a mainstream technique.</p>
<p id="p-0004" num="0003">A CELP type speech coding apparatus encodes input speech based on speech models stored beforehand. More specifically, the CELP speech coding apparatus divides a digitalized speech signal into frames of about 20 ms, performs linear prediction analysis of the speech signal on a frame-by-frame basis, obtains linear prediction coefficients and linear prediction residual vector, and encodes separately the linear prediction coefficients and linear prediction residual vector.</p>
<p id="p-0005" num="0004">In order to execute low-bit rate communications, since the amount of speech models to be stored is limited, phonation speech models are chiefly stored in the conventional CELP type speech coding/decoding scheme.</p>
<p id="p-0006" num="0005">In communication systems for transmitting packets such as Internet communications, packet losses occur depending on the state of the network, and it is preferable that speech and sound can be decoded from part of remaining coded information even when part of the coded information is lost. Similarly, in variable rate communication systems for varying the bit rate according to the communication capacity, when the communication capacity is decreased, it is desired that loads on the communication capacity can be reduced at ease by transmitting only part of the coded information. Thus, as a technique enabling decoding of speech and sound using all the coded information or part of the coded information, attention has recently been directed toward the scalable coding technique. Some scalable coding schemes are disclosed conventionally.</p>
<p id="p-0007" num="0006">The scalable coding system is generally comprised of a base layer and enhancement layer, and the layers constitute a hierarchical structure with the base layer being the lowest layer. In each layer, a residual signal is coded that is a difference between an input signal and output signal in a lower layer. According to this constitution, it is possible to decode speech and/or sound signals using the coded information of all the layers or using only the coded information of a lower layer.</p>
<p id="p-0008" num="0007">However, in the conventional scalable coding system, the CELP type speech coding/decoding system is used as the coding schemes for the base layer and enhancement layers, and considerable amounts are thereby required both in calculation and coded information.</p>
<heading id="h-0003" level="1">DISCLOSURE OF INVENTION</heading>
<p id="p-0009" num="0008">It is therefore an object of the present invention to provide a speech coding apparatus, speech decoding apparatus and methods thereof enabling scalable coding to be implemented with small amounts of calculation and coded information.</p>
<p id="p-0010" num="0009">The above-noted object is achieved by providing an enhancement layer to perform long term prediction, performing long term prediction of the residual signal in the enhancement layer using a long term correlation characteristic of speech or sound to improve the quality of the decoded signal, obtaining a long term prediction lag using long term prediction information of a base layer, and thereby reducing the computation amount.</p>
<?BRFSUM description="Brief Summary" end="tail"?>
<?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?>
<description-of-drawings>
<heading id="h-0004" level="1">BRIEF DESCRIPTION OF DRAWINGS</heading>
<p id="p-0011" num="0010"><figref idref="DRAWINGS">FIG. 1</figref> is a block diagram illustrating configurations of a speech coding apparatus and speech decoding apparatus according to Embodiment 1 of the invention;</p>
<p id="p-0012" num="0011"><figref idref="DRAWINGS">FIG. 2</figref> is a block diagram illustrating an internal configuration a base layer coding section according to the above Embodiment;</p>
<p id="p-0013" num="0012"><figref idref="DRAWINGS">FIG. 3</figref> is a diagram to explain processing for a parameter determining section in the base layer coding section to determine a signal generated from an adaptive excitation codebook according to the above Embodiment;</p>
<p id="p-0014" num="0013"><figref idref="DRAWINGS">FIG. 4</figref> is a block diagram illustrating an internal configuration of a base layer decoding section according to the above Embodiment;</p>
<p id="p-0015" num="0014"><figref idref="DRAWINGS">FIG. 5</figref> is a block diagram illustrating an internal configuration of an enhancement layer coding section according to the above Embodiment;</p>
<p id="p-0016" num="0015"><figref idref="DRAWINGS">FIG. 6</figref> is a block diagram illustrating an internal configuration of an enhancement layer decoding section according to the above Embodiment;</p>
<p id="p-0017" num="0016"><figref idref="DRAWINGS">FIG. 7</figref> is a block diagram illustrating an internal configuration of an enhancement layer coding section according to Embodiment 2 of the invention;</p>
<p id="p-0018" num="0017"><figref idref="DRAWINGS">FIG. 8</figref> is a block diagram illustrating an internal configuration of an enhancement layer decoding section according to the above Embodiment; and</p>
<p id="p-0019" num="0018"><figref idref="DRAWINGS">FIG. 9</figref> is a block diagram illustrating configurations of a speech signal transmission apparatus and speech signal reception apparatus according to Embodiment 3 of the invention.</p>
</description-of-drawings>
<?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?>
<?DETDESC description="Detailed Description" end="lead"?>
<heading id="h-0005" level="1">BEST MODE FOR CARRYING OUT THE INVENTION</heading>
<p id="p-0020" num="0019">Embodiments of the present invention will specifically be described below with reference to the accompanying drawings. A case will be described in each of the Embodiments where long term prediction is performed in an enhancement layer in a two layer speech coding/decoding method comprised of a base layer and the enhancement layer. However, the invention is not limited in layer structure, and applicable to any cases of performing long term prediction in an upper layer using long term prediction information of a lower layer in a hierarchical speech coding/decoding method with three or more layers. A hierarchical speech coding method refers to a method in which a plurality of speech coding methods for coding a residual signal (difference between an input signal of a lower layer and a decoded signal of the lower layer) by long term prediction to output coded information exist in upper layers and constitute a hierarchical structure. Further, a hierarchical speech decoding method refers to a method in which a plurality of speech decoding methods for decoding a residual signal exists in an upper layer and constitutes a hierarchical structure. Herein, a speech/sound coding/decoding method existing in the lowest layer will be referred to as a base layer. A speech/sound coding/decoding method existing in a layer higher than the base layer will be referred to as an enhancement layer.</p>
<p id="p-0021" num="0020">In each of the Embodiments of the invention, a case is described as an example where the base layer performs CELP type speech coding/decoding.</p>
<heading id="h-0006" level="1">Embodiment 1</heading>
<p id="p-0022" num="0021"><figref idref="DRAWINGS">FIG. 1</figref> is a block diagram illustrating configurations of a speech coding apparatus and speech decoding apparatus according to Embodiment 1 of the invention.</p>
<p id="p-0023" num="0022">In <figref idref="DRAWINGS">FIG. 1</figref>, speech coding apparatus <b>100</b> is mainly comprised of base layer coding section <b>101</b>, base layer decoding section <b>102</b>, adding section <b>103</b>, enhancement layer coding section <b>104</b>, and multiplexing section <b>105</b>. Speech decoding apparatus <b>150</b> is mainly comprised of demultiplexing section <b>151</b>, base layer decoding section <b>152</b>, enhancement layer decoding section <b>153</b>, and adding section <b>154</b>.</p>
<p id="p-0024" num="0023">Base layer coding section <b>101</b> receives a speech or sound signal, codes the input signal using the CELP type speech coding method, and outputs base layer coded information obtained by the coding, to base layer decoding section <b>102</b> and multiplexing section <b>105</b>.</p>
<p id="p-0025" num="0024">Base layer decoding section <b>102</b> decodes the base layer coded information using the CELP type speech decoding method, and outputs a base layer decoded signal obtained by the decoding, to adding section <b>103</b>. Further, base layer decoding section <b>102</b> outputs the pitch lag to enhancement layer coding section <b>104</b> as long term prediction information of the base layer.</p>
<p id="p-0026" num="0025">The “long term prediction information” is information indicating long term correlation of the speech or sound signal. The “pitch lag” refers to position information specified by the base layer, and will be described later in detail.</p>
<p id="p-0027" num="0026">Adding section <b>103</b> inverts the polarity of the base layer decoded signal output from base layer decoding section <b>102</b> to add to the input signal, and outputs a residual signal as a result of the addition to enhancement layer coding section <b>104</b>.</p>
<p id="p-0028" num="0027">Enhancement layer coding section <b>104</b> calculates long term prediction coefficients using the long term prediction information output from base layer decoding section <b>102</b> and the residual signal output from adding section <b>103</b>, codes the long term prediction coefficients, and outputs enhancement layer coded information obtained by coding to multiplexing section <b>105</b>.</p>
<p id="p-0029" num="0028">Multiplexing section <b>105</b> multiplexes the base layer coded information output from base layer coding section <b>101</b> and the enhancement layer coded information output from enhancement layer coding section <b>104</b> to output to demultiplexing section <b>151</b> as multiplexed information via a transmission channel.</p>
<p id="p-0030" num="0029">Demultiplexing section <b>151</b> demultiplexes the multiplexed information transmitted from speech coding apparatus <b>100</b> into the base layer coded information and enhancement layer coded information, and outputs the demultiplexed base layer coded information to base layer decoding section <b>152</b>, while outputting the demultiplexed enhancement layer coded information to enhancement layer decoding section <b>153</b>.</p>
<p id="p-0031" num="0030">Base layer decoding section <b>152</b> decodes the base layer coded information using the CELP type speech decoding method, and outputs a base layer decoded signal obtained by the decoding, to adding section <b>154</b>. Further, base layer decoding section <b>152</b> outputs the pitch lag to enhancement layer decoding section <b>153</b> as the long term prediction information of the base layer. Enhancement layer decoding section <b>153</b> decodes the enhancement layer coded information using the long term prediction information, and outputs an enhancement layer decoded signal obtained by the decoding, to adding section <b>154</b>.</p>
<p id="p-0032" num="0031">Adding section <b>154</b> adds the base layer decoded signal output from base layer decoding section <b>152</b> and the enhancement layer decoded signal output from enhancement layer decoding section <b>153</b>, and outputs a speech or sound signal as a result of the addition, to an apparatus for subsequent processing.</p>
<p id="p-0033" num="0032">The internal configuration of base layer coding section <b>101</b> of <figref idref="DRAWINGS">FIG. 1</figref> will be described below with reference to the block diagram of <figref idref="DRAWINGS">FIG. 2</figref>.</p>
<p id="p-0034" num="0033">An input signal of base layer coding section <b>101</b> is input to pre-processing section <b>200</b>. Pre-processing section <b>200</b> performs high-pass filtering processing to remove the DC component, waveform shaping processing and pre-emphasis processing to improve performance of subsequent coding processing, and outputs a signal (Xin) subjected to the processing, to LPC analyzing section <b>201</b> and adder <b>204</b>.</p>
<p id="p-0035" num="0034">LPC analyzing section <b>201</b> performs linear predictive analysis using Xin, and outputs a result of the analysis (linear prediction coefficients) to LPC quantizing section <b>202</b>. LPC quantizing section <b>202</b> performs quantization processing on the linear prediction coefficients (LPC) output from LPC analyzing section <b>201</b>, and outputs quantized LPC to synthesis filter <b>203</b>, while outputting code (L) representing the quantized LPC, to multiplexing section <b>213</b>.</p>
<p id="p-0036" num="0035">Synthesis filter <b>203</b> generates a synthesized signal by performing filter synthesis on an excitation vector output from adding section <b>210</b> described later using filter coefficients based on the quantized LPC, and outputs the synthesized signal to adder <b>204</b>.</p>
<p id="p-0037" num="0036">Adder <b>204</b> inverts the polarity of the synthesized signal, adds the resulting signal to Xin, calculates an error signal, and outputs the error signal to perceptual weighting section <b>211</b>.</p>
<p id="p-0038" num="0037">Adaptive excitation codebook <b>205</b> has excitation vector signals output earlier from adder <b>210</b> stored in a buffer, and fetches a sample corresponding to one frame from an earlier excitation vector signal sample specified by a signal output from parameter determining section <b>212</b> to output to multiplier <b>208</b>.</p>
<p id="p-0039" num="0038">Quantization gain generating section <b>206</b> outputs an adaptive excitation gain and fixed excitation gain specified by a signal output from parameter determining section <b>212</b> respectively to multipliers <b>208</b> and <b>209</b>.</p>
<p id="p-0040" num="0039">Fixed excitation codebook <b>207</b> multiplies a pulse excitation vector having a shape specified by the signal output from parameter determining section <b>212</b> by a spread vector, and outputs the obtained fixed excitation vector to multiplier <b>209</b>.</p>
<p id="p-0041" num="0040">Multiplier <b>208</b> multiplies the quantization adaptive excitation gain output from quantization gain generating section <b>206</b> by the adaptive excitation vector output from adaptive excitation codebook <b>205</b> and outputs the result to adder <b>210</b>. Multiplier <b>209</b> multiplies the quantization fixed excitation gain output from quantization gain generating section <b>206</b> by the fixed excitation vector output from fixed excitation codebook <b>207</b> and outputs the result to adder <b>210</b>.</p>
<p id="p-0042" num="0041">Adder <b>210</b> receives the adaptive excitation vector and fixed excitation vector both multiplied by the gain respectively input from multipliers <b>208</b> and <b>209</b> to add in vector, and outputs an excitation vector as a result of the addition to synthesis filter <b>203</b> and adaptive excitation codebook <b>205</b>. In addition, the excitation vector input to adaptive excitation codebook <b>205</b> is stored in the buffer.</p>
<p id="p-0043" num="0042">Perceptual weighting section <b>211</b> performs perceptual weighting on the error signal output from adder <b>204</b>, and calculates a distortion between Xin and the synthesized signal in a perceptual weighting region and outputs the result to parameter determining section <b>212</b>.</p>
<p id="p-0044" num="0043">Parameter determining section <b>212</b> selects the adaptive excitation vector, fixed excitation vector and quantization gain that minimize the coding distortion output from perceptual weighting section <b>211</b> respectively from adaptive excitation codebook <b>205</b>, fixed excitation codebook <b>207</b> and quantization gain generating section <b>206</b>, and outputs adaptive excitation vector code (A), excitation gain code (G) and fixed excitation vector code (F) representing the result of the selection to multiplexing section <b>213</b>. In addition, the adaptive excitation vector code (A) is code corresponding to the pitch lag.</p>
<p id="p-0045" num="0044">Multiplexing section <b>213</b> receives the code (L) representing quantized LPC from LPC quantizing section <b>202</b>, further receives the code (A) representing the adaptive excitation vector, the code (F) representing the fixed excitation vector and the code (G) representing the quantization gain from parameter determining section <b>212</b>, and multiplexes these pieces of information to output as base layer coded information.</p>
<p id="p-0046" num="0045">The foregoing is explanations of the internal configuration of base layer coding section <b>101</b> of <figref idref="DRAWINGS">FIG. 1</figref>.</p>
<p id="p-0047" num="0046">With reference to <figref idref="DRAWINGS">FIG. 3</figref>, the processing will briefly be described below for parameter determining section <b>212</b> to determine a signal to be generated from adaptive excitation codebook <b>205</b>. In <figref idref="DRAWINGS">FIG. 3</figref>, buffer <b>301</b> is the buffer provided in adaptive excitation codebook <b>205</b>, position <b>302</b> is a fetching position for the adaptive excitation vector, and vector <b>303</b> is a fetched adaptive excitation vector. Numeric values “41” and “296” respectively correspond to the lower limit and the upper limit of a range in which fetching position <b>302</b> is moved.</p>
<p id="p-0048" num="0047">The range for moving fetching position <b>302</b> is set at a range with a length of “256” (for example, from “41” to “296”), assuming that the number of bits assigned to the code (A) representing the adaptive excitation vector is “8.” The range for moving fetching position <b>302</b> can be set arbitrarily.</p>
<p id="p-0049" num="0048">Parameter determining section <b>212</b> moves fetching position <b>302</b> in the set range, and fetches adaptive excitation vector <b>303</b> by the frame length from each position. Then, parameter determining section <b>212</b> obtains fetching position <b>302</b> that minimizes the coding distortion output from perceptual weighting section <b>211</b>.</p>
<p id="p-0050" num="0049">Fetching position <b>302</b> in the buffer thus obtained by parameter determining section <b>212</b> is the “pitch lag”.</p>
<p id="p-0051" num="0050">The internal configuration of base layer decoding section <b>102</b> (<b>152</b>) of <figref idref="DRAWINGS">FIG. 1</figref> will be described below with reference to <figref idref="DRAWINGS">FIG. 4</figref>.</p>
<p id="p-0052" num="0051">In <figref idref="DRAWINGS">FIG. 4</figref>, the base layer coded information input to base layer decoding section <b>102</b> (<b>152</b>) is demultiplexed to separate codes (L, A, G and F) by demultiplexing section <b>401</b>. The demultiplexed LPC code (L) is output to LPC decoding section <b>402</b>, the demultiplexed adaptive excitation vector code (A) is output to adaptive excitation codebook <b>405</b>, the demultiplexed excitation gain code (G) is output to quantization gain generating section <b>406</b>, and the demultiplexed fixed excitation vector code (F) is output to fixed excitation codebook <b>407</b>.</p>
<p id="p-0053" num="0052">LPC decoding section <b>402</b> decodes the LPC from the code (L) output from demultiplexing section <b>401</b> and outputs the result to synthesis filter <b>403</b>.</p>
<p id="p-0054" num="0053">Adaptive excitation codebook <b>405</b> fetches a sample corresponding to one frame from a past excitation vector signal sample designated by the code (A) output from demultiplexing section <b>401</b> as an excitation vector and outputs the excitation vector to multiplier <b>408</b>. Further, adaptive excitation codebook <b>405</b> outputs the pitch lag as the long term prediction information to enhancement layer coding section <b>104</b> (enhancement layer decoding section <b>153</b>).</p>
<p id="p-0055" num="0054">Quantization gain generating section <b>406</b> decodes an adaptive excitation vector gain and fixed excitation vector gain designated by the excitation gain code (G) output from demultiplexing section <b>401</b> respectively and output the results to multipliers <b>408</b> and <b>409</b>.</p>
<p id="p-0056" num="0055">Fixed excitation codebook <b>407</b> generates a fixed excitation vector designated by the code (F) output from demultiplexing section <b>401</b> and outputs the result to adder <b>409</b>.</p>
<p id="p-0057" num="0056">Multiplier <b>408</b> multiplies the adaptive excitation vector by the adaptive excitation vector gain and outputs the result to adder <b>410</b>. Multiplier <b>409</b> multiplies the fixed excitation vector by the fixed excitation vector gain and outputs the result to adder <b>410</b>.</p>
<p id="p-0058" num="0057">Adder <b>410</b> adds the adaptive excitation vector and fixed excitation vector both multiplied by the gain respectively output from multipliers <b>408</b> and <b>409</b>, generates an excitation vector, and outputs this excitation vector to synthesis filter <b>403</b> and adaptive excitation codebook <b>405</b>.</p>
<p id="p-0059" num="0058">Synthesis filter <b>403</b> performs filter synthesis using the excitation vector output from adder <b>410</b> as an excitation signal and further using the filter coefficients decoded in LPC decoding section <b>402</b>, and outputs a synthesized signal to post-processing section <b>404</b>.</p>
<p id="p-0060" num="0059">Post-processing section <b>404</b> performs on the signal output from synthesis filter <b>403</b> processing for improving subjective quality of speech such as formant emphasis and pitch emphasis and other processing for improving subjective quality of stationary noise to output as a base layer decoded signal.</p>
<p id="p-0061" num="0060">The foregoing is explanations of the internal configuration of base layer decoding section <b>102</b> (<b>152</b>) of <figref idref="DRAWINGS">FIG. 1</figref>.</p>
<p id="p-0062" num="0061">The internal configuration of enhancement layer coding section <b>104</b> of <figref idref="DRAWINGS">FIG. 1</figref> will be described below with reference to <figref idref="DRAWINGS">FIG. 5</figref>.</p>
<p id="p-0063" num="0062">Enhancement layer coding section <b>104</b> divides the residual signal into segments of N samples (N is a natural number), and performs coding for each frame assuming N samples as one frame. Hereinafter, the residual signal is represented by e(<b>0</b>)˜e(X−1), and frames subject to coding is represented by e(n)˜e(n+N−1). Herein, X is a length of the residual signal, and N corresponds to the length of the frame. n is a sample positioned at the beginning of each frame, and corresponds to an integral multiple of N. In addition, the method of predicting a signal of some frame from previously generated signals is called long term prediction. A filter for performing long term prediction is called pitch filter, comb filter and the like.</p>
<p id="p-0064" num="0063">In <figref idref="DRAWINGS">FIG. 5</figref>, long term prediction lag instructing section <b>501</b> receives long term prediction information t obtained in base layer decoding section <b>102</b>, and based on the information, obtains long term prediction lag T of the enhancement layer to output to long term prediction signal storage <b>502</b>. In addition, when a difference in sampling frequency occurs between the base layer and enhancement layer, the long term prediction lag T is obtained from following equation (1). In addition, in equation (1), D is the sampling frequency of the enhancement layer, and d is the sampling frequency of the base layer.
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>T=D×t/d</i>  Equation.(1)<?in-line-formulae description="In-line Formulae" end="tail"?>
</p>
<p id="p-0065" num="0064">Long term prediction signal storage <b>502</b> is provided with a buffer for storing a long term prediction signal generated earlier. When the length of the buffer is assumed M, the buffer is comprised of sequence s(n−M−1)˜s(n−1) of the previously generated long term prediction signal. Upon receiving the long term prediction lag T from long term prediction lag instructing section <b>501</b>, long term prediction signal storage <b>502</b> fetches long term prediction signal s(n−T)˜s(n−T+N−1) the long term prediction lag T back from the previous long term prediction signal sequence stored in the buffer, and outputs the result to long term prediction coefficient calculating section <b>503</b> and long term prediction signal generating section <b>506</b>. Further, long term prediction signal storage <b>502</b> receives long term prediction signal s(n)˜s(n+N−1) from long term prediction signal generating section <b>506</b>, and updates the buffer by following equation (2).
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?>{circumflex over (<i>s</i>)}(<i>i</i>)=<i>s</i>(<i>i+N</i>)(<i>i=n−M−</i>1<i>, . . . , n−</i>1)<?in-line-formulae description="In-line Formulae" end="tail"?>
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>s</i>(<i>i</i>)={circumflex over (<i>s</i>)}(<i>i</i>)(<i>i=n−M−</i>1<i>, . . . , n−</i>1)  Equation (2)<?in-line-formulae description="In-line Formulae" end="tail"?>
</p>
<p id="p-0066" num="0065">In addition, when the long term prediction lag T is shorter than the frame length N and long term prediction signal storage <b>502</b> cannot fetch a long term prediction signal, the long term prediction lag T is multiplied by integrals until the T is longer than the frame length N, to enable the long term prediction signal to be fetched. Otherwise, long term prediction signal s(n−T)˜s(n−T+N−1) the long term prediction lag T back is repeated up to the frame length N to be fetched.</p>
<p id="p-0067" num="0066">Long term prediction coefficient calculating section <b>503</b> receives the residual signal e(n)˜e(n+N−1) and long term prediction signal s(n−T)˜s(n−T+N−1), and using these signals in following equation (3), calculates a long term prediction coefficient β to output to long term prediction coefficient coding section <b>504</b>.</p>
<p id="p-0068" num="0067">
<maths id="MATH-US-00001" num="00001">
<math overflow="scroll">
<mtable>
  <mtr>
    <mtd>
      <mrow>
        <mi>β</mi>
        <mo>=</mo>
        <mfrac>
          <mrow>
            <munderover>
              <mo>∑</mo>
              <mrow>
                <mi>i</mi>
                <mo>=</mo>
                <mn>0</mn>
              </mrow>
              <mrow>
                <mi>N</mi>
                <mo>-</mo>
                <mn>1</mn>
              </mrow>
            </munderover>
            <mo>⁢</mo>
            <mrow>
              <mrow>
                <mi>e</mi>
                <mo>⁡</mo>
                <mrow>
                  <mo>(</mo>
                  <mrow>
                    <mi>n</mi>
                    <mo>+</mo>
                    <mi>i</mi>
                  </mrow>
                  <mo>)</mo>
                </mrow>
              </mrow>
              <mo>⁢</mo>
              <mrow>
                <mi>s</mi>
                <mo>⁡</mo>
                <mrow>
                  <mo>(</mo>
                  <mrow>
                    <mi>n</mi>
                    <mo>-</mo>
                    <mi>T</mi>
                    <mo>+</mo>
                    <mi>i</mi>
                  </mrow>
                  <mo>)</mo>
                </mrow>
              </mrow>
            </mrow>
          </mrow>
          <mrow>
            <munderover>
              <mo>∑</mo>
              <mrow>
                <mi>i</mi>
                <mo>=</mo>
                <mn>0</mn>
              </mrow>
              <mrow>
                <mi>N</mi>
                <mo>-</mo>
                <mn>1</mn>
              </mrow>
            </munderover>
            <mo>⁢</mo>
            <msup>
              <mrow>
                <mi>s</mi>
                <mo>⁡</mo>
                <mrow>
                  <mo>(</mo>
                  <mrow>
                    <mi>n</mi>
                    <mo>-</mo>
                    <mi>T</mi>
                    <mo>+</mo>
                    <mi>i</mi>
                  </mrow>
                  <mo>)</mo>
                </mrow>
              </mrow>
              <mn>2</mn>
            </msup>
          </mrow>
        </mfrac>
      </mrow>
    </mtd>
    <mtd>
      <mrow>
        <mi>Equation</mi>
        <mo>⁢</mo>
        <mstyle>
          <mspace width="1.1em" height="1.1ex"/>
        </mstyle>
        <mo>⁢</mo>
        <mrow>
          <mo>(</mo>
          <mn>3</mn>
          <mo>)</mo>
        </mrow>
      </mrow>
    </mtd>
  </mtr>
</mtable>
</math>
</maths>
</p>
<p id="p-0069" num="0068">Long term prediction coefficient coding section <b>504</b> codes the long term prediction coefficient β, and outputs the enhancement layer coded information obtained by coding to long term prediction coefficient decoding section <b>505</b>, while further outputting the information to enhancement layer decoding section <b>153</b> via the transmission channel. In addition, as a method of coding the long term prediction coefficient β, there are known a method by scalar quantization and the like.</p>
<p id="p-0070" num="0069">Long term prediction coefficient decoding section <b>505</b> decodes the enhancement layer coded information, and outputs a decoded long term prediction coefficient βq obtained by decoding to long term prediction signal generating section <b>506</b>.</p>
<p id="p-0071" num="0070">Long term prediction signal generating section <b>506</b> receives as input the decoded long term prediction coefficient βq and long term prediction signal s(n−T) ˜s(n−T+N−1), and, using the input, calculates long term prediction signal s(n)˜s(n+N−1) by following equation (4), and outputs the result to long term prediction signal storage <b>502</b>.
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>s</i>(<i>n+i</i>)=β<sub>α</sub><i>×s</i>(<i>n−T+</i>1)(<i>i=</i>0<i>, . . . , N−</i>1)  Equation (4)<?in-line-formulae description="In-line Formulae" end="tail"?>
</p>
<p id="p-0072" num="0071">The foregoing is explanations of the internal configuration of enhancement layer coding section <b>104</b> of <figref idref="DRAWINGS">FIG. 1</figref>.</p>
<p id="p-0073" num="0072">The internal configuration of enhancement layer decoding section <b>153</b> of <figref idref="DRAWINGS">FIG. 1</figref> will be described below with reference to the block diagram of <figref idref="DRAWINGS">FIG. 6</figref>.</p>
<p id="p-0074" num="0073">In <figref idref="DRAWINGS">FIG. 6</figref>, long term prediction lag instructing section <b>601</b> obtains the long term prediction lag T of the enhancement layer using the long term prediction information output from base layer decoding section <b>152</b> to output to long term prediction signal storage <b>602</b>.</p>
<p id="p-0075" num="0074">Long term prediction signal storage <b>602</b> is provided with a buffer for storing a long term prediction signal generated earlier. When the length of the buffer is M, the buffer is comprised of sequence s(n−M−1)˜s(n−1) of the earlier generated long term prediction signal. Upon receiving the long term prediction lag T from long term prediction lag instructing section <b>601</b>, long term prediction signal storage <b>602</b> fetches long term prediction signal s(n−T)˜s(n−T+N−1) the long term prediction lag T back from the previous long term prediction signal sequence stored in the buffer to output to long term prediction signal generating section <b>604</b>. Further, long term prediction signal storage <b>602</b> receives long term prediction signals s(n)˜s(n+N−1) from long term prediction signal generating section <b>604</b>, and updates the buffer by equation (2) as described above.</p>
<p id="p-0076" num="0075">Long term prediction coefficient decoding section <b>603</b> decodes the enhancement layer coded information, and outputs the decoded long term prediction coefficient βq obtained by the decoding, to long term prediction signal generating section <b>604</b>.</p>
<p id="p-0077" num="0076">Long term prediction signal generating section <b>604</b> receives as its inputs the decoded long term prediction coefficient βq and long term prediction signal s(n−T) ˜s(n−T+N−1), and using the inputs, calculates long term prediction signal s(n)˜s(n+N−1) by Eq. (4) as described above, and outputs the result to long term prediction signal storage <b>602</b> and adding section <b>153</b> as an enhancement layer decoded signal.</p>
<p id="p-0078" num="0077">The foregoing is explanations of the internal configuration of enhancement layer decoding section <b>153</b> of <figref idref="DRAWINGS">FIG. 1</figref>.</p>
<p id="p-0079" num="0078">Thus, by providing the enhancement layer to perform long term prediction and performing long term prediction on the residual signal in the enhancement layer using the long term correlation characteristic of the speech or sound signal, it is possible to code/decode the speech/sound signal with a wide frequency range using less coded information and to reduce the computation amount.</p>
<p id="p-0080" num="0079">At this point, the coded information can be reduced by obtaining the long term prediction lag using the long term prediction information of the base layer, instead of coding/decoding the long term prediction lag.</p>
<p id="p-0081" num="0080">Further, by decoding the base layer coded information, it is possible to obtain only the decoded signal of the base layer, and implement the function for decoding the speech or sound from part of the coded information in the CELP type speech coding/decoding method (scalable coding).</p>
<p id="p-0082" num="0081">Furthermore, in the long term prediction, using the long term correlation of the speech or sound, a frame with the highest correlation with the current frame is fetched from the buffer, and using a signal of the fetched frame, a signal of the current frame is expressed. However, in the means for fetching the frame with the highest correlation with the current frame from the buffer, when there is no information to represent the long term correlation of speech or sound such as the pitch lag, it is necessary to vary the fetching position to fetch a frame from the buffer while calculating the auto-correlation function of the fetched frame and the current frame to search for the frame with the highest correlation, and the calculation amount for the search becomes significantly large.</p>
<p id="p-0083" num="0082">However, by determining the fetching position uniquely using the pitch lag obtained in base layer coding section <b>101</b>, it is possible to largely reduce the calculation amount required for general long term prediction.</p>
<p id="p-0084" num="0083">In addition, a case has been described above in the enhancement layer long term prediction method explained in this Embodiment where the long term prediction information output from the base layer decoding section is the pitch lag, but the invention is not limited to this, and any information may be used as the long term prediction information as long as the information represents the long term correlation of speech or sound.</p>
<p id="p-0085" num="0084">Further, the case is described in this Embodiment where the position for long term prediction signal storage <b>502</b> to fetch a long term prediction signal from the buffer is the long term prediction lag T, but the invention is applicable to a case where such a position is position T+α (α is a minute number and settable arbitrarily) around the long term prediction lag T, and it is possible to obtain the same effects and advantages as in this Embodiment even in the case where a minute error occurs in the long term prediction lag T.</p>
<p id="p-0086" num="0085">For example, long term prediction signal storage <b>502</b> receives the long term prediction lag T from long term prediction lag instructing section <b>501</b>, fetches long term prediction signal s(n−T−α)˜s(n−T−α+N−1) T+α back from the previous long term prediction signal sequence stored in the buffer, calculates a determination value C using following equation (5), and obtains a that maximizes the determination value C, and encodes this. Further, in the case of decoding, long term prediction signal storage <b>602</b> decodes the coded information of α, and using the long term prediction lag T, fetches long term prediction signal s(n−T−α)˜s(n−T−α+N−1).</p>
<p id="p-0087" num="0086">
<maths id="MATH-US-00002" num="00002">
<math overflow="scroll">
<mtable>
  <mtr>
    <mtd>
      <mrow>
        <mi>C</mi>
        <mo>=</mo>
        <mfrac>
          <msup>
            <mrow>
              <mo>[</mo>
              <mrow>
                <munderover>
                  <mo>∑</mo>
                  <mrow>
                    <mi>i</mi>
                    <mo>=</mo>
                    <mn>0</mn>
                  </mrow>
                  <mrow>
                    <mi>N</mi>
                    <mo>-</mo>
                    <mn>1</mn>
                  </mrow>
                </munderover>
                <mo>⁢</mo>
                <mrow>
                  <mrow>
                    <mi>e</mi>
                    <mo>⁡</mo>
                    <mrow>
                      <mo>(</mo>
                      <mrow>
                        <mi>n</mi>
                        <mo>+</mo>
                        <mi>i</mi>
                      </mrow>
                      <mo>)</mo>
                    </mrow>
                  </mrow>
                  <mo>⁢</mo>
                  <mrow>
                    <mi>s</mi>
                    <mo>⁡</mo>
                    <mrow>
                      <mo>(</mo>
                      <mrow>
                        <mi>n</mi>
                        <mo>-</mo>
                        <mi>T</mi>
                        <mo>-</mo>
                        <mi>α</mi>
                        <mo>+</mo>
                        <mi>i</mi>
                      </mrow>
                      <mo>)</mo>
                    </mrow>
                  </mrow>
                </mrow>
              </mrow>
              <mo>]</mo>
            </mrow>
            <mn>2</mn>
          </msup>
          <mrow>
            <munderover>
              <mo>∑</mo>
              <mrow>
                <mi>i</mi>
                <mo>=</mo>
                <mn>0</mn>
              </mrow>
              <mrow>
                <mi>N</mi>
                <mo>-</mo>
                <mn>1</mn>
              </mrow>
            </munderover>
            <mo>⁢</mo>
            <msup>
              <mrow>
                <mi>s</mi>
                <mo>⁡</mo>
                <mrow>
                  <mo>(</mo>
                  <mrow>
                    <mi>n</mi>
                    <mo>-</mo>
                    <mi>T</mi>
                    <mo>-</mo>
                    <mi>α</mi>
                    <mo>+</mo>
                    <mi>i</mi>
                  </mrow>
                  <mo>)</mo>
                </mrow>
              </mrow>
              <mn>2</mn>
            </msup>
          </mrow>
        </mfrac>
      </mrow>
    </mtd>
    <mtd>
      <mrow>
        <mi>Equation</mi>
        <mo>⁢</mo>
        <mstyle>
          <mspace width="0.8em" height="0.8ex"/>
        </mstyle>
        <mo>⁢</mo>
        <mrow>
          <mo>(</mo>
          <mn>5</mn>
          <mo>)</mo>
        </mrow>
      </mrow>
    </mtd>
  </mtr>
</mtable>
</math>
</maths>
</p>
<p id="p-0088" num="0087">Further, while a case has been described above in this Embodiment where long term prediction is carried out using a speech/sound signal, the invention is eventually applicable to a case of transforming a speech/sound signal from the time domain to the frequency domain using orthogonal transform such as MDCT and QMF, and performing long term prediction using a transformed signal (frequency parameter), and it is still possible to obtain the same effects and advantages as in this Embodiment. For example, in the case of performing enhancement layer long term prediction using the frequency parameter of a speech/sound signal, in <figref idref="DRAWINGS">FIG. 5</figref>, long term prediction coefficient calculating section <b>503</b> is newly provided with a function of transforming long term prediction signal s(n−T)˜s(n−T+N−1) from the time domain to the frequency domain and with another function of transforming a residual signal to the frequency parameter, and long term prediction signal generating section <b>506</b> is newly provided with a function of inverse-transforming long term prediction signals s(n) ˜s(n+N−1) from the frequency domain to time domain. Further, in <figref idref="DRAWINGS">FIG. 6</figref>, long term prediction signal generating section <b>604</b> is newly provided with the function of inverse-transforming long term prediction signal s(n)˜s(n+N−1) from the frequency domain to the time domain.</p>
<p id="p-0089" num="0088">It is general in the general speech/sound coding/decoding method adding redundant bits for use in error detection or error correction to the coded information and transmitting the coded information containing the redundant bits on the transmission channel. It is possible in the invention to weight a bit assignment of redundant bits assigned to the coded information (A) output from base layer coding section <b>101</b> and to the coded information (B) output from enhancement layer coding section <b>104</b> to the coded information (A) to assign.</p>
<heading id="h-0007" level="1">Embodiment 2</heading>
<p id="p-0090" num="0089">Embodiment 2 will be described with reference to a case of coding and decoding a difference (long term prediction residual signal) between the residual signal and long term prediction signal.</p>
<p id="p-0091" num="0090">Configurations of a speech coding apparatus and speech decoding apparatus of this Embodiment are the same as those in <figref idref="DRAWINGS">FIG. 1</figref> except for the internal configurations of enhancement layer coding section <b>104</b> and enhancement layer decoding section <b>153</b>.</p>
<p id="p-0092" num="0091"><figref idref="DRAWINGS">FIG. 7</figref> is a block diagram illustrating an internal configuration of enhancement layer coding section <b>104</b> according to this Embodiment. In addition, in <figref idref="DRAWINGS">FIG. 7</figref>, structural elements common to <figref idref="DRAWINGS">FIG. 5</figref> are assigned the same reference numerals as in <figref idref="DRAWINGS">FIG. 5</figref> to omit descriptions.</p>
<p id="p-0093" num="0092">As compared with <figref idref="DRAWINGS">FIG. 5</figref>, enhancement layer coding section <b>104</b> in <figref idref="DRAWINGS">FIG. 7</figref> is further provided with adding section <b>701</b>, long term prediction residual signal coding section <b>702</b>, coded information multiplexing section <b>703</b>, long term prediction residual signal decoding section <b>704</b> and adding section <b>705</b>.</p>
<p id="p-0094" num="0093">Long term prediction signal generating section <b>506</b> outputs calculated long term prediction signal s(n)˜s(n+N−1) to adding sections <b>701</b> and <b>702</b>.</p>
<p id="p-0095" num="0094">As expressed in following equation (6), adding section <b>701</b> inverts the polarity of long term prediction signal s(n)˜s(n+N−1), adds the result to residual signal e(n)˜e(n+N−1), and outputs long term prediction residual signal p(n)˜p(n+N−1) as a result of the addition to long term prediction residual signal coding section <b>702</b>.
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>p</i>(<i>n+i</i>)=<i>e</i>(<i>n+i</i>)−<i>s</i>(<i>n+i</i>)(<i>i=</i>0<i>, . . . , N−</i>1)  Equation (6)<?in-line-formulae description="In-line Formulae" end="tail"?>
</p>
<p id="p-0096" num="0095">Long term prediction residual signal coding section <b>702</b> codes long term prediction residual signal p(n)˜p(n+N−1), and outputs coded information (hereinafter, referred to as “long term prediction residual coded information”) obtained by coding to coded information multiplexing section <b>703</b> and long term prediction residual signal decoding section <b>704</b>.</p>
<p id="h-0008" num="0000">In addition, the coding of the long term prediction residual signal is generally performed by vector quantization.</p>
<p id="p-0097" num="0096">A method of coding long term prediction residual signal p(n)˜p(n+N−1) will be described below using as one example a case of performing vector quantization with 8 bits. In this case, a codebook storing beforehand generated 256 types of code vectors is prepared in long term prediction residual signal coding section <b>702</b>. The code vector CODE(k)(<b>0</b>)˜CODE(k)(N−1) is a vector with a length of N. k is an index of the code vector and takes values ranging from 0 to 255. Long term prediction residual signal coding section <b>702</b> obtains a square error er between long term prediction residual signal p(n)˜p(n+N−1) and code vector CODE(k)(<b>0</b>)˜CODE(k)(N−1) using following equation (7).</p>
<p id="p-0098" num="0097">
<maths id="MATH-US-00003" num="00003">
<math overflow="scroll">
<mtable>
  <mtr>
    <mtd>
      <mrow>
        <mi>er</mi>
        <mo>=</mo>
        <mrow>
          <munderover>
            <mo>∑</mo>
            <mrow>
              <mi>i</mi>
              <mo>=</mo>
              <mn>0</mn>
            </mrow>
            <mrow>
              <mi>N</mi>
              <mo>-</mo>
              <mn>1</mn>
            </mrow>
          </munderover>
          <mo>⁢</mo>
          <msup>
            <mrow>
              <mo>(</mo>
              <mrow>
                <mrow>
                  <mi>p</mi>
                  <mo>⁡</mo>
                  <mrow>
                    <mo>(</mo>
                    <mrow>
                      <mi>n</mi>
                      <mo>+</mo>
                      <mi>i</mi>
                    </mrow>
                    <mo>)</mo>
                  </mrow>
                </mrow>
                <mo>-</mo>
                <mrow>
                  <msup>
                    <mi>CODE</mi>
                    <mrow>
                      <mo>(</mo>
                      <mi>κ</mi>
                      <mo>)</mo>
                    </mrow>
                  </msup>
                  <mo>⁡</mo>
                  <mrow>
                    <mo>(</mo>
                    <mi>i</mi>
                    <mo>)</mo>
                  </mrow>
                </mrow>
              </mrow>
              <mo>)</mo>
            </mrow>
            <mn>2</mn>
          </msup>
        </mrow>
      </mrow>
    </mtd>
    <mtd>
      <mrow>
        <mi>Equation</mi>
        <mo>⁢</mo>
        <mstyle>
          <mspace width="0.8em" height="0.8ex"/>
        </mstyle>
        <mo>⁢</mo>
        <mrow>
          <mo>(</mo>
          <mn>7</mn>
          <mo>)</mo>
        </mrow>
      </mrow>
    </mtd>
  </mtr>
</mtable>
</math>
</maths>
</p>
<p id="p-0099" num="0098">Then, long term prediction residual signal coding section <b>702</b> determines a value of k that minimizes the square error er as long term prediction residual coded information.</p>
<p id="p-0100" num="0099">Coded information multiplexing section <b>703</b> multiplexes the enhancement layer coded information input from long term prediction coefficient coding section <b>504</b> and the long term prediction residual coded information input from long term prediction residual signal coding section <b>702</b>, and outputs the multiplexed information to enhancement layer decoding section <b>153</b> via the transmission channel.</p>
<p id="p-0101" num="0100">Long term prediction residual signal decoding section <b>704</b> decodes the long term prediction residual coded information, and outputs decoded long term prediction residual signal pq(n)˜pq(n+N−1) to adding section <b>705</b>.</p>
<p id="p-0102" num="0101">Adding section <b>705</b> adds long term prediction signal s(n)˜s(n+N−1) input from long term prediction signal generating section <b>506</b> and decoded long term prediction residual signal pq(n)˜pq(n+N−1) input from long term prediction residual signal decoding section <b>704</b>, and outputs the result of the addition to long term prediction signal storage <b>502</b>. As a result, long term prediction signal storage <b>502</b> updates the buffer using following equation (8).</p>
<p id="p-0103" num="0102">
<maths id="MATH-US-00004" num="00004">
<math overflow="scroll">
<mtable>
  <mtr>
    <mtd>
      <mrow>
        <mrow>
          <mtable>
            <mtr>
              <mtd>
                <mrow>
                  <mrow>
                    <mover>
                      <mi>s</mi>
                      <mo>^</mo>
                    </mover>
                    <mo>⁡</mo>
                    <mrow>
                      <mo>(</mo>
                      <mi>i</mi>
                      <mo>)</mo>
                    </mrow>
                  </mrow>
                  <mo>=</mo>
                  <mrow>
                    <mi>s</mi>
                    <mo>⁢</mo>
                    <mstyle>
                      <mspace width="0.6em" height="0.6ex"/>
                    </mstyle>
                    <mo>⁢</mo>
                    <mrow>
                      <mo>(</mo>
                      <mrow>
                        <mi>i</mi>
                        <mo>+</mo>
                        <mi>N</mi>
                      </mrow>
                      <mo>)</mo>
                    </mrow>
                    <mo>⁢</mo>
                    <mrow>
                      <mo>(</mo>
                      <mrow>
                        <mrow>
                          <mi>i</mi>
                          <mo>=</mo>
                          <mrow>
                            <mi>n</mi>
                            <mo>-</mo>
                            <mi>M</mi>
                            <mo>-</mo>
                            <mn>1</mn>
                          </mrow>
                        </mrow>
                        <mo>,</mo>
                        <mi>⋯</mi>
                        <mo>⁢</mo>
                        <mstyle>
                          <mspace width="0.8em" height="0.8ex"/>
                        </mstyle>
                        <mo>,</mo>
                        <mrow>
                          <mi>n</mi>
                          <mo>-</mo>
                          <mi>N</mi>
                          <mo>-</mo>
                          <mn>1</mn>
                        </mrow>
                      </mrow>
                      <mo>)</mo>
                    </mrow>
                  </mrow>
                </mrow>
              </mtd>
            </mtr>
            <mtr>
              <mtd>
                <mrow>
                  <mrow>
                    <mrow>
                      <mover>
                        <mi>s</mi>
                        <mo>^</mo>
                      </mover>
                      <mo>⁡</mo>
                      <mrow>
                        <mo>(</mo>
                        <mi>i</mi>
                        <mo>)</mo>
                      </mrow>
                    </mrow>
                    <mo>=</mo>
                    <mrow>
                      <mrow>
                        <mi>s</mi>
                        <mo>⁢</mo>
                        <mstyle>
                          <mspace width="0.6em" height="0.6ex"/>
                        </mstyle>
                        <mo>⁢</mo>
                        <mrow>
                          <mo>(</mo>
                          <mrow>
                            <mi>i</mi>
                            <mo>+</mo>
                            <mi>N</mi>
                          </mrow>
                          <mo>)</mo>
                        </mrow>
                      </mrow>
                      <mo>+</mo>
                      <mi>p</mi>
                    </mrow>
                  </mrow>
                  <mo>,</mo>
                  <mrow>
                    <mrow>
                      <mo>(</mo>
                      <mrow>
                        <mi>i</mi>
                        <mo>-</mo>
                        <mi>N</mi>
                      </mrow>
                      <mo>)</mo>
                    </mrow>
                    <mo>⁢</mo>
                    <mrow>
                      <mo>(</mo>
                      <mrow>
                        <mrow>
                          <mi>i</mi>
                          <mo>=</mo>
                          <mrow>
                            <mi>n</mi>
                            <mo>-</mo>
                            <mi>N</mi>
                          </mrow>
                        </mrow>
                        <mo>,</mo>
                        <mi>⋯</mi>
                        <mo>⁢</mo>
                        <mstyle>
                          <mspace width="0.8em" height="0.8ex"/>
                        </mstyle>
                        <mo>,</mo>
                        <mrow>
                          <mi>n</mi>
                          <mo>-</mo>
                          <mn>1</mn>
                        </mrow>
                      </mrow>
                      <mo>)</mo>
                    </mrow>
                  </mrow>
                </mrow>
              </mtd>
            </mtr>
          </mtable>
          <mo>}</mo>
        </mrow>
        <mo>⁢</mo>
        <mstyle>
          <mtext>
</mtext>
        </mstyle>
        <mo>⁢</mo>
        <mrow>
          <mrow>
            <mi>s</mi>
            <mo>⁡</mo>
            <mrow>
              <mo>(</mo>
              <mi>i</mi>
              <mo>)</mo>
            </mrow>
          </mrow>
          <mo>=</mo>
          <mrow>
            <mrow>
              <mover>
                <mi>s</mi>
                <mo>^</mo>
              </mover>
              <mo>⁡</mo>
              <mrow>
                <mo>(</mo>
                <mi>i</mi>
                <mo>)</mo>
              </mrow>
            </mrow>
            <mo>⁢</mo>
            <mrow>
              <mo>(</mo>
              <mrow>
                <mrow>
                  <mi>i</mi>
                  <mo>=</mo>
                  <mrow>
                    <mi>n</mi>
                    <mo>-</mo>
                    <mi>M</mi>
                    <mo>-</mo>
                    <mn>1</mn>
                  </mrow>
                </mrow>
                <mo>,</mo>
                <mi>⋯</mi>
                <mo>⁢</mo>
                <mstyle>
                  <mspace width="0.8em" height="0.8ex"/>
                </mstyle>
                <mo>,</mo>
                <mrow>
                  <mi>n</mi>
                  <mo>-</mo>
                  <mn>1</mn>
                </mrow>
              </mrow>
              <mo>)</mo>
            </mrow>
          </mrow>
        </mrow>
      </mrow>
    </mtd>
    <mtd>
      <mrow>
        <mi>Equation</mi>
        <mo>⁢</mo>
        <mstyle>
          <mspace width="0.8em" height="0.8ex"/>
        </mstyle>
        <mo>⁢</mo>
        <mrow>
          <mo>(</mo>
          <mn>8</mn>
          <mo>)</mo>
        </mrow>
      </mrow>
    </mtd>
  </mtr>
</mtable>
</math>
</maths>
</p>
<p id="p-0104" num="0103">The foregoing is explanations of the internal configuration of enhancement layer coding section <b>104</b> according to this Embodiment.</p>
<p id="p-0105" num="0104">An internal configuration of enhancement layer decoding section <b>153</b> according to this Embodiment will be described below with reference to the block diagram in <figref idref="DRAWINGS">FIG. 8</figref>. In addition, in <figref idref="DRAWINGS">FIG. 8</figref>, structural elements common to <figref idref="DRAWINGS">FIG. 6</figref> are assigned the same reference numerals as in <figref idref="DRAWINGS">FIG. 6</figref> to omit descriptions.</p>
<p id="p-0106" num="0105">Compared with <figref idref="DRAWINGS">FIG. 6</figref>, enhancement layer decoding section <b>153</b> in <figref idref="DRAWINGS">FIG. 8</figref> is further provided with coded information demultiplexing section <b>801</b>, long term prediction residual signal decoding section <b>802</b> and adding section <b>803</b>.</p>
<p id="p-0107" num="0106">Coded information demultiplexing section <b>801</b> demultiplexes the multiplexed coded information received via the transmission channel into the enhancement layer coded information and long term prediction residual coded information, and outputs the enhancement layer coded information to long term prediction coefficient decoding section <b>603</b>, and the long term prediction residual coded information to long term prediction residual signal decoding section <b>802</b>.</p>
<p id="p-0108" num="0107">Long term prediction residual signal decoding section <b>802</b> decodes the long term prediction residual coded information, obtains decoded long term prediction residual signal pq(n)˜pq(n+N−1), and outputs the signal to adding section <b>803</b>.</p>
<p id="p-0109" num="0108">Adding section <b>803</b> adds long term prediction signal s(n)˜s(n+N−1) input from long term prediction signal generating section <b>604</b> and decoded long term prediction residual signal pq(n)˜pq(n+N−1) input from long term prediction residual signal decoding section <b>802</b>, and outputs a result of the addition to long term prediction signal storage <b>602</b>, while outputting the result as an enhancement layer decoded signal.</p>
<p id="p-0110" num="0109">The foregoing is explanations of the internal configuration of enhancement layer decoding section <b>153</b> according to this Embodiment.</p>
<p id="p-0111" num="0110">By thus coding and decoding the difference (long term prediction residual signal) between the residual signal and long term prediction signal, it is possible to obtain a decoded signal with higher quality than previously described in Embodiment 1.</p>
<p id="p-0112" num="0111">In addition, a case has been described above in this Embodiment of coding a long term prediction residual signal by vector quantization. However, the present invention is not limited in coding method, and coding may be performed using shape-gain VQ, split VQ, transform VQ or multi-phase VQ, for example.</p>
<p id="p-0113" num="0112">A case will be described below of performing coding by shape-gain VQ of 13 bits of 8 bits in shape and 5 bits in gain. In this case, two types of codebooks are provided, a shape codebook and gain codebook. The shape codebook is comprised of 256 types of shape code vectors, and shape code vector SCODE(k<b>1</b>)(<b>0</b>)˜SCODE(k<b>1</b>)(N−1) is a vector with a length of N. k<b>1</b> is an index of the shape code vector and takes values ranging from 0 to 255. The gain codebook is comprised of 32 types of gain codes, and gain code GCODE(k<b>2</b>) takes a scalar value. k<b>2</b> is an index of the gain code and takes values ranging from 0 to 31. Long term prediction residual signal coding section <b>702</b> obtains the gain and shape vector shape(<b>0</b>)˜shape(N−1) of long term prediction residual signal p(n)˜p(n+N−1) using following equation (9), and further obtains a gain error gainer between the gain and gain code GCODE(k<b>2</b>) and a square error shapeer between shape vector shape(<b>0</b>)˜shape(N−1) and shape code vector SCODE(k<b>1</b>)(<b>0</b>)˜SCODE(k<b>1</b>)(N−1).</p>
<p id="p-0114" num="0113">
<maths id="MATH-US-00005" num="00005">
<math overflow="scroll">
<mtable>
  <mtr>
    <mtd>
      <mrow>
        <mrow>
          <mi>gain</mi>
          <mo>=</mo>
          <msqrt>
            <mrow>
              <munderover>
                <mo>∑</mo>
                <mrow>
                  <mi>i</mi>
                  <mo>=</mo>
                  <mn>0</mn>
                </mrow>
                <mrow>
                  <mi>N</mi>
                  <mo>-</mo>
                  <mn>1</mn>
                </mrow>
              </munderover>
              <mo>⁢</mo>
              <msup>
                <mrow>
                  <mi>p</mi>
                  <mo>⁡</mo>
                  <mrow>
                    <mo>(</mo>
                    <mrow>
                      <mi>n</mi>
                      <mo>+</mo>
                      <mi>i</mi>
                    </mrow>
                    <mo>)</mo>
                  </mrow>
                </mrow>
                <mn>2</mn>
              </msup>
            </mrow>
          </msqrt>
        </mrow>
        <mo>⁢</mo>
        <mstyle>
          <mtext>
</mtext>
        </mstyle>
        <mo>⁢</mo>
        <mrow>
          <mrow>
            <mi>shape</mi>
            <mo>⁡</mo>
            <mrow>
              <mo>(</mo>
              <mi>i</mi>
              <mo>)</mo>
            </mrow>
          </mrow>
          <mo>=</mo>
          <mrow>
            <mfrac>
              <mrow>
                <mi>p</mi>
                <mo>⁡</mo>
                <mrow>
                  <mo>(</mo>
                  <mrow>
                    <mi>n</mi>
                    <mo>+</mo>
                    <mi>i</mi>
                  </mrow>
                  <mo>)</mo>
                </mrow>
              </mrow>
              <mi>gain</mi>
            </mfrac>
            <mo>⁢</mo>
            <mrow>
              <mo>(</mo>
              <mrow>
                <mrow>
                  <mi>i</mi>
                  <mo>=</mo>
                  <mn>0</mn>
                </mrow>
                <mo>,</mo>
                <mi>…</mi>
                <mo>⁢</mo>
                <mstyle>
                  <mspace width="0.6em" height="0.6ex"/>
                </mstyle>
                <mo>,</mo>
                <mrow>
                  <mi>N</mi>
                  <mo>-</mo>
                  <mn>1</mn>
                </mrow>
              </mrow>
              <mo>)</mo>
            </mrow>
          </mrow>
        </mrow>
      </mrow>
    </mtd>
    <mtd>
      <mrow>
        <mi>Equation</mi>
        <mo>⁢</mo>
        <mstyle>
          <mspace width="0.8em" height="0.8ex"/>
        </mstyle>
        <mo>⁢</mo>
        <mrow>
          <mo>(</mo>
          <mn>9</mn>
          <mo>)</mo>
        </mrow>
      </mrow>
    </mtd>
  </mtr>
  <mtr>
    <mtd>
      <mrow>
        <mrow>
          <mi>gainer</mi>
          <mo>=</mo>
          <mrow>
            <mo></mo>
            <mrow>
              <mi>gatn</mi>
              <mo>-</mo>
              <msup>
                <mi>GCODE</mi>
                <mrow>
                  <mo>(</mo>
                  <mi>k2</mi>
                  <mo>)</mo>
                </mrow>
              </msup>
            </mrow>
            <mo></mo>
          </mrow>
        </mrow>
        <mo>⁢</mo>
        <mstyle>
          <mtext>
</mtext>
        </mstyle>
        <mo>⁢</mo>
        <mrow>
          <mi>shapeer</mi>
          <mo>=</mo>
          <mrow>
            <munderover>
              <mo>∑</mo>
              <mrow>
                <mi>i</mi>
                <mo>=</mo>
                <mn>0</mn>
              </mrow>
              <mrow>
                <mi>N</mi>
                <mo>-</mo>
                <mn>1</mn>
              </mrow>
            </munderover>
            <mo>⁢</mo>
            <msup>
              <mrow>
                <mo>(</mo>
                <mrow>
                  <mrow>
                    <mi>shape</mi>
                    <mo>⁡</mo>
                    <mrow>
                      <mo>(</mo>
                      <mi>i</mi>
                      <mo>)</mo>
                    </mrow>
                  </mrow>
                  <mo>-</mo>
                  <mrow>
                    <msup>
                      <mi>SCODE</mi>
                      <mrow>
                        <mo>(</mo>
                        <mi>k2</mi>
                        <mo>)</mo>
                      </mrow>
                    </msup>
                    <mo>⁡</mo>
                    <mrow>
                      <mo>(</mo>
                      <mi>i</mi>
                      <mo>)</mo>
                    </mrow>
                  </mrow>
                </mrow>
                <mo>)</mo>
              </mrow>
              <mn>2</mn>
            </msup>
          </mrow>
        </mrow>
      </mrow>
    </mtd>
    <mtd>
      <mrow>
        <mi>Equation</mi>
        <mo>⁢</mo>
        <mstyle>
          <mspace width="0.8em" height="0.8ex"/>
        </mstyle>
        <mo>⁢</mo>
        <mrow>
          <mo>(</mo>
          <mn>10</mn>
          <mo>)</mo>
        </mrow>
      </mrow>
    </mtd>
  </mtr>
</mtable>
</math>
</maths>
</p>
<p id="p-0115" num="0114">Then, long term prediction residual signal coding section <b>702</b> obtains a value of k<b>2</b> that minimizes the gain error gainer and a value of k<b>1</b> that minimizes the square error shapper, and determines the obtained values as long term prediction residual coded information.</p>
<p id="p-0116" num="0115">A case will be described below where coding is performed by split VQ of 8 bits. In this case, two types of codebooks are prepared, the first split codebook and second split codebook.</p>
<p id="p-0117" num="0116">The first split codebook is comprised of 16 types of first split code vectors SPCODE(k<b>3</b>)(<b>0</b>)˜SPCODE(k<b>3</b>)(N/2−1), second split codebook SPCODE(k<b>4</b>)(<b>0</b>)˜SPCODE(k<b>4</b>)(N/2−1) is comprised of 16 types of second split code vectors, and each code vector has a length of N/2. k<b>3</b> is an index of the first split code vector and takes values ranging from 0 to 15 k<b>4</b> is an index of the second split code vector and takes values ranging from 0 to 15. Long term prediction residual signal coding section <b>702</b> divides long term prediction residual signal p(n)˜p(n+N−1) into first split vector sp<b>1</b>(<b>0</b>)˜sp<b>1</b>(N/2−1) and second split vector sp<b>2</b>(<b>0</b>)˜sp<b>2</b>(N/2−1) using following equation (11), and obtains a square error splitter <b>1</b> between first split vector sp<b>1</b>(<b>0</b>)˜sp<b>1</b>(N/2−1) and first split code vector SPCODE(k<b>3</b>)(<b>0</b>)˜SPCODE(k<b>3</b>)(N/2−1), and a square error splitter <b>2</b> between second split vector sp<b>2</b>(<b>0</b>)˜sp<b>2</b>(N/2−1) and second split codebook SPCODE(k<b>4</b>)(<b>0</b>)˜SPCODE(k<b>4</b>)(N/2−1), using following equation (12).</p>
<p id="p-0118" num="0117">
<maths id="MATH-US-00006" num="00006">
<math overflow="scroll">
<mtable>
  <mtr>
    <mtd>
      <mrow>
        <mrow>
          <mrow>
            <msub>
              <mi>sp</mi>
              <mn>1</mn>
            </msub>
            <mo>⁡</mo>
            <mrow>
              <mo>(</mo>
              <mi>i</mi>
              <mo>)</mo>
            </mrow>
          </mrow>
          <mo>=</mo>
          <mrow>
            <mrow>
              <mi>p</mi>
              <mo>⁡</mo>
              <mrow>
                <mo>(</mo>
                <mrow>
                  <mi>n</mi>
                  <mo>+</mo>
                  <mn>1</mn>
                </mrow>
                <mo>)</mo>
              </mrow>
            </mrow>
            <mo>⁢</mo>
            <mrow>
              <mo>(</mo>
              <mrow>
                <mrow>
                  <mi>i</mi>
                  <mo>=</mo>
                  <mn>0</mn>
                </mrow>
                <mo>,</mo>
                <mi>…</mi>
                <mo>⁢</mo>
                <mstyle>
                  <mspace width="0.6em" height="0.6ex"/>
                </mstyle>
                <mo>,</mo>
                <mrow>
                  <mrow>
                    <mi>N</mi>
                    <mo>/</mo>
                    <mn>2</mn>
                  </mrow>
                  <mo>-</mo>
                  <mn>1</mn>
                </mrow>
              </mrow>
              <mo>)</mo>
            </mrow>
          </mrow>
        </mrow>
        <mo>⁢</mo>
        <mstyle>
          <mtext>
</mtext>
        </mstyle>
        <mo>⁢</mo>
        <mrow>
          <mrow>
            <msub>
              <mi>sp</mi>
              <mn>2</mn>
            </msub>
            <mo>⁡</mo>
            <mrow>
              <mo>(</mo>
              <mi>i</mi>
              <mo>)</mo>
            </mrow>
          </mrow>
          <mo>=</mo>
          <mrow>
            <mrow>
              <mi>p</mi>
              <mo>⁡</mo>
              <mrow>
                <mo>(</mo>
                <mrow>
                  <mi>n</mi>
                  <mo>+</mo>
                  <mrow>
                    <mi>N</mi>
                    <mo>/</mo>
                    <mn>2</mn>
                  </mrow>
                  <mo>+</mo>
                  <mi>i</mi>
                </mrow>
                <mo>)</mo>
              </mrow>
            </mrow>
            <mo>⁢</mo>
            <mrow>
              <mo>(</mo>
              <mrow>
                <mrow>
                  <mi>i</mi>
                  <mo>=</mo>
                  <mn>0</mn>
                </mrow>
                <mo>,</mo>
                <mi>…</mi>
                <mo>⁢</mo>
                <mstyle>
                  <mspace width="0.6em" height="0.6ex"/>
                </mstyle>
                <mo>,</mo>
                <mrow>
                  <mrow>
                    <mi>N</mi>
                    <mo>/</mo>
                    <mn>2</mn>
                  </mrow>
                  <mo>-</mo>
                  <mn>1</mn>
                </mrow>
              </mrow>
              <mo>)</mo>
            </mrow>
          </mrow>
        </mrow>
      </mrow>
    </mtd>
    <mtd>
      <mrow>
        <mi>Equation</mi>
        <mo>⁢</mo>
        <mstyle>
          <mspace width="0.8em" height="0.8ex"/>
        </mstyle>
        <mo>⁢</mo>
        <mrow>
          <mo>(</mo>
          <mn>11</mn>
          <mo>)</mo>
        </mrow>
      </mrow>
    </mtd>
  </mtr>
  <mtr>
    <mtd>
      <mrow>
        <mrow>
          <msub>
            <mi>spliter</mi>
            <mn>1</mn>
          </msub>
          <mo>=</mo>
          <mrow>
            <munderover>
              <mo>∑</mo>
              <mrow>
                <mi>i</mi>
                <mo>=</mo>
                <mn>0</mn>
              </mrow>
              <mrow>
                <mrow>
                  <mi>N</mi>
                  <mo>/</mo>
                  <mn>2</mn>
                </mrow>
                <mo>-</mo>
                <mn>1</mn>
              </mrow>
            </munderover>
            <mo>⁢</mo>
            <msup>
              <mrow>
                <mo>(</mo>
                <mrow>
                  <mrow>
                    <msub>
                      <mi>sp</mi>
                      <mn>1</mn>
                    </msub>
                    <mo>⁡</mo>
                    <mrow>
                      <mo>(</mo>
                      <mi>i</mi>
                      <mo>)</mo>
                    </mrow>
                  </mrow>
                  <mo>-</mo>
                  <mrow>
                    <msubsup>
                      <mi>SPCODE</mi>
                      <mn>1</mn>
                      <mrow>
                        <mo>(</mo>
                        <mi>k3</mi>
                        <mo>)</mo>
                      </mrow>
                    </msubsup>
                    <mo>⁡</mo>
                    <mrow>
                      <mo>(</mo>
                      <mi>i</mi>
                      <mo>)</mo>
                    </mrow>
                  </mrow>
                </mrow>
                <mo>)</mo>
              </mrow>
              <mn>2</mn>
            </msup>
          </mrow>
        </mrow>
        <mo>⁢</mo>
        <mstyle>
          <mtext>
</mtext>
        </mstyle>
        <mo>⁢</mo>
        <mrow>
          <msub>
            <mi>spliter</mi>
            <mn>2</mn>
          </msub>
          <mo>=</mo>
          <mrow>
            <munderover>
              <mo>∑</mo>
              <mrow>
                <mi>i</mi>
                <mo>=</mo>
                <mn>0</mn>
              </mrow>
              <mrow>
                <mrow>
                  <mi>N</mi>
                  <mo>/</mo>
                  <mn>2</mn>
                </mrow>
                <mo>-</mo>
                <mn>1</mn>
              </mrow>
            </munderover>
            <mo>⁢</mo>
            <msup>
              <mrow>
                <mo>(</mo>
                <mrow>
                  <mrow>
                    <msub>
                      <mi>sp</mi>
                      <mn>2</mn>
                    </msub>
                    <mo>⁡</mo>
                    <mrow>
                      <mo>(</mo>
                      <mi>i</mi>
                      <mo>)</mo>
                    </mrow>
                  </mrow>
                  <mo>-</mo>
                  <mrow>
                    <msubsup>
                      <mi>SPCODE</mi>
                      <mn>2</mn>
                      <mrow>
                        <mo>(</mo>
                        <mi>k4</mi>
                        <mo>)</mo>
                      </mrow>
                    </msubsup>
                    <mo>⁡</mo>
                    <mrow>
                      <mo>(</mo>
                      <mi>i</mi>
                      <mo>)</mo>
                    </mrow>
                  </mrow>
                </mrow>
                <mo>)</mo>
              </mrow>
              <mn>2</mn>
            </msup>
          </mrow>
        </mrow>
      </mrow>
    </mtd>
    <mtd>
      <mrow>
        <mi>Equation</mi>
        <mo>⁢</mo>
        <mstyle>
          <mspace width="0.8em" height="0.8ex"/>
        </mstyle>
        <mo>⁢</mo>
        <mrow>
          <mo>(</mo>
          <mn>12</mn>
          <mo>)</mo>
        </mrow>
      </mrow>
    </mtd>
  </mtr>
</mtable>
</math>
</maths>
</p>
<p id="p-0119" num="0118">Then, long term prediction residual signal coding section <b>702</b> obtains the value of k<b>3</b> that minimizes the square error splitter <b>1</b> and the value of k<b>4</b> that minimizes the square error splitter <b>2</b>, and determines the obtained values as long term prediction residual coded information.</p>
<p id="p-0120" num="0119">A case will be described below where coding is performed by transform VQ of 8 bits using discrete Fourier transform. In this case, a transform codebook comprised of 256 types of transform code vector is prepared, and transform code vector TCODE(k<b>5</b>)(<b>0</b>)˜TCODE(k<b>5</b>)(N/2−1) is a vector with a length of N/2. k<b>5</b> is an index of the transform code vector and takes values ranging from 0 to 255. Long term prediction residual signal coding section <b>702</b> performs discrete Fourier transform of long term prediction residual signal p(n)˜p(n+N−1) to obtain transform vector tp(<b>0</b>)˜tp(N−1) using following equation (13), and obtains a square error transer between transform vector tp(<b>0</b>)˜tp(N−1) and transform code vector TCODE(k<b>5</b>)(<b>0</b>)˜TCODE(k<b>5</b>)(N/2−1) using following equation (14).</p>
<p id="p-0121" num="0120">
<maths id="MATH-US-00007" num="00007">
<math overflow="scroll">
<mtable>
  <mtr>
    <mtd>
      <mrow>
        <mrow>
          <mi>tp</mi>
          <mo>⁡</mo>
          <mrow>
            <mo>(</mo>
            <mover>
              <mi>i</mi>
              <mo>^</mo>
            </mover>
            <mo>)</mo>
          </mrow>
        </mrow>
        <mo>=</mo>
        <mrow>
          <munderover>
            <mo>∑</mo>
            <mrow>
              <mi>i</mi>
              <mo>=</mo>
              <mn>0</mn>
            </mrow>
            <mrow>
              <mi>N</mi>
              <mo>-</mo>
              <mn>1</mn>
            </mrow>
          </munderover>
          <mo>⁢</mo>
          <mrow>
            <mrow>
              <mi>p</mi>
              <mo>⁡</mo>
              <mrow>
                <mo>(</mo>
                <mrow>
                  <mi>n</mi>
                  <mo>+</mo>
                  <mi>i</mi>
                </mrow>
                <mo>)</mo>
              </mrow>
            </mrow>
            <mo>⁢</mo>
            <mrow>
              <msup>
                <mi>ⅇ</mi>
                <mrow>
                  <mrow>
                    <mo>-</mo>
                    <mi>j</mi>
                  </mrow>
                  <mo>⁢</mo>
                  <mfrac>
                    <mrow>
                      <mn>2</mn>
                      <mo>⁢</mo>
                      <mstyle>
                        <mspace width="0.3em" height="0.3ex"/>
                      </mstyle>
                      <mo>⁢</mo>
                      <mi>r</mi>
                      <mo>⁢</mo>
                      <mstyle>
                        <mspace width="0.3em" height="0.3ex"/>
                      </mstyle>
                      <mo>⁢</mo>
                      <mi>σⅈ</mi>
                    </mrow>
                    <mi>N</mi>
                  </mfrac>
                </mrow>
              </msup>
              <mo>⁡</mo>
              <mrow>
                <mo>(</mo>
                <mrow>
                  <mrow>
                    <mover>
                      <mi>i</mi>
                      <mo>^</mo>
                    </mover>
                    <mo>=</mo>
                    <mn>0</mn>
                  </mrow>
                  <mo>,</mo>
                  <mrow>
                    <mrow>
                      <mi>…</mi>
                      <mo>⁢</mo>
                      <mstyle>
                        <mspace width="0.6em" height="0.6ex"/>
                      </mstyle>
                      <mo>⁢</mo>
                      <mi>N</mi>
                    </mrow>
                    <mo>-</mo>
                    <mn>1</mn>
                  </mrow>
                </mrow>
                <mo>)</mo>
              </mrow>
            </mrow>
          </mrow>
        </mrow>
      </mrow>
    </mtd>
    <mtd>
      <mrow>
        <mi>Equation</mi>
        <mo>⁢</mo>
        <mstyle>
          <mspace width="0.8em" height="0.8ex"/>
        </mstyle>
        <mo>⁢</mo>
        <mrow>
          <mo>(</mo>
          <mn>13</mn>
          <mo>)</mo>
        </mrow>
      </mrow>
    </mtd>
  </mtr>
  <mtr>
    <mtd>
      <mrow>
        <mi>transer</mi>
        <mo>=</mo>
        <mrow>
          <munderover>
            <mo>∑</mo>
            <mrow>
              <mi>i</mi>
              <mo>=</mo>
              <mn>0</mn>
            </mrow>
            <mrow>
              <mi>N</mi>
              <mo>-</mo>
              <mn>1</mn>
            </mrow>
          </munderover>
          <mo>⁢</mo>
          <msup>
            <mrow>
              <mo>(</mo>
              <mrow>
                <mrow>
                  <mi>tp</mi>
                  <mo>⁡</mo>
                  <mrow>
                    <mo>(</mo>
                    <mi>i</mi>
                    <mo>)</mo>
                  </mrow>
                </mrow>
                <mo>-</mo>
                <mrow>
                  <msup>
                    <mi>TCODE</mi>
                    <mrow>
                      <mo>(</mo>
                      <mi>k3</mi>
                      <mo>)</mo>
                    </mrow>
                  </msup>
                  <mo>⁡</mo>
                  <mrow>
                    <mo>(</mo>
                    <mi>i</mi>
                    <mo>)</mo>
                  </mrow>
                </mrow>
              </mrow>
              <mo>)</mo>
            </mrow>
            <mn>2</mn>
          </msup>
        </mrow>
      </mrow>
    </mtd>
    <mtd>
      <mrow>
        <mi>Equation</mi>
        <mo>⁢</mo>
        <mstyle>
          <mspace width="0.8em" height="0.8ex"/>
        </mstyle>
        <mo>⁢</mo>
        <mrow>
          <mo>(</mo>
          <mn>14</mn>
          <mo>)</mo>
        </mrow>
      </mrow>
    </mtd>
  </mtr>
</mtable>
</math>
</maths>
</p>
<p id="p-0122" num="0121">Then, long term prediction residual signal coding section <b>702</b> obtains a value of k<b>5</b> that minimizes the square error transfer, and determines the obtained value as long term prediction residual coded information.</p>
<p id="p-0123" num="0122">A case will be described below of performing coding by two-phase VQ of 13 bits of 5 bits for a first stage and 8 bits for a second stage. In this case, two types of codebooks are prepared, a first stage codebook and second stage codebook. The first stage codebook is comprised of 32 types of first stage code vectors PHCODE<b>1</b>(k<b>6</b>)(<b>0</b>)˜PHCODE<b>1</b>(k<b>6</b>)(N−1), the second stage codebook is comprised of 256 types of second stage code vectors PHCODE<b>2</b>(k<b>7</b>)(<b>0</b>)˜PHCODE<b>2</b>(k<b>7</b>)(N−1), and each code vector has a length of N/2.k<b>6</b> is an index of the first stage code vector and takes values ranging from 0 to 31.</p>
<p id="p-0124" num="0123">k<b>7</b> is an index of the second stage code vector and takes values ranging from 0 to 255. Long term prediction residual signal coding section <b>702</b> obtains a square error phaseer <b>1</b> between long term prediction residual signal p(n)˜p(n+N−1) and first stage code vector PHCODE<b>1</b>(k<b>6</b>)(<b>0</b>)˜PHCODE<b>1</b>(k<b>6</b>)(N−1) using following equation (15), further obtains the value of k<b>6</b> that minimizes the square error phaseer <b>1</b>, and determines the value as Kmax.</p>
<p id="p-0125" num="0124">
<maths id="MATH-US-00008" num="00008">
<math overflow="scroll">
<mtable>
  <mtr>
    <mtd>
      <mrow>
        <msub>
          <mi>phaseer</mi>
          <mn>1</mn>
        </msub>
        <mo>=</mo>
        <mrow>
          <munderover>
            <mo>∑</mo>
            <mrow>
              <mi>i</mi>
              <mo>=</mo>
              <mn>0</mn>
            </mrow>
            <mrow>
              <mi>N</mi>
              <mo>-</mo>
              <mn>1</mn>
            </mrow>
          </munderover>
          <mo>⁢</mo>
          <msup>
            <mrow>
              <mo>(</mo>
              <mrow>
                <mrow>
                  <mi>tp</mi>
                  <mo>⁡</mo>
                  <mrow>
                    <mo>(</mo>
                    <mi>i</mi>
                    <mo>)</mo>
                  </mrow>
                </mrow>
                <mo>-</mo>
                <mrow>
                  <msup>
                    <mi>TCODE</mi>
                    <mrow>
                      <mo>(</mo>
                      <mi>k3</mi>
                      <mo>)</mo>
                    </mrow>
                  </msup>
                  <mo>⁡</mo>
                  <mrow>
                    <mo>(</mo>
                    <mi>i</mi>
                    <mo>)</mo>
                  </mrow>
                </mrow>
              </mrow>
              <mo>)</mo>
            </mrow>
            <mn>2</mn>
          </msup>
        </mrow>
      </mrow>
    </mtd>
    <mtd>
      <mrow>
        <mi>Equation</mi>
        <mo>⁢</mo>
        <mstyle>
          <mspace width="0.8em" height="0.8ex"/>
        </mstyle>
        <mo>⁢</mo>
        <mrow>
          <mo>(</mo>
          <mn>15</mn>
          <mo>)</mo>
        </mrow>
      </mrow>
    </mtd>
  </mtr>
</mtable>
</math>
</maths>
</p>
<p id="p-0126" num="0125">Then, long term prediction residual signal coding section <b>702</b> obtains error vector ep(<b>0</b>)˜ep(N−1) using following equation (16), obtains a square error phaseer <b>2</b> between error vector ep(<b>0</b>)˜ep(N−1) and second stage code vector PHCODE<b>2</b>(k<b>7</b>)(<b>0</b>)˜PHCODE<b>2</b>(k<b>7</b>)(N−1) using following equation (17), further obtains a value of k<b>7</b> that minimizes the square error phaseer <b>2</b>, and determines the value and Kmax as long term prediction residual coded information.</p>
<p id="p-0127" num="0126">
<maths id="MATH-US-00009" num="00009">
<math overflow="scroll">
<mtable>
  <mtr>
    <mtd>
      <mrow>
        <mrow>
          <mi>ep</mi>
          <mo>⁡</mo>
          <mrow>
            <mo>(</mo>
            <mi>i</mi>
            <mo>)</mo>
          </mrow>
        </mrow>
        <mo>=</mo>
        <mrow>
          <mrow>
            <mi>p</mi>
            <mo>⁡</mo>
            <mrow>
              <mo>(</mo>
              <mrow>
                <mi>n</mi>
                <mo>+</mo>
                <mn>1</mn>
              </mrow>
              <mo>)</mo>
            </mrow>
          </mrow>
          <mo>-</mo>
          <mrow>
            <mrow>
              <msubsup>
                <mi>PHCODE</mi>
                <mn>1</mn>
                <mrow>
                  <mo>(</mo>
                  <mi>kmax</mi>
                  <mo>)</mo>
                </mrow>
              </msubsup>
              <mo>⁡</mo>
              <mrow>
                <mo>(</mo>
                <mi>i</mi>
                <mo>)</mo>
              </mrow>
            </mrow>
            <mo>⁢</mo>
            <mstyle>
              <mspace width="1.1em" height="1.1ex"/>
            </mstyle>
            <mo>⁢</mo>
            <mrow>
              <mo>(</mo>
              <mrow>
                <mrow>
                  <mi>i</mi>
                  <mo>=</mo>
                  <mn>0</mn>
                </mrow>
                <mo>,</mo>
                <mi>…</mi>
                <mo>⁢</mo>
                <mstyle>
                  <mspace width="0.6em" height="0.6ex"/>
                </mstyle>
                <mo>,</mo>
                <mrow>
                  <mi>N</mi>
                  <mo>-</mo>
                  <mn>1</mn>
                </mrow>
              </mrow>
              <mo>)</mo>
            </mrow>
          </mrow>
        </mrow>
      </mrow>
    </mtd>
    <mtd>
      <mrow>
        <mi>Equation</mi>
        <mo>⁢</mo>
        <mstyle>
          <mspace width="0.8em" height="0.8ex"/>
        </mstyle>
        <mo>⁢</mo>
        <mrow>
          <mo>(</mo>
          <mn>16</mn>
          <mo>)</mo>
        </mrow>
      </mrow>
    </mtd>
  </mtr>
  <mtr>
    <mtd>
      <mrow>
        <msub>
          <mi>phaseer</mi>
          <mn>2</mn>
        </msub>
        <mo>=</mo>
        <mrow>
          <munderover>
            <mo>∑</mo>
            <mrow>
              <mi>i</mi>
              <mo>=</mo>
              <mn>0</mn>
            </mrow>
            <mrow>
              <mi>N</mi>
              <mo>-</mo>
              <mn>1</mn>
            </mrow>
          </munderover>
          <mo>⁢</mo>
          <msup>
            <mrow>
              <mo>(</mo>
              <mrow>
                <mrow>
                  <mi>ep</mi>
                  <mo>⁡</mo>
                  <mrow>
                    <mo>(</mo>
                    <mi>i</mi>
                    <mo>)</mo>
                  </mrow>
                </mrow>
                <mo>-</mo>
                <mrow>
                  <msubsup>
                    <mi>PHCODE</mi>
                    <mn>2</mn>
                    <mrow>
                      <mo>(</mo>
                      <mi>k3</mi>
                      <mo>)</mo>
                    </mrow>
                  </msubsup>
                  <mo>⁡</mo>
                  <mrow>
                    <mo>(</mo>
                    <mi>i</mi>
                    <mo>)</mo>
                  </mrow>
                </mrow>
              </mrow>
              <mo>)</mo>
            </mrow>
            <mn>2</mn>
          </msup>
        </mrow>
      </mrow>
    </mtd>
    <mtd>
      <mrow>
        <mi>Equation</mi>
        <mo>⁢</mo>
        <mstyle>
          <mspace width="0.8em" height="0.8ex"/>
        </mstyle>
        <mo>⁢</mo>
        <mrow>
          <mo>(</mo>
          <mn>17</mn>
          <mo>)</mo>
        </mrow>
      </mrow>
    </mtd>
  </mtr>
</mtable>
</math>
</maths>
</p>
<heading id="h-0009" level="1">Embodiment 3</heading>
<p id="p-0128" num="0127"><figref idref="DRAWINGS">FIG. 9</figref> is a block diagram illustrating configurations of a speech signal transmission apparatus and speech signal reception apparatus respectively having the speech coding apparatus and speech decoding apparatus described in Embodiments 1 and 2.</p>
<p id="p-0129" num="0128">In <figref idref="DRAWINGS">FIG. 9</figref>, speech signal <b>901</b> is converted into an electric signal through input apparatus <b>902</b> and output to A/D conversion apparatus <b>903</b>. A/D conversion apparatus <b>903</b> converts the (analog) signal output from input apparatus <b>902</b> into a digital signal and outputs the result to speech coding apparatus <b>904</b>. Speech coding apparatus <b>904</b> is installed with speech coding apparatus <b>100</b> as shown in <figref idref="DRAWINGS">FIG. 1</figref>, encodes the digital speech signal output from A/D conversion apparatus <b>903</b>, and outputs coded information to RF modulation apparatus <b>905</b>. R/F modulation apparatus <b>905</b> converts the speech coded information output from speech coding apparatus <b>904</b> into a signal of propagation medium such as a radio signal to transmit the information, and outputs the signal to transmission antenna <b>906</b>. Transmission antenna <b>906</b> transmits the output signal output from RF modulation apparatus <b>905</b> as a radio signal (RF signal). In addition, RF signal <b>907</b> in <figref idref="DRAWINGS">FIG. 9</figref> represents a radio signal (RF signal) transmitted from transmission antenna <b>906</b>. The configuration and operation of the speech signal transmission apparatus are as described above.</p>
<p id="p-0130" num="0129">RF signal <b>908</b> is received by reception antenna <b>909</b> and then output to RF demodulation apparatus <b>910</b>. In addition, RF signal <b>908</b> in <figref idref="DRAWINGS">FIG. 9</figref> represents a radio signal received by reception antenna <b>909</b>, which is the same as RF signal <b>907</b> if attenuation of the signal and/or multiplexing of noise does not occur on the propagation path.</p>
<p id="p-0131" num="0130">RF demodulation apparatus <b>910</b> demodulates the speech coded information from the RF signal output from reception antenna <b>909</b> and outputs the result to speech decoding apparatus <b>911</b>. Speech decoding apparatus <b>911</b> is installed with speech decoding apparatus <b>150</b> as shown in <figref idref="DRAWINGS">FIG. 1</figref>, decodes the speech signal from the speech coded information output from RF demodulation apparatus <b>910</b>, and outputs the result to D/A conversion apparatus <b>912</b>. D/A conversion apparatus <b>912</b> converts the digital speech signal output from speech decoding apparatus <b>911</b> into an analog electric signal and outputs the result to output apparatus <b>913</b>.</p>
<p id="p-0132" num="0131">Output apparatus <b>913</b> converts the electric signal into vibration of air and outputs the result as a sound signal to be heard by human ear. In addition, in the figure, reference numeral <b>914</b> denotes an output sound signal. The configuration and operation of the speech signal reception apparatus are as described above.</p>
<p id="p-0133" num="0132">It is possible to obtain a decoded signal with high quality by providing a base station apparatus and communication terminal apparatus in a wireless communication system with the above-mentioned speech signal transmission apparatus and speech signal reception apparatus.</p>
<p id="p-0134" num="0133">As described above, according to the present invention, it is possible to code and decode speech and sound signals with a wide bandwidth using less coded information, and reduce the computation amount. Further, by obtaining a long term prediction lag using the long term prediction information of the base layer, the coded information can be reduced. Furthermore, by decoding the base layer coded information, it is possible to obtain only a decoded signal of the base layer, and in the CELP type speech coding/decoding method, it is possible to implement the function of decoding speech and sound from part of the coded information (scalable coding).</p>
<p id="p-0135" num="0134">This application is based on Japanese Patent Application No. 2003-125665 filed on Apr. 30, 2003, entire content of which is expressly incorporated by reference herein.</p>
<heading id="h-0010" level="1">INDUSTRIAL APPLICABILITY</heading>
<p id="p-0136" num="0135">The present invention is suitable for use in a speech coding apparatus and speech decoding apparatus used in a communication system for coding and transmitting speech and/or sound signals.</p>
<?DETDESC description="Detailed Description" end="tail"?>
</description>
<us-math idrefs="MATH-US-00001" nb-file="US07299174-20071120-M00001.NB">
<img id="EMI-M00001" he="15.92mm" wi="76.20mm" file="US07299174-20071120-M00001.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-math idrefs="MATH-US-00002" nb-file="US07299174-20071120-M00002.NB">
<img id="EMI-M00002" he="16.26mm" wi="76.20mm" file="US07299174-20071120-M00002.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-math idrefs="MATH-US-00003" nb-file="US07299174-20071120-M00003.NB">
<img id="EMI-M00003" he="8.81mm" wi="76.20mm" file="US07299174-20071120-M00003.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-math idrefs="MATH-US-00004" nb-file="US07299174-20071120-M00004.NB">
<img id="EMI-M00004" he="12.70mm" wi="76.20mm" file="US07299174-20071120-M00004.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-math idrefs="MATH-US-00005" nb-file="US07299174-20071120-M00005.NB">
<img id="EMI-M00005" he="30.73mm" wi="76.20mm" file="US07299174-20071120-M00005.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-math idrefs="MATH-US-00006" nb-file="US07299174-20071120-M00006.NB">
<img id="EMI-M00006" he="28.19mm" wi="76.20mm" file="US07299174-20071120-M00006.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-math idrefs="MATH-US-00007" nb-file="US07299174-20071120-M00007.NB">
<img id="EMI-M00007" he="19.05mm" wi="76.20mm" file="US07299174-20071120-M00007.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-math idrefs="MATH-US-00008" nb-file="US07299174-20071120-M00008.NB">
<img id="EMI-M00008" he="8.81mm" wi="76.20mm" file="US07299174-20071120-M00008.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-math idrefs="MATH-US-00009" nb-file="US07299174-20071120-M00009.NB">
<img id="EMI-M00009" he="14.48mm" wi="76.20mm" file="US07299174-20071120-M00009.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-claim-statement>The invention claimed is:</us-claim-statement>
<claims id="claims">
<claim id="CLM-00001" num="00001">
<claim-text>1. A speech coding apparatus comprising:
<claim-text>a base layer coder that codes an input signal and generates first coded information;</claim-text>
<claim-text>a base layer decoder that decodes the first coded information and generates a first decoded signal, while generating long term prediction information comprising information representing long term correlation of speech or sound;</claim-text>
<claim-text>an adder that obtains a residual signal representing a difference between the input signal and the first decoded signal; and</claim-text>
<claim-text>an enhancement layer coder that calculates a long term prediction coefficient using long term prediction information and the residual signal, and codes the long term prediction coefficient and generates second coded information,</claim-text>
<claim-text>wherein the enhancement layer coder comprises:</claim-text>
<claim-text>an obtainer that obtains a long term prediction lag of an enhancement layer based on long term prediction information;</claim-text>
<claim-text>a fetcher that fetches a long term prediction signal back by the long term prediction lag from a previous long term prediction signal sequence stored in a buffer;</claim-text>
<claim-text>a first calculator that calculates the long term prediction coefficient using the residual signal and the long term prediction signal;</claim-text>
<claim-text>a coder that codes the long term prediction coefficient and generates enhancement layer coded information;</claim-text>
<claim-text>a decoder that decodes enhancement layer coded information and generates a decoded long term prediction coefficient; and</claim-text>
<claim-text>a second calculator that calculates a new long term prediction signal using the decoded long term prediction coefficient and the long term prediction signal, and updates the buffer using the new long term prediction signal.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00002" num="00002">
<claim-text>2. The speech coding apparatus according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the enhancement layer coder further comprises:
<claim-text>a second obtainer that obtains a long term prediction residual signal representing a difference between the residual signal and the long term prediction signal;</claim-text>
<claim-text>a second coder that codes the long term prediction residual signal and generates long term prediction residual coded information;</claim-text>
<claim-text>a second decoder that decodes long term prediction residual coded information and calculates a decoded long term prediction residual signal; and</claim-text>
<claim-text>a second adder that adds the new long term prediction signal and the decoded long term prediction residual signal, and updates the buffer using a result of the addition.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00003" num="00003">
<claim-text>3. The speech coding apparatus according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the base layer decoder uses information specifying a fetching position where an adaptive excitation vector is fetched from an excitation vector signal sample, as long term prediction information.</claim-text>
</claim>
<claim id="CLM-00004" num="00004">
<claim-text>4. A speech decoding apparatus that receives first coded information and second coded information from the speech coding apparatus according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, and decodes speech, said speech decoding apparatus comprising:
<claim-text>a base layer decoder that decodes first coded information to generate a first decoded signal, while generating long term prediction information comprising information representing long term correlation of speech or sound;</claim-text>
<claim-text>an enhancement layer decoder that decodes second coded information using the long term prediction information and generates a second decoded signal; and</claim-text>
<claim-text>a second adder that adds the first decoded signal and the second decoded signal, and outputs a speech or sound signal as a result of the addition,</claim-text>
<claim-text>wherein the enhancement layer decoder comprises:</claim-text>
<claim-text>an obtainer that obtains a long term prediction lag of an enhancement layer based on long term prediction information;</claim-text>
<claim-text>a fetcher that fetches a long term prediction signal back by the long term prediction lag from a previous long term prediction signal sequence stored in a buffer;</claim-text>
<claim-text>a decoder that decodes enhancement layer coded information and obtains a decoded long term prediction coefficient; and</claim-text>
<claim-text>a calculator that calculates a long term prediction signal using the decoded long term prediction coefficient and the long term prediction signal, and updates the buffer using the long term prediction signal,</claim-text>
<claim-text>wherein the enhancement layer decoder uses the long term prediction signal as an enhancement layer decoded signal.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00005" num="00005">
<claim-text>5. The speech decoding apparatus according to <claim-ref idref="CLM-00004">claim 4</claim-ref>, wherein the enhancement layer decoder further comprises:
<claim-text>a decoder that decodes long term prediction residual coded information and obtains a decoded long term prediction residual signal; and</claim-text>
<claim-text>a third adder that adds the long term prediction signal and the decoded long term prediction residual signal, wherein the enhancement layer decoder uses a result of addition as an enhancement layer decoded signal.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00006" num="00006">
<claim-text>6. The speech decoding apparatus according to <claim-ref idref="CLM-00004">claim 4</claim-ref>, wherein the base layer decoder uses information specifying a fetching position where an adaptive excitation vector is fetched from an excitation vector signal sample, as long term prediction information.</claim-text>
</claim>
</claims>
</us-patent-grant>
