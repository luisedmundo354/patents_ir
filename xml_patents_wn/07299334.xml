<us-patent-grant lang="EN" dtd-version="v4.2 2006-08-23" file="US07299334-20071120.XML" status="PRODUCTION" id="us-patent-grant" country="US" date-produced="20071106" date-publ="20071120">
<us-bibliographic-data-grant>
<publication-reference>
<document-id>
<country>US</country>
<doc-number>07299334</doc-number>
<kind>B2</kind>
<date>20071120</date>
</document-id>
</publication-reference>
<application-reference appl-type="utility">
<document-id>
<country>US</country>
<doc-number>10886372</doc-number>
<date>20040707</date>
</document-id>
</application-reference>
<us-application-series-code>10</us-application-series-code>
<us-term-of-grant>
<us-term-extension>590</us-term-extension>
</us-term-of-grant>
<classifications-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>G</section>
<class>06</class>
<subclass>F</subclass>
<main-group>12</main-group>
<subgroup>00</subgroup>
<symbol-position>F</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20071120</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
</classifications-ipcr>
<classification-national>
<country>US</country>
<main-classification>711203</main-classification>
<further-classification>711118</further-classification>
</classification-national>
<invention-title id="d0e53">Storage system configurations</invention-title>
<references-cited>
<citation>
<patcit num="00001">
<document-id>
<country>US</country>
<doc-number>5694576</doc-number>
<kind>A</kind>
<name>Yamamoto et al.</name>
<date>19971200</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<patcit num="00002">
<document-id>
<country>US</country>
<doc-number>6434666</doc-number>
<kind>B1</kind>
<name>Takahashi et al.</name>
<date>20020800</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<patcit num="00003">
<document-id>
<country>US</country>
<doc-number>6457102</doc-number>
<kind>B1</kind>
<name>Lambright et al.</name>
<date>20020900</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<patcit num="00004">
<document-id>
<country>US</country>
<doc-number>6477618</doc-number>
<kind>B2</kind>
<name>Chilton</name>
<date>20021100</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<patcit num="00005">
<document-id>
<country>US</country>
<doc-number>6490615</doc-number>
<kind>B1</kind>
<name>Dias et al.</name>
<date>20021200</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<patcit num="00006">
<document-id>
<country>US</country>
<doc-number>6601137</doc-number>
<kind>B1</kind>
<name>Castro et al.</name>
<date>20030700</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>711113</main-classification></classification-national>
</citation>
<citation>
<patcit num="00007">
<document-id>
<country>US</country>
<doc-number>2001/0020260</doc-number>
<kind>A1</kind>
<name>Kanamaru et al.</name>
<date>20010900</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>711  4</main-classification></classification-national>
</citation>
<citation>
<patcit num="00008">
<document-id>
<country>US</country>
<doc-number>2003/0115408</doc-number>
<kind>A1</kind>
<name>Milillo et al.</name>
<date>20030600</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>711113</main-classification></classification-national>
</citation>
<citation>
<patcit num="00009">
<document-id>
<country>US</country>
<doc-number>2005/0015544</doc-number>
<kind>A1</kind>
<name>Zohar et al.</name>
<date>20050100</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>711113</main-classification></classification-national>
</citation>
</references-cited>
<number-of-claims>38</number-of-claims>
<us-exemplary-claim>1</us-exemplary-claim>
<us-field-of-classification-search>
<classification-national>
<country>US</country>
<main-classification>711201</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>711203</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>711204</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>711205</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>711209</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>711129</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>711130</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>711141</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>711147</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>711221</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>711154</main-classification>
</classification-national>
</us-field-of-classification-search>
<figures>
<number-of-drawing-sheets>13</number-of-drawing-sheets>
<number-of-figures>14</number-of-figures>
</figures>
<us-related-documents>
<continuation-in-part>
<relation>
<parent-doc>
<document-id>
<country>US</country>
<doc-number>10620080</doc-number>
<kind>00</kind>
<date>20030715</date>
</document-id>
<parent-status>PENDING</parent-status>
</parent-doc>
<child-doc>
<document-id>
<country>US</country>
<doc-number>10886372</doc-number>
</document-id>
</child-doc>
</relation>
</continuation-in-part>
<continuation-in-part>
<relation>
<parent-doc>
<document-id>
<country>US</country>
<doc-number>10620249</doc-number>
<kind>00</kind>
<date>20030715</date>
</document-id>
<parent-status>PENDING</parent-status>
</parent-doc>
<child-doc>
<document-id>
<country>US</country>
<doc-number>10620080</doc-number>
</document-id>
</child-doc>
</relation>
</continuation-in-part>
<related-publication>
<document-id>
<country>US</country>
<doc-number>20050015544</doc-number>
<kind>A1</kind>
<date>20050120</date>
</document-id>
</related-publication>
</us-related-documents>
<parties>
<applicants>
<applicant sequence="001" app-type="applicant-inventor" designation="us-only">
<addressbook>
<last-name>Zohar</last-name>
<first-name>Ofir</first-name>
<address>
<city>Alfe-Menashe</city>
<country>IL</country>
</address>
</addressbook>
<nationality>
<country>IL</country>
</nationality>
<residence>
<country>IL</country>
</residence>
</applicant>
<applicant sequence="002" app-type="applicant-inventor" designation="us-only">
<addressbook>
<last-name>Revah</last-name>
<first-name>Yaron</first-name>
<address>
<city>Tel-Aviv</city>
<country>IL</country>
</address>
</addressbook>
<nationality>
<country>IL</country>
</nationality>
<residence>
<country>IL</country>
</residence>
</applicant>
<applicant sequence="003" app-type="applicant-inventor" designation="us-only">
<addressbook>
<last-name>Helman</last-name>
<first-name>Haim</first-name>
<address>
<city>Ramat Gan</city>
<country>IL</country>
</address>
</addressbook>
<nationality>
<country>IL</country>
</nationality>
<residence>
<country>IL</country>
</residence>
</applicant>
<applicant sequence="004" app-type="applicant-inventor" designation="us-only">
<addressbook>
<last-name>Cohen</last-name>
<first-name>Dror</first-name>
<address>
<city>Petach-Tikva</city>
<country>IL</country>
</address>
</addressbook>
<nationality>
<country>IL</country>
</nationality>
<residence>
<country>IL</country>
</residence>
</applicant>
<applicant sequence="005" app-type="applicant-inventor" designation="us-only">
<addressbook>
<last-name>Schwartz</last-name>
<first-name>Shemer</first-name>
<address>
<city>Herzelia</city>
<country>IL</country>
</address>
</addressbook>
<nationality>
<country>IL</country>
</nationality>
<residence>
<country>IL</country>
</residence>
</applicant>
</applicants>
<agents>
<agent sequence="01" rep-type="attorney">
<addressbook>
<orgname>Katten Muchin Rosenman LLP</orgname>
<address>
<country>unknown</country>
</address>
</addressbook>
</agent>
</agents>
</parties>
<assignees>
<assignee>
<addressbook>
<orgname>XIV Ltd.</orgname>
<role>03</role>
<address>
<city>Tel-Aviv</city>
<country>IL</country>
</address>
</addressbook>
</assignee>
</assignees>
<examiners>
<primary-examiner>
<last-name>Peugh</last-name>
<first-name>Brian R.</first-name>
<department>2187</department>
</primary-examiner>
</examiners>
</us-bibliographic-data-grant>
<abstract id="abstract">
<p id="p-0001" num="0000">A storage system, including: one or more mass storage devices, coupled to store data at respective first ranges of logical addresses (LAs), and one or more interfaces, which are adapted to receive input/output (IO) requests from host processors directed to specified LAs. The system also includes a plurality of caches coupled to the one or more interfaces so as to receive the IO requests therefrom, each cache being assigned a respective second range of the LAs and being coupled to the one or more mass storage devices, the respective first ranges of which overlap the respective second range, so as to receive data from and provide data to the one or more mass storage devices, and being coupled to accept the IO requests within the respective second range directed thereto.</p>
</abstract>
<drawings id="DRAWINGS">
<figure id="Fig-EMI-D00000" num="00000">
<img id="EMI-D00000" he="161.88mm" wi="241.22mm" file="US07299334-20071120-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00001" num="00001">
<img id="EMI-D00001" he="231.06mm" wi="158.24mm" orientation="landscape" file="US07299334-20071120-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00002" num="00002">
<img id="EMI-D00002" he="243.76mm" wi="164.85mm" orientation="landscape" file="US07299334-20071120-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00003" num="00003">
<img id="EMI-D00003" he="242.40mm" wi="165.69mm" orientation="landscape" file="US07299334-20071120-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00004" num="00004">
<img id="EMI-D00004" he="240.11mm" wi="166.03mm" orientation="landscape" file="US07299334-20071120-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00005" num="00005">
<img id="EMI-D00005" he="170.01mm" wi="135.97mm" file="US07299334-20071120-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00006" num="00006">
<img id="EMI-D00006" he="221.40mm" wi="151.38mm" orientation="landscape" file="US07299334-20071120-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00007" num="00007">
<img id="EMI-D00007" he="221.40mm" wi="146.56mm" orientation="landscape" file="US07299334-20071120-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00008" num="00008">
<img id="EMI-D00008" he="216.83mm" wi="145.54mm" orientation="landscape" file="US07299334-20071120-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00009" num="00009">
<img id="EMI-D00009" he="215.73mm" wi="124.29mm" orientation="landscape" file="US07299334-20071120-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00010" num="00010">
<img id="EMI-D00010" he="242.49mm" wi="137.75mm" orientation="landscape" file="US07299334-20071120-D00010.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00011" num="00011">
<img id="EMI-D00011" he="225.38mm" wi="142.83mm" orientation="landscape" file="US07299334-20071120-D00011.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00012" num="00012">
<img id="EMI-D00012" he="228.01mm" wi="156.55mm" orientation="landscape" file="US07299334-20071120-D00012.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00013" num="00013">
<img id="EMI-D00013" he="218.78mm" wi="144.10mm" orientation="landscape" file="US07299334-20071120-D00013.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
</drawings>
<description id="description">
<?RELAPP description="Other Patent Relations" end="lead"?>
<heading id="h-0001" level="1">RELATED APPLICATIONS</heading>
<p id="p-0002" num="0001">This application is a continuation-in-part of application Ser. No. 10/620,080, titled “Data Allocation in a Distributed Storage System,” and of application Ser. No. 10/620,249, titled “Distributed Independent Cache Memory,” both filed 15 Jul. 2003, which are incorporated herein by reference.</p>
<?RELAPP description="Other Patent Relations" end="tail"?>
<?BRFSUM description="Brief Summary" end="lead"?>
<heading id="h-0002" level="1">FIELD OF THE INVENTION</heading>
<p id="p-0003" num="0002">The present invention relates generally to memory access, and specifically to distributed cache design in data storage systems.</p>
<heading id="h-0003" level="1">BACKGROUND OF THE INVENTION</heading>
<p id="p-0004" num="0003">The slow access time, of the order of 5-10 ms, for an input/output (IO) transaction performed on a disk has led to the need for a caching system between a host generating the IO transaction and the disk. A cache, a fast access time medium, stores a portion of the data contained in the disk. The IO transaction is first routed to the cache, and if the data required by the transaction exists in the cache, it may be used without accessing the disk.</p>
<p id="p-0005" num="0004">One goal of an efficient caching system is to achieve a high “hit” ratio, where a high proportion of the data requested by IO transactions already exists in the cache, so that access to the disk is minimized. Other desirable properties of an efficient caching system include scalability, the ability to maintain redundant caches and/or disks, and relatively few overhead management transactions.</p>
<p id="p-0006" num="0005">U.S. Pat. No. 5,694,576 to Yamamoto, et al., whose disclosure is incorporated herein by reference, describes a method for controlling writing from a cache to a disk by adding record identification information to a write request. The added information enables the cache to decide whether data written to the cache should or should not be written to the disk.</p>
<p id="p-0007" num="0006">U.S. Pat. No. 6,457,102 to Lambright, et al., whose disclosure is incorporated herein by reference, describes a system for storing data in a cache memory that is divided into a number of separate portions. Exclusive access to each of the portions is provided by software or hardware locks. The system may be used for choosing which data is to be erased from the cache in order to make room for new data.</p>
<p id="p-0008" num="0007">U.S. Pat. No. 6,434,666 to Takahashi, et al., whose disclosure is incorporated herein by reference, describes a caching system having a plurality of cache memories, and a memory control apparatus that selects the cache memory to be used. The memory control apparatus selects the cache so as to equalize use of the cache memories.</p>
<p id="p-0009" num="0008">U.S. Pat. No. 6,490,615 to Dias, et al., whose disclosure is incorporated herein by reference, describes a scalable cache having caches for storage servers. On receipt of a read request, the caches serve the request or communicate with each other to cooperatively serve the request.</p>
<p id="p-0010" num="0009">U.S. Pat. No. 6,477,618 to Chilton, whose disclosure is incorporated herein by reference, describes an architecture of a data storage cluster. The cluster includes integrated cached disk arrays which are coupled by a cluster interconnect. A request to one of the arrays is routed, as necessary, to another of the arrays via the cluster interconnect.</p>
<heading id="h-0004" level="1">SUMMARY OF THE INVENTION</heading>
<p id="p-0011" num="0010">In embodiments of the present invention, a data storage system comprises one or more interfaces which communicate with caches and mass storage devices. The system may be formed in a number of configurations, all of which comprise the mass storage devices storing data at respective first ranges of logical addresses (LAs). In all the configurations each cache is assigned a respective second range of the LAs. The one or more interfaces receive input/output (IO) requests from a host directed to specified LAs and direct all the IO requests to the cache to which the specified LAs are assigned. In some of the configurations one or more communication channels, typically switches, connect elements of the storage system. The communication channels convey IO requests and data between their connected elements.</p>
<p id="p-0012" num="0011">In a first embodiment, each cache is directly connected to one or more of the mass storage devices, the one or more mass storage devices having LAs within the second range of the cache. A communication channel connects the one or more interfaces and the caches.</p>
<p id="p-0013" num="0012">In a second embodiment, two or more of the caches are directly connected to one of the mass storage devices, the mass storage device having LAs within the respective second ranges of the two or more caches. A communication channel connects the one or more interfaces and the caches.</p>
<p id="p-0014" num="0013">In a third embodiment, the caches are connected to each other so that they are able to transfer data and IO requests between themselves. There are an equal number of interfaces and caches, each interface being directly connected to a respective cache. A communication channel connects the caches and the mass storage devices.</p>
<p id="p-0015" num="0014">In a fourth embodiment, there are an equal number of interfaces, caches and mass storage devices. Each interface connects to a respective cache, which in turn connects to a respective mass storage device. Each mass storage device has LAs within the second range of its connected cache. The caches are connected to each other so that they are able to transfer data and IO requests between themselves.</p>
<p id="p-0016" num="0015">In a fifth embodiment, there are an equal number of interfaces and caches, each interface being directly connected to a respective cache. The caches are connected to each other so that they are able to transfer data and IO requests between themselves. Two or more of the caches are directly connected to one of the mass storage devices, the mass storage device having LAs within the respective second ranges of the two or more caches.</p>
<p id="p-0017" num="0016">In a sixth embodiment, there are an equal number of interfaces and caches, each interface being directly connected to a respective cache. The caches are connected to each other so that they are able to transfer data and IO requests between themselves. Each cache is directly connected to one or more of the mass storage devices, the one or more mass storage devices having LAs within the second range of the cache.</p>
<p id="p-0018" num="0017">In a seventh embodiment, a first communication channel connects the caches and the mass storage devices. A plurality of interfaces are connected to the caches by a second communication channel.</p>
<p id="p-0019" num="0018">In an eighth embodiment, there are an equal number of caches and mass storage devices. Each cache connects to a respective mass storage device. Each mass storage device has LAs within the second range its connected cache. A plurality of interfaces are connected to the caches by a communication channel.</p>
<p id="p-0020" num="0019">In a ninth embodiment, the storage system operates as a network attached storage (NAS) system. The mass storage devices store data in a file-based format. The caches receive file-based data from and provide file-based data to the mass storage devices. The one or more interfaces receive file-based IO requests from host processors.</p>
<p id="p-0021" num="0020">In a tenth embodiment, the storage system operates as a storage area network (SAN) system. The mass storage devices store data in a block-based format. The caches receive block-based data from and provide block-based data to the mass storage devices. The one or more interfaces receive block-based IO requests from host processors.</p>
<p id="p-0022" num="0021">In some embodiments, a mapping of addresses of the second ranges is stored in the one or more interfaces, for use by the interfaces to direct the IO requests to the appropriate cache. In some of the embodiments, each cache comprises a listing of the second range of the cache, the listing being used by the cache to determine which IO requests are acted on by the cache.</p>
<p id="p-0023" num="0022">It will be appreciated that aspects of the disclosed embodiments described herein, such as operating as a SAN system and/or as a NAS system, may be combined to create other embodiments. All such embodiments are assumed to be within the scope of the present invention.</p>
<p id="p-0024" num="0023">In some embodiments, at least some of the interfaces, the caches, and the mass storage devices, are implemented separately, or in combination, from commercially available, off-the-shelf, components. Typically, such commercially available components include, but are not limited to, personal computers.</p>
<p id="p-0025" num="0024">There is therefore provided, according to an embodiment of the present invention, a storage system, including:</p>
<p id="p-0026" num="0025">one or more mass storage devices, coupled to store data at respective first ranges of logical addresses (LAs);</p>
<p id="p-0027" num="0026">one or more interfaces, which are adapted to receive input/output (IO) requests from host processors directed to specified LAs; and</p>
<p id="p-0028" num="0027">a plurality of caches coupled to the one or more interfaces so as to receive the IO requests therefrom, each cache being assigned a respective second range of the LAs and being coupled to the one or more mass storage devices, the respective first ranges of which overlap the respective second range, so as to receive data from and provide data to the one or more mass storage devices, and being coupled to accept the IO requests within the respective second range directed thereto.</p>
<p id="p-0029" num="0028">Typically, the one or more mass storage devices include a plurality of mass storage devices, and each cache is directly connected to one or more of the plurality of mass storage devices.</p>
<p id="p-0030" num="0029">In an embodiment, the one or more interfaces are adapted to direct the IO requests to all of the plurality of caches.</p>
<p id="p-0031" num="0030">The one or more interfaces may include a mapping between the second ranges of each of the caches and the LAs and may be adapted to convert the IO requests to one or more requests and to direct the one or more requests to respective one or more caches in response to the mapping.</p>
<p id="p-0032" num="0031">In an alternative embodiment each cache includes a listing of LAs corresponding to the second range of the each cache, and the cache is adapted to ignore IO requests directed to LAs not included in the listing.</p>
<p id="p-0033" num="0032">In an embodiment the plurality of caches includes a first cache and a second cache, and the first cache is coupled to write an IO request directed to the first cache to the second cache. In some embodiments the plurality of caches includes one or more third caches which are adapted to operate substantially independently of the first and second caches.</p>
<p id="p-0034" num="0033">Typically, each of the plurality of caches is adapted to operate substantially independently of remaining caches included in the plurality.</p>
<p id="p-0035" num="0034">In an embodiment, each of the plurality of caches are at an equal hierarchical level.</p>
<p id="p-0036" num="0035">In an alternative embodiment all of the LAs of the second ranges include all of the LAs of the one or more mass storage devices.</p>
<p id="p-0037" num="0036">In a further alternative embodiment one or more of the one or more mass storage devices, the one or more interfaces, and the plurality of caches, are implemented from an industrially available personal computer.</p>
<p id="p-0038" num="0037">In some embodiments one or more of the one or more mass storage devices, the one or more interfaces, and the plurality of caches, are housed in a single housing.</p>
<p id="p-0039" num="0038">There is further provided, according to an embodiment of the present invention, a storage system, including:</p>
<p id="p-0040" num="0039">one or more mass storage devices, coupled to store data at respective first ranges of logical addresses (LAs);</p>
<p id="p-0041" num="0040">a plurality of caches, each cache being assigned a respective second range of the LAs and being directly connected to one or more of the mass storage devices, the respective first ranges of which overlap the respective second range, so as to receive data from and provide data to the one or more mass storage devices;</p>
<p id="p-0042" num="0041">one or more interfaces, which are adapted to receive input/output (IO) requests from host processors directed to specified LAs and to direct all the IO requests to the cache to which the specified LAs are assigned; and</p>
<p id="p-0043" num="0042">a communication channel to which the one or more interfaces and the second plurality of caches are connected, and which is adapted to convey the data and the IO requests therebetween.</p>
<p id="p-0044" num="0043">In an embodiment the one or more interfaces include a mapping between the second ranges of each of the caches and the LAs and are adapted to convert the IO requests to one or more requests and to direct the one or more requests to respective one or more caches in response to the mapping.</p>
<p id="p-0045" num="0044">In an alternative embodiment one of the caches is coupled to two or more mass storage devices and includes a location table providing locations of the second range of the LAs assigned to the one cache in the two or more mass storage devices.</p>
<p id="p-0046" num="0045">In a further alternative embodiment the plurality of caches includes two or more caches, and the two or more caches are directly connected to one of the mass storage devices, the first range of which overlaps each of the respective second ranges of the two or more caches, so as to receive data from and provide data to the one mass storage device.</p>
<p id="p-0047" num="0046">There is further provided, according to an embodiment of the present invention, a storage system, including:</p>
<p id="p-0048" num="0047">one or more mass storage devices, coupled to store data at respective first ranges of logical addresses (LAs);</p>
<p id="p-0049" num="0048">a plurality of caches, each cache being assigned a respective second range of the LAs and being coupled to the one or more mass storage devices, the respective first ranges of which overlap the respective second range, so as to receive data from and provide data to the one or more mass storage devices; and</p>
<p id="p-0050" num="0049">a plurality of interfaces, each interface being directly connected to a respective cache and being adapted to receive input/output (IO) requests from host processors directed to specified LAs and to direct all the IO requests to the cache to which the specified LAs are assigned.</p>
<p id="p-0051" num="0050">The storage system typically includes a communication channel to which the one or more mass storage devices and the plurality of caches are connected, and which is adapted to convey data and the IO requests therebetween.</p>
<p id="p-0052" num="0051">In an embodiment each interface includes a mapping between the second ranges of each of the caches and the LAs and is adapted to convert the IO requests to one or more requests and to direct the one or more requests to respective one or more of the caches in response to the mapping.</p>
<p id="p-0053" num="0052">In an embodiment one of the caches and one of the interfaces are housed in a single housing.</p>
<p id="p-0054" num="0053">In an alternative embodiment the one or more mass storage devices includes a plurality of mass storage devices, and each of the plurality of mass storage devices is directly connected to a respective cache. In a further alternative embodiment the storage system includes a plurality of single housings which respectively house a respective interface, a respective cache, and a respective mass storage device.</p>
<p id="p-0055" num="0054">The one or more mass storage devices may include a multiplicity of mass storage devices, and two or more caches are directly coupled to one of the mass storage devices. In an embodiment, one of the caches and one of the interfaces are housed in a single housing.</p>
<p id="p-0056" num="0055">In an embodiment the one or more storage devices include a multiplicity of mass storage devices, and each of the caches is directly connected to one or more of the multiplicity of mass storage devices. In an alternative embodiment, one of the caches is coupled to two or more mass storage devices and includes a location table providing locations in the two or more mass storage devices of the second range of the LAs assigned to the one cache.</p>
<p id="p-0057" num="0056">There is further provided, according to an embodiment of the present invention, a storage system, including:</p>
<p id="p-0058" num="0057">one or more mass storage devices, coupled to store data at respective first ranges of logical addresses (LAs);</p>
<p id="p-0059" num="0058">a plurality of caches, each cache being assigned a respective second range of the LAs so that the LAs of all the respective second ranges comprise the LAs of all the respective first ranges;</p>
<p id="p-0060" num="0059">a first communication channel to which the one or more mass storage devices and the plurality of caches are connected, and which is adapted to convey data and input/output (IO) requests therebetween;</p>
<p id="p-0061" num="0060">one or more interfaces, which are adapted to receive the IO requests from host processors directed to specified LAs and to direct all the IO requests to the cache to which the specified LAs are assigned; and</p>
<p id="p-0062" num="0061">a second communication channel to which the one or more interfaces and the plurality of caches are connected, and which is adapted to convey the data and the IO requests therebetween.</p>
<p id="p-0063" num="0062">The one or more interfaces may include a mapping between the second ranges of the caches and the LAs, and the one or more interfaces may be adapted to convert the IO requests to one or more requests and to direct the one or more requests to respective one or more of the caches in response to the mapping.</p>
<p id="p-0064" num="0063">In an embodiment the plurality of caches include respective location tables, wherein each location table includes locations of the second range of the LAs assigned to the respective cache in the one or more mass storage devices.</p>
<p id="p-0065" num="0064">There is further provided, according to an embodiment of the present invention, a storage system, including:</p>
<p id="p-0066" num="0065">a plurality of mass storage devices, coupled to store data at respective first ranges of logical addresses (LAs);</p>
<p id="p-0067" num="0066">a plurality of caches, configured to operate independently of one another, each cache being directly connected to a respective mass storage device so as to receive data from and provide data to the respective mass storage device, and being assigned the respective range of LAs of the respective mass storage device;</p>
<p id="p-0068" num="0067">one or more interfaces, which are adapted to receive input/output (IO) requests from host processors directed to specified LAs and to direct all the IO requests to the cache to which the specified LAs are assigned; and</p>
<p id="p-0069" num="0068">a communication channel to which the one or more interfaces and the plurality of caches are connected, and which is adapted to convey data and the IO requests therebetween.</p>
<p id="p-0070" num="0069">Typically, the one or more interfaces include a mapping between the plurality of caches and the LAs, and the one or more interfaces are adapted to convert the IO requests to one or more requests and to direct the one or more requests to respective one or more of the caches in response to the mapping.</p>
<p id="p-0071" num="0070">There is further provided, according to an embodiment of the present invention, a network attached storage (NAS) system, including:</p>
<p id="p-0072" num="0071">one or more mass storage devices, coupled to store file-based data at respective first ranges of logical addresses (LAs);</p>
<p id="p-0073" num="0072">a plurality of caches, each cache being assigned a respective second range of the LAs so that the LAs of all the respective second ranges comprise the LAs of all the respective first ranges, the caches being coupled to receive file-based data from and provide file-based data to the one or more mass storage devices having LAs within the respective second range; and</p>
<p id="p-0074" num="0073">one or more interfaces, which are adapted to receive file-based input/output (IO) requests from host processors directed to specified LAs and to direct all the file-based IO requests to the caches to which the specified LAs are assigned.</p>
<p id="p-0075" num="0074">Typically, the one or more interfaces include a file-based mapping between the plurality of caches and the LAs, and the one or more interfaces are adapted to convert the file-based IO requests to one or more file-based requests and to direct the one or more file-based requests to respective one or more of the caches in response to the file-based mapping.</p>
<p id="p-0076" num="0075">There is further provided, according to an embodiment of the present invention, a storage area network (SAN) system, including:</p>
<p id="p-0077" num="0076">one or more mass storage devices, coupled to store block-based data at respective first ranges of logical addresses (LAs);</p>
<p id="p-0078" num="0077">a plurality of caches, each cache being assigned a respective second range of the LAs so that the LAs of all the respective second ranges comprise the LAs of all the respective first ranges, the caches being coupled to receive block-based data from and provide block-based data to the one or more mass storage devices having LAs within the respective second range; and</p>
<p id="p-0079" num="0078">one or more interfaces, which are adapted to receive block-based input/output (IO) requests from host processors directed to specified LAs and to direct all the block-based IO requests to the caches to which the specified LAs are assigned.</p>
<p id="p-0080" num="0079">The one or more interfaces typically include a block-based mapping between the plurality of caches and the LAs, and the one or more interfaces are typically adapted to convert the block-based IO requests to one or more block-based requests and to direct the one or more block-based requests to respective one or more of the caches in response to the block-based mapping.</p>
<p id="p-0081" num="0080">There is further provided, according to an embodiment of the present invention, a method for storing data, including:</p>
<p id="p-0082" num="0081">coupling one or more mass storage devices to store data at respective first ranges of logical addresses (LAs);</p>
<p id="p-0083" num="0082">receiving in one or more interfaces input/output (IO) requests from host processors directed to specified LAs; and</p>
<p id="p-0084" num="0083">coupling a plurality of caches to the one or more interfaces so as to receive the IO requests therefrom, each cache being assigned a respective second range of the LAs and being coupled to the one or more mass storage devices, the respective first ranges of which overlap the respective second range, so as to receive data from and provide data to the one or more mass storage devices, and being coupled to accept the IO requests within the respective second range directed thereto.</p>
<p id="p-0085" num="0084">There is further provided, according to an embodiment of the present invention, a method for storing data in a network attached storage (NAS) system, including:</p>
<p id="p-0086" num="0085">coupling one or more mass storage devices to store file-based data at respective first ranges of logical addresses (LAs);</p>
<p id="p-0087" num="0086">assigning each of a plurality of caches a respective second range of the LAs so that the LAs of all the respective second ranges comprise the LAs of all the respective first ranges;</p>
<p id="p-0088" num="0087">coupling the caches to receive the file-based data from and provide the file-based data to the one or more mass storage devices having LAs within the respective second range;</p>
<p id="p-0089" num="0088">receiving file-based input/output (IO) requests from host processors directed to specified LAs; and</p>
<p id="p-0090" num="0089">directing the file-based IO requests to the caches to which the specified LAs are assigned.</p>
<p id="p-0091" num="0090">There is further provided, according to an embodiment of the present invention, a method for storing data in a storage area network (SAN), including:</p>
<p id="p-0092" num="0091">coupling one or more mass storage devices to store block-based data at respective first ranges of logical addresses (LAs);</p>
<p id="p-0093" num="0092">assigning each of a plurality of caches a respective second range of the LAs so that the LAs of all the respective second ranges comprise the LAs of all the respective first ranges;</p>
<p id="p-0094" num="0093">coupling the caches to receive the block-based data from and provide the block-based data to the one or more mass storage devices having LAs within the respective second range;</p>
<p id="p-0095" num="0094">receiving block-based input/output (IO) requests from host processors directed to specified LAs; and</p>
<p id="p-0096" num="0095">directing the block-based IO requests to the caches to which the specified LAs are assigned.</p>
<p id="p-0097" num="0096">The present invention will be more fully understood from the following detailed description of the preferred embodiments thereof, taken together with the drawings, a brief description of which is given below.</p>
<?BRFSUM description="Brief Summary" end="tail"?>
<?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?>
<description-of-drawings>
<heading id="h-0005" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading>
<p id="p-0098" num="0097"><figref idref="DRAWINGS">FIG. 1</figref> is a schematic block diagram of a data storage system, according to an embodiment of the present invention;</p>
<p id="p-0099" num="0098"><figref idref="DRAWINGS">FIG. 2</figref> is a schematic diagram illustrating a mapping of data between different elements of the system of <figref idref="DRAWINGS">FIG. 1</figref> for an “all-caches-to-all-disks” configuration, according to an embodiment of the present invention;</p>
<p id="p-0100" num="0099"><figref idref="DRAWINGS">FIG. 3</figref> is a schematic diagram illustrating a mapping of data between different elements of the system of <figref idref="DRAWINGS">FIG. 1</figref> for a “one-cache-to-one-disk” configuration, according to an embodiment of the present invention;</p>
<p id="p-0101" num="0100"><figref idref="DRAWINGS">FIG. 4</figref> is a schematic diagram illustrating a mapping of data between different elements of the system of <figref idref="DRAWINGS">FIG. 1</figref> for an alternative “all-caches-to-all-disks” configuration, according to an embodiment of the present invention;</p>
<p id="p-0102" num="0101"><figref idref="DRAWINGS">FIG. 5</figref> is a flow chart showing steps followed by the system of <figref idref="DRAWINGS">FIG. 1</figref> on receipt of an input/output request from a host communicating with the system, according to an embodiment of the present invention;</p>
<p id="p-0103" num="0102"><figref idref="DRAWINGS">FIG. 6</figref> is a flow chart showing steps followed by the system of <figref idref="DRAWINGS">FIG. 1</figref> on addition or removal of a cache or disk to/from the system, according to an embodiment of the present invention;</p>
<p id="p-0104" num="0103"><figref idref="DRAWINGS">FIG. 7</figref> is a schematic block diagram of a configuration of the system of <figref idref="DRAWINGS">FIG. 1</figref>, according to an embodiment of the present invention;</p>
<p id="p-0105" num="0104"><figref idref="DRAWINGS">FIG. 8</figref> is a schematic block diagram of an alternative configuration of the system of <figref idref="DRAWINGS">FIG. 1</figref>, according to an embodiment of the present invention;</p>
<p id="p-0106" num="0105"><figref idref="DRAWINGS">FIG. 9</figref> is a schematic block diagram of a further alternative configuration of the system of <figref idref="DRAWINGS">FIG. 1</figref>, according to an embodiment of the present invention;</p>
<p id="p-0107" num="0106"><figref idref="DRAWINGS">FIG. 10</figref> is a schematic block diagram of a yet further alternative configuration of the system of <figref idref="DRAWINGS">FIG. 1</figref>, according to an embodiment of the present invention;</p>
<p id="p-0108" num="0107"><figref idref="DRAWINGS">FIG. 11</figref> is a schematic block diagram of another configuration of the system of <figref idref="DRAWINGS">FIG. 1</figref>, according to an embodiment of the present invention;</p>
<p id="p-0109" num="0108"><figref idref="DRAWINGS">FIG. 12</figref> is a schematic block diagram of another alternative configuration of the system of <figref idref="DRAWINGS">FIG. 1</figref>, according to an embodiment of the present invention;</p>
<p id="p-0110" num="0109"><figref idref="DRAWINGS">FIG. 13</figref> is a schematic block diagram of another configuration of the system of <figref idref="DRAWINGS">FIG. 1</figref>, according to an embodiment of the present invention; and</p>
<p id="p-0111" num="0110"><figref idref="DRAWINGS">FIG. 14</figref> is a schematic block diagram of another alternative configuration of the system of <figref idref="DRAWINGS">FIG. 1</figref>, according to an embodiment of the present invention.</p>
</description-of-drawings>
<?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?>
<?DETDESC description="Detailed Description" end="lead"?>
<heading id="h-0006" level="1">DETAILED DESCRIPTION OF PREFERRED EMBODIMENTS</heading>
<p id="p-0112" num="0111">Reference is now made to <figref idref="DRAWINGS">FIG. 1</figref>, which is a schematic block diagram of a storage system <b>10</b>, according to an embodiment of the present invention. System <b>10</b> acts as a data memory for one or more host processors <b>52</b>, which are coupled to the storage system by any means known in the art, for example, via a network such as the Internet or by a bus. Herein, by way of example, hosts <b>52</b> and system <b>10</b> are assumed to be coupled by a network <b>50</b>. The data stored within system <b>10</b> is stored at logical addresses (LAs) in one or more slow access time mass storage devices, hereinbelow assumed to be one or more disks <b>12</b>, by way of example, unless otherwise stated. LAs for system <b>10</b> are typically grouped into logical units (LUs) and both LAs and LUs are allocated by a system manager <b>54</b>, which also acts as central control unit for the system.</p>
<p id="p-0113" num="0112">System <b>10</b> is typically installed as part of a network attached storage (NAS) system, or as part of a storage area network (SAN) system, data and/or file transfer between system <b>10</b> and hosts <b>52</b> being implemented according to the protocol required by the type of system. For example, if system <b>10</b> is operative in a NAS system, data transfer is typically file based, using an Ethernet protocol; if system <b>10</b> is operative in a SAN system, data transfer is typically block based, using small computer system interface (SCSI) and fibre channel protocols. It will be appreciated, however, that embodiments of the present invention are not limited to any specific type of data transfer method or protocol. Moreover, it will be appreciated that elements of system <b>10</b> may be implemented from commercially available components. Such components include, but are not limited to, personal computers. Typically, an off-the-shelf personal computer may be used as one or more of the elements of system <b>10</b>.</p>
<p id="p-0114" num="0113">System <b>10</b> comprises one or more substantially similar interfaces <b>26</b> which receive input/output (IO) access requests for data in disks <b>12</b> from hosts <b>52</b>. Each interface <b>26</b> may be implemented in hardware and/or software, and may be located in storage system <b>10</b> or alternatively in any other suitable location, such as an element of network <b>50</b> or one of host processors <b>52</b>. Between disks <b>12</b> and the interfaces are a second plurality of interim caches <b>20</b>, each cache comprising memory having fast access time, and each cache being at an equal level hierarchically. Each cache <b>20</b> typically comprises random access memory (RAM), such as dynamic RAM, and may also comprise software. Specific caches are also referred to herein as Cache <b>0</b>, Cache <b>1</b>, . . . , Cache n, where n is a whole number. Caches <b>20</b> are coupled to interfaces <b>26</b> by any suitable fast communication channel <b>14</b> known in the art, such as a bus or a switch, so that each interface is able to communicate with, and transfer data to and from, any cache. Herein communication channel <b>14</b> between caches <b>20</b> and interfaces <b>26</b> is assumed, by way of example, to be by a first cross-point switch. Interfaces <b>26</b> operate substantially independently of each other. Caches <b>20</b> and interfaces <b>26</b> operate as a data transfer system <b>27</b>, transferring data between hosts <b>52</b> and disks <b>12</b>. Except where otherwise stated below, caches <b>20</b> operate substantially independently of each other.</p>
<p id="p-0115" num="0114">Caches <b>20</b> are typically coupled to disks <b>12</b> by a fast communication channel <b>24</b>, typically a second cross-point switch. The coupling between the caches and the disks may be by a “second plurality of caches to first plurality of disks” coupling, herein termed an “all-to-all” coupling. Alternatively, one or more subsets of the caches may be coupled to one or more subsets of the disks. Further alternatively, the coupling may be by a “one-cache-to-one-disk” coupling, herein termed a “one-to-one” coupling, so that one cache communicates with one disk. The coupling may also be configured as a combination of any of these types of coupling. Disks <b>12</b> operate substantially independently of each other.</p>
<p id="p-0116" num="0115">At setup of system <b>10</b> system manager <b>54</b> assigns a range of LAs to each cache <b>20</b>. Manager <b>54</b> may subsequently reassign the ranges during operation of system, and an example of steps to be taken in the event of a change is described below with reference to <figref idref="DRAWINGS">FIG. 5</figref>. The ranges are chosen so that the complete memory address space of disks <b>12</b> is covered, and so that each LA is mapped to at least one cache; typically more than one is used for redundancy purposes. The LAs are typically grouped by an internal unit termed a “track,” which is a group of sequential LAs, and which is described in more detail below. The assigned ranges for each cache <b>20</b> are typically stored in each interface <b>26</b> as a substantially similar table, and the table is used by the interfaces in routing IO requests from hosts <b>52</b> to the caches. Alternatively or additionally, the assigned ranges for each cache <b>20</b> are stored in each interface <b>26</b> as a substantially similar function, or by any other suitable method known in the art for generating a correspondence between ranges and caches. Hereinbelow, the correspondence between caches and ranges, in terms of tracks, is referred to as track-cache mapping <b>28</b>, and it will be understood that mapping <b>28</b> gives each interface <b>26</b> a general overview of the complete cache address space of system <b>10</b>.</p>
<p id="p-0117" num="0116">In arrangements of system <b>10</b> comprising an all-to-all configuration, each cache <b>20</b> contains a track location table <b>21</b> specific to the cache. Each track location table <b>21</b> gives its respective cache exact location details, on disks <b>12</b>, for tracks of the range assigned to the cache. Track location table <b>21</b> may be implemented as software, hardware, or a combination of software and hardware. The operations of track location table <b>21</b>, and also of mapping <b>28</b>, are explained in more detail below.</p>
<p id="p-0118" num="0117"><figref idref="DRAWINGS">FIG. 2</figref> is a schematic diagram illustrating a mapping of data between different elements of system <b>10</b> when the system comprises an all-to-all configuration <b>11</b>, according to an embodiment of the present invention. It will be appreciated that host processors <b>52</b> may communicate with storage system <b>10</b> using virtually any communication system known in the art. By way of example, hereinbelow it is assumed that the hosts communicate with system <b>10</b>, via network <b>50</b>, according to an Internet Small Computer System Interface (iSCSI) protocol, wherein blocks of size 512 bytes are transferred between the hosts and the system. The internal unit of data, i.e., the track, is defined by system manager <b>54</b> for system <b>10</b>, and is herein assumed to have a size of 128 iSCSI blocks, i.e., 64 KB, although it will be appreciated that substantially any other convenient size of track may be used to group the data.</p>
<p id="p-0119" num="0118">Also by way of example, system <b>10</b> is assumed to comprise 16 caches <b>20</b>, herein termed Ca<b>0</b>, Ca<b>1</b>, . . . , Ca<b>14</b>, Ca<b>15</b>, and 32 generally similar disks <b>12</b>, each disk having a 250 GB storage capacity, for a total disk storage of 8 TB. It will be understood that there is no requirement that disks <b>12</b> have equal capacities, and that the capacities of disks <b>12</b> have substantially no effect on the performance of caches <b>20</b>. The 32 disks are assumed to be partitioned into generally similar LUs, LU<sub>L</sub>, where L is an identifying LU integer from 0 to 79. The LUs include LU<sub>0 </sub>having a capacity of 100 GB. Each LU is sub-divided into tracks, so that LU<sub>0 </sub>comprises 100 GB/64 KB tracks i.e., 1,562,500 tracks, herein termed Tr<b>0</b>, Tr<b>1</b>, . . . , Tr<b>1562498</b>, Tr<b>1562499</b>. (Typically, as is described further below, the LAs for any particular LU may be spread over a number of disks <b>12</b>, to achieve well-balanced loading for the disks.)</p>
<p id="p-0120" num="0119">In system <b>10</b>, each track of LU<sub>0 </sub>is assigned to a cache according to the following general mapping:
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?>Tr(n)→Ca(n mod 16)  (1)<?in-line-formulae description="In-line Formulae" end="tail"?>
</p>
<p id="p-0121" num="0120">where n is the track number.</p>
<p id="p-0122" num="0121">Mapping (1) generates the following specific mappings between tracks and caches:
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?>Tr(0)→Ca(0)<?in-line-formulae description="In-line Formulae" end="tail"?>
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?>Tr(1)→Ca(1)<?in-line-formulae description="In-line Formulae" end="tail"?>
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?>M<?in-line-formulae description="In-line Formulae" end="tail"?>
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?>Tr(15)→Ca(15)<?in-line-formulae description="In-line Formulae" end="tail"?>
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?>Tr(16)→Ca(0)<?in-line-formulae description="In-line Formulae" end="tail"?>
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?>Tr(17)→Ca(1)<?in-line-formulae description="In-line Formulae" end="tail"?>
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?>M<?in-line-formulae description="In-line Formulae" end="tail"?>
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?>Tr(1562498)→Ca(2)<?in-line-formulae description="In-line Formulae" end="tail"?>
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?>Tr(1562499)→Ca(3)  (2)<?in-line-formulae description="In-line Formulae" end="tail"?>
</p>
<p id="p-0123" num="0122">A similar mapping for each LU comprising disks <b>12</b> may be generated. For example, an LU<sub>1 </sub>having a capacity of 50 GB is sub-divided into 781,250 tracks, and each track of LU<sub>1 </sub>is assigned the following specific mappings:
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?>Tr(0)→Ca(0)<?in-line-formulae description="In-line Formulae" end="tail"?>
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?>Tr(1)→Ca(1)<?in-line-formulae description="In-line Formulae" end="tail"?>
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?>M<?in-line-formulae description="In-line Formulae" end="tail"?>
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?>Tr(15)→Ca(15)<?in-line-formulae description="In-line Formulae" end="tail"?>
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?>Tr(16)→Ca(0)<?in-line-formulae description="In-line Formulae" end="tail"?>
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?>Tr(17)→Ca(1)<?in-line-formulae description="In-line Formulae" end="tail"?>
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?>M<?in-line-formulae description="In-line Formulae" end="tail"?>
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?>Tr(781248)→Ca(0)<?in-line-formulae description="In-line Formulae" end="tail"?>
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?>Tr(781249)→Ca(1)  (3)<?in-line-formulae description="In-line Formulae" end="tail"?>
</p>
<p id="p-0124" num="0123">Inspection of mappings (2) and (3) shows that the tracks of LU<sub>0 </sub>and of LU<sub>1 </sub>are substantially evenly mapped to caches <b>20</b>. In general, for any LU<sub>L</sub>, a general mapping for every track in disks <b>12</b> is given by:
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?>Tr(L,n)→Ca(n mod 16)  (4)<?in-line-formulae description="In-line Formulae" end="tail"?>
</p>
<p id="p-0125" num="0124">where n is the track number of LU<sub>L</sub>.</p>
<p id="p-0126" num="0125">It will be appreciated that mapping (4) is substantially equivalent to a look-up table, such as Table I below, that assigns specific tracks to specific caches, and that such a look-up table may be stored in each interface in place of the mapping.</p>
<p id="p-0127" num="0126">
<tables id="TABLE-US-00001" num="00001">
<table frame="none" colsep="0" rowsep="0">
<tgroup align="left" colsep="0" rowsep="0" cols="3">
<colspec colname="offset" colwidth="21pt" align="left"/>
<colspec colname="1" colwidth="126pt" align="center"/>
<colspec colname="2" colwidth="70pt" align="center"/>
<thead>
<row>
<entry/>
<entry namest="offset" nameend="2" rowsep="1">TABLE I</entry>
</row>
</thead>
<tbody valign="top">
<row>
<entry/>
<entry namest="offset" nameend="2" align="center" rowsep="1"/>
</row>
<row>
<entry/>
<entry>Track</entry>
<entry/>
</row>
</tbody>
</tgroup>
<tgroup align="left" colsep="0" rowsep="0" cols="3">
<colspec colname="1" colwidth="98pt" align="center"/>
<colspec colname="2" colwidth="49pt" align="center"/>
<colspec colname="3" colwidth="70pt" align="center"/>
<tbody valign="top">
<row>
<entry>L</entry>
<entry>n</entry>
<entry>Cache</entry>
</row>
<row>
<entry>(LU identifier)</entry>
<entry>(Track number)</entry>
<entry>(0-15)</entry>
</row>
<row>
<entry namest="1" nameend="3" align="center" rowsep="1"/>
</row>
</tbody>
</tgroup>
<tgroup align="left" colsep="0" rowsep="0" cols="3">
<colspec colname="1" colwidth="98pt" align="center"/>
<colspec colname="2" colwidth="49pt" align="char" char="."/>
<colspec colname="3" colwidth="70pt" align="center"/>
<tbody valign="top">
<row>
<entry>0</entry>
<entry>0</entry>
<entry>0</entry>
</row>
<row>
<entry>0</entry>
<entry>1</entry>
<entry>1</entry>
</row>
<row>
<entry>0</entry>
<entry>2</entry>
<entry>2</entry>
</row>
<row>
<entry>0</entry>
<entry>3</entry>
<entry>3</entry>
</row>
<row>
<entry>0</entry>
<entry>4</entry>
<entry>4</entry>
</row>
<row>
<entry>. . .</entry>
<entry>. . .</entry>
<entry>. . .</entry>
</row>
<row>
<entry>0</entry>
<entry>15</entry>
<entry>15 </entry>
</row>
<row>
<entry>0</entry>
<entry>16</entry>
<entry>0</entry>
</row>
<row>
<entry>. . .</entry>
<entry>. . .</entry>
<entry>. . .</entry>
</row>
<row>
<entry>0</entry>
<entry>1562498</entry>
<entry>2</entry>
</row>
<row>
<entry>0</entry>
<entry>1562499</entry>
<entry>3</entry>
</row>
<row>
<entry>1</entry>
<entry>0</entry>
<entry>0</entry>
</row>
<row>
<entry>1</entry>
<entry>1</entry>
<entry>1</entry>
</row>
<row>
<entry>. . .</entry>
<entry>. . .</entry>
<entry>. . .</entry>
</row>
<row>
<entry>1</entry>
<entry>17</entry>
<entry>1</entry>
</row>
<row>
<entry>. . .</entry>
<entry>. . .</entry>
<entry>. . .</entry>
</row>
<row>
<entry>1</entry>
<entry>781249</entry>
<entry>1</entry>
</row>
<row>
<entry>. . .</entry>
<entry>. . .</entry>
<entry>. . .</entry>
</row>
<row>
<entry namest="1" nameend="3" align="center" rowsep="1"/>
</row>
</tbody>
</tgroup>
</table>
</tables>
</p>
<p id="p-0128" num="0127">Mapping (4) and Table I are examples of correspondences that assign each track comprised in disks <b>12</b> to a specific cache. Other examples of such assignments will be apparent to those skilled in the art. While such assignments may always be defined in terms of a look-up table such as Table I, it will be appreciated that any particular assignment may not be defined by a simple function such as mapping (4). For example, an embodiment of the present invention comprises a Table II where each track of each LU is assigned by randomly or pseudo-randomly choosing a cache between 0 and 15.</p>
<p id="p-0129" num="0128">
<tables id="TABLE-US-00002" num="00002">
<table frame="none" colsep="0" rowsep="0">
<tgroup align="left" colsep="0" rowsep="0" cols="3">
<colspec colname="offset" colwidth="21pt" align="left"/>
<colspec colname="1" colwidth="126pt" align="center"/>
<colspec colname="2" colwidth="70pt" align="left"/>
<thead>
<row>
<entry/>
<entry namest="offset" nameend="2" rowsep="1">TABLE II</entry>
</row>
</thead>
<tbody valign="top">
<row>
<entry/>
<entry namest="offset" nameend="2" align="center" rowsep="1"/>
</row>
<row>
<entry/>
<entry>Track</entry>
<entry/>
</row>
</tbody>
</tgroup>
<tgroup align="left" colsep="0" rowsep="0" cols="3">
<colspec colname="1" colwidth="98pt" align="center"/>
<colspec colname="2" colwidth="49pt" align="center"/>
<colspec colname="3" colwidth="70pt" align="center"/>
<tbody valign="top">
<row>
<entry>L</entry>
<entry>n</entry>
<entry>Cache</entry>
</row>
<row>
<entry>(LU identifier)</entry>
<entry>(Track number)</entry>
<entry>(0-15)</entry>
</row>
<row>
<entry namest="1" nameend="3" align="center" rowsep="1"/>
</row>
</tbody>
</tgroup>
<tgroup align="left" colsep="0" rowsep="0" cols="3">
<colspec colname="1" colwidth="98pt" align="center"/>
<colspec colname="2" colwidth="49pt" align="char" char="."/>
<colspec colname="3" colwidth="70pt" align="center"/>
<tbody valign="top">
<row>
<entry>0</entry>
<entry>0</entry>
<entry>11</entry>
</row>
<row>
<entry>0</entry>
<entry>1</entry>
<entry> 0</entry>
</row>
<row>
<entry>. . .</entry>
<entry>. . .</entry>
<entry>. . .</entry>
</row>
<row>
<entry>0</entry>
<entry>15</entry>
<entry>12</entry>
</row>
<row>
<entry>0</entry>
<entry>16</entry>
<entry> 2</entry>
</row>
<row>
<entry>. . .</entry>
<entry>. . .</entry>
<entry>. . .</entry>
</row>
<row>
<entry>0</entry>
<entry>1562498</entry>
<entry>14</entry>
</row>
<row>
<entry>0</entry>
<entry>1562499</entry>
<entry>13</entry>
</row>
<row>
<entry>1</entry>
<entry>0</entry>
<entry> 7</entry>
</row>
<row>
<entry>1</entry>
<entry>1</entry>
<entry> 5</entry>
</row>
<row>
<entry>. . .</entry>
<entry>. . .</entry>
<entry>. . .</entry>
</row>
<row>
<entry>1</entry>
<entry>17</entry>
<entry>12</entry>
</row>
<row>
<entry>. . .</entry>
<entry>. . .</entry>
<entry>. . .</entry>
</row>
<row>
<entry>1</entry>
<entry>781249</entry>
<entry>15</entry>
</row>
<row>
<entry>. . .</entry>
<entry>. . .</entry>
<entry>. . .</entry>
</row>
<row>
<entry namest="1" nameend="3" align="center" rowsep="1"/>
</row>
</tbody>
</tgroup>
</table>
</tables>
</p>
<p id="p-0130" num="0129">Configurations of system <b>10</b> that include an all-to-all configuration such as configuration <b>11</b> include track location table <b>21</b> in each cache <b>20</b> of the all-to-all configuration. Track location table <b>21</b> is used by the cache to determine an exact disk location of a requested LU and track. Table III below is an example of track location table <b>21</b> for cache Ca<b>7</b>, assuming that mapping <b>28</b> corresponds to Table I. In Table III, the values a, b, . . . , f, . . . of the disk locations of the tracks, are allocated by system manager <b>54</b>.</p>
<p id="p-0131" num="0130">
<tables id="TABLE-US-00003" num="00003">
<table frame="none" colsep="0" rowsep="0">
<tgroup align="left" colsep="0" rowsep="0" cols="1">
<colspec colname="1" colwidth="217pt" align="center"/>
<thead>
<row>
<entry namest="1" nameend="1" rowsep="1">TABLE III</entry>
</row>
</thead>
<tbody valign="top">
<row>
<entry namest="1" nameend="1" align="center" rowsep="1"/>
</row>
<row>
<entry>Cache Ca7</entry>
</row>
</tbody>
</tgroup>
<tgroup align="left" colsep="0" rowsep="0" cols="3">
<colspec colname="offset" colwidth="14pt" align="left"/>
<colspec colname="1" colwidth="126pt" align="center"/>
<colspec colname="2" colwidth="77pt" align="left"/>
<tbody valign="top">
<row>
<entry/>
<entry>Track</entry>
<entry/>
</row>
</tbody>
</tgroup>
<tgroup align="left" colsep="0" rowsep="0" cols="3">
<colspec colname="1" colwidth="91pt" align="center"/>
<colspec colname="2" colwidth="49pt" align="center"/>
<colspec colname="3" colwidth="77pt" align="center"/>
<tbody valign="top">
<row>
<entry>L</entry>
<entry>n</entry>
<entry>Disk</entry>
</row>
<row>
<entry>(LU identifier)</entry>
<entry>(Track number)</entry>
<entry>Location</entry>
</row>
<row>
<entry namest="1" nameend="3" align="center" rowsep="1"/>
</row>
</tbody>
</tgroup>
<tgroup align="left" colsep="0" rowsep="0" cols="3">
<colspec colname="1" colwidth="91pt" align="center"/>
<colspec colname="2" colwidth="49pt" align="char" char="."/>
<colspec colname="3" colwidth="77pt" align="center"/>
<tbody valign="top">
<row>
<entry>0</entry>
<entry>7</entry>
<entry>a</entry>
</row>
<row>
<entry>0</entry>
<entry>23</entry>
<entry>b</entry>
</row>
<row>
<entry>. . .</entry>
<entry>. . .</entry>
<entry>. . .</entry>
</row>
<row>
<entry>0</entry>
<entry>1562487</entry>
<entry>c</entry>
</row>
<row>
<entry>1</entry>
<entry>7</entry>
<entry>d</entry>
</row>
<row>
<entry>1</entry>
<entry>23</entry>
<entry>e</entry>
</row>
<row>
<entry>. . .</entry>
<entry>. . .</entry>
<entry>. . .</entry>
</row>
<row>
<entry>1</entry>
<entry>1562487</entry>
<entry>f</entry>
</row>
<row>
<entry>. . .</entry>
<entry>. . .</entry>
<entry>. . .</entry>
</row>
<row>
<entry namest="1" nameend="3" align="center" rowsep="1"/>
</row>
</tbody>
</tgroup>
</table>
</tables>
</p>
<p id="p-0132" num="0131"><figref idref="DRAWINGS">FIG. 3</figref> is a schematic diagram illustrating a mapping of data between different elements of system <b>10</b> when the system comprises a one-to-one configuration <b>13</b>, according to am embodiment of the present invention. In one-to-one configuration <b>13</b>, tracks are assigned to caches on the basis of the disks wherein the tracks originate. <figref idref="DRAWINGS">FIG. 3</figref>, and Table IV below, shows an example of tracks so assigned. For the assignment of each track of system <b>10</b> defined by Table IV, there are assumed to be 16 generally similar disks <b>12</b>, each disk having a whole number disk identifier D range from 0 to 15 and 50 GB capacity, and each disk is assigned a cache. There are also assumed to be 8 LU<sub>L</sub>, where L is an integer from 0 to 7, of 100 GB evenly divided between the disks, according to mapping (5):
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?>Tr(L,n)→Disk(n mod 16)=Ca(n mod 16)  (5)<?in-line-formulae description="In-line Formulae" end="tail"?>
</p>
<p id="p-0133" num="0132">
<tables id="TABLE-US-00004" num="00004">
<table frame="none" colsep="0" rowsep="0">
<tgroup align="left" colsep="0" rowsep="0" cols="3">
<colspec colname="offset" colwidth="14pt" align="left"/>
<colspec colname="1" colwidth="84pt" align="center"/>
<colspec colname="2" colwidth="119pt" align="left"/>
<thead>
<row>
<entry/>
<entry namest="offset" nameend="2" rowsep="1">TABLE IV</entry>
</row>
</thead>
<tbody valign="top">
<row>
<entry/>
<entry namest="offset" nameend="2" align="center" rowsep="1"/>
</row>
<row>
<entry/>
<entry>Track</entry>
<entry/>
</row>
</tbody>
</tgroup>
<tgroup align="left" colsep="0" rowsep="0" cols="5">
<colspec colname="offset" colwidth="14pt" align="left"/>
<colspec colname="1" colwidth="35pt" align="center"/>
<colspec colname="2" colwidth="63pt" align="center"/>
<colspec colname="3" colwidth="56pt" align="center"/>
<colspec colname="4" colwidth="49pt" align="center"/>
<tbody valign="top">
<row>
<entry/>
<entry>L</entry>
<entry>n</entry>
<entry>D</entry>
<entry/>
</row>
<row>
<entry/>
<entry>(LU</entry>
<entry>(Track</entry>
<entry>(Disk identifier)</entry>
<entry>Cache</entry>
</row>
<row>
<entry/>
<entry>identifier)</entry>
<entry>number)</entry>
<entry>(0-15)</entry>
<entry>(0-15)</entry>
</row>
<row>
<entry/>
<entry namest="offset" nameend="4" align="center" rowsep="1"/>
</row>
</tbody>
</tgroup>
<tgroup align="left" colsep="0" rowsep="0" cols="5">
<colspec colname="offset" colwidth="14pt" align="left"/>
<colspec colname="1" colwidth="35pt" align="center"/>
<colspec colname="2" colwidth="63pt" align="char" char="."/>
<colspec colname="3" colwidth="56pt" align="center"/>
<colspec colname="4" colwidth="49pt" align="center"/>
<tbody valign="top">
<row>
<entry/>
<entry>0-7</entry>
<entry>0</entry>
<entry>0</entry>
<entry>0</entry>
</row>
<row>
<entry/>
<entry/>
<entry>1</entry>
<entry>1</entry>
<entry>1</entry>
</row>
<row>
<entry/>
<entry/>
<entry>2</entry>
<entry>2</entry>
<entry>2</entry>
</row>
<row>
<entry/>
<entry/>
<entry>. . .</entry>
<entry>. . .</entry>
<entry>. . .</entry>
</row>
<row>
<entry/>
<entry/>
<entry>329999</entry>
<entry>15 </entry>
<entry>15 </entry>
</row>
<row>
<entry/>
<entry/>
<entry>330000</entry>
<entry>0</entry>
<entry>0</entry>
</row>
<row>
<entry/>
<entry/>
<entry>. . .</entry>
<entry>. . .</entry>
<entry>. . .</entry>
</row>
<row>
<entry/>
<entry/>
<entry>761254</entry>
<entry>6</entry>
<entry>6</entry>
</row>
<row>
<entry/>
<entry/>
<entry>. . .</entry>
<entry>. . .</entry>
<entry>. . .</entry>
</row>
<row>
<entry/>
<entry/>
<entry>1002257</entry>
<entry>1</entry>
<entry>1</entry>
</row>
<row>
<entry/>
<entry/>
<entry>1002258</entry>
<entry>2</entry>
<entry>2</entry>
</row>
<row>
<entry/>
<entry/>
<entry>. . .</entry>
<entry>. . .</entry>
<entry>. . .</entry>
</row>
<row>
<entry/>
<entry/>
<entry>1562499</entry>
<entry>3</entry>
<entry>3</entry>
</row>
<row>
<entry/>
<entry namest="offset" nameend="4" align="center" rowsep="1"/>
</row>
</tbody>
</tgroup>
</table>
</tables>
</p>
<p id="p-0134" num="0133">A mapping such as mapping (4) or mapping (5), or a table such as Table I, II, or IV, or a combination of such types of mapping and tables, is incorporated into each interface <b>26</b> as its track-cache mapping <b>28</b>, and spreads the LAs of the LUs substantially evenly across caches <b>20</b>. The mapping used is a function of the coupling arrangement between caches <b>20</b> and disks <b>12</b>. Track-cache mapping <b>28</b> is used by the interfaces to process IO requests from hosts <b>52</b>, as is explained with respect to <figref idref="DRAWINGS">FIG. 5</figref> below. The application titled “Data Allocation in a Distributed Storage System,” describes a system for mapping LAs to devices such as caches <b>20</b> and/or disks <b>12</b>, and such a system is preferably used for generating track-cache mapping <b>28</b>.</p>
<p id="p-0135" num="0134">To achieve well-balanced loading across caches <b>20</b>, system <b>10</b> generates even and sufficiently fine “spreading” of all the LAs over the caches, and it will be appreciated that track-cache mapping <b>28</b> enables system <b>10</b> to implement the even and fine spread, and thus the well-balanced loading. For example, if in all-to-all configuration <b>11</b>, or in one-to-one configuration <b>13</b>, caches <b>20</b> comprise substantially equal capacities, it will be apparent that well-balanced loading occurs. Thus, referring back to mapping (1), statistical considerations make it clear that the average IO transaction related with the LAs of LU<sub>0 </sub>is likely to use evenly all the 16 caches available in the system, rather than any one of them, or any subset of them, in particular. This is because LU<sub>0 </sub>contains about 1.5 million tracks, and these tracks are now spread uniformly and finely across all 16 caches, thus yielding a well-balanced load for the IO activity pertaining to the caches, as may be true in general for any system where the number of tracks is far greater than the number of caches. Similarly, spreading LAs evenly and sufficiently finely amongst disks <b>12</b> leads to well-balanced IO activity for the disks.</p>
<p id="p-0136" num="0135">An example of a configuration with unequal cache capacities is described with reference to <figref idref="DRAWINGS">FIG. 4</figref>.</p>
<p id="p-0137" num="0136"><figref idref="DRAWINGS">FIG. 4</figref> is a schematic diagram illustrating a mapping of data between different elements of system <b>10</b> when the system comprises an alternative all-to-all configuration <b>15</b>, according to an embodiment of the present invention. Apart from the differences described below, configuration <b>15</b> is generally similar to configuration <b>11</b>, so that elements indicated by the same reference numerals in both configurations are generally identical in construction and in operation. All-to-all configuration <b>15</b> comprises two caches <b>20</b>, herein termed Ca<b>0</b> and Ca<b>1</b>, Ca<b>0</b> having approximately twice the capacity of Ca<b>1</b>.</p>
<p id="p-0138" num="0137">Track-cache mapping <b>28</b> is implemented as mapping (6) below, or as Table V below, which is derived from mapping (6).
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?>Tr(L,n)→Ca[(n mod 3)mod 2]  (6)<?in-line-formulae description="In-line Formulae" end="tail"?>
<br/>
where n is the track number of LU<sub>L</sub>.
</p>
<p id="p-0139" num="0138">
<tables id="TABLE-US-00005" num="00005">
<table frame="none" colsep="0" rowsep="0">
<tgroup align="left" colsep="0" rowsep="0" cols="3">
<colspec colname="offset" colwidth="21pt" align="left"/>
<colspec colname="1" colwidth="126pt" align="center"/>
<colspec colname="2" colwidth="70pt" align="left"/>
<thead>
<row>
<entry/>
<entry namest="offset" nameend="2" rowsep="1">TABLE V</entry>
</row>
</thead>
<tbody valign="top">
<row>
<entry/>
<entry namest="offset" nameend="2" align="center" rowsep="1"/>
</row>
<row>
<entry/>
<entry>Track</entry>
<entry/>
</row>
</tbody>
</tgroup>
<tgroup align="left" colsep="0" rowsep="0" cols="3">
<colspec colname="1" colwidth="98pt" align="center"/>
<colspec colname="2" colwidth="49pt" align="center"/>
<colspec colname="3" colwidth="70pt" align="center"/>
<tbody valign="top">
<row>
<entry>L</entry>
<entry>n</entry>
<entry>Cache</entry>
</row>
<row>
<entry>(LU identifier)</entry>
<entry>(Track number)</entry>
<entry>(0-1)</entry>
</row>
<row>
<entry namest="1" nameend="3" align="center" rowsep="1"/>
</row>
</tbody>
</tgroup>
<tgroup align="left" colsep="0" rowsep="0" cols="3">
<colspec colname="1" colwidth="98pt" align="center"/>
<colspec colname="2" colwidth="49pt" align="char" char="."/>
<colspec colname="3" colwidth="70pt" align="center"/>
<tbody valign="top">
<row>
<entry>0</entry>
<entry>0</entry>
<entry>0</entry>
</row>
<row>
<entry>0</entry>
<entry>1</entry>
<entry>1</entry>
</row>
<row>
<entry>0</entry>
<entry>2</entry>
<entry>0</entry>
</row>
<row>
<entry>0</entry>
<entry>3</entry>
<entry>0</entry>
</row>
<row>
<entry>0</entry>
<entry>4</entry>
<entry>1</entry>
</row>
<row>
<entry>0</entry>
<entry>5</entry>
<entry>0</entry>
</row>
<row>
<entry>0</entry>
<entry>6</entry>
<entry>0</entry>
</row>
<row>
<entry>. . .</entry>
<entry>. . .</entry>
<entry>. . .</entry>
</row>
<row>
<entry>0</entry>
<entry>15</entry>
<entry>0</entry>
</row>
<row>
<entry>0</entry>
<entry>16</entry>
<entry>1</entry>
</row>
<row>
<entry>0</entry>
<entry>17</entry>
<entry>0</entry>
</row>
<row>
<entry>0</entry>
<entry>18</entry>
<entry>0</entry>
</row>
<row>
<entry>. . .</entry>
<entry>. . .</entry>
<entry>. . .</entry>
</row>
<row>
<entry>0</entry>
<entry>1562499</entry>
<entry>0</entry>
</row>
<row>
<entry>1</entry>
<entry>0</entry>
<entry>0</entry>
</row>
<row>
<entry>1</entry>
<entry>1</entry>
<entry>1</entry>
</row>
<row>
<entry>. . .</entry>
<entry>. . .</entry>
<entry>. . .</entry>
</row>
<row>
<entry>1</entry>
<entry>15</entry>
<entry>0</entry>
</row>
<row>
<entry>1</entry>
<entry>16</entry>
<entry>1</entry>
</row>
<row>
<entry>1</entry>
<entry>17</entry>
<entry>0</entry>
</row>
<row>
<entry>. . .</entry>
<entry>. . .</entry>
<entry>. . .</entry>
</row>
<row>
<entry>1</entry>
<entry>781249</entry>
<entry>1</entry>
</row>
<row>
<entry>. . .</entry>
<entry>. . .</entry>
<entry>. . .</entry>
</row>
<row>
<entry namest="1" nameend="3" align="center" rowsep="1"/>
</row>
</tbody>
</tgroup>
</table>
</tables>
</p>
<p id="p-0140" num="0139">Mapping <b>28</b> is configured to accommodate the unequal capacities of Ca<b>0</b> and Ca<b>1</b> so that well-balanced loading of configuration <b>15</b> occurs.</p>
<p id="p-0141" num="0140">By the inspection of the exemplary mappings for configurations <b>11</b>, <b>13</b>, and <b>15</b>, it will be appreciated that mapping <b>28</b> may be configured to accommodate caches <b>20</b> in system <b>10</b> having substantially any capacities, so as to maintain substantially well-balanced loading for the system. It will also be appreciated that the loading generated by mapping <b>28</b> is substantially independent of the capacity of any specific disk in system <b>10</b>, since the mapping relates caches to tracks.</p>
<p id="p-0142" num="0141"><figref idref="DRAWINGS">FIG. 5</figref> is a flow chart showing steps followed by system <b>10</b> on receipt of an IO request from one of hosts <b>52</b>, according to an embodiment of the present invention. Each IO request from a specific host <b>52</b> comprises several parameters, such as whether the request is a read or a write command, the LU to which the request is addressed, the first LA requested, and a number of blocks of data included in the request.</p>
<p id="p-0143" num="0142">In an initial step <b>100</b>, the IO request is transmitted to system <b>10</b> in one or more packets according to the protocol under which the hosts and the system are operating. The request is received by system <b>10</b> at one of interfaces <b>26</b>, herein, for clarity, termed the request-receiving interface (RRI).</p>
<p id="p-0144" num="0143">In a track identification step <b>102</b>, the RRI identifies from the request the LAs from which data is to be read from, or to which data is to be written to. The RRI then determines one or more tracks corresponding to the LAs which have been identified.</p>
<p id="p-0145" num="0144">In a cache identification step <b>104</b>, the RRI refers to its mapping <b>28</b> to determine the caches corresponding to tracks determined in the third step. For each track so determined, the RRI transfers a respective track request to the cache corresponding to the track. It will be understood that each track request is a read or a write command, according to the originating IO request.</p>
<p id="p-0146" num="0145">In a cache response <b>106</b>, each cache <b>20</b> receiving a track request from the RRI responds to the request. The response is a function of, inter alia, the type of request, i.e., whether the track request is a read or a write command and whether the request is a “hit” or a “miss.” Thus, data may be written to the LA of the track request from the cache and/or read from the LA to the cache. Data may also be written to the RRI from the cache and/or read from the RRI to the cache. If system <b>10</b> comprises an all-to-all configuration, and the response includes writing to or reading from the LA, the cache uses its track location table <b>21</b> to determine the location on the corresponding disk of the track for the LA.</p>
<p id="p-0147" num="0146">The flow chart of <figref idref="DRAWINGS">FIG. 5</figref> illustrates that there is virtually no management activity of system <b>10</b> once an IO request has reached a specific interface <b>26</b>. This is because the only activity performed by the interface is, as described above for steps <b>102</b> and <b>104</b>, identifying track requests and transmitting the track requests to their respective caches <b>20</b>. Similarly, each cache <b>20</b> operates substantially independently, since once a track request reaches its cache, data is moved between the cache and the interface originating the request, and between the cache and the required disk, as necessary, to service the request.</p>
<p id="p-0148" num="0147"><figref idref="DRAWINGS">FIG. 6</figref> is a flow chart showing steps followed by system <b>10</b> on addition or removal of a cache or disk from system <b>10</b>, according to an embodiment of the present invention. In a first step <b>120</b>, a cache or disk is added or removed from system <b>10</b>. In an update step <b>122</b>, system manager <b>54</b> updates mapping <b>28</b> and/or track location table <b>21</b> to reflect the change in system <b>10</b>. In a redistribution step <b>124</b>, system manager <b>54</b> redistributes data on disks <b>12</b>, if the change has been a disk change, or data between caches <b>20</b>, if the change is a cache change. The redistribution is according to the updated mapping <b>28</b>, and it will be understood that the number of internal IO transactions generated for the redistribution is dependent on changes effected in mapping <b>28</b>. Once redistribution is complete, system <b>10</b> then proceeds to operate as described with reference to <figref idref="DRAWINGS">FIG. 4</figref>. It will thus be apparent that system <b>10</b> is substantially perfectly scalable.</p>
<p id="p-0149" num="0148">Referring back to <figref idref="DRAWINGS">FIGS. 1</figref>, <b>2</b>, and <b>3</b>, redundancy for caches <b>20</b> and/or disks <b>12</b> may be easily incorporated into system <b>10</b>. The redundancy may be implemented by modifying track-cache mapping <b>28</b> and/or track location table <b>21</b>, so that data is written to more than one cache <b>20</b>, and may be read from any of the caches, and also so that data is stored on more than one disk <b>12</b>.</p>
<p id="p-0150" num="0149">Mapping (7) below is an example of a mapping, similar to mapping (4), that assigns each track to two caches <b>20</b> of the 16 caches available, so that incorporating mapping (7) as track-cache mapping <b>28</b> in each interface <b>26</b> will form a redundant cache for each cache of system <b>10</b>.</p>
<p id="p-0151" num="0150">
<maths id="MATH-US-00001" num="00001">
<math overflow="scroll">
<mtable>
  <mtr>
    <mtd>
      <mrow>
        <mrow>
          <mi>Tr</mi>
          <mo>⁡</mo>
          <mrow>
            <mo>(</mo>
            <mrow>
              <mi>L</mi>
              <mo>,</mo>
              <mi>n</mi>
            </mrow>
            <mo>)</mo>
          </mrow>
        </mrow>
        <mo>-&gt;</mo>
        <mrow>
          <mo>{</mo>
          <mtable>
            <mtr>
              <mtd>
                <mrow>
                  <mi>Ca</mi>
                  <mo>⁡</mo>
                  <mrow>
                    <mo>(</mo>
                    <mrow>
                      <mi>n</mi>
                      <mo>⁢</mo>
                      <mstyle>
                        <mspace width="0.8em" height="0.8ex"/>
                      </mstyle>
                      <mo>⁢</mo>
                      <mi>mod</mi>
                      <mo>⁢</mo>
                      <mstyle>
                        <mspace width="0.3em" height="0.3ex"/>
                      </mstyle>
                      <mo>⁢</mo>
                      <mn>8</mn>
                    </mrow>
                    <mo>)</mo>
                  </mrow>
                </mrow>
              </mtd>
            </mtr>
            <mtr>
              <mtd>
                <mrow>
                  <mi>Ca</mi>
                  <mo>⁡</mo>
                  <mrow>
                    <mo>(</mo>
                    <mrow>
                      <mn>7</mn>
                      <mo>+</mo>
                      <mrow>
                        <mi>n</mi>
                        <mo>⁢</mo>
                        <mstyle>
                          <mspace width="0.8em" height="0.8ex"/>
                        </mstyle>
                        <mo>⁢</mo>
                        <mi>mod</mi>
                        <mo>⁢</mo>
                        <mstyle>
                          <mspace width="0.3em" height="0.3ex"/>
                        </mstyle>
                        <mo>⁢</mo>
                        <mn>8</mn>
                      </mrow>
                    </mrow>
                    <mo>)</mo>
                  </mrow>
                </mrow>
              </mtd>
            </mtr>
          </mtable>
        </mrow>
      </mrow>
    </mtd>
    <mtd>
      <mrow>
        <mo>(</mo>
        <mn>7</mn>
        <mo>)</mo>
      </mrow>
    </mtd>
  </mtr>
</mtable>
</math>
</maths>
</p>
<p id="p-0152" num="0151">In processing an IO request, as described above with reference to <figref idref="DRAWINGS">FIG. 5</figref>, the interface <b>26</b> that receives the IO request may generate a track request (cache identification step <b>104</b>) to either cache defined by mapping (7).</p>
<p id="p-0153" num="0152">Table VI below is an example of a table for cache Ca<b>7</b>, similar to Table III above, that assumes each track is written to two separate disks <b>12</b>, thus incorporating disk redundancy into system <b>10</b>. The specific disk locations for each track are assigned by system manager <b>54</b>. A table similar to Table VI is incorporated as track location table <b>21</b> into each respective cache <b>20</b>.</p>
<p id="p-0154" num="0153">
<tables id="TABLE-US-00006" num="00006">
<table frame="none" colsep="0" rowsep="0">
<tgroup align="left" colsep="0" rowsep="0" cols="1">
<colspec colname="1" colwidth="217pt" align="center"/>
<thead>
<row>
<entry namest="1" nameend="1" rowsep="1">TABLE VI</entry>
</row>
</thead>
<tbody valign="top">
<row>
<entry namest="1" nameend="1" align="center" rowsep="1"/>
</row>
<row>
<entry>Cache Ca7</entry>
</row>
</tbody>
</tgroup>
<tgroup align="left" colsep="0" rowsep="0" cols="3">
<colspec colname="offset" colwidth="14pt" align="left"/>
<colspec colname="1" colwidth="126pt" align="center"/>
<colspec colname="2" colwidth="77pt" align="left"/>
<tbody valign="top">
<row>
<entry/>
<entry>Track</entry>
<entry/>
</row>
</tbody>
</tgroup>
<tgroup align="left" colsep="0" rowsep="0" cols="3">
<colspec colname="1" colwidth="91pt" align="center"/>
<colspec colname="2" colwidth="49pt" align="center"/>
<colspec colname="3" colwidth="77pt" align="center"/>
<tbody valign="top">
<row>
<entry>L</entry>
<entry>n</entry>
<entry>Disk</entry>
</row>
<row>
<entry>(LU identifier)</entry>
<entry>(Track number)</entry>
<entry>Location</entry>
</row>
<row>
<entry namest="1" nameend="3" align="center" rowsep="1"/>
</row>
</tbody>
</tgroup>
<tgroup align="left" colsep="0" rowsep="0" cols="3">
<colspec colname="1" colwidth="91pt" align="center"/>
<colspec colname="2" colwidth="49pt" align="char" char="."/>
<colspec colname="3" colwidth="77pt" align="center"/>
<tbody valign="top">
<row>
<entry>0</entry>
<entry>7</entry>
<entry>a1, a2</entry>
</row>
<row>
<entry>0</entry>
<entry>23</entry>
<entry>b1, b2</entry>
</row>
<row>
<entry>. . .</entry>
<entry>. . .</entry>
<entry>. . .</entry>
</row>
<row>
<entry>0</entry>
<entry>1562487</entry>
<entry>c1, c2</entry>
</row>
<row>
<entry>1</entry>
<entry>7</entry>
<entry>d1, d2</entry>
</row>
<row>
<entry>1</entry>
<entry>23</entry>
<entry>e1, e2</entry>
</row>
<row>
<entry>. . .</entry>
<entry>. . .</entry>
<entry>. . .</entry>
</row>
<row>
<entry>1</entry>
<entry>1562487</entry>
<entry>f1, f2</entry>
</row>
<row>
<entry>. . .</entry>
<entry>. . .</entry>
<entry>. . .</entry>
</row>
<row>
<entry namest="1" nameend="3" align="center" rowsep="1"/>
</row>
</tbody>
</tgroup>
</table>
</tables>
</p>
<p id="p-0155" num="0154">As described above with reference to cache response step <b>106</b> (<figref idref="DRAWINGS">FIG. 5</figref>), the cache that receives a specific track request may need to refer to track location table <b>21</b>. This reference generates a read or a write, so that in the case of Table VI, the read may be to either disk assigned to the specific track, and the write is to both disks.</p>
<p id="p-0156" num="0155">It will be appreciated that other forms of redundancy known in the art, apart from those described above, may be incorporated into system <b>10</b>. For example, a write command to a cache may be considered to be incomplete until the command has also been performed on another cache. All such forms of redundancy are assumed to be comprised within the present invention.</p>
<p id="p-0157" num="0156">As stated above with reference to <figref idref="DRAWINGS">FIG. 1</figref>, disks <b>12</b> (<figref idref="DRAWINGS">FIGS. 1-4</figref>) are examples of mass storage devices, and it will be appreciated that other mass storage devices may be used in embodiments of the present invention. In the configurations described above, as well as those in the following description, it will thus be understood that a mass storage device may comprise one or more disks, one or more redundant arrays of independent disks (RAIDs), one or more optical storage devices, one or more non-volatile random access memories (RAMs), or combinations of such devices.</p>
<p id="p-0158" num="0157"><figref idref="DRAWINGS">FIGS. 7-14</figref> below are illustrative of configurations of storage systems, other than those represented by <figref idref="DRAWINGS">FIG. 1</figref> and <figref idref="DRAWINGS">FIG. 3</figref>. Apart from the differences described below, the functioning of the systems of <figref idref="DRAWINGS">FIGS. 7-14</figref> is generally similar to that of system <b>10</b>, such that elements indicated by the same terms and reference numerals within the systems of <figref idref="DRAWINGS">FIGS. 7-14</figref>, and in system <b>10</b>, are generally identical in construction and in operation. It will be understood that the configurations of <figref idref="DRAWINGS">FIGS. 7-14</figref> may be implemented using substantially any data transfer method; such methods include, but are not limited to, operation in a SAN or a NAS system. It will also be understood that, as for the configurations of <figref idref="DRAWINGS">FIGS. 1</figref> and <b>3</b>, the configurations of <figref idref="DRAWINGS">FIGS. 7-14</figref> may be implemented using commercially available components, including, but not limited to, personal computers.</p>
<p id="p-0159" num="0158">In operating as a SAN system, data transfer is typically block-based, and caches such as caches <b>20</b> are coupled to transfer block-based data between the mass storage devices where the data is stored. In the SAN system, interfaces <b>26</b> are typically adapted to receive block-based IO requests from host processors such as hosts <b>52</b>. As described above, the interfaces convert the block-based IO requests to internal block-based requests which are directed to the appropriate cache.</p>
<p id="p-0160" num="0159">In operating as a NAS system, data transfer is typically file-based, and caches such as caches <b>20</b> are coupled to transfer file-based data between the mass storage devices where the data is stored. In the NAS system, interfaces <b>26</b> are typically adapted to receive file-based IO requests from host processors such as hosts <b>52</b>. It will be understood that the interfaces may then convert the file-based IO requests to internal file-based requests which are directed to the appropriate cache.</p>
<p id="p-0161" num="0160"><figref idref="DRAWINGS">FIG. 7</figref> is a schematic block diagram of a storage system <b>150</b>, according to an embodiment of the present invention. In storage system <b>150</b>, each cache <b>20</b> may be coupled directly to one or more mass storage devices <b>152</b>. In cases when a specific cache <b>20</b> is coupled to one device <b>152</b>—corresponding to a one-one configuration such as described in more detail with respect to <figref idref="DRAWINGS">FIG. 3</figref> above—the tracks assigned to the cache correspond to those of the mass storage device to which the cache is attached.</p>
<p id="p-0162" num="0161">In cases when a specific cache <b>20</b> is coupled to more than one device <b>152</b>, the respective cache comprises a local track location table. This is exemplified by local track location tables <b>154</b>, <b>156</b>, and <b>158</b>, which respectively tabulate track locations on two, three, and two mass storage devices coupled to their respective caches <b>20</b>. Local track location tables <b>154</b>, <b>156</b>, and <b>158</b> are generally similar to track location tables <b>21</b> described above, but map exact location details for the tracks of the specific mass storage devices device attached to a particular cache <b>20</b>.</p>
<p id="p-0163" num="0162">In configurations such as that exemplified by system <b>150</b>, separation of the interfaces from the cache-mass storage devices allows for flexibility in locating the interfaces relative to the cache-mass storage devices. For example, the interfaces may be located in one or more devices physically distant from the cache-mass storage devices. By enabling each cache to be coupled to more than one mass storage device, further flexibility is available for the system, such as the ability to provide local redundancy for the devices coupled to a specific cache.</p>
<p id="p-0164" num="0163">In some embodiments of the present invention, each cache <b>20</b> is housed together with its directly coupled one or more mass storage device, in a single housing such as housings <b>151</b> and <b>153</b>. Typically the single housing is at least part of a personal computer. By coupling the cache and the one or more storage devices directly, there are no communication overheads such as are present with a switch, and there is an extremely large bandwidth between the storage devices and the cache.</p>
<p id="p-0165" num="0164"><figref idref="DRAWINGS">FIG. 8</figref> is a schematic block diagram of a storage system <b>160</b>, according to an embodiment of the present invention. In storage system <b>160</b>, more than one cache is coupled to each single mass storage device. By way of example, caches <b>170</b> and <b>172</b> are coupled to a single mass storage device <b>162</b>, and caches <b>174</b> and <b>176</b> are coupled to a single mass storage device <b>164</b>. Each of caches <b>170</b>, <b>172</b>, <b>174</b>, and <b>176</b> is substantially similar to cache <b>20</b>. Single mass storage device <b>162</b> comprises a single physical device which is divided into two logical partitions <b>178</b> and <b>180</b> which communicate respectively with cache <b>170</b> and cache <b>172</b>.</p>
<p id="p-0166" num="0165">Single mass storage device <b>164</b> comprises a single physical device having one logical partition <b>182</b>, and both cache <b>170</b> and cache <b>172</b> communicate with the one partition. System manager <b>54</b> and/or central processing units within the caches are implemented to track input/output requests to logical partition <b>182</b> so as to avoid conflicts. The implementation is typically in a “dual-write” format, described in more detail below with reference to <figref idref="DRAWINGS">FIG. 13</figref>.</p>
<p id="p-0167" num="0166">In the caches attached to device <b>162</b> and to device <b>164</b>, the tracks assigned to a specific cache correspond to those of the logical partition with which the cache communicates.</p>
<p id="p-0168" num="0167">In configurations such as system <b>160</b>, separation of the interfaces from the cache-mass storage devices provides the same advantages as described for system <b>150</b>. In addition, by coupling more than one cache to each mass storage device, the system benefits from improved data throughput and/or cache redundancy.</p>
<p id="p-0169" num="0168"><figref idref="DRAWINGS">FIG. 9</figref> is a schematic block diagram of a storage system <b>190</b>, according to an embodiment of the present invention. Each interface <b>26</b> is coupled to a respective cache <b>20</b> to form an interface-cache pair <b>192</b>, and each interface-cache pair is typically housed within a single housing <b>194</b>. The interface-cache pairs are coupled via channel <b>24</b> to mass storage devices <b>198</b>. Each cache <b>20</b> comprises a track location table <b>196</b>, substantially similar to track location tables <b>21</b> described above, each table <b>196</b> giving its respective cache exact location details on devices <b>198</b> for tracks of the range assigned to the cache. It will be appreciated that the cache—mass storage device arrangement of system <b>190</b> corresponds to the “all-to-all” configuration described above.</p>
<p id="p-0170" num="0169">Each cache <b>20</b> is implemented to communicate with other caches <b>20</b>, typically via a communication channel <b>200</b>, such as a bus, to which all the caches are coupled. An I/O request to a specific interface <b>26</b> is routed from the specific interface, using the interface's track-cache mapping <b>28</b>. If the cache <b>20</b> to which the request is routed is the cache coupled directly to the interface, the request is conveyed directly to the cache. If the cache <b>20</b> to which the request is routed is another cache, the request is routed to the other cache via channel <b>200</b>. Typically the routing may be implemented using a central processing unit comprised in the interface-cache pair <b>192</b> which receives the request. Alternatively or additionally, the routing may be implemented by system manager <b>54</b>.</p>
<p id="p-0171" num="0170">In configurations such as that of system <b>190</b>, housing an interface unit with a cache may provide extremely fast response to IO requests directed to the specific interface-cache combination. The overall ability of the system to respond to any IO request is maintained by the coupling between the caches. In addition, by separating the interface-cache combinations from the mass storage devices, the two combinations and the devices may be isolated, so that, for example, maintenance/removal/addition of a mass storage device has no effect on any other part of the system.</p>
<p id="p-0172" num="0171">Furthermore, interface-cache pairs <b>192</b>, in respective housings <b>194</b>, may be conveniently implemented from an off-the-shelf personal computer, typically leading to significant savings in cost compared to separate provision of the components. Other advantages to such an implementation include reduced maintenance, reduced power consumption, and reduced communication overhead. In addition, memory comprised in the personal computer may be allocated flexibly between the interface <b>26</b> and the cache <b>20</b> of the interface-cache pair.</p>
<p id="p-0173" num="0172"><figref idref="DRAWINGS">FIG. 10</figref> is a schematic block diagram of a storage system <b>210</b>, according to an embodiment of the present invention. Storage system <b>210</b> comprises a plurality of interface-cache pairs <b>192</b>, typically housed in respective housings <b>194</b> and coupled by communication channel <b>200</b> as described above with reference to <figref idref="DRAWINGS">FIG. 9</figref>. Each interface-cache pair <b>192</b> is coupled directly to a respective single mass storage device <b>212</b>, so that there are no track location tables <b>196</b> in caches <b>20</b>. Rather, as described above with reference to <figref idref="DRAWINGS">FIG. 3</figref>, there is a one-one configuration wherein the tracks assigned to each cache correspond to those of the mass storage device to which the cache is attached. In some embodiments of system <b>210</b>, each mass storage device <b>212</b> is included within a respective housing <b>194</b> of the interface-cache pair to which it is coupled. Such an interface-cache-mass storage device combination may be advantageously implemented from an off-the-shelf personal computer.</p>
<p id="p-0174" num="0173">An I/O request to a specific interface <b>26</b> is routed from the specific interface, using the interface's track-cache mapping <b>28</b>, as described above with reference for system <b>190</b>.</p>
<p id="p-0175" num="0174">In configurations such as that exemplified by system <b>210</b>, the interface-cache-mass storage device combination may operate as a “local” storage system, enabling local IO requests directed to a local mass storage device to be handled quickly. The ability of any local interface-cache-mass storage device combination to respond to any IO request is maintained by the coupling between the caches. It will be appreciated that in addition to the advantages described above (with reference to <figref idref="DRAWINGS">FIG. 9</figref>) in implementing interface-cache pairs <b>192</b>, the one-one configuration of system <b>210</b> has extremely high cache-mass storage device bandwidth.</p>
<p id="p-0176" num="0175"><figref idref="DRAWINGS">FIG. 11</figref> is a schematic block diagram of a storage system <b>220</b>, according to an embodiment of the present invention. Storage system <b>220</b> comprises interface-cache pairs coupled by communication channel <b>200</b>, as described above with reference to <figref idref="DRAWINGS">FIG. 9</figref>. In storage system <b>220</b>, each single mass storage device is coupled to more than one interface-cache pair. By way of example, a single mass storage device <b>222</b> is coupled to three interface-cache pairs <b>224</b>, <b>226</b>, and <b>228</b>, and a single mass storage device <b>230</b> is coupled to two interface-cache pairs <b>232</b> and <b>234</b>. Each interface-cache pair <b>224</b>, <b>226</b>, <b>228</b>, <b>232</b>, and <b>234</b> is substantially the same as interface-cache pair <b>192</b>, described above, and is typically housed in a respective housing <b>194</b>.</p>
<p id="p-0177" num="0176">Single mass storage device <b>222</b> comprises a single physical device which is divided into three logical partitions <b>236</b>, <b>238</b> and <b>240</b> which communicate respectively with interface-cache pairs <b>224</b>, <b>226</b>, and <b>228</b>.</p>
<p id="p-0178" num="0177">Single mass storage device <b>230</b> comprises a single physical unit having one logical partition <b>242</b>, and both interface-cache pairs <b>232</b> and <b>234</b> communicate with the one partition. System manager <b>54</b> and/or central processing units within the interface-cache pairs are implemented to track input/output requests to logical partition <b>242</b> so as to avoid conflicts. The implementation is typically in a dual-write format.</p>
<p id="p-0179" num="0178">In the caches attached to device <b>222</b> and to device <b>230</b>, the tracks assigned to a specific cache correspond to those of the logical partition with which the cache communicates.</p>
<p id="p-0180" num="0179">In configurations such as that exemplified by system <b>220</b>, connecting more than one interface-cache combination to a single mass storage device provides all the connected interfaces with the ability to quickly access the single mass storage device. Such a configuration extends the local storage system advantages of system <b>210</b>, so that multiple interfaces, each with a respective cache, may operate in a local mode. The overall ability of any interface-cache combination to respond to any IO request is maintained by the coupling between the caches.</p>
<p id="p-0181" num="0180"><figref idref="DRAWINGS">FIG. 12</figref> is a schematic block diagram of a storage system <b>250</b>, according to an embodiment of the present invention. Storage system <b>250</b> comprises interface-cache pairs coupled by communication channel <b>200</b>, as described above with reference to <figref idref="DRAWINGS">FIG. 9</figref>. In storage system <b>250</b>, each interface-cache pair may be coupled to one or more single mass storage devices. By way of example, an interface-cache pair <b>252</b> is coupled to a single mass storage device <b>254</b>, an interface-cache pair <b>256</b> is coupled to three single mass storage devices <b>258</b>, <b>260</b>, and <b>262</b>, and an interface-cache pair <b>264</b> is coupled to two single mass storage devices <b>266</b> and <b>268</b>.</p>
<p id="p-0182" num="0181">The tracks assigned to the cache of interface-cache pair <b>252</b> correspond to those of single mass storage device <b>254</b>. The cache of interface-cache pair <b>256</b> comprises a local track location table <b>270</b>, and the cache of interface-cache pair <b>264</b> comprises a local track location table <b>272</b>. Tables <b>270</b> and <b>272</b> are generally similar to track location tables <b>21</b> described above. Table <b>270</b> gives exact locations for tracks of single mass storage devices <b>258</b>, <b>260</b>, and <b>262</b>; table <b>272</b> gives exact locations for tracks of single mass storage devices <b>266</b> and <b>268</b>.</p>
<p id="p-0183" num="0182">Configurations such as system <b>250</b> provide the advantages described above, with reference to systems <b>190</b> and <b>210</b>, for the interface-cache combination. In addition, providing each cache with the ability to be connected to more than one mass storage device increases the flexibility of the system, such as by enabling local redundancy for the devices coupled to the specific cache.</p>
<p id="p-0184" num="0183"><figref idref="DRAWINGS">FIG. 13</figref> is a schematic block diagram of a storage system <b>280</b>, according to an embodiment of the present invention. Except as described hereinbelow, system <b>280</b> is generally configured and operates as system <b>150</b> (<figref idref="DRAWINGS">FIG. 7</figref>). System <b>280</b> comprises interfaces <b>282</b>, which differ from interfaces <b>26</b> in not having a track-cache mapping <b>28</b>. Rather, interfaces <b>282</b> are configured to receive IO requests from hosts <b>52</b>, and to convey the requests to all caches <b>20</b> coupled to communication channel <b>14</b>.</p>
<p id="p-0185" num="0184">Each cache <b>20</b> comprises a respective track listing <b>284</b>, specific to the cache. The cache is implemented to respond to track requests for tracks in its listing, and to ignore track requests not in its listing. It will be understood that track listings <b>284</b> derive from track-cache mapping <b>28</b>, which in system <b>280</b> acts as a virtual mapping. For example, if track-cache mapping <b>28</b> corresponds to mapping (4) or Table I above, then Table VII below shows the track listings <b>284</b> of cache <b>0</b> (Ca<b>0</b>) and cache <b>1</b> (Ca<b>1</b>).</p>
<p id="p-0186" num="0185">
<tables id="TABLE-US-00007" num="00007">
<table frame="none" colsep="0" rowsep="0">
<tgroup align="left" colsep="0" rowsep="0" cols="5">
<colspec colname="offset" colwidth="21pt" align="left"/>
<colspec colname="1" colwidth="77pt" align="center"/>
<colspec colname="2" colwidth="14pt" align="left"/>
<colspec colname="3" colwidth="84pt" align="center"/>
<colspec colname="4" colwidth="21pt" align="left"/>
<thead>
<row>
<entry/>
<entry namest="offset" nameend="4" rowsep="1">TABLE VII</entry>
</row>
</thead>
<tbody valign="top">
<row>
<entry/>
<entry namest="offset" nameend="4" align="center" rowsep="1"/>
</row>
<row>
<entry/>
<entry>Cache 0</entry>
<entry/>
<entry>Cache 1</entry>
<entry/>
</row>
<row>
<entry/>
<entry>Track Listing</entry>
<entry/>
<entry>Track Listing</entry>
</row>
</tbody>
</tgroup>
<tgroup align="left" colsep="0" rowsep="0" cols="5">
<colspec colname="offset" colwidth="21pt" align="left"/>
<colspec colname="1" colwidth="35pt" align="center"/>
<colspec colname="2" colwidth="56pt" align="center"/>
<colspec colname="3" colwidth="35pt" align="center"/>
<colspec colname="4" colwidth="70pt" align="center"/>
<tbody valign="top">
<row>
<entry/>
<entry>L</entry>
<entry>n</entry>
<entry>L</entry>
<entry>n</entry>
</row>
<row>
<entry/>
<entry>(LU</entry>
<entry>(Track</entry>
<entry>(LU</entry>
<entry>(Track</entry>
</row>
<row>
<entry/>
<entry>identifier)</entry>
<entry>number)</entry>
<entry>identifier)</entry>
<entry>number)</entry>
</row>
<row>
<entry/>
<entry namest="offset" nameend="4" align="center" rowsep="1"/>
</row>
<row>
<entry/>
<entry>0</entry>
<entry> 0</entry>
<entry>0</entry>
<entry> 1</entry>
</row>
<row>
<entry/>
<entry>0</entry>
<entry>16</entry>
<entry>0</entry>
<entry>17</entry>
</row>
<row>
<entry/>
<entry>. . .</entry>
<entry>. . .</entry>
<entry>. . .</entry>
<entry>. . .</entry>
</row>
<row>
<entry/>
<entry>1</entry>
<entry> 0</entry>
<entry>1</entry>
<entry> 1</entry>
</row>
<row>
<entry/>
<entry>1</entry>
<entry>16</entry>
<entry>1</entry>
<entry>17</entry>
</row>
<row>
<entry/>
<entry>. . .</entry>
<entry>. . .</entry>
<entry>. . .</entry>
<entry>. . .</entry>
</row>
<row>
<entry/>
<entry namest="offset" nameend="4" align="center" rowsep="1"/>
</row>
</tbody>
</tgroup>
</table>
</tables>
</p>
<p id="p-0187" num="0186">Caches <b>20</b> in system <b>280</b> operate in a dual-write configuration, herein by way of example assumed to be a cyclic dual-write system wherein Cache <b>0</b> writes to Cache <b>1</b>, Cache <b>1</b> writes to Cache <b>2</b>, . . . , Cache n writes to cache <b>0</b>. Thus, in the event of a failure of Cache <b>1</b> during processing of an IO request to a track of the cache, Cache <b>2</b> completes processing the IO request. It will be appreciated that system <b>280</b> may incorporate other types of dual-write system known in the art, such as having caches paired with each other, or having each cache “dual-writing” to more than one other cache. It will also be understood that in operating in a dual-write configuration, each specific cache <b>20</b> operates substantially independently of all other caches <b>20</b>, other than the caches to which it is coupled in its dual-write configuration.</p>
<p id="p-0188" num="0187">At least some of caches <b>20</b> and their associated one or more storage devices <b>152</b> are housed in single housings, typically as off-the-shelf personal computers. By way of example, cache <b>0</b> and its associated storage device <b>152</b> are implemented from a personal computer <b>286</b>, and cache <b>3</b> and its associated storage devices are implemented from a personal computer <b>288</b>.</p>
<p id="p-0189" num="0188">Not requiring track-cache mapping <b>28</b> in interfaces <b>282</b> reduces the memory needed for the interfaces. In addition, using track listings <b>284</b> rather than the track-cache mapping <b>28</b> reduces the memory required by each specific cache.</p>
<p id="p-0190" num="0189"><figref idref="DRAWINGS">FIG. 14</figref> is a schematic block diagram of a storage system <b>300</b>, according to an embodiment of the present invention. Except as described hereinbelow, system <b>300</b> is generally configured and operates as system <b>190</b> (<figref idref="DRAWINGS">FIG. 9</figref>). System <b>300</b> comprises interfaces <b>302</b>, which differ from interfaces <b>26</b> in not having track-cache mapping <b>28</b>. Rather, interfaces <b>302</b> are configured to receive IO requests from hosts <b>52</b>, and to convey the requests to the cache <b>20</b> to which they are coupled. Each cache <b>20</b>, in addition to its track location table <b>196</b>, comprises a respective track-cache mapping <b>28</b>.</p>
<p id="p-0191" num="0190">As for system <b>190</b>, each IO request received by an interface is conveyed to the cache coupled to the interface. According to the track to which the request is directed, the cache then transfers the IO request to another cache on channel <b>200</b> using its track-cache mapping <b>28</b>, or, if required, uses its track location table <b>196</b> to convey the IO request to the appropriate storage device <b>198</b>.</p>
<p id="p-0192" num="0191">Caches <b>20</b> in system <b>300</b> preferably operate in a dual-write configuration, such as the cyclically configured dual-write system described above with reference to <figref idref="DRAWINGS">FIG. 13</figref>. Also, at least some of interfaces <b>302</b> and their associated caches <b>152</b> are housed in single housings, typically as off-the-shelf personal computers. By way of example, a first interface <b>302</b> and its associated cache <b>20</b> are implemented from a personal computer <b>304</b>, and a second interface <b>302</b> and its associated cache <b>20</b> are implemented from a personal computer <b>306</b>.</p>
<p id="p-0193" num="0192">It will be appreciated that system <b>300</b> has generally similar advantages to those described above for system <b>190</b>, with the added advantage of including a dual-write system.</p>
<p id="p-0194" num="0193">It will be understood that features described above for specific storage systems, such as track listings <b>284</b> in system <b>280</b>, incorporation of track-cache mappings <b>28</b> into caches <b>20</b> in system <b>300</b>, and use of one or more personal computers to implement interfaces, caches, and/or mass storage devices, typically in a single housing, may be advantageously implemented in other storage systems not specifically described above. It will also be understood that a storage system may be implemented from combinations of systems, and parts of those systems, described above, such as configuring a storage system to partly comprise elements of system <b>150</b> and elements of system <b>250</b>.</p>
<p id="p-0195" num="0194">It will thus be appreciated that the embodiments described above are cited by way of example, and that the present invention is not limited to what has been particularly shown and described hereinabove. Rather, the scope of the present invention includes both combinations and subcombinations of the various features described hereinabove, as well as variations and modifications thereof which would occur to persons skilled in the art upon reading the foregoing description and which are not disclosed in the prior art.</p>
<?DETDESC description="Detailed Description" end="tail"?>
</description>
<us-math idrefs="MATH-US-00001" nb-file="US07299334-20071120-M00001.NB">
<img id="EMI-M00001" he="7.45mm" wi="76.20mm" file="US07299334-20071120-M00001.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-claim-statement>We claim:</us-claim-statement>
<claims id="claims">
<claim id="CLM-00001" num="00001">
<claim-text>1. A storage system, comprising:
<claim-text>one or more mass storage devices, coupled to store data at respective first ranges of logical addresses (LAs);</claim-text>
<claim-text>one or more interfaces, which are adapted to receive input/output (IO) requests from host processors directed to specified LAs; and</claim-text>
<claim-text>a plurality of caches coupled to the one or more interfaces so as to receive the IO requests therefrom, each cache being assigned a respective second range of the LAs and being coupled to the one or more mass storage devices, the respective first ranges of which overlap the respective second ranges, so as to receive data from and provide data to the one or more mass storage devices, and being coupled to accept the IO requests within the respective second range directed thereto.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00002" num="00002">
<claim-text>2. The storage system according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the one or more mass storage devices comprise a plurality of mass storage devices, and wherein each cache is directly connected to one or more of the plurality of mass storage devices.</claim-text>
</claim>
<claim id="CLM-00003" num="00003">
<claim-text>3. The storage system according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the one or more interfaces are adapted to direct the IO requests to all of the plurality of caches.</claim-text>
</claim>
<claim id="CLM-00004" num="00004">
<claim-text>4. The storage system according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the one or more interfaces comprise a mapping between the second ranges of each of the caches and the LAs and are adapted to convert the IO requests to one or mow requests and to direct the one or more requests to respective one or more caches in response to the mapping.</claim-text>
</claim>
<claim id="CLM-00005" num="00005">
<claim-text>5. The storage system according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein each cache comprises a listing of LAs corresponding to the second range of the each cache, and wherein the each cache is adapted to ignore IO requests directed to LAs not comprised in the listing.</claim-text>
</claim>
<claim id="CLM-00006" num="00006">
<claim-text>6. The storage system according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the plurality of caches comprises a first cache and a second cache, and wherein the first cache is coupled to write an IO request directed to the first cache to the second cache.</claim-text>
</claim>
<claim id="CLM-00007" num="00007">
<claim-text>7. The storage system according to <claim-ref idref="CLM-00006">claim 6</claim-ref>, wherein the plurality of caches further comprises one or more third caches which are adapted to operate substantially independently of the first and second caches.</claim-text>
</claim>
<claim id="CLM-00008" num="00008">
<claim-text>8. The storage system according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein each of the plurality of caches is adapted to operate substantially independently of remaining caches comprised in the plurality.</claim-text>
</claim>
<claim id="CLM-00009" num="00009">
<claim-text>9. The storage system according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein each of the plurality of caches are at an equal hierarchical level.</claim-text>
</claim>
<claim id="CLM-00010" num="00010">
<claim-text>10. The storage system according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein all of the LAs of the second ranges comprise all of the LAs of the one or more mass storage devices.</claim-text>
</claim>
<claim id="CLM-00011" num="00011">
<claim-text>11. The storage system according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein one or more of the one or more mass storage devices, the one or more interfaces, and the plurality of caches, are implemented from an industrially available personal computer.</claim-text>
</claim>
<claim id="CLM-00012" num="00012">
<claim-text>12. The storage system according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein one or more of the one or more mass storage devices, the one or more interfaces, and the plurality of caches, are housed in a single housing.</claim-text>
</claim>
<claim id="CLM-00013" num="00013">
<claim-text>13. A storage system, comprising:
<claim-text>one or more mass storage devices, coupled to store data at respective first ranges of logical addresses (LAs);</claim-text>
<claim-text>a plurality of caches, each cache being assigned a respective second range of the LAs and being directly connected to one or more of the mass storage devices, the respective first ranges of which overlap the respective second ranges, so as to receive data from and provide data to the one or more mass storage devices;</claim-text>
<claim-text>one or more interfaces, which are adapted to receive input/output (IO) requests from host processors directed to specified LAs and to direct all the IO requests to the cache to which the specified LAs are assigned; and</claim-text>
<claim-text>a communication channel to which the one or more interfaces and a second plurality of caches of the plurality of caches are connected, and which is adapted to convey the data and the IO requests therebetween.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00014" num="00014">
<claim-text>14. The storage system according to <claim-ref idref="CLM-00013">claim 13</claim-ref>, wherein the one or more interfaces comprise a mapping between the second ranges of each of the caches and the LAs and are adapted to convert the IO requests to one or more requests and to direct the one or more requests to respective one or more caches in response to the mapping.</claim-text>
</claim>
<claim id="CLM-00015" num="00015">
<claim-text>15. The storage system according to <claim-ref idref="CLM-00013">claim 13</claim-ref>, wherein one of the caches is coupled to two or more mass storage devices and comprises a location table providing locations of the second range of the LAs assigned to the one cache in the two or more mass storage devices.</claim-text>
</claim>
<claim id="CLM-00016" num="00016">
<claim-text>16. The storage system according to <claim-ref idref="CLM-00013">claim 13</claim-ref>, wherein the plurality of caches comprises two or more caches, and wherein the two or more caches are directly connected to one of the mass storage devices, the first range of which overlaps each of the respective second ranges of the two or more caches, so as to receive data from and provide data to the one mass storage device.</claim-text>
</claim>
<claim id="CLM-00017" num="00017">
<claim-text>17. A storage system, comprising:
<claim-text>one or more mass storage devices, coupled to store data at respective first ranges of logical addresses (LAs);</claim-text>
<claim-text>a plurality of caches, each cache being assigned a respective second range of the LAs and being coupled to the one or more mass storage devices, the respective first ranges of which overlap the respective second ranges, so as to receive data from and provide data to the one or more mass storage devices; and</claim-text>
<claim-text>a plurality of interfaces, each interface being directly connected to a respective cache and being adapted to receive input/output (IO) requests from host processors directed to specified LAs and to direct all the IO requests to the cache to which the specified LAs are assigned.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00018" num="00018">
<claim-text>18. The storage system according to <claim-ref idref="CLM-00017">claim 17</claim-ref>, and comprising a communication channel to which the one or more mass storage devices and the plurality of caches are connected, and which is adapted to convey data and the IO requests therebetween.</claim-text>
</claim>
<claim id="CLM-00019" num="00019">
<claim-text>19. The storage system according to <claim-ref idref="CLM-00017">claim 17</claim-ref>, wherein each interface comprises a mapping between the second ranges of each of the caches and the LAs and is adapted to convert the IO requests to one or more requests and to direct the one or more requests to respective one or more of the caches in response to the mapping.</claim-text>
</claim>
<claim id="CLM-00020" num="00020">
<claim-text>20. The storage system according to <claim-ref idref="CLM-00017">claim 17</claim-ref>, wherein one of the plurality of caches and one of the interfaces are housed in a single housing.</claim-text>
</claim>
<claim id="CLM-00021" num="00021">
<claim-text>21. The storage system according to <claim-ref idref="CLM-00017">claim 17</claim-ref>, wherein the one or more mass storage devices comprises a plurality of mass storage devices, and wherein each of the plurality of mass storage devices is directly connected to a respective cache.</claim-text>
</claim>
<claim id="CLM-00022" num="00022">
<claim-text>22. The storage system according to <claim-ref idref="CLM-00021">claim 21</claim-ref>, and comprising a plurality of single housings which respectively house a respective interface, a respective cache, and a respective mass storage device.</claim-text>
</claim>
<claim id="CLM-00023" num="00023">
<claim-text>23. The storage system according to <claim-ref idref="CLM-00017">claim 17</claim-ref>, wherein the one or more mass storage devices comprises a multiplicity of mass storage devices, and wherein two or more caches of the plurality of caches are directly coupled to one of the mass storage devices.</claim-text>
</claim>
<claim id="CLM-00024" num="00024">
<claim-text>24. The storage system according to <claim-ref idref="CLM-00023">claim 23</claim-ref>, wherein one of the caches of the two or more caches and one of the interfaces are housed in a single housing.</claim-text>
</claim>
<claim id="CLM-00025" num="00025">
<claim-text>25. The storage system according to <claim-ref idref="CLM-00017">claim 17</claim-ref>, wherein the one or more storage devices comprise a multiplicity of mass storage devices, and wherein each of the caches is directly connected to one or more of the multiplicity of mass storage devices.</claim-text>
</claim>
<claim id="CLM-00026" num="00026">
<claim-text>26. The storage system according to <claim-ref idref="CLM-00025">claim 25</claim-ref>, wherein one of the caches is coupled to two or more mass storage devices and comprises a location table providing locations in the two or more mass storage devices of the second range of the LAs assigned to the one cache.</claim-text>
</claim>
<claim id="CLM-00027" num="00027">
<claim-text>27. A storage system, comprising: one or more mass storage devices, coupled to store data at respective first ranges of logical addresses (LAs); a plurality of caches, each cache being assigned a respective second range of the LAs so that the LAs of all the respective second ranges comprise the LAs of all the respective first ranges; a first communication channel to which the one or more mass storage devices and the plurality of caches are connected, and which is adapted to convey data and input/output (IO) requests therebetween; one or more interfaces, which are adapted to receive the IO requests from host processors directed to specified LAs and to direct all the IO requests to the cache to which the specified LAs are assigned; and a second communication channel to which the one or more interfaces and the plurality of caches are connected, and which is adapted to convey the data and the IO requests therebetween.</claim-text>
</claim>
<claim id="CLM-00028" num="00028">
<claim-text>28. The storage system according to <claim-ref idref="CLM-00027">claim 27</claim-ref>, wherein the one or more interfaces comprise a mapping between the second ranges of the caches and the LAs, and wherein the one or more interfaces are adapted to convert the IO requests to one or more requests and to direct the one or more requests to respective one or more of the caches in response to the mapping.</claim-text>
</claim>
<claim id="CLM-00029" num="00029">
<claim-text>29. The storage system according to <claim-ref idref="CLM-00027">claim 27</claim-ref>, wherein the plurality of caches comprise respective location tables, wherein each location table comprises locations of the second range of the LAs assigned to the respective cache in the one or more mass storage devices.</claim-text>
</claim>
<claim id="CLM-00030" num="00030">
<claim-text>30. A storage system, comprising: a plurality of mass storage devices, coupled to store data at respective first ranges of logical addresses (LAs); a plurality of caches, configured to operate independently of one another, each cache being directly connected to a respective mass storage device so as to receive data from and provide data to the respective mass storage device, and being assigned the respective range of LAs of the respective mass storage device; one or more interfaces, which are adapted to receive input/output (IO) requests from host processors directed to specified LAs and to direct all the IO requests to the cache to which the specified LAs are assigned; and a communication channel to which the one or more interfaces and the plurality of caches are connected, and which is adapted to convey data and the IO requests therebetween.</claim-text>
</claim>
<claim id="CLM-00031" num="00031">
<claim-text>31. The storage system according to <claim-ref idref="CLM-00030">claim 30</claim-ref>, wherein the one or more interfaces comprise a mapping between the plurality of caches and the LAs, and wherein the one or more interfaces are adapted to convert the IO requests to one or more requests and to direct the one or more requests to respective one or more of the caches in response to the mapping.</claim-text>
</claim>
<claim id="CLM-00032" num="00032">
<claim-text>32. A network attached storage (NAS) system, comprising:
<claim-text>one or more mass storage devices, coupled to store file-based data at respective first ranges of logical addresses (LAs);</claim-text>
<claim-text>a plurality of caches, each cache being assigned a respective second range of the LAs so that the LAs of all the respective second ranges comprise the LAs of all the respective first ranges, the caches being coupled to receive file-based data from and provide file-based data to the one or more mass storage devices having LAs within the respective second ranges; and</claim-text>
<claim-text>one or more interfaces, which are adapted to receive file-based input/output (IO) requests from host processors directed to specified LAs and to direct all the file-based IO requests to the caches to which the specified LAs are assigned.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00033" num="00033">
<claim-text>33. The NAS system according to <claim-ref idref="CLM-00032">claim 32</claim-ref>, wherein the one or more interfaces comprise a file-based mapping between the plurality of caches and the LAs, and wherein the one or more interfaces are adapted to convert the file-based IO requests to one or more file-based requests and to direct the one or more file-based requests to respective one or more of the plurality of caches in response to the file-based mapping.</claim-text>
</claim>
<claim id="CLM-00034" num="00034">
<claim-text>34. A storage area network (SAN) system, comprising:
<claim-text>one or more mass storage devices, coupled to store block-based data at respective first ranges of logical addresses (LAs);</claim-text>
<claim-text>a plurality of caches, each cache being assigned a respective second range of the LAs so that the LAs of all the respective second ranges comprise the LAs of all the respective first ranges, the caches being coupled to receive block-based data from and provide block-based data to the one or more mass storage devices having LAs within the respective second range; and</claim-text>
<claim-text>one or more interfaces, which are adapted to receive block-based input/output (IO) requests from host processors directed to specified LAs and to direct all the block-based IO requests to the caches to which the specified LAs are assigned.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00035" num="00035">
<claim-text>35. The SAN system according to <claim-ref idref="CLM-00034">claim 34</claim-ref>, wherein the one or more interfaces comprise a block-based mapping between the plurality of caches and the LAs, and wherein the one or more interfaces are adapted to convert the block-based IO requests to one or more block-based requests anti to direct the one or more block-based requests to respective one or more of the plurality of caches in response to the block-based mapping.</claim-text>
</claim>
<claim id="CLM-00036" num="00036">
<claim-text>36. A method for storing data, comprising:
<claim-text>coupling one or more mass storage devices to store data at respective first ranges of logical addresses (LAs);</claim-text>
<claim-text>receiving in one or more interfaces input/output (IO) requests from host processors directed to specified LAs; and</claim-text>
<claim-text>coupling a plurality of caches to the one or more interfaces so as to receive the IO requests therefrom, each cache being assigned a respective second range of the LAs and being coupled to the one or more mass storage devices, the respective first ranges of which overlap the respective second ranges, so as to receive data from and provide data to the one or more mass storage devices, and being coupled to accept the IO requests within the respective second range directed thereto.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00037" num="00037">
<claim-text>37. A method for storing data in a network attached storage (NAS) system, comprising:
<claim-text>coupling one or more mass storage devices to store file-based data at respective first ranges of logical addresses (LAs);</claim-text>
<claim-text>assigning each of a plurality of caches a respective second range of the LAs so that the LAs of all the respective second ranges comprise the LAs of all the respective first ranges;</claim-text>
<claim-text>coupling the caches to receive the file-based data from and provide the file-based data to the one or more mass storage devices having LAs within the respective second range;</claim-text>
<claim-text>receiving file-based input/output (IO) requests from host processors directed to specified LAs; and</claim-text>
<claim-text>directing the file-based IO requests to the caches to which the specified LAs are assigned.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00038" num="00038">
<claim-text>38. A method for storing data in a storage area network (SAN), comprising:
<claim-text>coupling one or more mass storage devices to store block-based data at respective first ranges of logical addresses (LAs);</claim-text>
<claim-text>assigning each of a plurality of caches a respective second range of the LAs so that the LAs of all the respective second ranges comprise the LAs of all the respective first ranges;</claim-text>
<claim-text>coupling the caches to receive the block-based data from and provide the block-based data to the one or more mass storage devices having LAs within the respective second range;</claim-text>
<claim-text>receiving block-based input/output (IO) requests from host processors directed to specified LAs; and</claim-text>
<claim-text>directing the block-based IO requests to the caches to which the specified LAs are assigned.</claim-text>
</claim-text>
</claim>
</claims>
</us-patent-grant>
