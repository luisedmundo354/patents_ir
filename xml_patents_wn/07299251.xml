<us-patent-grant lang="EN" dtd-version="v4.2 2006-08-23" file="US07299251-20071120.XML" status="PRODUCTION" id="us-patent-grant" country="US" date-produced="20071106" date-publ="20071120">
<us-bibliographic-data-grant>
<publication-reference>
<document-id>
<country>US</country>
<doc-number>07299251</doc-number>
<kind>B2</kind>
<date>20071120</date>
</document-id>
</publication-reference>
<application-reference appl-type="utility">
<document-id>
<country>US</country>
<doc-number>10399848</doc-number>
<date>20011025</date>
</document-id>
</application-reference>
<us-application-series-code>10</us-application-series-code>
<priority-claims>
<priority-claim sequence="01" kind="national">
<country>GB</country>
<doc-number>0027238.5</doc-number>
<date>20001108</date>
</priority-claim>
</priority-claims>
<classifications-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>G</section>
<class>06</class>
<subclass>F</subclass>
<main-group>17</main-group>
<subgroup>10</subgroup>
<symbol-position>F</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20071120</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
</classifications-ipcr>
<classification-national>
<country>US</country>
<main-classification>708322</main-classification>
</classification-national>
<invention-title id="d0e61">Adaptive filter</invention-title>
<references-cited>
<citation>
<patcit num="00001">
<document-id>
<country>US</country>
<doc-number>5353307</doc-number>
<kind>A</kind>
<name>Lester et al.</name>
<date>19941000</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<patcit num="00002">
<document-id>
<country>US</country>
<doc-number>5436928</doc-number>
<kind>A</kind>
<name>Fukuawa et al.</name>
<date>19950700</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>375232</main-classification></classification-national>
</citation>
<citation>
<patcit num="00003">
<document-id>
<country>US</country>
<doc-number>5517239</doc-number>
<kind>A</kind>
<name>Nakayama</name>
<date>19960500</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>3482221</main-classification></classification-national>
</citation>
<citation>
<patcit num="00004">
<document-id>
<country>US</country>
<doc-number>5677951</doc-number>
<kind>A</kind>
<name>Gay</name>
<date>19971000</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>37940608</main-classification></classification-national>
</citation>
<citation>
<patcit num="00005">
<document-id>
<country>US</country>
<doc-number>6445792</doc-number>
<kind>B1</kind>
<name>Shiraki et al.</name>
<date>20020900</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>370286</main-classification></classification-national>
</citation>
<citation>
<nplcit num="00006">
<othercit>Moonen, et al., “Using a lattice algorithm to estimate the Kalman gain vector in fast Newton-type adaptive filtering” <i>Acoustics, Speech, and Signal Processing</i>, pp. 2265-2268 (1997).</othercit>
</nplcit>
<category>cited by other</category>
</citation>
<citation>
<nplcit num="00007">
<othercit>Yuan, J., “QR-Decomposition-Based Least Squares Lattice Interpolators”, <i>IEEE Transaction on Signal Processing</i>, vol. 48, pp. 70-79 (2000).</othercit>
</nplcit>
<category>cited by other</category>
</citation>
<citation>
<nplcit num="00008">
<othercit>Moustakides, et al., “Fast Newton Transversal Filters-A New Class of Adaptive Estimation Algorithms,” <i>IEEE Transactions on Signal Processing</i>, vol. 39, pp. 2184-2193 (1991).</othercit>
</nplcit>
<category>cited by other</category>
</citation>
</references-cited>
<number-of-claims>29</number-of-claims>
<us-exemplary-claim>1</us-exemplary-claim>
<us-field-of-classification-search>
<classification-national>
<country>US</country>
<main-classification>708322</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>708323</main-classification>
</classification-national>
</us-field-of-classification-search>
<figures>
<number-of-drawing-sheets>7</number-of-drawing-sheets>
<number-of-figures>8</number-of-figures>
</figures>
<us-related-documents>
<related-publication>
<document-id>
<country>US</country>
<doc-number>20040071207</doc-number>
<kind>A1</kind>
<date>20040415</date>
</document-id>
</related-publication>
</us-related-documents>
<parties>
<applicants>
<applicant sequence="001" app-type="applicant-inventor" designation="us-only">
<addressbook>
<last-name>Skidmore</last-name>
<first-name>Ian David</first-name>
<address>
<city>Malvern Worcestershire</city>
<country>GB</country>
</address>
</addressbook>
<nationality>
<country>GB</country>
</nationality>
<residence>
<country>GB</country>
</residence>
</applicant>
<applicant sequence="002" app-type="applicant-inventor" designation="us-only">
<addressbook>
<last-name>Proudler</last-name>
<first-name>Ian Keith</first-name>
<address>
<city>Malvern Worcestershire</city>
<country>GB</country>
</address>
</addressbook>
<nationality>
<country>GB</country>
</nationality>
<residence>
<country>GB</country>
</residence>
</applicant>
</applicants>
<agents>
<agent sequence="01" rep-type="attorney">
<addressbook>
<orgname>McDonnell Boehnen Hulbert &amp; Berghoff LLP</orgname>
<address>
<country>unknown</country>
</address>
</addressbook>
</agent>
</agents>
</parties>
<assignees>
<assignee>
<addressbook>
<orgname>Qinetiq Limited</orgname>
<role>03</role>
<address>
<country>GB</country>
</address>
</addressbook>
</assignee>
</assignees>
<examiners>
<primary-examiner>
<last-name>Ngo</last-name>
<first-name>Chuong D.</first-name>
<department>2193</department>
</primary-examiner>
</examiners>
<pct-or-regional-filing-data>
<document-id>
<country>WO</country>
<doc-number>PCT/GB01/04717</doc-number>
<kind>00</kind>
<date>20011025</date>
</document-id>
<us-371c124-date>
<date>20030421</date>
</us-371c124-date>
</pct-or-regional-filing-data>
<pct-or-regional-publishing-data>
<document-id>
<country>WO</country>
<doc-number>WO02/39584</doc-number>
<kind>A </kind>
<date>20020516</date>
</document-id>
</pct-or-regional-publishing-data>
</us-bibliographic-data-grant>
<abstract id="abstract">
<p id="p-0001" num="0000">An adaptive filter is implemented by a computer (<b>10</b>) processing an input signal using a recursive least squares lattice (RLSL) algorithm (<b>12</b>) to obtain forward and backward least squares prediction residuals. A prediction residual is the difference between a data element in a sequence of elements and a prediction of that element from other sequence elements. Forward and backward residuals are converted at (<b>14</b>) to interpolation residuals which are unnormalized Kalman gain vector coefficients. Interpolation residuals are normalized to produce the Kalman gain vector at (<b>16</b>). The Kalman gain vector is combined at (<b>18</b>) with input and reference signals x(t) and y(t), which provides updates for the filter coefficients or weights to reflect these signals as required to provide adaptive filtering.</p>
</abstract>
<drawings id="DRAWINGS">
<figure id="Fig-EMI-D00000" num="00000">
<img id="EMI-D00000" he="168.06mm" wi="155.11mm" file="US07299251-20071120-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00001" num="00001">
<img id="EMI-D00001" he="186.52mm" wi="162.48mm" file="US07299251-20071120-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00002" num="00002">
<img id="EMI-D00002" he="249.34mm" wi="158.75mm" orientation="landscape" file="US07299251-20071120-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00003" num="00003">
<img id="EMI-D00003" he="234.10mm" wi="107.87mm" file="US07299251-20071120-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00004" num="00004">
<img id="EMI-D00004" he="177.72mm" wi="141.65mm" orientation="landscape" file="US07299251-20071120-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00005" num="00005">
<img id="EMI-D00005" he="136.40mm" wi="141.22mm" file="US07299251-20071120-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00006" num="00006">
<img id="EMI-D00006" he="139.62mm" wi="138.60mm" file="US07299251-20071120-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00007" num="00007">
<img id="EMI-D00007" he="234.78mm" wi="158.33mm" file="US07299251-20071120-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
</drawings>
<description id="description">
<?BRFSUM description="Brief Summary" end="lead"?>
<p id="p-0002" num="0001">This invention relates to an adaptive filter and to a method and a computer program for implementing it.</p>
<p id="p-0003" num="0002">Filtering is one of the most common procedures in signal processing. A filter processes a signal or signals at its input to produce an output signal having certain desirable characteristics. The exact form which these characteristics take is determined by the values taken by certain filter parameters. In a linear filter, the output is a weighted linear combination of the input signals and delayed versions thereof as generated by a tapped delay-line. The filter has controllable parameters which are weights used to weight this linear combination.</p>
<p id="p-0004" num="0003">One known form of filter is a time-series filter (or transversal filter, which might be adaptive) with only one input signal. The filter forms a linear combination of delayed versions of the signal. The filter weights may be chosen to select or reject different frequency components of the input signal. Another form of filter is a phased array radar beamformer: in receive mode such a beamformer receives signals from an array of spatially separated sensors, the signals being generated in response to reception of radar waves reflected from a scene. The filter weights may be chosen, for example, to select or reject components of the sensor array signals depending on the radar wave direction of arrival at the sensors: such a beamformer is said to be a spatial filter because of its spatially selective properties. A sonar array beamformer is a related example of a spatial filter: here in receive mode signals from an array of sonar transducers and delayed versions of these signals constitute the input to the filter. By varying the filter weights, the sonar beamformer can select or reject signals based on both direction of arrival and frequency. Radar and sonar beamformers are also operative in a transmit mode in which filter techniques may be used to configure the transmitted beam.</p>
<p id="p-0005" num="0004">A finite impulse response (FIR) filter, or transversal filter, is a known form of filter which may be implemented in hardware as a tapped delay line comprising a series-connected set of clock-activated registers each associated with a respective multiplier and multiplicative coefficient. Increasingly though FIR filters are implemented equivalently in software running on a computer system and accepting data from a sensor after conversion from analogue to digital electronic form. In a delay line version, a sequence of data elements x(i), where i denotes a clock cycle index, is clocked along the line, each register holding one element at a time. The total number m of data elements is very much larger than the number N of registers, so only N successive elements occupy the line at any time. The line is occupied by data elements x(1) to x(N) on the Nth clock cycle. Clocking the line advances the sequence by one register, so that the leading element leaves one end of the line and a new element is introduced at the other. On the pth clock cycle the line is occupied by data elements x(p−N+1) to x(p). On each clock cycle the data element in each register is multiplied by the respective filter coefficient in the associated multiplier in each case (w<sub>i </sub>for the ith register, i=1 to N), and the resulting products are added to form a result which is Z<sub>N</sub>(p) for the pth cycle.I.e.:</p>
<p id="p-0006" num="0005">
<maths id="MATH-US-00001" num="00001">
<math overflow="scroll">
<mtable>
  <mtr>
    <mtd>
      <mrow>
        <mrow>
          <msub>
            <mi>z</mi>
            <mi>N</mi>
          </msub>
          <mo>⁡</mo>
          <mrow>
            <mo>(</mo>
            <mi>p</mi>
            <mo>)</mo>
          </mrow>
        </mrow>
        <mo>=</mo>
        <mrow>
          <munderover>
            <mo>∑</mo>
            <mrow>
              <mi>i</mi>
              <mo>=</mo>
              <mn>1</mn>
            </mrow>
            <mi>N</mi>
          </munderover>
          <mo>⁢</mo>
          <mrow>
            <msub>
              <mi>w</mi>
              <mi>i</mi>
            </msub>
            <mo>⁢</mo>
            <mrow>
              <mi>x</mi>
              <mo>⁡</mo>
              <mrow>
                <mo>(</mo>
                <mrow>
                  <mi>p</mi>
                  <mo>-</mo>
                  <mi>i</mi>
                  <mo>+</mo>
                  <mn>1</mn>
                </mrow>
                <mo>)</mo>
              </mrow>
            </mrow>
          </mrow>
        </mrow>
      </mrow>
    </mtd>
    <mtd>
      <mrow>
        <mo>(</mo>
        <mn>1</mn>
        <mo>)</mo>
      </mrow>
    </mtd>
  </mtr>
</mtable>
</math>
</maths>
</p>
<p id="p-0007" num="0006">Equation (1) may alternatively be expressed as the scalar (dot) product of a weight vector w having elements w<sub>1 </sub>to w<sub>N </sub>(the transversal filter coefficients or weights) and a data vector x<sub>N</sub>(p) having elements x(p) to x(p−N+1):
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>z</i><sub>N</sub>(<i>p</i>)=<i>w</i><sup>T</sup><i>x</i><sub>N</sub>(<i>p</i>)  (2)<?in-line-formulae description="In-line Formulae" end="tail"?>
<br/>
where the superscript index T indicates a vector transpose.
</p>
<p id="p-0008" num="0007">It is a common requirement to obtain meaningful information from a set of data containing unwanted noise or other corrupting components in addition to a useful signal, and it is known to use linear filtering for this to improve signal to noise ratio. However, the choice of filter is difficult if not impossible if noise and signal characteristics are unknown: it then becomes desirable to use an adaptive filter with characteristics which are modifiable in response to data input to the system: in this regard filter adaptivity is important so that the filter properties will be determined from input data to be filtered, instead of a priori from making an estimate or assumption regarding the data characteristics. Filter adaptivity is realised by an adaptive filter with coefficients and therefore also properties which can be changed.</p>
<p id="p-0009" num="0008">Adaptive filters have applications in fields such as radar and sonar as mentioned above, and also in communications and processing of images and speech. They can be used inter alia to perform channel equalisation, signal detection (i.e. matched filtering), acoustic echo cancellation and adaptive noise cancellation.</p>
<p id="p-0010" num="0009">A typical example of an adaptive filter is that of an echo canceller in a ‘hands-free’ telephone system. Here a telephone microphone picks up not only speech from a person making the call, but also that of a recipient of the call whose speech is output by the telephone's loudspeaker. If no corrective action is taken the recipient's speech is also sent to the recipient and appears as an echo. If the recipient is using a hands-free telephone then the reply may be returned to the caller and positive feedback or ‘howl around’ may result.</p>
<p id="p-0011" num="0010">The echo may be removed by an adaptive filter. The telephone's loudspeaker is driven by a speech signal x(t) which becomes an input signal to the filter, and a reference signal y(t) is picked up by the telephone's microphone. The filter coefficients are adjusted adaptively to cancel the echo signal in y(t). Once the echo is removed, the residual signal, which contains only the caller's speech, is transmitted to the telephone call recipient. This residual signal may be termed an error signal e<sub>N</sub>(t), and it is used in adjusting the filter coefficients as will be described later.</p>
<p id="p-0012" num="0011">There has been considerable work on the development of digital signal processing algorithms for adaptive filtering. There are two broad classes of algorithm: block-based algorithms and recursive algorithms. The former are used to process data in finite length blocks and the latter are used for processing a continuous data stream. The choice of algorithm depends on the intended field of use but for many problems recursive algorithms are more attractive.</p>
<p id="p-0013" num="0012">For many years only two types of recursive adaptive filtering algorithms were known: Stochastic Gradient Descent (SGD) algorithms and Recursive Least Squares (RLS) algorithms. More recently recursive adaptive filtering algorithms of a new type have been introduced, and can be viewed as being hybrids of SGD and RLS algorithms.</p>
<p id="p-0014" num="0013">An SGD algorithm (as typified by the well known Least Mean Squares (LMS) algorithm) requires very few mathematical operations per time instant and is robust to finite precision numerical effects, i.e. large fractional errors in small digital quantities; unfortunately the LMS algorithm is, in most cases, very slow to converge to an optimum solution.</p>
<p id="p-0015" num="0014">RLS algorithms on the other hand are very quick to converge. However they require more mathematical operations per unit time, and this can make them completely impractical. For example, a filter for a hands-free telephone would require 4,000,000 floating-point arithmetic operations (flops) in one clock cycle of 100 μsec, equivalent to 4×10<sup>10 </sup>flops/sec, which is not currently achievable using a single computer processor.</p>
<p id="p-0016" num="0015">Furthermore RLS algorithms may exhibit mathematical instability; i.e. they may fail to operate properly due finite precision numerical effects, such as division by small inaccurate quantities: e.g. an eight-bit number may be inaccurate as regards the value of the least significant bit; if all bits of this number of other than the least significant bit are zero, i.e. the number is 00000001, then inaccuracy can lead to it becoming 00000000: the calculation may then involve division by zero.</p>
<p id="p-0017" num="0016">A further consideration is that some RLS algorithms do not calculate the required filter coefficients explicitly but instead related quantities which are less useful for some applications.</p>
<p id="p-0018" num="0017">A new class of hybrid algorithm was introduced in order to address the conflicting problems of convergence speed and computational burden. Examples are the Affine Projection algorithm and the Fast Newton algorithm. Here an LMS-type algorithm and an RLS algorithm work together to solve the adaptive filtering problem. The RLS algorithm is used to solve a problem that is much smaller than the original adaptive filter problem and hence does not present too much additional computational load. Together the LMS and RLS algorithms allow the adaptive filter to converge more quickly than the LMS on its own but usually not as fast as a proper RLS-based adaptive filter. These algorithms suffer from the fact that the RLS component to the algorithm can become numerically unstable.</p>
<p id="p-0019" num="0018">At present there are three classes of RLS algorithms: quadratic RLS algorithms, fast transversal filter algorithms and RLS lattice algorithms. Quadratic RLS algorithms can be numerically stable but they have the disadvantage that, for a problem with N unknown coefficients to be determined, they require a number of mathematical operations per clock cycle that is proportional to N<sup>2</sup>. For applications with large N (e.g. the acoustic echo cancellation problem where typically N=2000) this results in expensive hardware or is even impractical given current technology.</p>
<p id="p-0020" num="0019">Fast transversal filter algorithms and RLS lattice algorithms on the other hand only require a number of mathematical operation per clock cycle that is proportional to N (termed ‘fast’ algorithms). Unfortunately, fast transversal filter algorithms are virtually unusable for large problems, due their numerical instability, i.e. sensitivity to finite precision numerical effects arising from inaccuracies and rounding errors in calculations. Moreover, although it is possible for RLS lattice algorithms to be numerically stable, they do not produce the most desirable parameters (the filter coefficients) directly.</p>
<p id="p-0021" num="0020">In summary, RLS algorithms have optimal performance to the extent that they calculate the optimal set of filter coefficients (or related quantities) at all times. For a single channel problem of order N, i.e. having a number N of quantities to be determined, RLS algorithms fall into three categories:
<ul id="ul0001" list-style="none">
    <li id="ul0001-0001" num="0000">
    <ul id="ul0002" list-style="none">
        <li id="ul0002-0001" num="0021">O(N<sup>2</sup>) algorithms: computationally expensive—prohibitively so for large values of N;</li>
        <li id="ul0002-0002" num="0022">O(N) LS Lattice algorithms: computationally inexpensive (fast) but transversal filter weights are difficult to extract;</li>
        <li id="ul0002-0003" num="0023">O(N) Fast Transversal Filter algorithms: computationally inexpensive (fast) but numerically unstable;
<br/>
here “O” indicates the order of magnitude (N or N<sup>2</sup>) of the number of calculations to be performed per clock cycle as being that in the associated parentheses in each case.
</li>
    </ul>
    </li>
</ul>
</p>
<p id="p-0022" num="0024">Regardless of the field of application, most adaptive filtering applications demand the use of an algorithm with as high a performance but as low a complexity as possible. However, it is a prerequisite that the algorithm be numerically stable at least for all practical intents. Furthermore, many adaptive filtering applications require the actual filter weights themselves, not some other related parameters. In the prior art there is no RLS algorithm for an adaptive filter with these properties when the input to the filter consists of, in part or whole, delayed versions of one or more signals.</p>
<p id="p-0023" num="0025">It is an object of this invention to provide an alternative form of adaptive filter.</p>
<p id="p-0024" num="0026">The present invention provides an adaptive filter characterised in that it is arranged to update filter weights by means of a gain vector derived from interpolation residuals of a sequence of signal samples applied to the filter.</p>
<p id="p-0025" num="0027">In this connection, a ‘Kalman’ gain vector is known in the prior art for the purposes of updating adaptive filter weights or coefficients, although not its derivation from interpolation residuals. An interpolation residual is one kind of prediction residual: a prediction residual is the difference between a directly obtained data element and a prediction of that element extrapolated from other data associated with it. An interpolation residual is obtained using data both preceding and following the data element. There are also forward and backward prediction residuals obtained using forward and backward extrapolation respectively from a directly obtained element.</p>
<p id="p-0026" num="0028">The invention provides a number of advantages: it is an alternative form of adaptive filter; unlike prior art filters based on RLS lattice algorithms, the required filter weights are derived directly, not some other related parameters, and the invention can be implemented with an RLS algorithm or an algorithm with reduced computational load based on a simplifying assumption regarding the characteristics of the signal samples. There are also good theoretical reasons indicating that the invention is expected to provide results that are more numerically robust than alternative approaches.</p>
<p id="p-0027" num="0029">In a preferred embodiment, the present invention also provides an adaptive filter characterised in that it is arranged to:
<ul id="ul0003" list-style="none">
    <li id="ul0003-0001" num="0000">
    <ul id="ul0004" list-style="none">
        <li id="ul0004-0001" num="0030">a) process an input sequence of signal samples to derive prediction residuals;</li>
        <li id="ul0004-0002" num="0031">b) convert prediction residuals to interpolation residuals,</li>
        <li id="ul0004-0003" num="0032">c) derive elements of a gain vector from the interpolation residuals; and</li>
        <li id="ul0004-0004" num="0033">d) combine the gain vector with input and reference signals and update the filter coefficients or weights as required to provide adaptive filtering.</li>
    </ul>
    </li>
</ul>
</p>
<p id="p-0028" num="0034">The gain vector may be a Kalman gain vector. The prediction residuals may be least squares prediction residuals, and these may be obtained by processing a sequence of signal samples using a recursive least squares lattice algorithm.</p>
<p id="p-0029" num="0035">Prediction residuals may be converted to interpolation residuals corresponding to gain vector elements by an iterative approach in which each iteration changes an index (to be defined later) of a residual or of an intermediate quantity derived therefrom. The iterative approach may be a divide and conquer approach implemented by treating prediction residuals as interpolation residuals, iteration being arranged to proceed until the index is changed appropriately to convert the prediction residual to an interpolation residual providing an element of the gain vector in un-normalised form.</p>
<p id="p-0030" num="0036">The iterative approach may be implemented by treating prediction residuals as interpolation residuals with zero values for one of two indices: this corresponds to an absence of succeeding or preceding time series signal samples. The iterations are arranged to proceed until the zero index in each case is changed sufficiently to convert the forward or backward residual to an interpolation residual which is also an element of the gain vector in un-normalised form. It may also involve treating as an intermediate result an iteration in a sequence thereof leading to an element of the un-normalised gain vector, iteration being arranged to proceed until an index of the intermediate result is changed sufficiently to convert such result to an interpolation residual corresponding to an element of the gain vector.</p>
<p id="p-0031" num="0037">For an adaptive filter of order N where N is equal to 2<sup>x </sup>and x is a positive integer, iteration in a sequence to generate an un-normalised element of the gain vector may begin with use of one of two residuals immediately adjacent and on opposite sides of a halfway point, the residual of use being that having a relatively higher value of the index not to be altered in the iteration sequence, and the relevant halfway point being halfway between two quantities:
<ul id="ul0005" list-style="none">
    <li id="ul0005-0001" num="0000">
    <ul id="ul0006" list-style="none">
        <li id="ul0006-0001" num="0038">a) one of which is an interpolation residual and the other a member of the relevant time series for which the gain vector is generated, or</li>
        <li id="ul0006-0002" num="0039">b) one of which is an interpolation residual and the other a starting point for an earlier iteration, or</li>
        <li id="ul0006-0003" num="0040">c) which are respectively starting and end points for an earlier iteration.</li>
    </ul>
    </li>
</ul>
</p>
<p id="p-0032" num="0041">For an adaptive filter of order N where N is not a power of two, the iterative approach may still be used but will involve treating the filter weight vector as a combination of weight vectors of non-equal orders e.g. if N is equal to a sum of integers each of which is a power of two, the filter can be treated as a combination of filters each of order a respective power of two.</p>
<p id="p-0033" num="0042">Iteration in a sequence to generate an un-normalised element of a gain vector may begin with use of one of two residuals immediately adjacent and on opposite sides of a halfway point, the residual of use being that having a relatively higher value of the index not to be altered in the iteration sequence, and the relevant halfway point being halfway between two quantities:
<ul id="ul0007" list-style="none">
    <li id="ul0007-0001" num="0000">
    <ul id="ul0008" list-style="none">
        <li id="ul0008-0001" num="0043">a) one of which is an interpolation residual and the other a member of the relevant time series for which the gain vector is generated, or</li>
        <li id="ul0008-0002" num="0044">b) one of which is an interpolation residual and the other a starting point for an earlier iteration, or</li>
        <li id="ul0008-0003" num="0045">c) which are respectively starting and end points for an earlier iteration.</li>
    </ul>
    </li>
</ul>
</p>
<p id="p-0034" num="0046">The filter of the invention may arranged to convert prediction residuals to interpolation residuals corresponding to elements of a gain vector by a QR decomposition approach in which equations of the form:
<ul id="ul0009" list-style="none">
    <li id="ul0009-0001" num="0000">
    <ul id="ul0010" list-style="none">
        <li id="ul0010-0001" num="0047">a) ζ(i)=ψ(i)+k(i)ξ(i) are solved for ζ(i) and k(i) given ψ(i) and ξ(i) subject to a constraint that in so doing there is minimisation of a sum of squares of ψ(j)+k(i)ξ(j) obtained for different sample indexes j; and</li>
        <li id="ul0010-0002" num="0048">b) ψ(i)=ζ(i)−k(i)ξ(i) are solved for ψ(i) and k(i) given ζ(i) and ξ(i) subject to a constraint that in so doing the value of k(i) that is obtained is substantially that which would be obtained in solving ζ(i)=ψ(i)+k(i)ξ(i) for ζ(i) and k(i).</li>
    </ul>
    </li>
</ul>
</p>
<p id="p-0035" num="0049">QR decomposition may be arranged to employ square root free equivalents of sine and cosine rotation parameters.</p>
<p id="p-0036" num="0050">In an alternative aspect, the present invention provides a method for adaptive filtering characterised in that it includes updating filter weights with a gain vector derived from interpolation residuals of a sequence of signal samples provided as data for filtering.</p>
<p id="p-0037" num="0051">In a preferred embodiment, the method of the invention includes:
<ul id="ul0011" list-style="none">
    <li id="ul0011-0001" num="0000">
    <ul id="ul0012" list-style="none">
        <li id="ul0012-0001" num="0052">a) processing an input sequence of signal samples to derive prediction residuals;</li>
        <li id="ul0012-0002" num="0053">b) converting prediction residuals to interpolation residuals;</li>
        <li id="ul0012-0003" num="0054">c) deriving elements of a gain vector from the interpolation residuals; and</li>
        <li id="ul0012-0004" num="0055">d) combining the gain vector with input and reference signals and update the filter coefficients or weights as required to provide adaptive filtering.</li>
    </ul>
    </li>
</ul>
</p>
<p id="p-0038" num="0056">The prediction residuals may be least squares prediction residuals obtained by processing a sequence of signal samples using a recursive least squares lattice algorithm.</p>
<p id="p-0039" num="0057">Prediction residuals may be converted to interpolation residuals corresponding to gain vector elements by an iterative approach in which each iteration changes an index (to be defined later) of a residual or of an intermediate quantity derived therefrom. The iterative approach may be a divide and conquer approach which treats prediction residuals as interpolation residuals and changes the index appropriately to convert the prediction residual to an interpolation residual providing an element of the gain vector in un-normalised form. It may treat prediction residuals as interpolation residuals with zero values for one of two indices: this corresponds to absence of succeeding or preceding time series signal samples, and changes the zero index in each case sufficiently to convert the forward or backward residual to an interpolation residual which is also an element of the gain vector in un-normalised form</p>
<p id="p-0040" num="0058">The iterative approach may also treat as an intermediate result an iteration in a sequence thereof leading to an element of the gain vector and changes an index of the intermediate result sufficiently to convert such result to an interpolation residual corresponding to an element of the gain vector.</p>
<p id="p-0041" num="0059">The filtering method may implement filtering of order N where N is equal to 2<sup>x </sup>and x is a positive integer, characterised in that it includes iteration in a sequence to generate an un-normalised element of the gain vector, the iteration beginning with use of one of two residuals immediately adjacent and on opposite sides of a halfway point, the residual of use being that having a relatively higher value of an index not to be altered in the iteration sequence, and the relevant halfway point being halfway between two quantities:
<ul id="ul0013" list-style="none">
    <li id="ul0013-0001" num="0000">
    <ul id="ul0014" list-style="none">
        <li id="ul0014-0001" num="0060">a) one of which is an interpolation residual and the other a member of the relevant time series for which the gain vector is generated, or</li>
        <li id="ul0014-0002" num="0061">b) one of which is an interpolation residual and the other a starting point for an earlier iteration, or</li>
        <li id="ul0014-0003" num="0062">c) which are respectively starting and end points for an earlier iteration.</li>
    </ul>
    </li>
</ul>
</p>
<p id="p-0042" num="0063">The filtering method may implement filtering of order N where N is not a power of two, characterised in that the iterative approach involves treating the filter as a combination of weight vectors each of order a respective power of two. It may include iteration in a sequence to generate an un-normalised element of the gain vector, the iteration beginning with use of one of two residuals immediately adjacent and on opposite sides of a halfway point, the residual of use being that having a relatively higher value of an index not to be altered in the iteration sequence, and the relevant halfway point being halfway between two quantities:
<ul id="ul0015" list-style="none">
    <li id="ul0015-0001" num="0000">
    <ul id="ul0016" list-style="none">
        <li id="ul0016-0001" num="0064">a) one of which is an interpolation residual and the other a member of the relevant time series for which the gain vector is generated, or</li>
        <li id="ul0016-0002" num="0065">b) one of which is an interpolation residual and the other a starting point for an earlier iteration, or</li>
        <li id="ul0016-0003" num="0066">c) which are respectively starting and end points for an earlier iteration.</li>
    </ul>
    </li>
</ul>
</p>
<p id="p-0043" num="0067">Prediction residuals may be converted to interpolation residuals corresponding to elements of a gain vector by a QR decomposition approach in which equations of the form:
<ul id="ul0017" list-style="none">
    <li id="ul0017-0001" num="0000">
    <ul id="ul0018" list-style="none">
        <li id="ul0018-0001" num="0068">a) ζ(i)=ψ(i)+k(i)ξ(i) are solved for ζ(i) and k(i) given ψ(i) and ξ(i) subject to a constraint that in so doing there is minimisation of a sum of squares of ψ(j)+k(i)ξ(j) obtained for different sample indexes j; and</li>
        <li id="ul0018-0002" num="0069">b) ψ(i)=ζ(i)−k(i)ξ(i) are solved for ψ(i) and k(i) given ζ(i) and ξ(i) subject to a constraint that in so doing the value of k(i) that is obtained is substantially that which would be obtained in solving ζ(i)=ψ(i)+k(i)ξ(i) for ζ(i) and k(i).</li>
    </ul>
    </li>
</ul>
</p>
<p id="p-0044" num="0070">QR decomposition may employ square root free equivalents of sine and cosine rotation parameters.</p>
<p id="p-0045" num="0071">In a further aspect, the present invention provides a computer program for implementing an adaptive filter characterised in that it is arranged to generate updated filter weights by means of a gain vector derived from interpolation residuals of a sequence of signal samples provided as data for filtering.</p>
<p id="p-0046" num="0072">In a preferred embodiment, the present invention provides a computer program for implementing an adaptive filter characterised in that it is arranged to:
<ul id="ul0019" list-style="none">
    <li id="ul0019-0001" num="0000">
    <ul id="ul0020" list-style="none">
        <li id="ul0020-0001" num="0073">a) process an input sequence of signal samples to derive prediction residuals;</li>
        <li id="ul0020-0002" num="0074">b) convert prediction residuals to interpolation residuals;</li>
        <li id="ul0020-0003" num="0075">c) derive elements of a gain vector from the interpolation residuals; and</li>
        <li id="ul0020-0004" num="0076">d) combine the gain vector with input and reference signals and update the filter coefficients or weights as required to provide adaptive filtering.</li>
    </ul>
    </li>
</ul>
</p>
<p id="p-0047" num="0077">The computer program may be arranged to convert prediction residuals to interpolation residuals corresponding to gain vector elements by an iterative approach in which each iteration changes an index, to be defined later, of a residual or of an intermediate quantity derived therefrom. The iterative approach may be a divide and conquer approach implemented by treating prediction residuals as interpolation residuals, and wherein iteration is arranged to proceed until the index is changed appropriately to convert the prediction residual to an interpolation residual providing an element of the gain vector in un-normalised form. It may be implemented by treating the prediction residuals as interpolation residuals with zero values for one of two indices. This corresponds to absence of succeeding or preceding time series signal samples, and wherein iteration is arranged to proceed until the zero index in each case is changed sufficiently to convert the forward or backward residual to an interpolation residual which is also an element of the gain vector in un-normalised form.</p>
<p id="p-0048" num="0078">The iterative approach may also involve treating as an intermediate result an iteration in a sequence thereof leading to an element of the gain vector, wherein iteration is also arranged to proceed until an index of the intermediate result is changed sufficiently to convert such result to an interpolation residual corresponding to an element of the gain vector.</p>
<p id="p-0049" num="0079">The computer program may implement a adaptive filter of order N where N is equal to 2<sup>x </sup>and x is a positive integer, characterised in that iteration in a sequence to generate an un-normalised element of the gain vector begins with use of one of two residuals immediately adjacent and on opposite sides of a halfway point, the residual of use being that having a relatively higher value of an index not to be altered in the iteration sequence, and the relevant halfway point being halfway between two quantities:
<ul id="ul0021" list-style="none">
    <li id="ul0021-0001" num="0000">
    <ul id="ul0022" list-style="none">
        <li id="ul0022-0001" num="0080">a) one of which is an interpolation residual and the other a member of the relevant time series for which the gain vector is generated, or</li>
        <li id="ul0022-0002" num="0081">b) one of which is an interpolation residual and the other a starting point for an earlier iteration, or</li>
        <li id="ul0022-0003" num="0082">c) which are respectively starting and end points for an earlier iteration.</li>
    </ul>
    </li>
</ul>
</p>
<p id="p-0050" num="0083">The computer program may implement a adaptive filter of order N where N is not a power of two, characterised in that the iterative approach involves treating the weight vector as a combination of weight vectors each of order a respective power of two.</p>
<p id="p-0051" num="0084">Iteration in a sequence to generate an un-normalised element of the gain vector may begin with use of one of two residuals immediately adjacent and on opposite sides of a halfway point, the residual of use being that having a relatively higher value of an index not to be altered in the iteration sequence, and the relevant halfway point being halfway between two quantities:
<ul id="ul0023" list-style="none">
    <li id="ul0023-0001" num="0000">
    <ul id="ul0024" list-style="none">
        <li id="ul0024-0001" num="0085">a) one of which is an interpolation residual and the other a member of the relevant time series for which the gain vector is generated, or</li>
        <li id="ul0024-0002" num="0086">b) one of which is an interpolation residual and the other a starting point for an earlier iteration, or</li>
        <li id="ul0024-0003" num="0087">c) which are respectively starting and end points for an earlier iteration.</li>
    </ul>
    </li>
</ul>
</p>
<p id="p-0052" num="0088">The computer program may be arranged to convert prediction residuals to interpolation residuals corresponding to elements of a gain vector by a QR decomposition approach in which equations of the form:
<ul id="ul0025" list-style="none">
    <li id="ul0025-0001" num="0000">
    <ul id="ul0026" list-style="none">
        <li id="ul0026-0001" num="0089">a) ζ(i)=ψ(i)+k(i)ξ(i) are solved for ζ(i) and k(i) given ψ(i) and ξ(i) subject to a constraint that in so doing there is minimisation of a sum of squares of ψ(j)+k(i)ξ(j) obtained for different sample indexes j; and</li>
        <li id="ul0026-0002" num="0090">b) ψ(i)=ζ(i)−k(i)ξ(i) are solved for ψ(i) and k(i) given ζ(i) and ξ(i) subject to a constraint that in so doing the value of k(i) that is obtained is substantially that which would be obtained in solving ζ(i)=ψ(i)+k(i)ξ(i) for ζ(i) ad ζ(i).</li>
    </ul>
    </li>
</ul>
</p>
<p id="p-0053" num="0091">QR decomposition may be arranged to employ square root free equivalents of sine and cosine rotation parameters.</p>
<?BRFSUM description="Brief Summary" end="tail"?>
<?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?>
<description-of-drawings>
<p id="p-0054" num="0092">In order that the invention might be more fully understood, an embodiment thereof will now be described, by way of example only, with reference to the accompanying drawings, in which:</p>
<p id="p-0055" num="0093"><figref idref="DRAWINGS">FIG. 1</figref> is a block diagram of an adaptive filter of the invention;</p>
<p id="p-0056" num="0094"><figref idref="DRAWINGS">FIG. 2</figref> shows four successive stages of a recursive least squares lattice algorithm indicated in <figref idref="DRAWINGS">FIG. 1</figref>;</p>
<p id="p-0057" num="0095"><figref idref="DRAWINGS">FIGS. 3 and 4</figref> illustrate conversion of a first interpolation residual into a second such residual with a relative change of unity in an index;</p>
<p id="p-0058" num="0096"><figref idref="DRAWINGS">FIGS. 5 to 7</figref> illustrate a “divide and conquer” approach to reducing computational load in a filter of the invention; and</p>
<p id="p-0059" num="0097"><figref idref="DRAWINGS">FIG. 8</figref> illustrates weight updating in a hardware version of the invention.</p>
</description-of-drawings>
<?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?>
<?DETDESC description="Detailed Description" end="lead"?>
<p id="p-0060" num="0098">In those prior art RLS algorithms which are not Lattice algorithms, what is referred to as the ‘Kalman gain’ vector k<sup>N</sup>(t) is used to update N adaptive filter coefficients on receipt of each of successive input signals forming a time series, i.e. a series of data values representing the variation of an input signal x(t) as a function of time t. Here t is in units of an interval between successive digital signal samples (clock cycles), and has integer values 1, 2 etc.</p>
<p id="p-0061" num="0099">Designating the vector of adaptive filter coefficients at times t and t−1 as w<sub>N</sub>(t) and w<sub>N</sub>(t−1), where N is the number of filter coefficients (the order of the filter), then an operation to update filter coefficients adaptively in response to input of a new signal sample is defined by:
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>w</i><sub>N</sub>(<i>t</i>)=<i>w</i><sub>N</sub>(<i>t</i>−1)+<i>e</i><sub>N</sub>(<i>t</i>)<i>k</i><sup>N</sup>(<i>t</i>)  (3)<?in-line-formulae description="In-line Formulae" end="tail"?>
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?>where:<?in-line-formulae description="In-line Formulae" end="tail"?>
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>e</i><sub>N</sub>(<i>t</i>)=<i>y</i>(<i>t</i>)−<i>w</i><sub>N</sub><sup>T</sup>(<i>t</i>−1)<i>x</i><sub>N</sub>(<i>t</i>)  (4)<?in-line-formulae description="In-line Formulae" end="tail"?>
<br/>
and is known as the a priori error, x<sub>N</sub>(t) is a vector with elements consisting of input signal values from x(t) to x(t−N+1) as discussed above, superscript index T indicates a matrix transpose and y(t) is the most recent value of a reference signal detected for use in adaptive filtration as mentioned earlier in the example of a hands-free telephone system: x<sub>N</sub>(t) and y(t) are therefore known quantities. The Kalman gain vector k<sup>N</sup>(t) is conventionally defined as:
</p>
<p id="p-0062" num="0100">
<maths id="MATH-US-00002" num="00002">
<math overflow="scroll">
<mtable>
  <mtr>
    <mtd>
      <mrow>
        <mrow>
          <msup>
            <munder>
              <mi>k</mi>
              <mi>_</mi>
            </munder>
            <mi>N</mi>
          </msup>
          <mo>⁡</mo>
          <mrow>
            <mo>(</mo>
            <mi>t</mi>
            <mo>)</mo>
          </mrow>
        </mrow>
        <mo>=</mo>
        <mrow>
          <mrow>
            <msubsup>
              <mi>M</mi>
              <mi>XX</mi>
              <mrow>
                <mo>-</mo>
                <mn>1</mn>
              </mrow>
            </msubsup>
            <mo>⁡</mo>
            <mrow>
              <mo>(</mo>
              <mi>t</mi>
              <mo>)</mo>
            </mrow>
          </mrow>
          <mo>⁢</mo>
          <mrow>
            <msub>
              <munder>
                <mi>x</mi>
                <mi>_</mi>
              </munder>
              <mi>N</mi>
            </msub>
            <mo>⁡</mo>
            <mrow>
              <mo>(</mo>
              <mi>t</mi>
              <mo>)</mo>
            </mrow>
          </mrow>
        </mrow>
      </mrow>
    </mtd>
    <mtd>
      <mrow>
        <mo>(</mo>
        <mn>5</mn>
        <mo>)</mo>
      </mrow>
    </mtd>
  </mtr>
</mtable>
</math>
</maths>
<br/>
where M<sub>xx </sub>(t) is a covariance matrix of input signals given by:
</p>
<p id="p-0063" num="0101">
<maths id="MATH-US-00003" num="00003">
<math overflow="scroll">
<mtable>
  <mtr>
    <mtd>
      <mrow>
        <mrow>
          <mrow>
            <msub>
              <mi>M</mi>
              <mi>XX</mi>
            </msub>
            <mo>⁡</mo>
            <mrow>
              <mo>(</mo>
              <mi>t</mi>
              <mo>)</mo>
            </mrow>
          </mrow>
          <mo>=</mo>
          <mrow>
            <munderover>
              <mo>∑</mo>
              <mrow>
                <mi>i</mi>
                <mo>=</mo>
                <mn>0</mn>
              </mrow>
              <mrow>
                <msub>
                  <mi>L</mi>
                  <mi>t</mi>
                </msub>
                <mo>-</mo>
                <mn>1</mn>
              </mrow>
            </munderover>
            <mo>⁢</mo>
            <mrow>
              <msubsup>
                <mi>β</mi>
                <mn>1</mn>
                <mrow>
                  <mn>2</mn>
                  <mo>⁢</mo>
                  <mi>i</mi>
                </mrow>
              </msubsup>
              <mo>⁢</mo>
              <mrow>
                <msub>
                  <munder>
                    <mi>x</mi>
                    <mi>_</mi>
                  </munder>
                  <mi>N</mi>
                </msub>
                <mo>⁡</mo>
                <mrow>
                  <mo>(</mo>
                  <mrow>
                    <mi>t</mi>
                    <mo>-</mo>
                    <mi>i</mi>
                  </mrow>
                  <mo>)</mo>
                </mrow>
              </mrow>
              <mo>⁢</mo>
              <mrow>
                <msubsup>
                  <munder>
                    <mi>x</mi>
                    <mi>_</mi>
                  </munder>
                  <mi>N</mi>
                  <mi>T</mi>
                </msubsup>
                <mo>⁡</mo>
                <mrow>
                  <mo>(</mo>
                  <mrow>
                    <mi>t</mi>
                    <mo>-</mo>
                    <mi>i</mi>
                  </mrow>
                  <mo>)</mo>
                </mrow>
              </mrow>
            </mrow>
          </mrow>
        </mrow>
        <mo>,</mo>
      </mrow>
    </mtd>
    <mtd>
      <mrow>
        <mo>(</mo>
        <mn>6</mn>
        <mo>)</mo>
      </mrow>
    </mtd>
  </mtr>
</mtable>
</math>
</maths>
<br/>
x<sub>N</sub>(t−i) is a N-dimensional (column) vector with elements consisting of input signal values from x(t−i) to x(t−i−N+1) as discussed above, and x<sub>N</sub><sup>T</sup>(t−i) is its transpose (row vector). The parameter L<sub>1 </sub>determines the number of samples that are included in the summation. Sometimes L<sub>1 </sub>is chosen to be a fixed number and sometimes it is arranged to vary with time: e.g. L<sub>1</sub>=t+1 is common. The Equation (6) summation could also start from a non-zero value of i if it were decided to discard the most recent data values. The term β<sub>1 </sub>is what is referred to as a “forget factor”: it has a value in the range 0&lt;β<sub>1</sub>≦1, normally 0.99&lt;β<sub>1</sub>&lt;1.0, and it decrements data elements so that the most recent data element x(t) is undecremented, that immediately preceding it is decremented once, the next twice and so on with the nth preceding it decremented n times. The effect of this is to bias the Equation (6) summation in favour of more recent data: i.e. it reduces the value of a data element each time the latter is used so that older data has progressively less influence on the solution compared to more recent data. Use of forget factors in signal processing is known, see e.g. U.S. Pat No. 4,727,503 to McWhirter.
</p>
<p id="p-0064" num="0102">Most RLS algorithms that use the Kalman gain vector calculate it using the above formulation or something related.</p>
<p id="p-0065" num="0103">However, it has been discovered in accordance with the invention that the ith element k<sub>i</sub><sup>N</sup>(t) of the Nth order Kalman gain vector k<sup>N</sup>(t) for a signal at time t is also given by:</p>
<p id="p-0066" num="0104">
<maths id="MATH-US-00004" num="00004">
<math overflow="scroll">
<mtable>
  <mtr>
    <mtd>
      <mrow>
        <mrow>
          <msubsup>
            <mi>k</mi>
            <mi>i</mi>
            <mi>N</mi>
          </msubsup>
          <mo>⁡</mo>
          <mrow>
            <mo>(</mo>
            <mi>t</mi>
            <mo>)</mo>
          </mrow>
        </mrow>
        <mo>=</mo>
        <mfrac>
          <mrow>
            <msub>
              <mi>ɛ</mi>
              <mrow>
                <mrow>
                  <mi>N</mi>
                  <mo>-</mo>
                  <mi>i</mi>
                </mrow>
                <mo>,</mo>
                <mrow>
                  <mi>i</mi>
                  <mo>-</mo>
                  <mn>1</mn>
                </mrow>
              </mrow>
            </msub>
            <mo>⁡</mo>
            <mrow>
              <mo>(</mo>
              <mrow>
                <mi>t</mi>
                <mo>-</mo>
                <mi>i</mi>
                <mo>+</mo>
                <mn>1</mn>
              </mrow>
              <mo>)</mo>
            </mrow>
          </mrow>
          <mrow>
            <msub>
              <mi>E</mi>
              <mrow>
                <mrow>
                  <mi>N</mi>
                  <mo>-</mo>
                  <mi>i</mi>
                </mrow>
                <mo>,</mo>
                <mrow>
                  <mi>i</mi>
                  <mo>-</mo>
                  <mn>1</mn>
                </mrow>
              </mrow>
            </msub>
            <mo>⁡</mo>
            <mrow>
              <mo>(</mo>
              <mrow>
                <mi>t</mi>
                <mo>-</mo>
                <mi>i</mi>
                <mo>+</mo>
                <mn>1</mn>
              </mrow>
              <mo>)</mo>
            </mrow>
          </mrow>
        </mfrac>
      </mrow>
    </mtd>
    <mtd>
      <mrow>
        <mo>(</mo>
        <mn>7</mn>
        <mo>)</mo>
      </mrow>
    </mtd>
  </mtr>
</mtable>
</math>
</maths>
</p>
<p id="p-0067" num="0105">where ε<sub>N−i,i−1</sub>(t−i+1) is what is referred to as an a posteriori least squares interpolation residual whose definition and derivation is described later. E<sub>N−i,i−1</sub>(t−i+1) is a normalisation factor (also described later) corresponding to the power of the interpolation residual ε<sub>N−i,i−1</sub>(t−i+1). Equation (7) indicates that the time parameter (i.e. t−i+1) of the terms on its right hand side depends on the index i of the ith element k<sub>i</sub><sup>N</sup>(t), and therefore the time parameter varies from element to element of the Kalman gain vector k<sup>N</sup>(t).</p>
<p id="p-0068" num="0106">Changing indices for convenience, for a signal element x(t−ƒ) in a sequence comprising a time series x(1) to x(t), a least squares interpolation residual is designated ε<sub>p,ƒ</sub>(t−ƒ): the indices p and ƒindicate that p signal elements before x(t−ƒ) and f signal elements after it in the time series are used in the interpolation: this interpolation residual is derived by subtracting from the element x(t−ƒ) itself an estimated interpolation or prediction of it derived from the signal elements both before (a total of p) and after (a total of f) it in the time series. An interpolation for the signal element x(t−ƒ) is given by a weighted linear combination or summation of p elements before it: x(t−ƒ−p) to x(t−ƒ−1) inclusive, together with a second such combination of the f elements after it: x(t−ƒ+1) to x(t). The least squares interpolation residual for the signal element x(t−ƒ) is then the difference between the element and the interpolation,</p>
<p id="p-0069" num="0107">
<maths id="MATH-US-00005" num="00005">
<math overflow="scroll">
<mtable>
  <mtr>
    <mtd>
      <mrow>
        <mrow>
          <mi>i</mi>
          <mo>.</mo>
          <mi>e</mi>
          <mo>.</mo>
          <mstyle>
            <mspace width="0.8em" height="0.8ex"/>
          </mstyle>
          <mo>⁢</mo>
          <mrow>
            <msub>
              <mi>ɛ</mi>
              <mrow>
                <mi>p</mi>
                <mo>,</mo>
                <mi>f</mi>
              </mrow>
            </msub>
            <mo>⁡</mo>
            <mrow>
              <mo>(</mo>
              <mrow>
                <mi>t</mi>
                <mo>-</mo>
                <mi>f</mi>
              </mrow>
              <mo>)</mo>
            </mrow>
          </mrow>
        </mrow>
        <mo>=</mo>
        <mrow>
          <mrow>
            <mi>x</mi>
            <mo>⁡</mo>
            <mrow>
              <mo>(</mo>
              <mrow>
                <mi>t</mi>
                <mo>-</mo>
                <mi>f</mi>
              </mrow>
              <mo>)</mo>
            </mrow>
          </mrow>
          <mo>-</mo>
          <mrow>
            <munderover>
              <mo>∑</mo>
              <mrow>
                <mi>i</mi>
                <mo>=</mo>
                <mn>1</mn>
              </mrow>
              <mi>f</mi>
            </munderover>
            <mo>⁢</mo>
            <mrow>
              <mrow>
                <msub>
                  <mover>
                    <mi>w</mi>
                    <mo>^</mo>
                  </mover>
                  <mrow>
                    <mi>p</mi>
                    <mo>,</mo>
                    <mi>f</mi>
                    <mo>,</mo>
                    <mi>i</mi>
                  </mrow>
                </msub>
                <mo>⁡</mo>
                <mrow>
                  <mo>(</mo>
                  <mrow>
                    <mi>t</mi>
                    <mo>-</mo>
                    <mi>f</mi>
                  </mrow>
                  <mo>)</mo>
                </mrow>
              </mrow>
              <mo>⁢</mo>
              <mrow>
                <mi>x</mi>
                <mo>⁡</mo>
                <mrow>
                  <mo>(</mo>
                  <mrow>
                    <mi>t</mi>
                    <mo>-</mo>
                    <mi>i</mi>
                    <mo>+</mo>
                    <mn>1</mn>
                  </mrow>
                  <mo>)</mo>
                </mrow>
              </mrow>
            </mrow>
          </mrow>
          <mo>-</mo>
          <mrow>
            <munderover>
              <mo>∑</mo>
              <mrow>
                <mi>i</mi>
                <mo>=</mo>
                <mrow>
                  <mi>f</mi>
                  <mo>+</mo>
                  <mn>1</mn>
                </mrow>
              </mrow>
              <mrow>
                <mi>f</mi>
                <mo>+</mo>
                <mi>p</mi>
              </mrow>
            </munderover>
            <mo>⁢</mo>
            <mrow>
              <mrow>
                <msub>
                  <mover>
                    <mi>w</mi>
                    <mo>^</mo>
                  </mover>
                  <mrow>
                    <mi>p</mi>
                    <mo>,</mo>
                    <mi>f</mi>
                    <mo>,</mo>
                    <mi>i</mi>
                  </mrow>
                </msub>
                <mo>⁡</mo>
                <mrow>
                  <mo>(</mo>
                  <mrow>
                    <mi>t</mi>
                    <mo>-</mo>
                    <mi>f</mi>
                  </mrow>
                  <mo>)</mo>
                </mrow>
              </mrow>
              <mo>⁢</mo>
              <mrow>
                <mi>x</mi>
                <mo>⁡</mo>
                <mrow>
                  <mo>(</mo>
                  <mrow>
                    <mi>t</mi>
                    <mo>-</mo>
                    <mi>i</mi>
                  </mrow>
                  <mo>)</mo>
                </mrow>
              </mrow>
            </mrow>
          </mrow>
        </mrow>
      </mrow>
    </mtd>
    <mtd>
      <mrow>
        <mo>(</mo>
        <mn>8</mn>
        <mo>)</mo>
      </mrow>
    </mtd>
  </mtr>
</mtable>
</math>
</maths>
<br/>
where the vector ŵ<sub>p,ƒ</sub>(t−ƒ) has (p+f) dimensions and contains adjustable coefficients ŵ<sub>p,ƒ,i</sub>(t−ƒ) (i=1 to f+p) and the summation terms are the weighted linear combinations. The reference to “least squares” indicates that these coefficients are those that would be produced by any least squares optimisation procedure. In a preferred embodiment of this invention, these coefficients are never calculated and the interpolation residuals are calculated via an alternative method as will be described later. Nevertheless, the least squares minimisation procedure involves determining coefficients in ŵ<sub>p,ƒ</sub>(t−ƒ) in such a way as to minimise a sum of a preselected number L<sub>2 </sub>of the squares of the residuals determined up to and including the most recent residual, i.e. ŵ<sub>p,ƒ</sub>(t−ƒ) is the value of the vector ω that minimises:
</p>
<p id="p-0070" num="0108">
<maths id="MATH-US-00006" num="00006">
<math overflow="scroll">
<mtable>
  <mtr>
    <mtd>
      <mrow>
        <mrow>
          <msub>
            <mi>J</mi>
            <mrow>
              <mrow>
                <mi>p</mi>
                <mo>.</mo>
              </mrow>
              <mo>,</mo>
              <mi>f</mi>
            </mrow>
          </msub>
          <mo>⁡</mo>
          <mrow>
            <mo>(</mo>
            <mrow>
              <mi>t</mi>
              <mo>-</mo>
              <mi>f</mi>
            </mrow>
            <mo>)</mo>
          </mrow>
        </mrow>
        <mo>=</mo>
        <mrow>
          <munderover>
            <mo>∑</mo>
            <mrow>
              <mi>n</mi>
              <mo>=</mo>
              <mn>0</mn>
            </mrow>
            <mrow>
              <msub>
                <mi>L</mi>
                <mn>2</mn>
              </msub>
              <mo>-</mo>
              <mn>1</mn>
            </mrow>
          </munderover>
          <mo>⁢</mo>
          <msup>
            <mrow>
              <msubsup>
                <mi>β</mi>
                <mn>2</mn>
                <mrow>
                  <mn>2</mn>
                  <mo>⁢</mo>
                  <mi>n</mi>
                </mrow>
              </msubsup>
              <mo>⁡</mo>
              <mrow>
                <mo>(</mo>
                <mrow>
                  <mrow>
                    <mi>x</mi>
                    <mo>⁡</mo>
                    <mrow>
                      <mo>(</mo>
                      <mrow>
                        <mi>t</mi>
                        <mo>-</mo>
                        <mi>n</mi>
                        <mo>-</mo>
                        <mi>f</mi>
                      </mrow>
                      <mo>)</mo>
                    </mrow>
                  </mrow>
                  <mo>-</mo>
                  <mrow>
                    <munderover>
                      <mo>∑</mo>
                      <mrow>
                        <mi>i</mi>
                        <mo>=</mo>
                        <mn>1</mn>
                      </mrow>
                      <mi>f</mi>
                    </munderover>
                    <mo>⁢</mo>
                    <mrow>
                      <msub>
                        <mi>ω</mi>
                        <mi>i</mi>
                      </msub>
                      <mo>⁢</mo>
                      <mrow>
                        <mi>x</mi>
                        <mo>⁡</mo>
                        <mrow>
                          <mo>(</mo>
                          <mrow>
                            <mi>t</mi>
                            <mo>-</mo>
                            <mi>n</mi>
                            <mo>-</mo>
                            <mi>i</mi>
                            <mo>+</mo>
                            <mn>1</mn>
                          </mrow>
                          <mo>)</mo>
                        </mrow>
                      </mrow>
                    </mrow>
                  </mrow>
                  <mo>-</mo>
                  <mrow>
                    <munderover>
                      <mo>∑</mo>
                      <mrow>
                        <mi>i</mi>
                        <mo>=</mo>
                        <mrow>
                          <mi>f</mi>
                          <mo>+</mo>
                          <mn>1</mn>
                        </mrow>
                      </mrow>
                      <mrow>
                        <mi>f</mi>
                        <mo>+</mo>
                        <mi>p</mi>
                      </mrow>
                    </munderover>
                    <mo>⁢</mo>
                    <mrow>
                      <msub>
                        <mi>ω</mi>
                        <mi>i</mi>
                      </msub>
                      <mo>⁢</mo>
                      <mrow>
                        <mi>x</mi>
                        <mo>⁡</mo>
                        <mrow>
                          <mo>(</mo>
                          <mrow>
                            <mi>t</mi>
                            <mo>-</mo>
                            <mi>n</mi>
                            <mo>-</mo>
                            <mi>i</mi>
                          </mrow>
                          <mo>)</mo>
                        </mrow>
                      </mrow>
                    </mrow>
                  </mrow>
                </mrow>
                <mo>)</mo>
              </mrow>
            </mrow>
            <mn>2</mn>
          </msup>
        </mrow>
      </mrow>
    </mtd>
    <mtd>
      <mrow>
        <mo>(</mo>
        <mn>9</mn>
        <mo>)</mo>
      </mrow>
    </mtd>
  </mtr>
</mtable>
</math>
</maths>
</p>
<p id="p-0071" num="0109">As L<sub>1 </sub>above, L<sub>2 </sub>may be chosen to be invariant or may vary with time: β<sub>2 </sub>is a “forget factor” like β<sub>1 </sub>previously discussed.</p>
<p id="p-0072" num="0110">Referring to Equation (7) once more, as has been said E<sub>N−i,i−1</sub>(t−i+1) is a normalisation factor which corresponds to the power of the interpolation residual signal ε<sub>N−i,i−1</sub>(t−i+1): it can be calculated from the interpolation residuals themselves. There are many ways in which this can be done such as by forming a sum of a preselected number L<sub>3 </sub>of the product of the a posteriori residuals and what are referred to as a priori residuals, which are defined below, and are determined up to including the most recent time instant, i.e.:</p>
<p id="p-0073" num="0111">
<maths id="MATH-US-00007" num="00007">
<math overflow="scroll">
<mtable>
  <mtr>
    <mtd>
      <mrow>
        <mrow>
          <msub>
            <mi>E</mi>
            <mrow>
              <mrow>
                <mi>N</mi>
                <mo>-</mo>
                <mi>i</mi>
              </mrow>
              <mo>,</mo>
              <mrow>
                <mi>i</mi>
                <mo>-</mo>
                <mn>1</mn>
              </mrow>
            </mrow>
          </msub>
          <mo>⁡</mo>
          <mrow>
            <mo>(</mo>
            <mrow>
              <mi>t</mi>
              <mo>-</mo>
              <mi>i</mi>
              <mo>+</mo>
              <mn>1</mn>
            </mrow>
            <mo>)</mo>
          </mrow>
        </mrow>
        <mo>=</mo>
        <mrow>
          <munderover>
            <mo>∑</mo>
            <mrow>
              <mi>n</mi>
              <mo>=</mo>
              <mn>0</mn>
            </mrow>
            <mrow>
              <msub>
                <mi>L</mi>
                <mn>3</mn>
              </msub>
              <mo>-</mo>
              <mn>1</mn>
            </mrow>
          </munderover>
          <mo>⁢</mo>
          <mrow>
            <msubsup>
              <mi>β</mi>
              <mn>3</mn>
              <mrow>
                <mn>2</mn>
                <mo>⁢</mo>
                <mi>n</mi>
              </mrow>
            </msubsup>
            <mo>⁢</mo>
            <mrow>
              <msub>
                <mi>e</mi>
                <mrow>
                  <mrow>
                    <mi>N</mi>
                    <mo>-</mo>
                    <mi>i</mi>
                  </mrow>
                  <mo>,</mo>
                  <mrow>
                    <mi>i</mi>
                    <mo>-</mo>
                    <mn>1</mn>
                  </mrow>
                </mrow>
              </msub>
              <mo>⁡</mo>
              <mrow>
                <mo>(</mo>
                <mrow>
                  <mi>t</mi>
                  <mo>-</mo>
                  <mi>n</mi>
                  <mo>-</mo>
                  <mi>i</mi>
                  <mo>+</mo>
                  <mn>1</mn>
                </mrow>
                <mo>)</mo>
              </mrow>
            </mrow>
            <mo>⁢</mo>
            <mrow>
              <msub>
                <mi>ɛ</mi>
                <mrow>
                  <mrow>
                    <mi>N</mi>
                    <mo>-</mo>
                    <mi>i</mi>
                  </mrow>
                  <mo>,</mo>
                  <mrow>
                    <mi>i</mi>
                    <mo>-</mo>
                    <mn>1</mn>
                  </mrow>
                </mrow>
              </msub>
              <mo>⁡</mo>
              <mrow>
                <mo>(</mo>
                <mrow>
                  <mi>t</mi>
                  <mo>-</mo>
                  <mi>n</mi>
                  <mo>-</mo>
                  <mi>i</mi>
                  <mo>+</mo>
                  <mn>1</mn>
                </mrow>
                <mo>)</mo>
              </mrow>
            </mrow>
          </mrow>
        </mrow>
      </mrow>
    </mtd>
    <mtd>
      <mrow>
        <mo>(</mo>
        <mn>10</mn>
        <mo>)</mo>
      </mrow>
    </mtd>
  </mtr>
</mtable>
</math>
</maths>
</p>
<p id="p-0074" num="0112">The term e<sub>N−i,i−1</sub>(t−i+1) is what is referred to as an a priori least squares interpolation and is calculated in a manner similar to the a posteriori residual (Equation (8)) except that the weight vector used is that calculated at the previous time instant i.e.</p>
<p id="p-0075" num="0113">
<maths id="MATH-US-00008" num="00008">
<math overflow="scroll">
<mtable>
  <mtr>
    <mtd>
      <mrow>
        <mrow>
          <msub>
            <mi>e</mi>
            <mrow>
              <mi>p</mi>
              <mo>,</mo>
              <mi>f</mi>
            </mrow>
          </msub>
          <mo>⁡</mo>
          <mrow>
            <mo>(</mo>
            <mrow>
              <mi>t</mi>
              <mo>-</mo>
              <mi>f</mi>
            </mrow>
            <mo>)</mo>
          </mrow>
        </mrow>
        <mo>=</mo>
        <mrow>
          <mrow>
            <mi>x</mi>
            <mo>⁡</mo>
            <mrow>
              <mo>(</mo>
              <mrow>
                <mi>t</mi>
                <mo>-</mo>
                <mi>f</mi>
              </mrow>
              <mo>)</mo>
            </mrow>
          </mrow>
          <mo>-</mo>
          <mrow>
            <munderover>
              <mo>∑</mo>
              <mrow>
                <mi>i</mi>
                <mo>=</mo>
                <mn>1</mn>
              </mrow>
              <mi>f</mi>
            </munderover>
            <mo>⁢</mo>
            <mrow>
              <mrow>
                <msub>
                  <mover>
                    <mi>w</mi>
                    <mo>^</mo>
                  </mover>
                  <mrow>
                    <mi>p</mi>
                    <mo>,</mo>
                    <mi>f</mi>
                    <mo>,</mo>
                    <mi>i</mi>
                  </mrow>
                </msub>
                <mo>⁡</mo>
                <mrow>
                  <mo>(</mo>
                  <mrow>
                    <mi>t</mi>
                    <mo>-</mo>
                    <mi>f</mi>
                    <mo>-</mo>
                    <mn>1</mn>
                  </mrow>
                  <mo>)</mo>
                </mrow>
              </mrow>
              <mo>⁢</mo>
              <mrow>
                <mi>x</mi>
                <mo>⁡</mo>
                <mrow>
                  <mo>(</mo>
                  <mrow>
                    <mi>t</mi>
                    <mo>-</mo>
                    <mi>i</mi>
                    <mo>+</mo>
                    <mn>1</mn>
                  </mrow>
                  <mo>)</mo>
                </mrow>
              </mrow>
            </mrow>
          </mrow>
          <mo>-</mo>
          <mrow>
            <munderover>
              <mo>∑</mo>
              <mrow>
                <mi>i</mi>
                <mo>=</mo>
                <mrow>
                  <mi>f</mi>
                  <mo>+</mo>
                  <mn>1</mn>
                </mrow>
              </mrow>
              <mrow>
                <mi>f</mi>
                <mo>+</mo>
                <mi>p</mi>
              </mrow>
            </munderover>
            <mo>⁢</mo>
            <mrow>
              <mrow>
                <msub>
                  <mover>
                    <mi>w</mi>
                    <mo>^</mo>
                  </mover>
                  <mrow>
                    <mi>p</mi>
                    <mo>,</mo>
                    <mi>f</mi>
                    <mo>,</mo>
                    <mi>i</mi>
                  </mrow>
                </msub>
                <mo>⁡</mo>
                <mrow>
                  <mo>(</mo>
                  <mrow>
                    <mi>t</mi>
                    <mo>-</mo>
                    <mi>f</mi>
                    <mo>-</mo>
                    <mn>1</mn>
                  </mrow>
                  <mo>)</mo>
                </mrow>
              </mrow>
              <mo>⁢</mo>
              <mrow>
                <mi>x</mi>
                <mo>⁡</mo>
                <mrow>
                  <mo>(</mo>
                  <mrow>
                    <mi>t</mi>
                    <mo>-</mo>
                    <mi>i</mi>
                  </mrow>
                  <mo>)</mo>
                </mrow>
              </mrow>
            </mrow>
          </mrow>
        </mrow>
      </mrow>
    </mtd>
    <mtd>
      <mrow>
        <mo>(</mo>
        <mn>11</mn>
        <mo>)</mo>
      </mrow>
    </mtd>
  </mtr>
</mtable>
</math>
</maths>
</p>
<p id="p-0076" num="0114">An a priori residual is related to an a posteriori equivalent by a so-called conversion factor δ<sub>p,ƒ</sub>(t−ƒ) (calculation of which will be described later) as follows:
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?>ε<sub>p,ƒ</sub>(<i>t</i>−ƒ)=δ<sub>p,ƒ</sub>(<i>t</i>−ƒ)<i>e</i><sub>p,ƒ</sub>(<i>t</i>−ƒ)  (12)<?in-line-formulae description="In-line Formulae" end="tail"?>
</p>
<p id="p-0077" num="0115">Equations (7) to (11) demonstrate the discovery in accordance with the invention that the gain vector (in the present example the Kalman gain vector) required to update filter coefficients can be generated from interpolation residuals. In this regard Equation (7) shows that each element of the Kalman gain vector is a ratio of a respective interpolation residual to the normalisation coefficient given by Equation (10), and Equations (8) and (9) yield the residuals themselves for derivation of the normalisation coefficient and Kalman gain vector.</p>
<p id="p-0078" num="0116">In the present example all forget factors β<sub>1 </sub>etc. are made equal: in this regard the value chosen depends on the nature of the signals being filtered. The associated number of summation terms is such that L<sub>1</sub>=L<sub>2</sub>=L<sub>4</sub>=L<sub>5</sub>=t+1, i.e. all available terms are included: NB L<sub>4 </sub>and L<sub>5 </sub>are defined in later equations.</p>
<p id="p-0079" num="0117">In an adaptive filter in accordance with the invention therefore, a gain vector is determined from interpolation residuals such as for example that given by:
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?>ε<sub>p,ƒ</sub>(<i>t</i>−ƒ)=x(<i>t</i>−ƒ)−<i>ŵ</i><sub>p,ƒ</sub><sup>T</sup>(<i>t</i>−ƒ)<i>{circumflex over (x)}</i><sub>p+ƒ</sub>(<i>t,t</i>−ƒ)  (13)<?in-line-formulae description="In-line Formulae" end="tail"?>
<br/>
where x(t−f) is the (t−f)th data element of the time series, ŵ<sub>p,ƒ</sub><sup>T</sup>(t−f){circumflex over (x)}<sub>p+ƒ</sub>(t,t−ƒ) represents an estimate of x(t−ƒ) obtained by interpolation from both earlier and later elements (subject to availability) of the time series, ŵ<sub>p,ƒ</sub><sup>T</sup>(t−ƒ) is an interpolation coefficient vector and as would be obtained in a least squares minimisation procedure. As mentioned earlier, the interpolation residuals may obtained without calculating ŵ<sub>p,ƒ</sub><sup>T</sup>(t−ƒ) as described later. The quantity {circumflex over (x)}<sub>p+ƒ</sub>(t,t−ƒ) is a vector consisting of p+f elements of the time series from x(t−ƒ−p) to x(t) inclusive except for the omission of x(t−ƒ).
</p>
<p id="p-0080" num="0118">Referring to <figref idref="DRAWINGS">FIG. 1</figref>, stages for calculating an updated weight vector for an adaptive filter in response to a new input signal are shown in outline. These will be described briefly initially and more detail will be given later. A computer indicated by a box <b>10</b> is programmed to carry out four generalised processing stages <b>12</b> to <b>18</b>. Of the latter, stage <b>12</b> comprises processing an input signal using an RLS lattice algorithm and obtaining forward and backward least squares prediction residuals. As indicated earlier, a prediction residual is the difference between a directly obtained data element and a prediction of that element extrapolated from other data associated with it; a forward prediction residual is obtained using forward extrapolation of data elements preceding a directly obtained element, and the backward equivalent is obtained by using backward extrapolation of data following that value. Forward and backward prediction residuals are naturally generated by an RLS lattice algorithm in a fast, numerically stable manner.</p>
<p id="p-0081" num="0119">In stage <b>14</b>, the forward and backward prediction residuals are used to generate interpolation residuals as appropriate to provide elements of an un-normalised Kalman gain vector with elements as indicated in the numerator of the right hand side of Equation (7). In the present example, a priori interpolation residuals are calculated together with conversion factors to convert them to a posteriori interpolation residuals using Equation (12). This approach is taken as it makes initialisation of various stored parameters easier than if a posteriori residuals are used.</p>
<p id="p-0082" num="0120">In stage <b>16</b>, to produce the normalised Kalman gain vector, each a posteriori interpolation residual is normalised by division by E<sub>N−i,i−1</sub>(t−i+1) the normalisation factor: this factor is calculated as the weighted sum of products, each product being that between a priori and a posteriori interpolation residuals for a respective time instant and the summation is over such products for all time instants for which signal samples have been obtained up to and including a most recent time instant as in Equation (10).</p>
<p id="p-0083" num="0121">In stage <b>18</b>, as indicated in Equations (3) and (4), the Kalman gain vector derived from input signal elements up to and including x(t) is combined with x(t) itself and a reference signal y(t); this produces an updated version of the filter coefficients or weights which have therefore been modified in response to both input and reference signal elements x(t) and y(t). To allow for processing delay in deriving the Kalman gain vector in stages <b>12</b> to <b>16</b>, to achieve simultaneity at input to stage <b>18</b>, the signal elements x(t) and y(t) are stored, or, if necessary in a real time system, delayed.</p>
<p id="p-0084" num="0122">The RLS lattice algorithm is implemented in stage <b>12</b> as follows. A Least Squares (LS) lattice algorithm is an efficient algorithm for calculating least squares forward and backwards prediction residuals of a sequence of signals such as a time series: a time series is a series of data values representing the variation of a signal x(t) as a function of time.</p>
<p id="p-0085" num="0123">An Nth order adaptive filter corresponds to a weight vector w<sub>N </sub>having N coefficients—the filter coefficients. Linear prediction residuals are produced as a first step in obtaining these coefficients and are of order 1 to N−1 inclusive. Residuals may be a posteriori or a priori, i.e. they may be produced using either a most recently determined weight vector or such a vector determined immediately previously. The nth order a posteriori LS forward prediction residual ε<sub>n</sub><sup>F</sup>(t) for a time series of signals with a value x(t) at time t is defined as:
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?>ε<sub>n</sub><sup>F</sup>(<i>t</i>)=<i>x</i>(<i>t</i>)−<i>a</i><sub>n</sub><sup>T</sup>(<i>t</i>)<i>x</i><sub>n</sub>(<i>t</i>−1)  (14)<?in-line-formulae description="In-line Formulae" end="tail"?>
<br/>
where x<sub>n</sub>(t−1) is a data vector having elements x(t−1) to x(t−n) inclusive, and a<sub>n</sub>(t) is the forward prediction coefficient vector which is again determined in a least squares optimisation procedure described later: this vector is determined in such a way as to minimise a weighted sum J<sub>n</sub><sup>F</sup>(t) of a preselected number (L<sub>4</sub>) of squares of residuals determined up to and including a most recent residual, i.e. a<sub>n</sub>(t) is the value of the vector ω that minimises:
</p>
<p id="p-0086" num="0124">
<maths id="MATH-US-00009" num="00009">
<math overflow="scroll">
<mtable>
  <mtr>
    <mtd>
      <mrow>
        <mrow>
          <msubsup>
            <mi>J</mi>
            <mi>n</mi>
            <mi>F</mi>
          </msubsup>
          <mo>⁡</mo>
          <mrow>
            <mo>(</mo>
            <mi>t</mi>
            <mo>)</mo>
          </mrow>
        </mrow>
        <mo>=</mo>
        <mrow>
          <munderover>
            <mo>∑</mo>
            <mrow>
              <mi>m</mi>
              <mo>=</mo>
              <mn>0</mn>
            </mrow>
            <mrow>
              <msub>
                <mi>L</mi>
                <mn>4</mn>
              </msub>
              <mo>-</mo>
              <mn>1</mn>
            </mrow>
          </munderover>
          <mo>⁢</mo>
          <msup>
            <mrow>
              <msubsup>
                <mi>β</mi>
                <mn>4</mn>
                <mrow>
                  <mn>2</mn>
                  <mo>⁢</mo>
                  <mi>m</mi>
                </mrow>
              </msubsup>
              <mo>⁡</mo>
              <mrow>
                <mo>(</mo>
                <mrow>
                  <mrow>
                    <mi>x</mi>
                    <mo>⁡</mo>
                    <mrow>
                      <mo>(</mo>
                      <mrow>
                        <mi>t</mi>
                        <mo>-</mo>
                        <mi>m</mi>
                      </mrow>
                      <mo>)</mo>
                    </mrow>
                  </mrow>
                  <mo>-</mo>
                  <mrow>
                    <msup>
                      <munder>
                        <mi>ω</mi>
                        <mi>_</mi>
                      </munder>
                      <mi>T</mi>
                    </msup>
                    <mo>⁢</mo>
                    <mrow>
                      <msub>
                        <munder>
                          <mi>x</mi>
                          <mi>_</mi>
                        </munder>
                        <mi>n</mi>
                      </msub>
                      <mo>⁡</mo>
                      <mrow>
                        <mo>(</mo>
                        <mrow>
                          <mi>t</mi>
                          <mo>-</mo>
                          <mi>m</mi>
                          <mo>-</mo>
                          <mn>1</mn>
                        </mrow>
                        <mo>)</mo>
                      </mrow>
                    </mrow>
                  </mrow>
                </mrow>
                <mo>)</mo>
              </mrow>
            </mrow>
            <mn>2</mn>
          </msup>
        </mrow>
      </mrow>
    </mtd>
    <mtd>
      <mrow>
        <mo>(</mo>
        <mn>15</mn>
        <mo>)</mo>
      </mrow>
    </mtd>
  </mtr>
</mtable>
</math>
</maths>
<br/>
where β<sub>4 </sub>is a forget factor as described earlier.
</p>
<p id="p-0087" num="0125">Similarly, the nth order a posteriori LS backward prediction residual ε<sub>n</sub><sup>B</sup>(t) for a time series of signals with a value x(t) at time t is defined as:
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?>ε<sub>n</sub><sup>B</sup>(<i>t</i>)=<i>x</i>(<i>t−n</i>)−<i>c</i><sub>n</sub><sup>T</sup>(<i>t</i>)<i>x</i><sub>n</sub>(<i>t</i>)  (16)<?in-line-formulae description="In-line Formulae" end="tail"?>
<br/>
where c<sub>n</sub>(t) is a coefficient vector for backward prediction determined as before by a least squares optimisation procedure which minimises a weighted sum of a number L<sub>5 </sub>of squares of residuals determined up to and including the most recent residual, i.e. c<sub>n</sub>(t) is the value of the vector ω that minimises:
</p>
<p id="p-0088" num="0126">
<maths id="MATH-US-00010" num="00010">
<math overflow="scroll">
<mtable>
  <mtr>
    <mtd>
      <mrow>
        <mrow>
          <msubsup>
            <mi>J</mi>
            <mi>n</mi>
            <mi>B</mi>
          </msubsup>
          <mo>⁡</mo>
          <mrow>
            <mo>(</mo>
            <mi>t</mi>
            <mo>)</mo>
          </mrow>
        </mrow>
        <mo>=</mo>
        <mrow>
          <munderover>
            <mo>∑</mo>
            <mrow>
              <mi>m</mi>
              <mo>=</mo>
              <mn>0</mn>
            </mrow>
            <mrow>
              <msub>
                <mi>L</mi>
                <mn>5</mn>
              </msub>
              <mo>-</mo>
              <mn>1</mn>
            </mrow>
          </munderover>
          <mo>⁢</mo>
          <msup>
            <mrow>
              <msubsup>
                <mi>β</mi>
                <mn>5</mn>
                <mrow>
                  <mn>2</mn>
                  <mo>⁢</mo>
                  <mi>m</mi>
                </mrow>
              </msubsup>
              <mo>⁡</mo>
              <mrow>
                <mo>(</mo>
                <mrow>
                  <mrow>
                    <mi>x</mi>
                    <mo>⁡</mo>
                    <mrow>
                      <mo>(</mo>
                      <mrow>
                        <mi>t</mi>
                        <mo>-</mo>
                        <mi>m</mi>
                        <mo>-</mo>
                        <mi>n</mi>
                      </mrow>
                      <mo>)</mo>
                    </mrow>
                  </mrow>
                  <mo>-</mo>
                  <mrow>
                    <msup>
                      <munder>
                        <mi>ω</mi>
                        <mi>_</mi>
                      </munder>
                      <mi>T</mi>
                    </msup>
                    <mo>⁢</mo>
                    <mrow>
                      <msub>
                        <munder>
                          <mi>x</mi>
                          <mi>_</mi>
                        </munder>
                        <mi>n</mi>
                      </msub>
                      <mo>⁡</mo>
                      <mrow>
                        <mo>(</mo>
                        <mrow>
                          <mi>t</mi>
                          <mo>-</mo>
                          <mi>m</mi>
                        </mrow>
                        <mo>)</mo>
                      </mrow>
                    </mrow>
                  </mrow>
                </mrow>
                <mo>)</mo>
              </mrow>
            </mrow>
            <mn>2</mn>
          </msup>
        </mrow>
      </mrow>
    </mtd>
    <mtd>
      <mrow>
        <mo>(</mo>
        <mn>17</mn>
        <mo>)</mo>
      </mrow>
    </mtd>
  </mtr>
</mtable>
</math>
</maths>
<br/>
where β<sub>5 </sub>is a forget factor.
</p>
<p id="p-0089" num="0127">The corresponding a priori forward and backward LS prediction residuals e<sub>n</sub><sup>F</sup>(t) and e<sub>n</sub><sup>B</sup>(t) respectively are defined similarly as follows:
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>e</i><sub>n</sub><sup>F</sup>(<i>t</i>)=<i>x</i>(<i>t</i>)−<i>a</i><sub>n</sub><sup>T</sup>(<i>t</i>−1)<i>x</i><sub>n</sub>(<i>t</i>−1)  (18)<?in-line-formulae description="In-line Formulae" end="tail"?>
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>e</i><sub>n</sub><sup>B</sup>(<i>t</i>)=<i>x</i>(<i>t−n</i>)−<i>c</i><sub>n</sub><sup>T</sup>(<i>t</i>−1)<i>x</i><sub>n</sub>(<i>t</i>)  (19)<?in-line-formulae description="In-line Formulae" end="tail"?>
</p>
<p id="p-0090" num="0128">Conversion factors δ<sub>n</sub><sup>F</sup>(t) and δ<sub>n</sub><sup>B</sup>(t) which relate forward and backward a priori residuals to respective a posteriori equivalents are defined as:</p>
<p id="p-0091" num="0129">
<maths id="MATH-US-00011" num="00011">
<math overflow="scroll">
<mtable>
  <mtr>
    <mtd>
      <mrow>
        <mrow>
          <msubsup>
            <mi>δ</mi>
            <mi>n</mi>
            <mi>F</mi>
          </msubsup>
          <mo>⁡</mo>
          <mrow>
            <mo>(</mo>
            <mi>t</mi>
            <mo>)</mo>
          </mrow>
        </mrow>
        <mo>=</mo>
        <mfrac>
          <mrow>
            <msubsup>
              <mi>ɛ</mi>
              <mi>n</mi>
              <mi>F</mi>
            </msubsup>
            <mo>⁡</mo>
            <mrow>
              <mo>(</mo>
              <mi>t</mi>
              <mo>)</mo>
            </mrow>
          </mrow>
          <mrow>
            <msubsup>
              <mi>e</mi>
              <mi>n</mi>
              <mi>F</mi>
            </msubsup>
            <mo>⁡</mo>
            <mrow>
              <mo>(</mo>
              <mi>t</mi>
              <mo>)</mo>
            </mrow>
          </mrow>
        </mfrac>
      </mrow>
    </mtd>
    <mtd>
      <mrow>
        <mo>(</mo>
        <mn>20</mn>
        <mo>)</mo>
      </mrow>
    </mtd>
  </mtr>
  <mtr>
    <mtd>
      <mrow>
        <mrow>
          <msubsup>
            <mi>δ</mi>
            <mi>n</mi>
            <mi>B</mi>
          </msubsup>
          <mo>⁡</mo>
          <mrow>
            <mo>(</mo>
            <mi>t</mi>
            <mo>)</mo>
          </mrow>
        </mrow>
        <mo>=</mo>
        <mfrac>
          <mrow>
            <msubsup>
              <mi>ɛ</mi>
              <mi>n</mi>
              <mi>B</mi>
            </msubsup>
            <mo>⁡</mo>
            <mrow>
              <mo>(</mo>
              <mi>t</mi>
              <mo>)</mo>
            </mrow>
          </mrow>
          <mrow>
            <msubsup>
              <mi>e</mi>
              <mi>n</mi>
              <mi>B</mi>
            </msubsup>
            <mo>⁡</mo>
            <mrow>
              <mo>(</mo>
              <mi>t</mi>
              <mo>)</mo>
            </mrow>
          </mrow>
        </mfrac>
      </mrow>
    </mtd>
    <mtd>
      <mrow>
        <mo>(</mo>
        <mn>21</mn>
        <mo>)</mo>
      </mrow>
    </mtd>
  </mtr>
</mtable>
</math>
</maths>
</p>
<p id="p-0092" num="0130">An Nth order Least Squares Lattice (LSL) algorithm generates a set of forward and backward a posteriori prediction residuals, N of each, i.e. {ε<sub>1</sub><sup>ƒ</sup>(t), . . . ε<sub>N</sub><sup>ƒ</sup>(t), ε<sub>1</sub><sup>b</sup>(t), . . . ε<sub>N</sub><sup>b</sup>(t)}. Here N is used in a general sense, and is not necessarily the same value as N used elsewhere herein. The particular LSL algorithm used in the present example is a Square Root Free QR Decomposition-based Recursive Least Squares Lattice (RLSL) Algorithm. Internally it calculates not the a posteriori prediction residuals directly but instead it calculates the a priori prediction residuals and the corresponding conversion factors. The algorithm is related to recursive square root free QR decomposition as described in U.S. Pat. No. 4,727,503 to McWhirter, which describes boundary and internal cell functions for a systolic array carrying out this function using inter alia a square root free equivalent of Givens rotations. See also I K Proudler, J G McWhirter and T J Shepherd, “Computationally Efficient, QR Decomposition Approach to Least Squares Adaptive Filtering”, IEE Proceedings, vol.138, Pt. F, no.4, August 1991, pp.341-353.</p>
<p id="p-0093" num="0131">QR decomposition is a well-known procedure which is used in solving sets of linear equations by determining a matrix Q of rotation parameters which will transform a data matrix X into an upper right triangular matrix R (i.e. with all sub-diagonal elements zero) by rotation of its co-ordinate system: such an R matrix has a last row corresponding to one equation with a single unknown and allowing the equations to be solved by back substitution. Rotation parameters are sines and cosines or square root free equivalents of these.</p>
<p id="p-0094" num="0132"><figref idref="DRAWINGS">FIG. 2</figref> is a diagram illustrating the first four successive steps of the RLSL algorithm, which are concatenated stages: the ith stage (i=1, 2, 3, or 4) includes two internal cells (squares) Ia<sub>i </sub>and Ib<sub>i</sub>, two boundary cells Ba<sub>i </sub>and Bb<sub>i </sub>and two delay cells Da<sub>i </sub>and Db<sub>i</sub>. The expressions “internal cell” and “boundary cell” are conventional in QR processing and originally distinguished above-diagonal cells from on-diagonal cells in a triangular array: they are now used to distinguish cells which apply rotation parameters from those that evaluate them.</p>
<p id="p-0095" num="0133">As described in U.S. Pat. No. 4,727,503, boundary cells evaluate rotation parameters from input data and cumulatively multiply cosine-like rotation parameters; internal cells apply rotation parameters to input data. Each of these cells also recomputes and stores a quantity in response to each input, and their processing functions will be described later in more detail. Each delay cell Da<sub>i </sub>and Db<sub>i </sub>store an input signal for a single time interval and outputs an immediately preceding input signal: e.g. when Da<sub>1 </sub>receives a new input signal such as data element x(t) it outputs the preceding input signal x(t−1) and stores the new one until the next input signal x(t+1) is received. The ith stage provides both forward and backward prediction residuals of order i using input signals x(t) when i=1 and when i is 2 or more using residuals corresponding to outputs from stage i−1.</p>
<p id="p-0096" num="0134">At time instant t, a data element x(t) is input to the first stage of the RLSL algorithm: it passes to the first internal cell Ia<sub>1</sub>, first delay cell Da<sub>1 </sub>and second boundary cell Bb<sub>1</sub>. The first delay cell Da<sub>1 </sub>outputs x(t−1) to the second internal cell Ib<sub>1 </sub>and first boundary cell Ba<sub>1</sub>. The first boundary cell Ba<sub>1 </sub>and second boundary cell Bb<sub>1 </sub>also receive inputs of 1, the latter from the second delay cell Db<sub>1 </sub>(unnecessary but included for conformity between stages). The sequence of operations then flows from left to right.</p>
<p id="p-0097" num="0135">The cells of the first RLSL algorithm stage generate quantities and pass them as follows:
<ul id="ul0027" list-style="none">
    <li id="ul0027-0001" num="0000">
    <ul id="ul0028" list-style="none">
        <li id="ul0028-0001" num="0136">first boundary cell Ba<sub>1</sub>: uses x(t−1) to update a quantity stored within it, and it calculates square root free rotation parameters from the stored quantity and passes them to the first internal cell Ia<sub>1</sub>; it calculates the conversion factor δ<sub>1</sub><sup>F</sup>(t) (as defined in Equation (20) with n=1 relating a priori and a posteriori forward prediction residuals) and passes it as an input to the second stage first boundary cell Ba<sub>2</sub>;</li>
        <li id="ul0028-0002" num="0137">second boundary cell Bb<sub>1</sub>: uses x(t) to update a stored quantity, and it calculates square root free rotation parameters from the stored quantity and passes them to second internal cell Ib<sub>1</sub>; it calculates the conversion factor δ<sub>1</sub><sup>B</sup>(t) relating a priori and a posteriori backward prediction residuals and passes it an input to the second stage second delay cell Db<sub>2</sub>, which responds by outputting δ<sub>1</sub><sup>B</sup>(t−1) (received previously) to the second stage second boundary cell Bb<sub>2</sub>;</li>
        <li id="ul0028-0003" num="0138">first internal cell Ia<sub>1</sub>: uses x(t) to update a stored quantity; it then calculates the a priori forward prediction residual e<sub>1</sub><sup>F</sup>(t) (i.e. n=1 in Equation (18)) and passes it to the second stage first internal and second boundary cells Ia<sub>2 </sub>and Bb<sub>2</sub>;</li>
        <li id="ul0028-0004" num="0139">second internal cell Ib<sub>1</sub>: uses x(t−1) to update a stored quantity; it then calculates the a priori backward prediction residual e<sub>1</sub><sup>B</sup>(t) (i.e. n=1 in Equation (19)) passes it to the second stage first delay cell Da<sub>2</sub>, which responds by outputting e<sub>1</sub><sup>B</sup>(t−1) (received previously) to the second stage second internal cell Ib<sub>2 </sub>and first boundary cell Ba<sub>2</sub>;</li>
    </ul>
    </li>
</ul>
</p>
<p id="p-0098" num="0140">The values e<sub>1</sub><img id="CUSTOM-CHARACTER-00001" he="4.23mm" wi="1.78mm" file="US07299251-20071120-P00001.TIF" alt="custom character" img-content="character" img-format="tif"/>(t), δ<sub>1</sub><sup>F</sup>(t), e<sub>1</sub><sup>B</sup>(t) and δ<sub>1</sub><sup>B</sup>(t) are evaluated and passed on in this way by the first stage solving the first order (n=1) forward and backward linear prediction problems: the second stage solves the second order (n=2) equivalent of this, i.e. the second stage internal and boundary cells Ia<sub>2</sub>, Ba<sub>2</sub>, Ib<sub>2 </sub>and Bb<sub>2 </sub>calculate results equivalent to those of the first stage but with the subscript index 2, i.e. e<sub>2</sub><sup>F</sup>(t), δ<sub>2</sub><sup>F</sup>(t), e<sub>2</sub><sup>B</sup>(t) and δ<sub>2</sub><sup>B</sup>(t). Similarly, the third and fourth stages subsequently calculate third and fourth order equivalents with index n=3 and n=4. Nth order prediction residuals require N stages four of which are shown in <figref idref="DRAWINGS">FIG. 2</figref>.</p>
<p id="p-0099" num="0141">As has been said, the terms e<sub>n</sub><sup>F</sup>(t) and e<sub>n</sub><sup>B</sup>(t) generated as described above are a priori prediction residuals: they can be converted to a posteriori residuals ε<sub>n</sub><sup>F</sup>(t) and ε<sub>n</sub><sup>B</sup>(t) using terms δ<sub>n</sub><sup>F</sup>(t) and δ<sub>n</sub><sup>B</sup>(t) also generated by the RLSL algorithm and substituting into Equations (20) and (21) above as follows:
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?>ε<sub>n</sub><sup>F</sup>(<i>t</i>)=δ<sub>n</sub><sup>F</sup>(<i>t</i>)<i>e</i><sub>n</sub><sup>F</sup>(<i>t</i>)  (22)<?in-line-formulae description="In-line Formulae" end="tail"?>
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?>ε<sub>n</sub><sup>F</sup>(<i>t</i>)=δ<sub>n</sub><sup>B</sup>(<i>t</i>)<i>e</i><sub>n</sub><sup>B</sup>(<i>t</i>).  (23)<?in-line-formulae description="In-line Formulae" end="tail"?>
</p>
<p id="p-0100" num="0142">Once the a posteriori prediction residuals have been calculated using Equations (22) and (23), they may be used as hereinafter described to calculate interpolation residuals that make up the un-normalised Kalman gain vector, which is the numerator of the right hand side of Equation (7). However it is advantageous for initialisation purposes as noted above to calculate the interpolation residuals in a priori form together with corresponding conversion factors. Hence in the present example Equations (22) and (23) are not implemented.</p>
<p id="p-0101" num="0143"><figref idref="DRAWINGS">FIGS. 3 and 4</figref> show how an a priori interpolation residual such as e<sub>p,f</sub>(t−f) (and the corresponding conversion factor) can be used, together with certain prediction residuals, to generate two other interpolation residuals: relative to the indices and time instant of the original interpolation residual, one of the two residuals generated from it has a p index increased by 1 and the other has an f index increased likewise and a time instant one sample interval earlier, i.e. e<sub>p+1,f</sub>(t−f) and e<sub>p,f+1</sub>(t−f−1).</p>
<p id="p-0102" num="0144">In <figref idref="DRAWINGS">FIG. 3</figref>, the a priori interpolation residual e<sub>p,f</sub>(t−f) and its associated conversion factor δ<sub>p,f</sub>(t−f) are converted into the a priori interpolation residual e<sub>p+1,f</sub>(t−f) and its associated conversion factor δ<sub>p+1,f</sub>(t−f). The process involves two internal cells (squares) I<sub>1 </sub>and I<sub>2 </sub>and two boundary cells B<sub>1 </sub>and B<sub>2</sub>. Boundary cells evaluate rotation parameters and, in the case of B<sub>2</sub>, cumulatively multiply cosine-like rotation parameters; internal cells apply rotation parameters to data. Each of these cells also recomputes and stores a quantity in response to each input, and their processing functions will be described next in more detail.
<ul id="ul0029" list-style="none">
    <li id="ul0029-0001" num="0000">
    <ul id="ul0030" list-style="none">
        <li id="ul0030-0001" num="0145">Boundary cell B<sub>1</sub>: uses the a priori interpolation residual e<sub>p,f</sub>(t−ƒ) and conversion factor δ<sub>p,ƒ</sub>(t−ƒ) to update a quantity stored within it, and it calculates square root free rotation parameters from the stored quantity and passes them to first internal cell I<sub>1</sub>; it passes the conversion factor δ<sub>p,ƒ</sub>(t−ƒ), relating a priori and a posteriori residuals, at its input to the boundary cell B<sub>2</sub>;</li>
        <li id="ul0030-0002" num="0146">internal cell I<sub>1</sub>: uses the a priori backward prediction residual e<sub>p+f+1</sub><sup>B</sup>(t) and rotation parameters from boundary cell B<sub>1 </sub>to update a stored quantity; it then calculates the quantity ê<sub>p+ƒ</sub><sup>B</sup>(t,t−ƒ) and passes it to the boundary cell B<sub>2</sub>;</li>
        <li id="ul0030-0003" num="0147">boundary cell B<sub>2</sub>: uses ê<sub>p+ƒ</sub><sup>B</sup>(t,t−ƒ) e<sub>p,ƒ</sub>(t−ƒ) and conversion factor δ<sub>p,ƒ</sub>(t−ƒ) to update a stored quantity, and it calculates square root free rotation parameters from the stored quantity and passes them to internal cell I<sub>2</sub>; it calculates the conversion factor δ<sub>p+1,ƒ</sub>(t−ƒ) relating a priori and a posteriori residuals;</li>
        <li id="ul0030-0004" num="0148">internal cell I<sub>2</sub>: uses e<sub>p,ƒ</sub>(t−ƒ) e<sub>p+ƒ+1</sub><sup>B</sup>(t) and rotation parameters from boundary cell B<sub>2 </sub>to update a stored quantity; it then calculates the a priori interpolation residual e<sub>p+1,f</sub>(t−f);</li>
    </ul>
    </li>
</ul>
</p>
<p id="p-0103" num="0149">In <figref idref="DRAWINGS">FIG. 4</figref>, the a priori interpolation residual e<sub>p,f</sub>(t−f) and its associated conversion factor δ<sub>p,f</sub>(t−f) are converted into the a priori interpolation residual e<sub>p,f+1</sub>(t−f−1) and its associated conversion factor δ<sub>p,ƒ+1</sub>(t−ƒ−1). With the exception of the two delay cells (D<sub>1 </sub>and D<sub>2</sub>), the underlying operations are exactly the same as described in relation to <figref idref="DRAWINGS">FIG. 3</figref> it is the use of different input signals that distinguishes the approaches described with reference to <figref idref="DRAWINGS">FIGS. 3 and 4</figref>.</p>
<p id="p-0104" num="0150">In <figref idref="DRAWINGS">FIG. 4</figref>, the process involves two delay cells D<sub>1 </sub>and D<sub>2</sub>, two internal cells (squares) I<sub>3 </sub>and I<sub>4 </sub>and two boundary cells B<sub>3 </sub>and B<sub>4</sub>. Each of the delay cells D<sub>1 </sub>and D<sub>2 </sub>stores an input signal for a single time interval: when it receives a new input signal h(t) it outputs the preceding input signal h(t−1) and stores the new one until the next input signal h(t+1) is received. Boundary cells B<sub>3 </sub>and B<sub>4 </sub>evaluate rotation parameters and, in the case of B<sub>4</sub>, cumulatively multiply cosine-like rotation parameters; internal cells apply rotation parameters to data. Each of these cells also recomputes and stores a quantity in response to each input, and their processing functions will be described later in more detail.
<ul id="ul0031" list-style="none">
    <li id="ul0031-0001" num="0000">
    <ul id="ul0032" list-style="none">
        <li id="ul0032-0001" num="0151">Boundary cell B<sub>3</sub>: uses the a priori interpolation residual e<sub>p,ƒ</sub>(t−ƒ−1), output from delay cell D<sub>2</sub>, and conversion factor δ<sub>p,f</sub>(t−f−1) to update a quantity stored within it, and it calculates square root free rotation parameters from the stored quantity and passes them to first internal cell I<sub>1</sub>; it passes the conversion factor δ<sub>p,ƒ</sub>(t−ƒ−1), output from delay cell D<sub>1</sub>, relating a priori and a posteriori residuals, at its input to the boundary cell B<sub>4</sub>;</li>
        <li id="ul0032-0002" num="0152">internal cell I<sub>3</sub>: uses the a priori forward prediction residual</li>
    </ul>
    </li>
</ul>
</p>
<p id="p-0105" num="0153">
<maths id="MATH-US-00012" num="00012">
<math overflow="scroll">
<mrow>
  <msubsup>
    <mi>e</mi>
    <mrow>
      <mi>p</mi>
      <mo>+</mo>
      <mi>f</mi>
      <mo>+</mo>
      <mn>1</mn>
    </mrow>
    <mi>F</mi>
  </msubsup>
  <mo>⁡</mo>
  <mrow>
    <mo>(</mo>
    <mi>t</mi>
    <mo>)</mo>
  </mrow>
</mrow>
</math>
</maths>
<br/>
and rotation parameters from boundary cell B<sub>3 </sub>to update a stored quantity; it then calculates the quantity
</p>
<p id="p-0106" num="0154">
<maths id="MATH-US-00013" num="00013">
<math overflow="scroll">
<mrow>
  <msubsup>
    <mover>
      <mi>e</mi>
      <mo>^</mo>
    </mover>
    <mrow>
      <mi>p</mi>
      <mo>+</mo>
      <mi>f</mi>
    </mrow>
    <mi>F</mi>
  </msubsup>
  <mo>⁡</mo>
  <mrow>
    <mo>(</mo>
    <mrow>
      <mi>t</mi>
      <mo>,</mo>
      <mrow>
        <mi>t</mi>
        <mo>-</mo>
        <mi>f</mi>
        <mo>-</mo>
        <mn>1</mn>
      </mrow>
    </mrow>
    <mo>)</mo>
  </mrow>
</mrow>
</math>
</maths>
<br/>
and passes it to the boundary cell B<sub>4</sub>;
<ul id="ul0033" list-style="none">
    <li id="ul0033-0001" num="0000">
    <ul id="ul0034" list-style="none">
        <li id="ul0034-0001" num="0155">boundary cell B<sub>4</sub>: uses</li>
    </ul>
    </li>
</ul>
</p>
<p id="p-0107" num="0156">
<maths id="MATH-US-00014" num="00014">
<math overflow="scroll">
<mrow>
  <msubsup>
    <mover>
      <mi>e</mi>
      <mo>^</mo>
    </mover>
    <mrow>
      <mi>p</mi>
      <mo>+</mo>
      <mi>f</mi>
    </mrow>
    <mi>F</mi>
  </msubsup>
  <mo>⁡</mo>
  <mrow>
    <mo>(</mo>
    <mrow>
      <mi>t</mi>
      <mo>,</mo>
      <mrow>
        <mi>t</mi>
        <mo>-</mo>
        <mi>f</mi>
        <mo>-</mo>
        <mn>1</mn>
      </mrow>
    </mrow>
    <mo>)</mo>
  </mrow>
</mrow>
</math>
</maths>
<br/>
and conversion factor δ<sub>p,ƒ</sub>(t−ƒ−1) to update a stored quantity, and it calculates square root free rotation parameters from the stored quantity and passes them to internal cell I<sub>2</sub>; it calculates the conversion factor δ<sub>p,ƒ+1</sub>(t−ƒ−1) relating a priori and a posteriori residuals;
<ul id="ul0035" list-style="none">
    <li id="ul0035-0001" num="0000">
    <ul id="ul0036" list-style="none">
        <li id="ul0036-0001" num="0157">internal cell I<sub>4</sub>: uses e<sub>p,ƒ</sub>(t−ƒ−1) and rotation parameters from boundary cell B<sub>4 </sub>to update a stored quantity; it then calculates the a priori interpolation residual e<sub>p,ƒ+1</sub>(t−ƒ−1);</li>
    </ul>
    </li>
</ul>
</p>
<p id="p-0108" num="0158">The details of the function of the cells in <figref idref="DRAWINGS">FIGS. 3 and 4</figref> will now be described. In IEEE Trans SP January 2000 vol 48(1) pp.70-79, “QR decomposition based least squares lattice interpolators”, J T Yuan shows that it is possible to use an a posteriori interpolation residual ε<sub>p,ƒ</sub>(t−ƒ), or a one step delayed equivalent ε<sub>p,ƒ</sub>(t−ƒ−1), to generate a posteriori interpolation residuals ε<sub>p+1,ƒ</sub>(t−ƒ) and ε<sub>p,ƒ+1</sub>(t−ƒ−1) as set out in Equations (24) and (25) below:</p>
<p id="p-0109" num="0159">
<maths id="MATH-US-00015" num="00015">
<math overflow="scroll">
<mtable>
  <mtr>
    <mtd>
      <mrow>
        <mrow>
          <msub>
            <mi>ɛ</mi>
            <mrow>
              <mrow>
                <mi>p</mi>
                <mo>+</mo>
                <mn>1</mn>
              </mrow>
              <mo>,</mo>
              <mi>f</mi>
            </mrow>
          </msub>
          <mo>⁡</mo>
          <mrow>
            <mo>(</mo>
            <mrow>
              <mi>t</mi>
              <mo>-</mo>
              <mi>f</mi>
            </mrow>
            <mo>)</mo>
          </mrow>
        </mrow>
        <mo>=</mo>
        <mrow>
          <mrow>
            <msub>
              <mi>ɛ</mi>
              <mrow>
                <mi>p</mi>
                <mo>,</mo>
                <mi>f</mi>
              </mrow>
            </msub>
            <mo>⁡</mo>
            <mrow>
              <mo>(</mo>
              <mrow>
                <mi>t</mi>
                <mo>-</mo>
                <mi>f</mi>
              </mrow>
              <mo>)</mo>
            </mrow>
          </mrow>
          <mo>+</mo>
          <mrow>
            <mrow>
              <msub>
                <mi>k</mi>
                <mrow>
                  <mrow>
                    <mi>p</mi>
                    <mo>+</mo>
                    <mn>1</mn>
                  </mrow>
                  <mo>,</mo>
                  <mi>f</mi>
                </mrow>
              </msub>
              <mo>⁡</mo>
              <mrow>
                <mo>(</mo>
                <mi>t</mi>
                <mo>)</mo>
              </mrow>
            </mrow>
            <mo>⁢</mo>
            <mrow>
              <msubsup>
                <mover>
                  <mi>ɛ</mi>
                  <mo>^</mo>
                </mover>
                <mrow>
                  <mi>p</mi>
                  <mo>+</mo>
                  <mi>f</mi>
                </mrow>
                <mi>B</mi>
              </msubsup>
              <mo>⁡</mo>
              <mrow>
                <mo>(</mo>
                <mrow>
                  <mi>t</mi>
                  <mo>,</mo>
                  <mrow>
                    <mi>t</mi>
                    <mo>-</mo>
                    <mi>f</mi>
                  </mrow>
                </mrow>
                <mo>)</mo>
              </mrow>
            </mrow>
          </mrow>
        </mrow>
      </mrow>
    </mtd>
    <mtd>
      <mrow>
        <mo>(</mo>
        <mn>24</mn>
        <mo>)</mo>
      </mrow>
    </mtd>
  </mtr>
  <mtr>
    <mtd>
      <mrow>
        <mrow>
          <msub>
            <mi>ɛ</mi>
            <mrow>
              <mi>p</mi>
              <mo>,</mo>
              <mrow>
                <mi>f</mi>
                <mo>+</mo>
                <mn>1</mn>
              </mrow>
            </mrow>
          </msub>
          <mo>⁡</mo>
          <mrow>
            <mo>(</mo>
            <mrow>
              <mi>t</mi>
              <mo>-</mo>
              <mi>f</mi>
              <mo>-</mo>
              <mn>1</mn>
            </mrow>
            <mo>)</mo>
          </mrow>
        </mrow>
        <mo>=</mo>
        <mrow>
          <mrow>
            <msub>
              <mi>ɛ</mi>
              <mrow>
                <mi>p</mi>
                <mo>,</mo>
                <mi>f</mi>
              </mrow>
            </msub>
            <mo>⁡</mo>
            <mrow>
              <mo>(</mo>
              <mrow>
                <mi>t</mi>
                <mo>-</mo>
                <mi>f</mi>
                <mo>-</mo>
                <mn>1</mn>
              </mrow>
              <mo>)</mo>
            </mrow>
          </mrow>
          <mo>+</mo>
          <mrow>
            <mrow>
              <msub>
                <mi>μ</mi>
                <mrow>
                  <mi>p</mi>
                  <mo>,</mo>
                  <mrow>
                    <mi>f</mi>
                    <mo>+</mo>
                    <mn>1</mn>
                  </mrow>
                </mrow>
              </msub>
              <mo>⁡</mo>
              <mrow>
                <mo>(</mo>
                <mi>t</mi>
                <mo>)</mo>
              </mrow>
            </mrow>
            <mo>⁢</mo>
            <mrow>
              <msubsup>
                <mover>
                  <mi>ɛ</mi>
                  <mo>^</mo>
                </mover>
                <mrow>
                  <mi>p</mi>
                  <mo>+</mo>
                  <mi>f</mi>
                </mrow>
                <mi>F</mi>
              </msubsup>
              <mo>⁡</mo>
              <mrow>
                <mo>(</mo>
                <mrow>
                  <mi>t</mi>
                  <mo>,</mo>
                  <mrow>
                    <mi>t</mi>
                    <mo>-</mo>
                    <mi>f</mi>
                    <mo>-</mo>
                    <mn>1</mn>
                  </mrow>
                </mrow>
                <mo>)</mo>
              </mrow>
            </mrow>
          </mrow>
        </mrow>
      </mrow>
    </mtd>
    <mtd>
      <mrow>
        <mo>(</mo>
        <mn>25</mn>
        <mo>)</mo>
      </mrow>
    </mtd>
  </mtr>
</mtable>
</math>
</maths>
<br/>
where k<sub>p+1,ƒ</sub>(t) and μ<sub>p,ƒ+1</sub>(t) are coefficients determined by a least squares minimisation procedure, i.e. k<sub>p+1,ƒ</sub>(t) and μ<sub>p,ƒ+1</sub>(t) are set equal to the value of the coefficients ω<sub>1 </sub>and ω<sub>2 </sub>(respectively) that minimise the weighted sum of the squares of values of the terms
</p>
<p id="p-0110" num="0160">
<maths id="MATH-US-00016" num="00016">
<math overflow="scroll">
<mrow>
  <mrow>
    <msub>
      <mi>ɛ</mi>
      <mrow>
        <mi>p</mi>
        <mo>,</mo>
        <mi>f</mi>
      </mrow>
    </msub>
    <mo>⁡</mo>
    <mrow>
      <mo>(</mo>
      <mrow>
        <mi>t</mi>
        <mo>-</mo>
        <mi>f</mi>
      </mrow>
      <mo>)</mo>
    </mrow>
  </mrow>
  <mo>+</mo>
  <mrow>
    <msub>
      <mi>ω</mi>
      <mn>1</mn>
    </msub>
    <mo>⁢</mo>
    <mrow>
      <msubsup>
        <mover>
          <mi>ɛ</mi>
          <mo>^</mo>
        </mover>
        <mrow>
          <mi>p</mi>
          <mo>+</mo>
          <mi>f</mi>
        </mrow>
        <mi>B</mi>
      </msubsup>
      <mo>⁡</mo>
      <mrow>
        <mo>(</mo>
        <mrow>
          <mi>t</mi>
          <mo>,</mo>
          <mrow>
            <mi>t</mi>
            <mo>-</mo>
            <mi>f</mi>
          </mrow>
        </mrow>
        <mo>)</mo>
      </mrow>
    </mrow>
  </mrow>
</mrow>
</math>
</maths>
<br/>
and
</p>
<p id="p-0111" num="0161">
<maths id="MATH-US-00017" num="00017">
<math overflow="scroll">
<mrow>
  <mrow>
    <msub>
      <mi>ɛ</mi>
      <mrow>
        <mi>p</mi>
        <mo>,</mo>
        <mi>f</mi>
      </mrow>
    </msub>
    <mo>⁡</mo>
    <mrow>
      <mo>(</mo>
      <mrow>
        <mi>t</mi>
        <mo>-</mo>
        <mi>f</mi>
        <mo>-</mo>
        <mn>1</mn>
      </mrow>
      <mo>)</mo>
    </mrow>
  </mrow>
  <mo>+</mo>
  <mrow>
    <msub>
      <mi>ω</mi>
      <mn>2</mn>
    </msub>
    <mo>⁢</mo>
    <mrow>
      <msubsup>
        <mover>
          <mi>ɛ</mi>
          <mo>^</mo>
        </mover>
        <mrow>
          <mi>p</mi>
          <mo>+</mo>
          <mi>f</mi>
        </mrow>
        <mi>F</mi>
      </msubsup>
      <mo>⁡</mo>
      <mrow>
        <mo>(</mo>
        <mrow>
          <mi>t</mi>
          <mo>,</mo>
          <mrow>
            <mi>t</mi>
            <mo>-</mo>
            <mi>f</mi>
            <mo>-</mo>
            <mn>1</mn>
          </mrow>
        </mrow>
        <mo>)</mo>
      </mrow>
    </mrow>
  </mrow>
</mrow>
</math>
</maths>
<br/>
(respectively) over a succession of evaluations at different times t. The quantities
</p>
<p id="p-0112" num="0162">
<maths id="MATH-US-00018" num="00018">
<math overflow="scroll">
<mrow>
  <msubsup>
    <mover>
      <mi>ɛ</mi>
      <mo>^</mo>
    </mover>
    <mrow>
      <mi>p</mi>
      <mo>+</mo>
      <mi>f</mi>
    </mrow>
    <mi>B</mi>
  </msubsup>
  <mo>⁡</mo>
  <mrow>
    <mo>(</mo>
    <mrow>
      <mi>t</mi>
      <mo>,</mo>
      <mrow>
        <mi>t</mi>
        <mo>-</mo>
        <mi>f</mi>
      </mrow>
    </mrow>
    <mo>)</mo>
  </mrow>
</mrow>
</math>
</maths>
<br/>
and
</p>
<p id="p-0113" num="0163">
<maths id="MATH-US-00019" num="00019">
<math overflow="scroll">
<mrow>
  <msubsup>
    <mover>
      <mi>ɛ</mi>
      <mo>^</mo>
    </mover>
    <mrow>
      <mi>p</mi>
      <mo>+</mo>
      <mi>f</mi>
    </mrow>
    <mi>F</mi>
  </msubsup>
  <mo>⁡</mo>
  <mrow>
    <mo>(</mo>
    <mrow>
      <mi>t</mi>
      <mo>,</mo>
      <mrow>
        <mi>t</mi>
        <mo>-</mo>
        <mi>f</mi>
        <mo>-</mo>
        <mn>1</mn>
      </mrow>
    </mrow>
    <mo>)</mo>
  </mrow>
</mrow>
</math>
</maths>
<br/>
are calculated from other known quantities as will be described later. Here the circumflex symbol above ε in each case indicates that the quantities
</p>
<p id="p-0114" num="0164">
<maths id="MATH-US-00020" num="00020">
<math overflow="scroll">
<mrow>
  <msubsup>
    <mover>
      <mi>ɛ</mi>
      <mo>^</mo>
    </mover>
    <mrow>
      <mi>p</mi>
      <mo>+</mo>
      <mi>f</mi>
    </mrow>
    <mi>B</mi>
  </msubsup>
  <mo>⁡</mo>
  <mrow>
    <mo>(</mo>
    <mrow>
      <mi>t</mi>
      <mo>,</mo>
      <mrow>
        <mi>t</mi>
        <mo>-</mo>
        <mi>f</mi>
      </mrow>
    </mrow>
    <mo>)</mo>
  </mrow>
</mrow>
</math>
</maths>
<br/>
and
</p>
<p id="p-0115" num="0165">
<maths id="MATH-US-00021" num="00021">
<math overflow="scroll">
<mrow>
  <msubsup>
    <mover>
      <mi>ɛ</mi>
      <mo>^</mo>
    </mover>
    <mrow>
      <mi>p</mi>
      <mo>+</mo>
      <mi>f</mi>
    </mrow>
    <mi>F</mi>
  </msubsup>
  <mo>⁡</mo>
  <mrow>
    <mo>(</mo>
    <mrow>
      <mi>t</mi>
      <mo>,</mo>
      <mrow>
        <mi>t</mi>
        <mo>-</mo>
        <mi>f</mi>
        <mo>-</mo>
        <mn>1</mn>
      </mrow>
    </mrow>
    <mo>)</mo>
  </mrow>
</mrow>
</math>
</maths>
<br/>
are, respectively, not the same as the backward and forward linear prediction residuals given by Equations (16) and (14);
</p>
<p id="p-0116" num="0166">
<maths id="MATH-US-00022" num="00022">
<math overflow="scroll">
<mrow>
  <msubsup>
    <mover>
      <mi>ɛ</mi>
      <mo>^</mo>
    </mover>
    <mrow>
      <mi>p</mi>
      <mo>+</mo>
      <mi>f</mi>
    </mrow>
    <mi>B</mi>
  </msubsup>
  <mo>⁡</mo>
  <mrow>
    <mo>(</mo>
    <mrow>
      <mi>t</mi>
      <mo>,</mo>
      <mrow>
        <mi>t</mi>
        <mo>-</mo>
        <mi>f</mi>
      </mrow>
    </mrow>
    <mo>)</mo>
  </mrow>
</mrow>
</math>
</maths>
<br/>
is a backward prediction residual of order (p+ƒ) at time (t) derived with the omission of data element x(t−ƒ), and
</p>
<p id="p-0117" num="0167">
<maths id="MATH-US-00023" num="00023">
<math overflow="scroll">
<mrow>
  <msubsup>
    <mover>
      <mi>ɛ</mi>
      <mo>^</mo>
    </mover>
    <mrow>
      <mi>p</mi>
      <mo>+</mo>
      <mi>f</mi>
    </mrow>
    <mi>F</mi>
  </msubsup>
  <mo>⁡</mo>
  <mrow>
    <mo>(</mo>
    <mrow>
      <mi>t</mi>
      <mo>,</mo>
      <mrow>
        <mi>t</mi>
        <mo>-</mo>
        <mi>f</mi>
        <mo>-</mo>
        <mn>1</mn>
      </mrow>
    </mrow>
    <mo>)</mo>
  </mrow>
</mrow>
</math>
</maths>
<br/>
is a forward prediction residual of order (p+ƒ) at time (t) derived with the omission of data element x(t−ƒ−1).
</p>
<p id="p-0118" num="0168">In the above analysis the time index (t−ƒ) has been used for convenience, although strictly speaking it can be shown mathematically that the parameter f is superfluous.</p>
<p id="p-0119" num="0169">There are several well known methods for solving least squares minimisation problems. In this example such a method based on QR Decomposition (QRD) is employed: it will be described in terms of determining a coefficient k(t) in a generalised equation as follows. Each of Equations (24) and (25) is of the form:
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?>ζ(<i>t</i>)=ψ(<i>t</i>)+<i>k</i>(<i>t</i>)ξ(<i>t</i>)  (26)<?in-line-formulae description="In-line Formulae" end="tail"?>
</p>
<p id="p-0120" num="0170">Equation (26) it is required to calculate, at time instant t, the coefficient k(t) given input data ξ(t) and ψ(t) (a posteriori modified prediction and interpolation residuals respectively), and subject to the constraint that in so doing there is minimisation of a sum of squares of the quantity ψ(i)+ωξ(i) calculated for certain different times i and weighted by forget factors β: i.e. k(t) is equal to the value of a parameter ω that delivers a minimum value for the expression:</p>
<p id="p-0121" num="0171">
<maths id="MATH-US-00024" num="00024">
<math overflow="scroll">
<mtable>
  <mtr>
    <mtd>
      <mrow>
        <munderover>
          <mo>∑</mo>
          <mrow>
            <mi>i</mi>
            <mo>=</mo>
            <mn>0</mn>
          </mrow>
          <mi>L</mi>
        </munderover>
        <mo>⁢</mo>
        <mstyle>
          <mspace width="0.3em" height="0.3ex"/>
        </mstyle>
        <mo>⁢</mo>
        <msup>
          <mrow>
            <msup>
              <mi>β</mi>
              <mrow>
                <mn>2</mn>
                <mo>⁢</mo>
                <mstyle>
                  <mspace width="0.3em" height="0.3ex"/>
                </mstyle>
                <mo>⁢</mo>
                <mi>i</mi>
              </mrow>
            </msup>
            <mo>⁡</mo>
            <mrow>
              <mo>(</mo>
              <mrow>
                <mrow>
                  <mi>ψ</mi>
                  <mo>⁡</mo>
                  <mrow>
                    <mo>(</mo>
                    <mrow>
                      <mi>t</mi>
                      <mo>-</mo>
                      <mi>i</mi>
                    </mrow>
                    <mo>)</mo>
                  </mrow>
                </mrow>
                <mo>+</mo>
                <mrow>
                  <mi>ωξ</mi>
                  <mo>⁡</mo>
                  <mrow>
                    <mo>(</mo>
                    <mrow>
                      <mi>t</mi>
                      <mo>-</mo>
                      <mi>i</mi>
                    </mrow>
                    <mo>)</mo>
                  </mrow>
                </mrow>
              </mrow>
              <mo>)</mo>
            </mrow>
          </mrow>
          <mn>2</mn>
        </msup>
      </mrow>
    </mtd>
    <mtd>
      <mrow>
        <mo>(</mo>
        <mn>27</mn>
        <mo>)</mo>
      </mrow>
    </mtd>
  </mtr>
</mtable>
</math>
</maths>
</p>
<p id="p-0122" num="0172">The parameter L determines the number of past samples that are included in the summation. As mentioned earlier L may be fixed or may vary with time, and β is a forget factor. The effect of calculating k(t) in this way is to attenuate any component of ξ(t) which also appears in ψ(t). The term ζ(t) is an a posteriori least squares residual for the problem.</p>
<p id="p-0123" num="0173">In this example the preferred method for solving the least squares problem given in Equations (26) and (27) involves not the a posteriori residuals ξ(t) and ψ(t) but corresponding a priori residuals, u(t) and v(t), respectively, to which a conversion factor δ<sub>m</sub>(t) is applied: it might be anticipated that there is a different conversion factor for each residual, but in fact in the present example it has been found that they are the same. In addition the preferred method does not solve the least squares problem given in Equations (26) and (27) directly but instead uses a recursive procedure that calculates k(t) based on the a priori residuals u(t) and v(t) and the conversion factor δ<sub>m</sub>(t) at only the current time instant and knowledge of the values of various parameters calculated at the previous time instant and stored for later use. One such stored parameter is the value of the least squares coefficient at the previous time instant i.e. k(t−1).</p>
<p id="p-0124" num="0174">By an analysis related to that of S Hammarling, “A Note on Modifications to the Givens Plane Rotation”, J. Inst. Maths. Applics., vol. 13, pp. 215-218, 1974, and described in U.S. Pat. No. 4,727,503 to McWhirter, it can be shown that k(t) can be calculated from the following recursions—which also yield the a priori residual associated with ζ(t) (i.e. z(t)) and the corresponding conversion factor (δ<sub>out</sub>(t)):
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>d</i>(<i>t</i>)=β<sup>2</sup><i>d</i>(<i>t</i>−1)+δ<sub>in</sub>(<i>t</i>)|<i>u</i>(<i>t</i>)|<sup>2</sup>  (28)<?in-line-formulae description="In-line Formulae" end="tail"?>
</p>
<p id="p-0125" num="0175">
<maths id="MATH-US-00025" num="00025">
<math overflow="scroll">
<mtable>
  <mtr>
    <mtd>
      <mrow>
        <mi>s</mi>
        <mo>=</mo>
        <mfrac>
          <mrow>
            <mrow>
              <msub>
                <mi>δ</mi>
                <mrow>
                  <mi>i</mi>
                  <mo>⁢</mo>
                  <mstyle>
                    <mspace width="0.3em" height="0.3ex"/>
                  </mstyle>
                  <mo>⁢</mo>
                  <mi>n</mi>
                </mrow>
              </msub>
              <mo>⁡</mo>
              <mrow>
                <mo>(</mo>
                <mi>t</mi>
                <mo>)</mo>
              </mrow>
            </mrow>
            <mo>⁢</mo>
            <mrow>
              <mi>u</mi>
              <mo>⁡</mo>
              <mrow>
                <mo>(</mo>
                <mi>t</mi>
                <mo>)</mo>
              </mrow>
            </mrow>
          </mrow>
          <mrow>
            <mi>d</mi>
            <mo>⁡</mo>
            <mrow>
              <mo>(</mo>
              <mi>t</mi>
              <mo>)</mo>
            </mrow>
          </mrow>
        </mfrac>
      </mrow>
    </mtd>
    <mtd>
      <mrow>
        <mo>(</mo>
        <mn>29</mn>
        <mo>)</mo>
      </mrow>
    </mtd>
  </mtr>
  <mtr>
    <mtd>
      <mrow>
        <mrow>
          <msub>
            <mi>δ</mi>
            <mi>out</mi>
          </msub>
          <mo>⁡</mo>
          <mrow>
            <mo>(</mo>
            <mi>t</mi>
            <mo>)</mo>
          </mrow>
        </mrow>
        <mo>=</mo>
        <mfrac>
          <mrow>
            <msup>
              <mi>β</mi>
              <mn>2</mn>
            </msup>
            <mo>⁢</mo>
            <mrow>
              <mo>ⅆ</mo>
              <mrow>
                <mo>(</mo>
                <mrow>
                  <mi>t</mi>
                  <mo>-</mo>
                  <mn>1</mn>
                </mrow>
                <mo>)</mo>
              </mrow>
            </mrow>
            <mo>⁢</mo>
            <mrow>
              <msub>
                <mi>δ</mi>
                <mrow>
                  <mi>i</mi>
                  <mo>⁢</mo>
                  <mstyle>
                    <mspace width="0.3em" height="0.3ex"/>
                  </mstyle>
                  <mo>⁢</mo>
                  <mi>n</mi>
                </mrow>
              </msub>
              <mo>⁡</mo>
              <mrow>
                <mo>(</mo>
                <mi>t</mi>
                <mo>)</mo>
              </mrow>
            </mrow>
          </mrow>
          <mrow>
            <mo>ⅆ</mo>
            <mrow>
              <mo>(</mo>
              <mi>t</mi>
              <mo>)</mo>
            </mrow>
          </mrow>
        </mfrac>
      </mrow>
    </mtd>
    <mtd>
      <mrow>
        <mo>(</mo>
        <mn>30</mn>
        <mo>)</mo>
      </mrow>
    </mtd>
  </mtr>
</mtable>
</math>
</maths>
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>z</i>(<i>t</i>)=<i>v</i>(<i>t</i>)+<i>k</i>(<i>t</i>−1)<i>u</i>(<i>t</i>)  (31)<?in-line-formulae description="In-line Formulae" end="tail"?>
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>k</i>(<i>t</i>)=<i>k</i>(<i>t</i>−1)−<i>sz</i>(<i>t</i>)  (32)<?in-line-formulae description="In-line Formulae" end="tail"?>
</p>
<p id="p-0126" num="0176">The term d(t) is defined by Equation (28) and represents an estimate of the energy of the time series u(0) to u(t) inclusive but with more weight given to more recent samples. All terms on the right-hand side of Equation (28) are known at time t: the term d(t−1) was obtained in the immediately preceding time instant by calculation if t=2 or more; for t=1 the corresponding initial term d(0) is given a predefined value such as zero; the terms δ<sub>in</sub>(t) and u(t) are input data. The term s is a generalised rotation parameter of the kind used in QR decomposition: it is defined by Equation (29) in terms of input data δ<sub>in</sub>(t) and u(t) together with a quantity d(t) calculated for the relevant time instant using Equation (28). Equation (30) defines the conversion factor δ<sub>out</sub>(t) for the residual z(t) in terms of known quantities: i.e. d(t−1) is known from the previous time instant and δ<sub>in</sub>(t) and d(t) have been calculated for this time instant using Equation (28). Together Equations (28) to (30) inclusive represent the calculation that takes place for each time instant in the boundary cells Ba<sub>1 </sub>etc. shown in <figref idref="DRAWINGS">FIG. 2</figref> and the boundary cells B<sub>2 </sub>and B<sub>4 </sub>shown in <figref idref="DRAWINGS">FIGS. 3 and 4</figref>.</p>
<p id="p-0127" num="0177">Equations (31) and (32) represent the calculation that takes place for each time instant in the internal cells Ia<sub>1 </sub>etc. in <figref idref="DRAWINGS">FIG. 2</figref> and the internal cells I<sub>2 </sub>and I<sub>4 </sub>shown in <figref idref="DRAWINGS">FIGS. 3 and 4</figref>. Equation (31) expresses z(t) in terms of two known quantities (input data) u(t) and v(t), and a value k(t−1) evaluated earlier for data of time instant (t−1) where t=2 or more; t=1 corresponds to the term k(0) which is set equal to a predefined value such as zero. Equation (32) expresses k(t) in terms of quantities s and z(t) calculated from Equations (29) and (31) together with k(t−1) from the previous time instant. With the knowledge of any starting value eg k(0) it is therefore possible to generate a succession of subsequent values of z(t) and k(t) using Equations (28) to (32):</p>
<p id="p-0128" num="0178">Referring to Equations (24) and (25) once more, the quantities</p>
<p id="p-0129" num="0179">
<maths id="MATH-US-00026" num="00026">
<math overflow="scroll">
<mrow>
  <msubsup>
    <mover>
      <mi>ɛ</mi>
      <mo>^</mo>
    </mover>
    <mrow>
      <mi>p</mi>
      <mo>+</mo>
      <mi>f</mi>
    </mrow>
    <mi>B</mi>
  </msubsup>
  <mo>⁡</mo>
  <mrow>
    <mo>(</mo>
    <mrow>
      <mi>t</mi>
      <mo>,</mo>
      <mrow>
        <mi>t</mi>
        <mo>-</mo>
        <mi>f</mi>
      </mrow>
    </mrow>
    <mo>)</mo>
  </mrow>
</mrow>
</math>
</maths>
<br/>
and
</p>
<p id="p-0130" num="0180">
<maths id="MATH-US-00027" num="00027">
<math overflow="scroll">
<mrow>
  <msubsup>
    <mover>
      <mi>ɛ</mi>
      <mo>^</mo>
    </mover>
    <mrow>
      <mi>p</mi>
      <mo>+</mo>
      <mi>f</mi>
    </mrow>
    <mi>F</mi>
  </msubsup>
  <mo>⁡</mo>
  <mrow>
    <mo>(</mo>
    <mrow>
      <mi>t</mi>
      <mo>,</mo>
      <mrow>
        <mi>t</mi>
        <mo>-</mo>
        <mi>f</mi>
        <mo>-</mo>
        <mn>1</mn>
      </mrow>
    </mrow>
    <mo>)</mo>
  </mrow>
</mrow>
</math>
</maths>
<br/>
are required in the foregoing calculations, and they are in fact obtained from other known quantities according to:
</p>
<p id="p-0131" num="0181">
<maths id="MATH-US-00028" num="00028">
<math overflow="scroll">
<mtable>
  <mtr>
    <mtd>
      <mrow>
        <mrow>
          <msubsup>
            <mover>
              <mi>ɛ</mi>
              <mo>^</mo>
            </mover>
            <mrow>
              <mi>p</mi>
              <mo>+</mo>
              <mi>f</mi>
            </mrow>
            <mi>B</mi>
          </msubsup>
          <mo>⁡</mo>
          <mrow>
            <mo>(</mo>
            <mrow>
              <mi>t</mi>
              <mo>,</mo>
              <mrow>
                <mi>t</mi>
                <mo>-</mo>
                <mi>f</mi>
              </mrow>
            </mrow>
            <mo>)</mo>
          </mrow>
        </mrow>
        <mo>=</mo>
        <mrow>
          <mrow>
            <msubsup>
              <mi>ɛ</mi>
              <mrow>
                <mi>p</mi>
                <mo>+</mo>
                <mi>f</mi>
                <mo>+</mo>
                <mn>1</mn>
              </mrow>
              <mi>B</mi>
            </msubsup>
            <mo>⁡</mo>
            <mrow>
              <mo>(</mo>
              <mi>t</mi>
              <mo>)</mo>
            </mrow>
          </mrow>
          <mo>-</mo>
          <mrow>
            <mrow>
              <msub>
                <mi>η</mi>
                <mrow>
                  <mrow>
                    <mi>p</mi>
                    <mo>+</mo>
                    <mn>1</mn>
                  </mrow>
                  <mo>,</mo>
                  <mi>f</mi>
                </mrow>
              </msub>
              <mo>⁡</mo>
              <mrow>
                <mo>(</mo>
                <mi>t</mi>
                <mo>)</mo>
              </mrow>
            </mrow>
            <mo>⁢</mo>
            <mrow>
              <msub>
                <mi>ɛ</mi>
                <mi>pf</mi>
              </msub>
              <mo>⁡</mo>
              <mrow>
                <mo>(</mo>
                <mrow>
                  <mi>t</mi>
                  <mo>-</mo>
                  <mi>f</mi>
                </mrow>
                <mo>)</mo>
              </mrow>
            </mrow>
          </mrow>
        </mrow>
      </mrow>
    </mtd>
    <mtd>
      <mrow>
        <mo>(</mo>
        <mn>33</mn>
        <mo>)</mo>
      </mrow>
    </mtd>
  </mtr>
  <mtr>
    <mtd>
      <mrow>
        <mrow>
          <msubsup>
            <mover>
              <mi>ɛ</mi>
              <mo>^</mo>
            </mover>
            <mrow>
              <mi>p</mi>
              <mo>+</mo>
              <mi>f</mi>
            </mrow>
            <mi>F</mi>
          </msubsup>
          <mo>⁡</mo>
          <mrow>
            <mo>(</mo>
            <mrow>
              <mi>t</mi>
              <mo>,</mo>
              <mrow>
                <mi>t</mi>
                <mo>-</mo>
                <mi>f</mi>
                <mo>-</mo>
                <mn>1</mn>
              </mrow>
            </mrow>
            <mo>)</mo>
          </mrow>
        </mrow>
        <mo>=</mo>
        <mrow>
          <mrow>
            <msubsup>
              <mi>ɛ</mi>
              <mrow>
                <mi>p</mi>
                <mo>+</mo>
                <mi>f</mi>
                <mo>+</mo>
                <mn>1</mn>
              </mrow>
              <mi>F</mi>
            </msubsup>
            <mo>⁡</mo>
            <mrow>
              <mo>(</mo>
              <mi>t</mi>
              <mo>)</mo>
            </mrow>
          </mrow>
          <mo>-</mo>
          <mrow>
            <mrow>
              <msub>
                <mi>𝓋</mi>
                <mrow>
                  <mi>p</mi>
                  <mo>,</mo>
                  <mrow>
                    <mi>f</mi>
                    <mo>+</mo>
                    <mn>1</mn>
                  </mrow>
                </mrow>
              </msub>
              <mo>⁡</mo>
              <mrow>
                <mo>(</mo>
                <mi>t</mi>
                <mo>)</mo>
              </mrow>
            </mrow>
            <mo>⁢</mo>
            <mrow>
              <msub>
                <mi>ɛ</mi>
                <mrow>
                  <mi>p</mi>
                  <mo>,</mo>
                  <mi>f</mi>
                </mrow>
              </msub>
              <mo>⁡</mo>
              <mrow>
                <mo>(</mo>
                <mrow>
                  <mi>t</mi>
                  <mo>-</mo>
                  <mi>f</mi>
                  <mo>-</mo>
                  <mn>1</mn>
                </mrow>
                <mo>)</mo>
              </mrow>
            </mrow>
          </mrow>
        </mrow>
      </mrow>
    </mtd>
    <mtd>
      <mrow>
        <mo>(</mo>
        <mn>34</mn>
        <mo>)</mo>
      </mrow>
    </mtd>
  </mtr>
</mtable>
</math>
</maths>
<br/>
where η<sub>p+1,f</sub>(t) and v<sub>p,f+1</sub>(t) are coefficients determined by a modified least squares minimisation procedure which will now be described: this procedure involves recursive least squares minimisation, and the two least squares problems in Equations (33) and (34) have the same form, i.e.:
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?>ψ(<i>t</i>)=ζ(<i>t</i>)−<i>k</i>(<i>t</i>)ξ(<i>t</i>)  (35)<?in-line-formulae description="In-line Formulae" end="tail"?>
<br/>
where ζ(t) and ξ(t) are known input data (a posteriori prediction and interpolation residuals respectively), and k(t) and ψ(t) are to be calculated. Although it is a modified procedure as compared to that associated with Expressions (26) and (27), it can be shown that this recursive least squares procedure calculates the same value for the coefficient k(t) that would have been obtained by solving the conventional least squares problem described by Expressions (26) and (27) and implemented by recursion Equations (28) to (32): the latter had ψ(t) and ξ(t) as input data and yielded k(t) and ζ(t), but in the present case ψ(t) is not available. However it can be shown that k(t) can be derived without knowledge of ψ(t) provided k(t−1) and ζ(t) are known. This is explained below but in terms of the corresponding a priori residuals and associated conversion factors: a priori residuals (Roman symbols) z(t), u(t) and v(t) correspond respectively to a posteriori residuals (Greek symbols) ζ(t), ξ(t) and ψ(t).
</p>
<p id="p-0132" num="0182">Equations (28) to (32) show that k(t) can be derived from k(t−1) with knowledge of v(t). Furthermore the recursion produces z(t) as a by-product. For the purpose of implementing the aforementioned modified recursive least squares procedure, it can be shown that these equations can be reordered in a way such that it is possible to derive a value for k(t) from k(t−1) without knowledge of v(t) provided z(t) is known which is the case. Moreover, these equations, so reordered, produce v(t) as a by-product and are set out in Equations (36) to (40) below:
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>d</i>(<i>t</i>)=β<sup>2</sup><i>d</i>(<i>t</i>−1)+δ<sub>in</sub>(<i>t</i>)|<i>u</i>(<i>t</i>)|<sup>2</sup>  (36)<?in-line-formulae description="In-line Formulae" end="tail"?>
</p>
<p id="p-0133" num="0183">
<maths id="MATH-US-00029" num="00029">
<math overflow="scroll">
<mtable>
  <mtr>
    <mtd>
      <mrow>
        <mi>s</mi>
        <mo>=</mo>
        <mfrac>
          <mrow>
            <mrow>
              <msub>
                <mi>δ</mi>
                <mrow>
                  <mi>i</mi>
                  <mo>⁢</mo>
                  <mstyle>
                    <mspace width="0.3em" height="0.3ex"/>
                  </mstyle>
                  <mo>⁢</mo>
                  <mi>n</mi>
                </mrow>
              </msub>
              <mo>⁡</mo>
              <mrow>
                <mo>(</mo>
                <mi>t</mi>
                <mo>)</mo>
              </mrow>
            </mrow>
            <mo>⁢</mo>
            <mrow>
              <mi>u</mi>
              <mo>⁡</mo>
              <mrow>
                <mo>(</mo>
                <mi>t</mi>
                <mo>)</mo>
              </mrow>
            </mrow>
          </mrow>
          <mrow>
            <mi>d</mi>
            <mo>⁡</mo>
            <mrow>
              <mo>(</mo>
              <mi>t</mi>
              <mo>)</mo>
            </mrow>
          </mrow>
        </mfrac>
      </mrow>
    </mtd>
    <mtd>
      <mrow>
        <mo>(</mo>
        <mn>37</mn>
        <mo>)</mo>
      </mrow>
    </mtd>
  </mtr>
</mtable>
</math>
</maths>
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?>δ<sub>out</sub>(<i>t</i>)=δ<sub>in</sub>(<i>t</i>)  (38)<?in-line-formulae description="In-line Formulae" end="tail"?>
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>v</i>(<i>t</i>)=<i>z</i>(<i>t</i>)−<i>k</i>(<i>t</i>−1)<i>u</i>(<i>t</i>)  (39)<?in-line-formulae description="In-line Formulae" end="tail"?>
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>k</i>(<i>t</i>)=<i>k</i>(<i>t</i>−1)−<i>s z</i>(<i>t</i>)  (40)<?in-line-formulae description="In-line Formulae" end="tail"?>
</p>
<p id="p-0134" num="0184">It is reiterated that Equations (36) to (40) are in terms of a priori residuals (Roman symbols) z(t), u(t) and v(t), which correspond respectively to a posteriori residuals (Greek symbols) ζ(t), ξ(t) and ψ(t).</p>
<p id="p-0135" num="0185">The term d(t) is defined by Equation (36) and represents an estimate of the energy of the time series u(0) to u(t) inclusive but with more weight given to more recent samples. All terms on the right-hand side of Equation (36) are known at time t: for t=2 or more, the term d(t−1) is known from the previous time instant; in the case of t=1 and d(0), d(0) is set equal to a predefined value such as zero; the terms δ<sub>in</sub>(t) and u(t) are input data. The term s is a generalised rotation parameter: it is defined by Equation (37) in terms of input data δ<sub>in</sub>(t) and u(t) and d(t) calculated using Equation (36). Equation (38) defines the conversion factor δ<sub>out</sub>(t) for the residual v(t) as being equal to the known input conversion factor δ<sub>in</sub>(t). Together Equations (36) to (38) inclusive represent the calculation carried out in the boundary cells B<sub>1 </sub>and B<sub>3 </sub>and described with reference to <figref idref="DRAWINGS">FIGS. 3 and 4</figref>.</p>
<p id="p-0136" num="0186">Equations (39) and (40) express the calculation carried out in the internal cells I<sub>1 </sub>and I<sub>3 </sub>shown in <figref idref="DRAWINGS">FIGS. 3 and 4</figref>. Equation (39) expresses v(t) in terms of known quantities, i.e. input data u(t) and z(t) together with a value k(t−1) evaluated earlier for data of time (t−1), or, in the case of k(0), set to a predefined values such as zero. Equation (40) expresses k(t) in terms of the quantities s calculated using Equation (37) and the input datum z(t) together with k(t−1) from the previous time instant. With the knowledge of any starting value eg k(0) it is therefore possible to generate a succession of subsequent values of v(t) and k(t) for subsequent time instants using Equations (36) to (40).</p>
<p id="p-0137" num="0187">Equations (24) et sequi show that an a posteriori interpolation residual such as ε<sub>p,f</sub>(t−f) (or equivalently an a priori interpolation residual e<sub>p,f</sub>(t−f) and the corresponding conversion factor δ<sub>p,f</sub>(t−f)) or a one sample time interval delayed equivalent e<sub>p,f</sub>(t−f−1) can be used, together with certain prediction residuals, to generate two other interpolation residuals: relative to the indices of the original interpolation residual, one of the two residuals generated from it has a p index increased by 1 and the other has an f index increased likewise, i.e. ε<sub>p+1,f</sub>(t−f) and ε<sub>p,f+1</sub>(t−f−1). This is shown in <figref idref="DRAWINGS">FIGS. 3 and 4</figref> in terms of a priori interpolation residuals and the corresponding conversion factors.</p>
<p id="p-0138" num="0188">For convenience the vector <u style="single">{circumflex over (k)}</u><sup>N</sup>(t) is defined to be the un-normalized Kalman gain vector of order N: i.e. the ith element of <u style="single">{circumflex over (k)}</u><sup>N</sup>(t) is {circumflex over (k)}<sub>i</sub><sup>N</sup>(t)=ε<sub>N−i,i−1</sub>(t−i+1)—see Equation (7). There are many possible ways of using the method described with reference to <figref idref="DRAWINGS">FIGS. 3 and 4</figref> to generate the interpolation residuals which are the elements {circumflex over (k)}<sub>Q</sub><sup>N</sup>(t) (Q=1 to N) of the un-normalized Kalman gain vector {circumflex over (k)}<sup>N</sup>(t) for an Nth order adaptive filter.</p>
<p id="p-0139" num="0189">For economy of description, the following discussion is in terms of a posteriori residuals, whereas equivalently in the present example a priori residuals and conversion factors are in fact used. Interpolation residuals ε<sub>p,0</sub>(t) and ε<sub>0,f</sub>(t−f) have a special property. The interpolation residual ε<sub>N−i,0</sub>(t) has p=N−i and f=0: f=0 corresponds to there being no subsequent terms in the time series. This residual is therefore calculated only from preceding terms, i.e. from the same terms in the time series and in the same way as the forward prediction residual</p>
<p id="p-0140" num="0190">
<maths id="MATH-US-00030" num="00030">
<math overflow="scroll">
<mrow>
  <msubsup>
    <mi>ɛ</mi>
    <mrow>
      <mi>N</mi>
      <mo>-</mo>
      <mi>i</mi>
    </mrow>
    <mi>F</mi>
  </msubsup>
  <mo>⁡</mo>
  <mrow>
    <mo>(</mo>
    <mi>t</mi>
    <mo>)</mo>
  </mrow>
</mrow>
</math>
</maths>
<br/>
see Equation (14): these two residuals are therefore equal. Moreover, since the forward prediction residual can obtained using a least squares lattice processor, so also can the interpolation residual ε<sub>N−i,0</sub>(t).
</p>
<p id="p-0141" num="0191">As stated there are many ways that the methods described with reference to <figref idref="DRAWINGS">FIGS. 3 and 4</figref> can be utilised to generate the elements of the un-normalized Kalman gain vector {circumflex over (k)}<sup>N</sup>(t). For example, following the principle elicited above the interpolation residual ε<sub>N−i,0</sub>(t) could be used to generate another interpolation residual with an f index increased by 1 and corresponding to the immediately preceding or (t−1)th time sample, namely ε<sub>N−i,1</sub>(t−1); this procedure is repeated to produce ε<sub>N−i,2</sub>(t−2) from ε<sub>N−i,1</sub>(t−1). Iteration is repeated a total of (i−1) times until ε<sub>N−i,i−1</sub>(t−i+1) is generated, i.e. the iteration stages are:
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?>ε<sub>N−i,0</sub>(<i>t</i>)<img id="CUSTOM-CHARACTER-00002" he="2.12mm" wi="2.79mm" file="US07299251-20071120-P00002.TIF" alt="custom character" img-content="character" img-format="tif"/>ε<sub>N−i,1</sub>(<i>t</i>−1)<img id="CUSTOM-CHARACTER-00003" he="2.12mm" wi="2.79mm" file="US07299251-20071120-P00003.TIF" alt="custom character" img-content="character" img-format="tif"/>ε<sub>N−i,2</sub>(<i>t</i>−2)<img id="CUSTOM-CHARACTER-00004" he="2.12mm" wi="2.79mm" file="US07299251-20071120-P00004.TIF" alt="custom character" img-content="character" img-format="tif"/>. . . <img id="CUSTOM-CHARACTER-00005" he="2.12mm" wi="2.79mm" file="US07299251-20071120-P00005.TIF" alt="custom character" img-content="character" img-format="tif"/>ε<sub>N−i,i−1</sub>(<i>t−i</i>+1)  (41)<?in-line-formulae description="In-line Formulae" end="tail"?>
</p>
<p id="p-0142" num="0192">An alternative method for generating ε<sub>N−i,i−1</sub>(t−i+1) is as follows. The interpolation residual ε<sub>0,i−1</sub>(t−i+1) (i.e. p=0, f=i−1) is the same as the (i−1)th backward prediction residual for time t: ε<sub>i−1</sub><sup>B</sup>(t)—see Equation (16). Following the principle elicited above the interpolation residual ε<sub>0,i−1</sub>(t−i+1) is used to generate ε<sub>1,i−1</sub>(t−i+1) with a p index increased by 1; this procedure is iterated to produce ε<sub>2,i−1</sub>(t−i+1) from ε<sub>1,i−1</sub>(t−i+1). Iteration is repeated a total of (N−i) times until ε<sub>N−i,i−1</sub>(t−i+1) is generated, i.e.:
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?>ε<sub>0,i−1</sub>(<i>t−i</i>+1)<img id="CUSTOM-CHARACTER-00006" he="2.12mm" wi="2.79mm" file="US07299251-20071120-P00006.TIF" alt="custom character" img-content="character" img-format="tif"/>ε<sub>1,i−1</sub>(<i>t−i</i>−1)<img id="CUSTOM-CHARACTER-00007" he="2.12mm" wi="2.79mm" file="US07299251-20071120-P00007.TIF" alt="custom character" img-content="character" img-format="tif"/>ε<sub>2,i−1</sub>(<i>t−i</i>+1)<img id="CUSTOM-CHARACTER-00008" he="2.12mm" wi="2.79mm" file="US07299251-20071120-P00008.TIF" alt="custom character" img-content="character" img-format="tif"/>. . . <img id="CUSTOM-CHARACTER-00009" he="2.12mm" wi="2.79mm" file="US07299251-20071120-P00009.TIF" alt="custom character" img-content="character" img-format="tif"/>ε<sub>N−i,i−1</sub>(<i>t−i,i</i>−1).  (42)<?in-line-formulae description="In-line Formulae" end="tail"?>
</p>
<p id="p-0143" num="0193">The term ε<sub>N−i,i−1</sub>(t−i+1) is the ith element of the un-normalized Kalman gain vector {circumflex over (k)}<sub>i</sub><sup>N</sup>(t).</p>
<p id="p-0144" num="0194">The two methods indicated by iterations (41) and (42) above require of the order of N<sup>2 </sup>(O(N<sup>2</sup>)) computations to generate all of the required interpolation residuals making up the un-normalised Kalman gain vector, where N is the order of the adaptive filter. An alternative procedure will now be described to reduce the number of computations to O(Nlog<sub>2</sub>N).</p>
<p id="p-0145" num="0195">For convenience of description it will be assumed that N=2<sup>x</sup>, where N is the order of the filter and x is a positive integer. As described in connection with iterations (41) and (42), an interpolation residual of the kind ε<sub>p,ƒ</sub>(t−ƒ) such as ε<sub>M,M−1</sub>(t−M+1) (M=N/2) can be calculated by iteration starting from an interpolation residual ε<sub>M,0</sub>(t); because ε<sub>M,0</sub>(t) has p=M and f=0, it is derived from M preceding but 0 (no) succeeding time series data elements: it is therefore identical to the a posteriori forward prediction residual ε<sub>M</sub><sup>ƒ</sup>(t) which is obtained from the same data elements by the same calculation. In consequence, ε<sub>M,M−1</sub>(t−M+1) is obtainable by iteration of the type shown in Equation (41) from ε<sub>M</sub><sup>ƒ</sup>(t) itself obtained as described earlier from the least squares lattice processor. Furthermore, in the process of this iteration the residuals e<sub>M,ƒ</sub>(t−ƒ) with values of f between 0 and M−1 are calculated, i.e. f=1 to M−2, namely:
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?>ε<sub>M,1</sub>(<i>t</i>−1), ε<sub>M,2</sub>(<i>t</i>−2), ε<sub>M,3</sub>(<i>t</i>−3), . . . ,ε<sub>M,M−2</sub>(<i>t−M</i>+2)  (43)<?in-line-formulae description="In-line Formulae" end="tail"?>
</p>
<p id="p-0146" num="0196">The interpolation residuals in (43) are also intermediate quantities that would be calculated using the iteration shown in Equation (42) to calculate the residuals ε<sub>N−i,i−1</sub>(t−i+1) where (2≦i≦M−1). Hence if the intermediate quantities at (43) are stored for later reuse it is possible to avoid some of the computation involved in calculating other residuals making up the un-normalised Kalman gain vector.</p>
<p id="p-0147" num="0197">Similarly, the interpolation residual ε<sub>M−1,M</sub>(t−M) is obtainable by the iteration shown in Equation (42) from ε<sub>0,M</sub>(t−M), which itself is identical to the a posteriori backward prediction residual ε<sub>M</sub><sup>B</sup>(t): this iteration also generates intermediate quantities ε<sub>p,M</sub>(t−M) with values of p from 1 to M−2.</p>
<p id="p-0148" num="0198">Similar savings are also possible for calculation of other terms, i.e. ε<sub>N−i,i−1</sub>(t−i+1) with values of i given by (M+2≦i≦N−1) via iterations (41).</p>
<p id="p-0149" num="0199">Storing intermediate quantities whilst calculating ε<sub>N−3P,3P−1</sub>(t−3P+1) from ε<sub>N−3P,M</sub>(t−M) (where P=N/4) gives a reduction in computation of residuals ε<sub>N−i,i−1</sub>(t−i+1) with values of i given by (M+2≦i≦3P−1) calculated via Equation (42). In total there are four such sets of computations, i.e. ε<sub>N−i,i−1</sub>(t−i+1) with values of i given by (M+2≦i≦3P−1), (3P+2≦i≦N−1), (2≦i≦P−1) and (P+2≦i≦M−1). Furthermore these four set of computations can then be split in to eight in a similar manner and so on doubling repeatedly until N residuals needed for the Kalman gain vector are generated.</p>
<p id="p-0150" num="0200">The procedure starts by calculating two elements of the un-normalised Kalman gain vector near its centre using the iteration approaches of (41) and (42). The Mth element ε<sub>M,M−1</sub>(t−M+1) is calculated via repeated use of Equation (41) starting from the forward residual e<sub>M,0</sub>(t). The (M+1)th element ε<sub>M−1,M</sub>(t−M) is calculated by repeated use of Equation (42) starting from the backward residual ε<sub>0,M</sub>(t−M). It is not in fact essential to use these starting residuals but it is believed that they are the most economical ones to use in terms of the number of computations required.</p>
<p id="p-0151" num="0201"><figref idref="DRAWINGS">FIG. 5</figref> graphically depicts interpolation residuals ε<sub>p,ƒ</sub>(t−ƒ) plotted against horizontal and vertical axes <b>50</b> and <b>52</b> in accordance with their ƒand p index values. Each point in the triangular space <b>54</b> represents an interpolation residual. The horizontal position of a point on the diagram and the associated ƒindex value represent the number of future data samples used to generate the interpolation residual. The vertical position and the associated p index value represent the number of past data samples used to generate the interpolation residual. Values on the horizontal axis <b>50</b> are interpolation residuals with ƒ=0 to N−1 and p=0, and therefore they are also backward prediction residuals ε<sub>ƒ</sub><sup>B</sup>(t) obtained as previously described and for reasons given then. Similarly, values on the vertical axis <b>52</b> are interpolation residuals with ƒ=0 and p=0 to N−1, and therefore they are also forward prediction residuals ε<sub>p</sub><sup>F</sup>(t) obtained as before.</p>
<p id="p-0152" num="0202">The interpolation residuals required to be calculated are positioned on a diagonal dotted line <b>56</b> shown as line halves <b>56</b><i>a </i>and <b>56</b><i>b</i>—they are as has been said the elements {circumflex over (k)}<sub>Q</sub><sup>N </sup>of the un-normalised Kalman gain vector <u style="single">{circumflex over (k)}</u><sup>N</sup>(t). The sequence of residuals generated in an iteration to calculate two residuals ε<sub>M,M−1</sub>(t−M +1) and ε<sub>M−1,M</sub>(t−M) at <b>58</b> and <b>60</b> collectively central to the diagonal <b>56</b> are shown as chain lines <b>62</b> and <b>64</b> respectively.</p>
<p id="p-0153" num="0203">There are now two sub-problems, identical in principle to the original problem, but of order M=N/2: they are indicated by respective line halves <b>56</b><i>a </i>and <b>56</b><i>b</i>. The next step of the procedure is to calculate two pairs of elements, each pair collectively central to a respective line half <b>56</b><i>a </i>or <b>56</b><i>b</i>. Let N/4=M2=P. The Pth element of <u style="single">{circumflex over (k)}</u><sup>N</sup>(t), i.e. ε<sub>3P,P−1</sub>(t−P+1) is calculated by iteration of Equation (41) starting from the forward residual ε<sub>3P,0</sub>(t).</p>
<p id="p-0154" num="0204">The (P+1)th element of <u style="single">{circumflex over (k)}</u><sup>N</sup>(t), i.e. ε<sub>3P−1,P</sub>(t−P) is calculated by iteration of Equation (42) staring from the interpolation residual ε<sub>M,P</sub>(t−P), which itself was calculated during the calculation of {circumflex over (k)}<sub>M</sub><sup>N</sup>=ε<sub>M,M−1</sub>(t−M+1).</p>
<p id="p-0155" num="0205">The 3Pth element of {circumflex over (k)}<sup>N</sup>(t), i.e. ε<sub>P,3P−1</sub>(t−3P+1) is calculated by iteration of Equation (41) starting from the residual ε<sub>P,M</sub>(t−M).</p>
<p id="p-0156" num="0206">The (3P+1)th element of <u style="single">{circumflex over (k)}</u><sup>N</sup>(t), i.e. ε<sub>P−1,3P</sub>(t−3P) is calculated by iteration of Equation (42) starting from the residual ε<sub>0,3P</sub>(t−3P).</p>
<p id="p-0157" num="0207">The evolution of the computation to this point is illustrated in <figref idref="DRAWINGS">FIG. 6</figref>, in which parts equivalent to those described earlier are like-referenced: four horizontal and vertical chain lines <b>66</b> have become added to this drawing as compared to <figref idref="DRAWINGS">FIG. 5</figref> indicating calculation of four additional interpolation residuals or elements of the un-normalised Kalman gain vector.</p>
<p id="p-0158" num="0208"><figref idref="DRAWINGS">FIG. 6</figref> illustrates that there are now four sub-problems of order P, as indicated by four diagonal quarter lines such as <b>56</b><i>c</i>. This procedure of splitting each sub-problem into two half-sized sub-problems and evaluating two adjacent residuals at a line portion central region is continued until all elements of the un-normalised Kalman gain vector are calculated. It is referred to as a “divide and conquer” approach: it reduces the number of computations required from O(N<sup>2</sup>) to O(Nlog<sub>2</sub>N) because it is not necessary to evaluate residuals indicated by spaces between lines within the drawing.</p>
<p id="p-0159" num="0209">For the case N=64 the set of interpolation residuals actually calculated is illustrated in <figref idref="DRAWINGS">FIG. 7</figref>. As indicated earlier, vertical lines such as <b>70</b> indicate residuals iterated downwards using Equation (42) and horizontal lines such as <b>72</b> indicate residuals iterated to the right using Equation (41).</p>
<p id="p-0160" num="0210">From inspection of <figref idref="DRAWINGS">FIG. 7</figref>, it is seen that the general approach is to generate two orthogonal lines of residuals, select a residual approximately half way along each line and iterate orthogonally to the direction of the line, generating a respective new line of iterations until the required value of the un-normalised Kalman gain vector is reached. This procedure is repeated using residuals approximately half way to previously selected residuals and also using residuals approximately half way along each new line of iterations.</p>
<p id="p-0161" num="0211">Strictly speaking, “half way along each line” is inaccurate because there is no residual at the half way point: instead the half way point is midway between two values and, in the case of the original prediction residuals (upon the uppermost and leftmost axes <b>50</b> and <b>52</b> in <figref idref="DRAWINGS">FIG. 5</figref>), on each side of the halfway point there are N/2 residuals (thirty-two for N=64). One begins an iteration with one of the two residuals immediately adjacent to and on opposite sides of a halfway point: the residual with which one begins is that having the higher value of the index which will not be incremented in the next iteration. This is not actually essential but it is believed to result in a numerically robust procedure with the smallest the number of iterative steps. In this connection, referring back to <figref idref="DRAWINGS">FIG. 6</figref> once more, to produce the un-normalised Kalman gain vector the starting point was the forward prediction residual ε<sub>M,0</sub>(t) and iteration was carried out for M−1 steps until the second or ƒindex (initially zero) became M−1. One could also begin with the backward prediction residual ε<sub>0,M−1</sub>(t) and iterate until the first or p index becomes M, but this requires one more iteration and does not help the “divide and conquer” process. Later iterations begin similarly with a residual immediately adjacent a halfway point and having a higher value of the index not to be incremented, and the relevant halfway point is halfway between an interpolation residual providing a value of {circumflex over (k)}<sub>Q</sub><sup>N</sup>(t) obtained earlier and a starting residual for an earlier iteration.</p>
<p id="p-0162" num="0212">For filters with order N not equal to an integer power of two, the divide and conquer principle described above dan still used but the problem is divided up in a different way. Splitting each problem into two equal sub-problems will no longer be wholly appropriate to give a complete solution. A problem or sub-problem can be split into two or more not necessarily equal sub-problems: e.g. N=48 could be split into sub-problems of order 32 and 16 one of which yields 32 weight vector elements and the other 16. Any integer value of N can be treated as a sum of numbers each of which is a power of two and yields a subset of the required weight vector elements: hence the problem could be solved in this way. Many other “divide and conquer” schemes are possible based on the many different ways of dividing filter weight vectors with order N not equal to an integer power of two into sub-weight-vectors with unequal orders.</p>
<p id="p-0163" num="0213">Referring to Stage <b>16</b> of <figref idref="DRAWINGS">FIG. 1</figref> the jth element of the Kalman gain vector is calculated using Equation 7: in this equation the numerator is an a posteriori interpolation residual ε<sub>N−j,j−1</sub>(t−j+1) which is the jth element of the un-normalised Kalman gain vector. The a posteriori residual will have been calculated as in <figref idref="DRAWINGS">FIGS. 3 and 4</figref> as an a priori residual e<sub>N−j,j−1</sub>(t−j+1) and a conversion factor δ<sub>N−j,j−1</sub>(t−j+1).</p>
<p id="p-0164" num="0214">The a posteriori residual itself is calculated as
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?>ε<sub>N−j,j−1</sub>(<i>t−j</i>+1)=<i>e</i><sub>N−j,j−1</sub>(<i>t−j</i>+1)δ<sub>N−j,j−1</sub>(<i>t−j</i>+1)  (44)<?in-line-formulae description="In-line Formulae" end="tail"?>
</p>
<p id="p-0165" num="0215">The denominator in Equation 7 is of the form E<sub>N−j,j−1</sub>(t−j+1) which is updated at each iteration by
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>E</i><sub>N−j,j−1</sub>(<i>t−j</i>+1)=β<sub>3</sub><sup>2</sup><i>E</i><sub>N−j,j−1</sub>(<i>t−j</i>)+<i>e</i><sub>N−j,j−1</sub>(<i>t−j</i>+1)ε<sub>N−j,j−1</sub>(<i>t−j</i>+1)  (45)<?in-line-formulae description="In-line Formulae" end="tail"?>
</p>
<p id="p-0166" num="0216">Referring now to <figref idref="DRAWINGS">FIG. 8</figref>, an Nth order adaptive filter is shown implemented electronically. It consists of a chain of delay cells <b>102</b><sub>1</sub>, to <b>102</b><sub>N−1 </sub>connected in series between respective pairs of connection nodes <b>104</b><sub>1</sub>, to <b>104</b><sub>N</sub>. These nodes are connected to respective amplifiers <b>106</b><sub>1</sub>, to <b>106</b><sub>N </sub>with respective amplification factors w<sub>1</sub>(t) to w<sub>N</sub>(t) and connected to an update bus <b>108</b>. The amplifiers <b>106</b> provide output to a summer <b>110</b>.</p>
<p id="p-0167" num="0217">Each delay cell <b>102</b> provides a delay equal to the time between successive values of an input signal sample x(t) input to the first node <b>104</b><sub>1</sub>, first delay cell <b>102</b><sub>1</sub>, and first amplifier <b>106</b><sub>1</sub>. Upon clock activation it outputs a signal sample input x(t−1) received on an immediately preceding clock cycle and inputs a new signal sample x(t). In consequence, when x(t) is input to the first cell <b>102</b><sub>1 </sub>and first amplifier <b>106</b><sub>1</sub>, the ith (i=2 to N−1) cell <b>102</b><sub>1</sub>, and ith amplifier <b>106</b><sub>i </sub>(i=2 to N) receive input of x(t−i+1). The amplifiers have respective gain factors which collectively apply a weight vector to the signals x(t) to x(t−N+1): i.e. the ith amplifier <b>106</b><sub>i </sub>has a gain of w<sub>i</sub>(t) a time t and produces an output w<sub>i</sub>(t)x(t−i+1). The summer <b>110</b> sums the amplifier outputs, i.e. its output S<sub>0 </sub>is:</p>
<p id="p-0168" num="0218">
<maths id="MATH-US-00031" num="00031">
<math overflow="scroll">
<mtable>
  <mtr>
    <mtd>
      <mrow>
        <msub>
          <mi>S</mi>
          <mn>0</mn>
        </msub>
        <mo>=</mo>
        <mrow>
          <munderover>
            <mo>∑</mo>
            <mrow>
              <mi>i</mi>
              <mo>=</mo>
              <mn>1</mn>
            </mrow>
            <mi>N</mi>
          </munderover>
          <mo>⁢</mo>
          <mrow>
            <mrow>
              <msub>
                <mi>w</mi>
                <mi>i</mi>
              </msub>
              <mo>⁡</mo>
              <mrow>
                <mo>(</mo>
                <mi>t</mi>
                <mo>)</mo>
              </mrow>
            </mrow>
            <mo>⁢</mo>
            <mrow>
              <mi>x</mi>
              <mo>⁡</mo>
              <mrow>
                <mo>(</mo>
                <mrow>
                  <mi>t</mi>
                  <mo>-</mo>
                  <mi>i</mi>
                  <mo>+</mo>
                  <mn>1</mn>
                </mrow>
                <mo>)</mo>
              </mrow>
            </mrow>
          </mrow>
        </mrow>
      </mrow>
    </mtd>
    <mtd>
      <mrow>
        <mo>(</mo>
        <mn>46</mn>
        <mo>)</mo>
      </mrow>
    </mtd>
  </mtr>
</mtable>
</math>
</maths>
<br/>
which is the required adaptive filter output as in Equation (1). The weights w<sub>i</sub>(t) are updated via the update bus <b>108</b> in accordance with Equations (3) and (4) given earlier.
</p>
<p id="p-0169" num="0219">Provided the Kalman gain vector is known, the equations given in the foregoing description can clearly be evaluated by an appropriate computer program comprising program instructions recorded on an appropriate carrier medium and running on a conventional computer system. The carrier medium may be a memory, a floppy or compact or optical disc or other hardware recordal medium, or an electrical signal. Such a program is straightforward for a skilled programmer to implement from the foregoing description without requiring invention, because it involves well known computational procedures. An outline for a program to calculate the Kalman gain vector will now be described assuming N=2<sup>x </sup>where x is a positive integer. The code is written using the rules of the MATLAB® computer program: the semicolon (;) indicates the end of an instruction, the epsis ( . . . ) indicates the instruction continues on the next line, the symbol ‘.*’ indicates element-wise multiplication of two vectors: element-wise multiplication of two vectors a and b having respective elements a<sub>i</sub>, b<sub>i </sub>is defined as forming a vector c with an i<sup>th </sup>element c<sub>i </sub>equal to the product a<sub>i</sub>b<sub>i </sub>of like-indexed pairs of elements a<sub>i</sub>, b<sub>i</sub>(i=1, 2, 3, . . . etc).</p>
<p id="p-0170" num="0220">The Nth order un-normalised Kalman gain vector (the symbol ‘k’ below) at a given time instance is given by the four line program. The conversion of the un-normalised Kalman gain vector into the true (i.e. normalised) Kalman gain vector according to Equation (10) is straightforward for a skilled programmer to implement without requiring invention, because it involves well known computational procedures.
<ul id="ul0037" list-style="none">
    <li id="ul0037-0001" num="0000">
    <ul id="ul0038" list-style="none">
        <li id="ul0038-0001" num="0221">[km, deltakm, store state]=interpolate (N, fr, br, deltaf, deltab, fr, br, beta, store, state);</li>
        <li id="ul0038-0002" num="0222">kp=[fr(N), km, br (N)];</li>
        <li id="ul0038-0003" num="0223">deltak=[deltaf(N), deltakm, deltab(N)];</li>
        <li id="ul0038-0004" num="0224">k=kp .* deltak;
<br/>
where the N-dimensional vectors ‘fr’ and ‘br’ contain the a priori forward and backward predictions residuals of order 0 to N−1 for that time instance, deltaf, deltab are N-dimensional vectors containing the corresponding conversion factors linking the a priori residuals to the a posteriori residuals (the a posteriori residual is equal to the product of the a priori residual and the conversion factor) and the variables ‘store’ and ‘state’ are used to store intermediate quantities and are arrays of dimension N×N×2 and N×N×4 respectively. The forward and backward prediction residuals and the conversion factors can be produced as previously described using for example a QRD-based LSL program. The intermediate quantities ‘store’ and ‘state’ need to be initialised before running the program for the first time. This will be described later.
</li>
    </ul>
    </li>
</ul>
</p>
<p id="p-0171" num="0225">Defining n=2m and m being an integer not less than 2, the function ‘interpolate’ calculates an (n−2)-dimensional vector of interpolation residuals that constitute the diagonal of a triangular region in the space depicted in <figref idref="DRAWINGS">FIG. 4</figref> defined by the horizontal line <b>50</b> and the vertical line <b>52</b>. The two residuals near the centre of the diagonal are calculated using the recursions (41) and (42) via calls to the functions ‘across’ and ‘down’. If n is greater than 4, the remaining diagonal elements are calculated by two further (recursive) calls to the function ‘interpolate’ consisting of data of dimension n/2 (the “divide and conquer” technique). The function ‘interpolate’ is:
<ul id="ul0039" list-style="none">
    <li id="ul0039-0001" num="0000">
    <ul id="ul0040" list-style="none">
        <li id="ul0040-0001" num="0226">function [k, deltak, store, state]=interpolate(n, v, h, deltav, deltah, fr, br, beta, store0, state0) store=store0; state=state0;</li>
        <li id="ul0040-0002" num="0227">if (n&gt;4)
        <ul id="ul0041" list-style="none">
            <li id="ul0041-0001" num="0228">[v1, deltav1, state(1:n/2,n/2+1,:)]= . . . down(n/2, h(n/2+1), deltah(n/2+1), br(n/2+1:n), beta, state(1:n/2,n/2+1,:));</li>
            <li id="ul0041-0002" num="0229">[h1, deltah1, store((n/2+1),1:(n/2),:), state((n/2+1),1:(n/2),:)]= . . . across(n/2, v(n/2+1), deltav(n/2+1), fr((n/2+1):n), beta, store((n/2+1),1:(n/2),:), . . . state((n/2+1),1:(n/2),:));</li>
            <li id="ul0041-0003" num="0230">[ku, deltaku, store(1:n/2,n/2+1:n,:), state(1:n/2,n/2+1:n,:)]= . . . interpolate(n/2, v1, h(n/2+1:n), deltav1, deltah(n/2+1:n), fr(n/2+1:n), br(n/2+1:n), . . . beta, store(1:n/2,((n/2)+1):n,:), state(1:n/2,n/2+1:n,:));</li>
            <li id="ul0041-0004" num="0231">[k1, deltak1, store(n/2+1:n,1:n/2,:), state(n/2+1:n,1:n/2,:)]= . . . interpolate(n/2,v(n/2+1:n),h1, deltav(n/2+1:n), deltah1, fr(n/2+1:n), br(n/2+1:n), . . . beta, store(((n/2)+1):n,1:n/2,:), state(n/2+1:n,1:n/2,:));</li>
            <li id="ul0041-0005" num="0232">k=[k1, h1(n/2), v1(n/2), ku];</li>
            <li id="ul0041-0006" num="0233">deltak=[deltak1, deltah1(n/2), deltav1(n/2), deltaku];</li>
        </ul>
        </li>
        <li id="ul0040-0003" num="0234">else
        <ul id="ul0042" list-style="none">
            <li id="ul0042-0001" num="0235">[v1, deltav1, state(1:n/2,n/2+1,:)]= . . . down(n/2, h(n/2+1), deltah(n/2+1), br(n/2+1:n), beta, state(1:n/2,n/2+1,:));</li>
            <li id="ul0042-0002" num="0236">[h1, deltah1, store(n/2+1,1:n/2,:), state(n/2+1,1:n/2,:)]= . . . across(n/2, v(n/2+1), deltav(n/2+1), fr(n/2+1:n), beta, store(n/2+1,1:n/2,:), state(n/2+1,1:n/2,:));</li>
            <li id="ul0042-0003" num="0237">k=[h1(n/2), v1(n/2)];</li>
            <li id="ul0042-0004" num="0238">deltak=[deltah1(n/2), deltav1(n/2)];</li>
        </ul>
        </li>
        <li id="ul0040-0004" num="0239">end</li>
    </ul>
    </li>
</ul>
</p>
<p id="p-0172" num="0240">The function ‘down’ implements the iteration (42) i.e. starting from a residual of the form ε<sub>p,f</sub>(t−ƒ) (actually represented by the a priori residual h=e<sub>p,ƒ</sub>(t−ƒ) and the conversion factor delta0=δ<sub>p,f</sub>(t−ƒ)) and produces the sequence of residuals ε<sub>p+i,f</sub>(t−ƒ) for i=1 to n−1 (again, represented by a priori residuals and conversion factors stored in v and delta<b>1</b> respectively).</p>
<p id="p-0173" num="0241">The function ‘update’ is common to the function ‘across’ both of which are described below. The function ‘down’ is
<ul id="ul0043" list-style="none">
    <li id="ul0043-0001" num="0000">
    <ul id="ul0044" list-style="none">
        <li id="ul0044-0001" num="0242">function [v, delta<b>1</b>1, state]=down (n, h, delta0, br, beta, state0) state=state0;</li>
        <li id="ul0044-0002" num="0243">v(1)=h; delta1(1)=delta0;</li>
        <li id="ul0044-0003" num="0244">for i=2:n,
        <ul id="ul0045" list-style="none">
            <li id="ul0045-0001" num="0245">[v(i), delta1(i), state(i,1,:)]=update (v(i−1), delta1(i−1), br(i), beta, state(i,1,:));</li>
        </ul>
        </li>
        <li id="ul0044-0004" num="0246">end</li>
    </ul>
    </li>
</ul>
</p>
<p id="p-0174" num="0247">The function ‘across’ implements iteration (41) i.e. starting from a residual of the form ε<sub>p,f</sub>(t−ƒ) (actually represented by the a priori residual v=e<sub>p,f</sub>(t−ƒ) and the conversion factor delta0=δ<sub>p,f</sub>(t−ƒ)) and produces the sequence of residuals ε<sub>p,f+i</sub>(t−ƒ−i) for i=1 to n−1 (again, represented by a priori residuals and conversion factors stored in h and delta<b>1</b> respectively). Extra storage is needed compared to the function ‘down’ in order to implement the change in time index. The function ‘update’ is common to the function ‘down’ and is described below. The function ‘across’ is
<ul id="ul0046" list-style="none">
    <li id="ul0046-0001" num="0000">
    <ul id="ul0047" list-style="none">
        <li id="ul0047-0001" num="0248">function [h, delta1, store, state]=across(n, v, delta0, fr, beta, store0, state0) state=state0; store=store0;</li>
        <li id="ul0047-0002" num="0249">h(1)=v; delta1(1)=delta0;
        <ul id="ul0048" list-style="none">
            <li id="ul0048-0001" num="0250">for i=2:n,</li>
            <li id="ul0048-0002" num="0251">[h(i), delta1(i), state(1,i,:)]=update (store(1,i,1), store(1,i,2), fr(i), beta, state(1,i,:));</li>
            <li id="ul0048-0003" num="0252">store(1,i,:)=[h(i−1), delta1(i−1)];</li>
        </ul>
        </li>
        <li id="ul0047-0003" num="0253">end</li>
    </ul>
    </li>
</ul>
</p>
<p id="p-0175" num="0254">The function ‘update’ takes a residual ε<sub>p,f</sub>(t−ƒ) (actually represented by the a priori residual x=e<sub>p,f</sub>(t−ƒ) and the conversion factor delta=δ<sub>p,f</sub>(t−ƒ)) and increases either the index ‘f’ or the index ‘p’ by one depending upon the value of the input ‘r’. No ‘switching’ is required: if ‘r’ is the appropriate backward prediction residual then the index ‘p’ is increased (see Equations (24) and (33)); conversely if ‘r’ is the appropriate forward prediction residual then the index ‘f’ is increased (see equations (25) and (34)). As described earlier this requires that an RLS and a modified RLS problem be solved This is done in the functions ‘rls’ and ‘mrls’ respectively. The function ‘update’ is
<ul id="ul0049" list-style="none">
    <li id="ul0049-0001" num="0000">
    <ul id="ul0050" list-style="none">
        <li id="ul0050-0001" num="0255">function [y, delta1, state1]=update (x, delta, r, beta, state)
        <ul id="ul0051" list-style="none">
            <li id="ul0051-0001" num="0256">d=state(1); k=state(2);</li>
            <li id="ul0051-0002" num="0257">dm=state(3); km=state(4);</li>
            <li id="ul0051-0003" num="0258">[z,dm1,km1]=mris (x, r, delta,dm,beta,km);</li>
            <li id="ul0051-0004" num="0259">[y,delta1,d1,k1]=ris (z, x,delta,d,beta,k);</li>
            <li id="ul0051-0005" num="0260">state1(1)=d1; state1(2)=k1;</li>
            <li id="ul0051-0006" num="0261">state1(3)=dm1; state1(4)=km1;</li>
        </ul>
        </li>
    </ul>
    </li>
</ul>
</p>
<p id="p-0176" num="0262">The function ‘rls’ solves a RLS problem as described in Equations (28) to (32) inclusive. The function ‘rls’ is
<ul id="ul0052" list-style="none">
    <li id="ul0052-0001" num="0000">
    <ul id="ul0053" list-style="none">
        <li id="ul0053-0001" num="0263">function [z,delta1,d1,k1]=ris (x, y,delta,d,beta,k)
        <ul id="ul0054" list-style="none">
            <li id="ul0054-0001" num="0264">if ((x==0)|(delta==0))</li>
            <li id="ul0054-0002" num="0265">d1=beta^2*d;</li>
            <li id="ul0054-0003" num="0266">s=0;</li>
            <li id="ul0054-0004" num="0267">delta1=delta;</li>
        </ul>
        </li>
        <li id="ul0053-0002" num="0268">else
        <ul id="ul0055" list-style="none">
            <li id="ul0055-0001" num="0269">d1=beta^2*d+delta*abs(x)^2;</li>
            <li id="ul0055-0002" num="0270">s=delta*x/d1;</li>
            <li id="ul0055-0003" num="0271">delta1=beta^2*delta*d/d1;</li>
        </ul>
        </li>
        <li id="ul0053-0003" num="0272">end</li>
        <li id="ul0053-0004" num="0273">z=y+k*x;</li>
        <li id="ul0053-0005" num="0274">k1=k−s*z;</li>
    </ul>
    </li>
</ul>
</p>
<p id="p-0177" num="0275">The function ‘mrls’ solves a modified RLS problem as described in Equations (36) to (40) inclusive. The function ‘mrls’ is
<ul id="ul0056" list-style="none">
    <li id="ul0056-0001" num="0000">
    <ul id="ul0057" list-style="none">
        <li id="ul0057-0001" num="0276">function [y,d1,k1]=mris (x, z,delta,d,beta,k)</li>
        <li id="ul0057-0002" num="0277">if ((x==0)|(delta==0))
        <ul id="ul0058" list-style="none">
            <li id="ul0058-0001" num="0278">d1beta^2*d;</li>
            <li id="ul0058-0002" num="0279">s=0;</li>
        </ul>
        </li>
        <li id="ul0057-0003" num="0280">else
        <ul id="ul0059" list-style="none">
            <li id="ul0059-0001" num="0281">d1=beta^2*d+delta*abs(x)^2;</li>
            <li id="ul0059-0002" num="0282">s=delta*x/d1;</li>
        </ul>
        </li>
        <li id="ul0057-0004" num="0283">end</li>
        <li id="ul0057-0005" num="0284">y=z−k*x;</li>
        <li id="ul0057-0006" num="0285">k1=k−s*z;</li>
    </ul>
    </li>
</ul>
</p>
<p id="p-0178" num="0286">In the above program listing and in the description of the associated mathematics, there are certain quantities that are updated from time instance to time instance e.g. k(t−1)→k(t). Hence an initial value must be specified. An example of a way to perform this initialisation is as follows: Firstly, all stored variables involved in the computation are set to zero with the exception of the stored conversion factors which are set to unity. Then the program or hardware instantiation, up to but not including the normalisation stage, is arranged to process an initialising input time series consisting of a one followed by N zeros, where as before N is the order of the digital filter. This procedure gives appropriate initial values as required, after which a real time series can be processed.</p>
<p id="p-0179" num="0287">In recent years a new type of algorithm (termed a ‘Fast Newton’ algorithm) has appeared in the academic literature: see e.g.
<ul id="ul0060" list-style="none">
    <li id="ul0060-0001" num="0000">
    <ul id="ul0061" list-style="none">
        <li id="ul0061-0001" num="0288">M. Moonen and I. K. Proudler, “Using a Lattice Algorithm to Estimate the Kalman Gain Vector in Fast Newton-type Adaptive Filtering”, Proc. ICASSP'97, Munich, April 1997</li>
        <li id="ul0061-0002" num="0289">G. V. Moustakides &amp; S. Theodoridis, “Fast Newton Transversal Filters—A New Class of Adaptive Estimation Algorithms”, IEEE trans. SP-39(6); pp. 2184-2193. 1991.</li>
        <li id="ul0061-0003" num="0290">D. K. Phillips and C. F. N. Cowan, “Zero-Phase Signal Conditioning for Improved NLMS Performance”, 13th Int. Conf. on DSP, 2-4 Jul. 1997, Santorini, Greece, pp.37-39.</li>
        <li id="ul0061-0004" num="0291">K. Maouche and D. T. M. Slock, “A Fast Instrumental Variable Affine Projection Algorithm”, Proc. ICASSP'98, Seattle, Wash., USA, pp. 1481-4.</li>
    </ul>
    </li>
</ul>
</p>
<p id="p-0180" num="0292">Fast Newton algorithms are based on RLS algorithms and on an assumption that the input signal to be filtered can be modelled as a low order autoregressive (AR) process. This assumption allows some of the computation in the RLS algorithm to be ignored with potentially very little loss in convergence performance but a large saving in computation. In accordance with the invention it has been discovered that in order for this approach to be viable it is necessary for the RLS algorithm in use to be of a type that produces filter coefficients explicitly—which excludes the use of RLS lattice algorithms known before this invention.</p>
<p id="p-0181" num="0293">It is also advantageous, especially in acoustic applications, for the algorithm to be convertible into a Fast Newton (U) algorithm in order to be able to take advantage of any autoregressive nature in the input signal. It can be shown that the present invention is suitable for implementation using an FN approach.</p>
<p id="p-0182" num="0294">The invention is suitable for use in a variety of applications: such applications for adaptive filters are given by S Haykin, Adaptive Filter Theory, 2nd Edition, Prentice-Hall, Englewood Cliffs, N.J., USA, 1991. One such is system identification—here the requirement is to observe a real system and generate a computer model of its operation. The model can be used for many things such as health monitoring, developing a control system or to design a distortion compensation processor (e.g. for manipulating the acoustic character of rooms). System identification is achieved by monitoring the input and output of the system and feeding these signals as input data in digital electronic form to the adaptive filter implemented on a computer system. Although not strictly necessary, having the filter weights rather than some other parameterisation of the system is usually considered desirable as they are easier to interpret and manipulate.</p>
<p id="p-0183" num="0295">Adaptive filters are also applicable to communications channel equalisation: in modem digital communication systems a high data rate is often a requirement, where ‘high’ means high having regard to the maximum data rate of the communications channel in use. Individual data bits in the system may be sent so close together that their signal wave forms merge due characteristics of the communications channel. This adversely affects the ability of a receiver in the system to recover the information correctly. The standard approach is to ‘equalise’ the channel. This can be done either via a channel equalisation filter (an adaptive filter) or via Maximum Likelihood Sequence Estimation (usually implemented using the Viterbi algorithm). Such equalisation is needed, for example, in the GSM mobile telephone system, digital TV transmissions over cable networks, and data modems for telephone lines.</p>
<p id="p-0184" num="0296">One of the most effective equalisation filter structures is the ‘Decision Feedback Equaliser’. It uses previous decoding decisions (assumed to be correct) to counteract communications channel effects. It requires the ability to filter received signals independently of any updating mechanism for filter weights. Obtaining the filter weights rather than some other parameterisation of the filter makes this task easier to implement.</p>
<p id="p-0185" num="0297">Maximum Likelihood Sequence Estimation is a very powerful means of equalising a communications channel and is used in the GSM mobile telephone system. Unlike channel equalisation filtering, it works by directly estimating a transmitted information signal in a communications channel: to achieve this an estimate of the distortion introduced by the channel is required and can be provided by an adaptive filter (cf. system identification above). Again it is desirable to model the communications channel directly in terms of filter weights, as opposed to a less direct approach.</p>
<p id="p-0186" num="0298">In some applications, e.g. data modems for telephone lines, although the appropriate equalisation filter is unknown a priori and hence must be determined by an adaptive filter, once it is known it changes little with time. Here there is little point in continually updating the filter weights. The simplest alternative is to separate filtering from calculation of filter weights (sometimes called ‘offline processing’) so that the latter is, done only when required. This gives an additional advantage for high speed data, when calculation of filter weights does not need to cope with the data rate even though filtering must do so. Again, obtaining the filter weights themselves rather than some other filter parameterisation makes this approach easier to implement.</p>
<p id="p-0187" num="0299">In contrast to the aforementioned unknown but unchanging case, in some situations, for example HF radio communication, communication channel properties may vary with time requiring continual updating of equalisation filter weights. This may be achieved in two basic ways: by the use of either recursive or block adaptive filters. A recursive adaptive filter continually updates filter weights as new data arrives. A block adaptive filter partitions data into blocks and processes each block independently of the others. Recursive adaptive filters may be required to process many millions of data samples, and it is therefore vital that they behave correctly even when implemented with computer arithmetic circuits. Block adaptive filters are less susceptible to this problem since each data block represents only a limited amount of data to be processed at one time. However block adaptive filters have several drawbacks such as extra storage requirements for the blocks of data and the need for smooth ‘joining’ of the resulting processed blocks of data together.</p>
<p id="p-0188" num="0300">Constrained adaptive filters are known. They have a variety of applications such as minimum variance spectral analysis and for processing data from antenna arrays. Here an adaptive filter is required to minimise some signal subject to a constraint on the filter weights such as zero response at a particular frequency. Such adaptive filters can be updated efficiently using the Kalman gain vector. The relevant parts of the invention described herein can thus be used in a constrained adaptive filter.</p>
<p id="p-0189" num="0301">In the example of the invention described with reference to <figref idref="DRAWINGS">FIG. 1</figref> et sequi, the adaptive filter is shown as consisting of four blocks: RLS Lattice Algorithm <b>12</b>, Interpolation Residual Calculation <b>14</b>, Normalisation <b>16</b>, and Weight Update <b>18</b>: highly detailed examples of the blocks <b>12</b> to <b>18</b> have been described in the embodiment of the invention hereinbefore set out. However, with the benefit of the embodiment described it will be clear to those of ordinary skill in the art of filtering that there are many different ways in which the blocks <b>12</b> to <b>18</b> can be implemented.</p>
<p id="p-0190" num="0302">The embodiment described herein derives the Kalman gain vector itself, ignoring arithmetic inaccuracies, i.e. rounding errors. It is known that adaptive filters are realisable (possibly with reduced performance) if the Kalman gain vector is replaced by an approximation to it provided by one of a variety of alternative gain vectors: see e.g. the literature on Fast Newton Algorithms referred to above. The advantage of these alternative gain vectors is that they can be calculated with fewer operations than the Kalman gain vector.</p>
<p id="p-0191" num="0303">In the example of the invention, the first step of generating interpolation residuals is to generate least squares (LS) prediction residuals via a recursive LS Lattice (RLSL) algorithm. The RLSL algorithm could be replaced by any other method of generating prediction residuals. Other algorithms which could be used to generate LS prediction residuals are RLS algorithms such as the Conventional RLS algorithm, the Fast Transversal Filter FTF) algorithm or the QR RLS algorithm.</p>
<p id="p-0192" num="0304">One could also use Non-RLS algorithms such as the Least Mean Squares GEMS) algorithm, the Gradient Adaptive Lattice (GAL) algorithm, a Clamped LSL algorithm or a Fast Newton Transversal Filter (FNTF) algorithm. In this case (at least some of) the residuals would be approximate RLS prediction residuals. The use of a non-RLS algorithm will lead to a gain vector that is not identical to the Kalman gain vector.</p>
<p id="p-0193" num="0305">Prediction residuals need not be the only input to interpolation residual calculation at <b>14</b> in <figref idref="DRAWINGS">FIG. 1</figref>. The basis of interpolation residual calculation <b>14</b> is that iterations increase interpolation residuals' order—their p and f subscripts. Prediction residuals correspond to p=0 or f=0, and hence are possible starting variables. Clearly one can begin from any suitable set of variables. For example, a set of interpolation residuals with p=1, and a set of residuals with f=1, could be calculated (or approximated) by any means, and these could then be used to generate the desired interpolation residuals which make up the un-normalised Kalman gain vector. One could use for example the recursive interpolation algorithm given by J T Yuan, “QR decomposition based least squares lattice interpolators”, IEEE Trans SP January 2000 vol 48(1) pages 70-79.</p>
<p id="p-0194" num="0306">By suitable mathematical manipulation of Equations (24) and (25), it is also possible to envisage the calculation of interpolation residuals by iterations that decrease the interpolation residuals' order—their p and f subscripts. Hence the residuals that constitute the un-normalised Kalman gain vector could be obtained from many other residuals as starting variables. In particular it is possible to replace the divide and conquer structure in Stage <b>14</b> of <figref idref="DRAWINGS">FIG. 1</figref> with a concatenation of processing stages which “zig-zag” (see <figref idref="DRAWINGS">FIG. 5</figref>) from the first element to the last element of the un-normalised Kalman gain vector. The set of residuals generated between these two extremes can include the residuals corresponding to all other elements of the un-normalised Kalman gain vector. This method requires fewer operations than the divide and conquer process used in the preferred embodiment but has not been used therein because it has inferior numerical properties when performed using finite precision arithmetic.</p>
<p id="p-0195" num="0307">There are several options for generating the un-normalised Kalman gain vector other than the divide and conquer approach described above. For example the same vector could be calculated from Equation (42) starting from backwards prediction residuals. This would require O(N<sup>2</sup>) operations as opposed to the O(Nlog<sub>2</sub>N) operations of the divide and conquer method. An interpolation residual may be calculated using basic iterations (whereby the order of the interpolation residuals (the p and f subscripts) is increased by one) from an interpolation residual with lower indices via a variety of routes. If RLS iterations are used the result is substantially the same regardless of the route taken.</p>
<p id="p-0196" num="0308">There are therefore many potential sets of iterations which may be performed to generate the un-normalised Kalman gain vector. Furthermore, there is a whole range of other vectors which could be generated instead of the un-normalised Kalman gain vector. The Kalman gain vector is the vector used to update the filter weights in a RLS algorithm. There are many other gain vectors which could be used to update the filter weights other than the Kalman gain vector. An algorithm with a gain vector which is close to the Kalman gain vector is expected to perform similarly to a RLS algorithm.</p>
<p id="p-0197" num="0309">Approximations to the Kalman gain vector could be generated for example by replacing some (or all) of the iterations which increase the interpolation order by computationally cheaper iterations. For example, the iterations could be replaced by their LMS equivalents, or any other approximations. In some cases the computation could reduce to merely a time delay or nothing at all (NB this is based on the same underlying idea behind the ‘Fast Newton’ algorithm mentioned above). This is equivalent to making the approximation that a higher order interpolation residual is equal to a lower order interpolation residual.</p>
<p id="p-0198" num="0310">Referring to <figref idref="DRAWINGS">FIG. 5</figref> once more, a second possibility is that, rather than trying to calculate the interpolation residuals on the diagonal <b>56</b><i>a</i>/<b>56</b><i>b</i>, one could use instead the residuals that form a horizontal line <b>80</b> starting from the bottom-left element of the Kalman gain vector. This corresponds to a vector consisting of the elements ε<sub>N−1,ƒ</sub>(t−ƒ) with ƒ=0, . . . , N−1.</p>
<p id="p-0199" num="0311">In the example the un-normalised Kalman Gain vector is normalised by dividing each element of the vector by its power; The power of each residual which is an element of the un-normalised Kalman Gain vector is calculated via Equation (10). This quantity could be approximated for example from just the a priori residuals or the a posteriori residuals or via many other means. Alternative normalisation factors could be used to alter an algorithm's convergence characteristics: for example choosing a larger normalisation factor will slow down convergence, but it allows filter weights to become closer to their ideal values once converged.</p>
<p id="p-0200" num="0312">The foregoing example is based on the Kalman gain vector defined by Equation (5). In the prior art RLS algorithms are known that use the so-called “dual Kalman-gain vector”. This is a scaled version of the Kalman gain vector which is based on normalised a priori interpolation residuals rather than a posteriori residuals. It requires a slightly modified weight updating formula but it is also an RLS algorithm. Approximations (such as those mentioned above) to the dual Kalman-gain vector could also be used.</p>
<p id="p-0201" num="0313">As described above in detail, it has been discovered in accordance with the invention that a Kalman gain vector can be derived as a set of normalised interpolation residuals which can be used in an O(Nlog<sub>2</sub>N) adaptive filter. However, the Kalman gain is an integral part of one form of the ubiquitous recursive least squares (RIS) procedure. Apart from applications such as noise cancellation and communications channel equalisation which use adaptive filtering, the filtering procedure (and approximate alternatives) can be used in adaptive beamforming. It can also be used to construct multichannel adaptive filters which are sometimes used in sonar applications where they are known as broadband beamformers. The residuals that make up the Kalman gain vector are conventionally referred to in recent prior art as “interpolation residuals” only if the data being processed is in the form of a time series. If data is taken from a set of spatial distributed sensors for example, corresponding residuals do not have an accepted common descriptive name even though the basic signal processing approach still holds good For the purposes of the present invention, the expression “interpolation residual” shall be taken as referring to residuals obtained by interpolation from a sequence of data elements distributed over any dimension without limitation, i.e. time or space or any other.</p>
<p id="p-0202" num="0314">Although it is as yet unverified for all practical situations, from theoretical considerations it is believed that—where filter parameters are updated using Equation (3)—calculating the Kalman gain vector via interpolation residuals is numerically more robust than a conventional approach with similar computational requirements.</p>
<p id="p-0203" num="0315">The foregoing example used real valued data. The invention can also be applied to complex valued data such as in-phase and quadrature (I and Q) channels of data used in digital communications or radar systems.</p>
<p id="p-0204" num="0316">Whereas it is preferred to obtain the Kalman gain vector with a divide-and-conquer approach of O(Nlog<sub>2</sub>N) operations, it is quite possible to use some other method to obtain interpolation residuals such as one of the O(N<sup>2</sup>) methods described earlier if the resulting increase in number of operations is acceptable: The latter will still have an advantage because as indicated above for good theoretical reasons it is likely to be more numerically robust than alternative approaches.</p>
<?DETDESC description="Detailed Description" end="tail"?>
</description>
<us-math idrefs="MATH-US-00001" nb-file="US07299251-20071120-M00001.NB">
<img id="EMI-M00001" he="8.81mm" wi="76.20mm" file="US07299251-20071120-M00001.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-math idrefs="MATH-US-00002" nb-file="US07299251-20071120-M00002.NB">
<img id="EMI-M00002" he="4.23mm" wi="76.20mm" file="US07299251-20071120-M00002.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-math idrefs="MATH-US-00003" nb-file="US07299251-20071120-M00003.NB">
<img id="EMI-M00003" he="9.14mm" wi="76.20mm" file="US07299251-20071120-M00003.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-math idrefs="MATH-US-00004" nb-file="US07299251-20071120-M00004.NB">
<img id="EMI-M00004" he="7.03mm" wi="76.20mm" file="US07299251-20071120-M00004.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-math idrefs="MATH-US-00005" nb-file="US07299251-20071120-M00005.NB">
<img id="EMI-M00005" he="9.57mm" wi="96.69mm" file="US07299251-20071120-M00005.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-math idrefs="MATH-US-00006" nb-file="US07299251-20071120-M00006.NB">
<img id="EMI-M00006" he="10.92mm" wi="96.69mm" file="US07299251-20071120-M00006.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-math idrefs="MATH-US-00007" nb-file="US07299251-20071120-M00007.NB">
<img id="EMI-M00007" he="9.14mm" wi="96.69mm" file="US07299251-20071120-M00007.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-math idrefs="MATH-US-00008" nb-file="US07299251-20071120-M00008.NB">
<img id="EMI-M00008" he="14.14mm" wi="76.20mm" file="US07299251-20071120-M00008.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-math idrefs="MATH-US-00009" nb-file="US07299251-20071120-M00009.NB">
<img id="EMI-M00009" he="9.14mm" wi="76.20mm" file="US07299251-20071120-M00009.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-math idrefs="MATH-US-00010" nb-file="US07299251-20071120-M00010.NB">
<img id="EMI-M00010" he="9.14mm" wi="76.20mm" file="US07299251-20071120-M00010.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-math idrefs="MATH-US-00011" nb-file="US07299251-20071120-M00011.NB">
<img id="EMI-M00011" he="16.26mm" wi="76.20mm" file="US07299251-20071120-M00011.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-math idrefs="MATH-US-00012" nb-file="US07299251-20071120-M00012.NB">
<img id="EMI-M00012" he="4.23mm" wi="76.20mm" file="US07299251-20071120-M00012.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-math idrefs="MATH-US-00013" nb-file="US07299251-20071120-M00013.NB">
<img id="EMI-M00013" he="4.23mm" wi="76.20mm" file="US07299251-20071120-M00013.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-math idrefs="MATH-US-00014" nb-file="US07299251-20071120-M00014.NB">
<img id="EMI-M00014" he="4.23mm" wi="76.20mm" file="US07299251-20071120-M00014.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-math idrefs="MATH-US-00015" nb-file="US07299251-20071120-M00015.NB">
<img id="EMI-M00015" he="10.58mm" wi="96.69mm" file="US07299251-20071120-M00015.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-math idrefs="MATH-US-00016" nb-file="US07299251-20071120-M00016.NB">
<img id="EMI-M00016" he="4.23mm" wi="76.20mm" file="US07299251-20071120-M00016.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-math idrefs="MATH-US-00017" nb-file="US07299251-20071120-M00017.NB">
<img id="EMI-M00017" he="4.23mm" wi="76.20mm" file="US07299251-20071120-M00017.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-math idrefs="MATH-US-00018" nb-file="US07299251-20071120-M00018.NB">
<img id="EMI-M00018" he="4.23mm" wi="76.20mm" file="US07299251-20071120-M00018.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-math idrefs="MATH-US-00019" nb-file="US07299251-20071120-M00019.NB">
<img id="EMI-M00019" he="4.23mm" wi="76.20mm" file="US07299251-20071120-M00019.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-math idrefs="MATH-US-00020" nb-file="US07299251-20071120-M00020.NB">
<img id="EMI-M00020" he="4.23mm" wi="76.20mm" file="US07299251-20071120-M00020.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-math idrefs="MATH-US-00021" nb-file="US07299251-20071120-M00021.NB">
<img id="EMI-M00021" he="4.23mm" wi="76.20mm" file="US07299251-20071120-M00021.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-math idrefs="MATH-US-00022" nb-file="US07299251-20071120-M00022.NB">
<img id="EMI-M00022" he="4.23mm" wi="76.20mm" file="US07299251-20071120-M00022.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-math idrefs="MATH-US-00023" nb-file="US07299251-20071120-M00023.NB">
<img id="EMI-M00023" he="4.23mm" wi="76.20mm" file="US07299251-20071120-M00023.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-math idrefs="MATH-US-00024" nb-file="US07299251-20071120-M00024.NB">
<img id="EMI-M00024" he="8.81mm" wi="76.20mm" file="US07299251-20071120-M00024.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-math idrefs="MATH-US-00025" nb-file="US07299251-20071120-M00025.NB">
<img id="EMI-M00025" he="15.49mm" wi="76.20mm" file="US07299251-20071120-M00025.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-math idrefs="MATH-US-00026" nb-file="US07299251-20071120-M00026.NB">
<img id="EMI-M00026" he="4.23mm" wi="76.20mm" file="US07299251-20071120-M00026.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-math idrefs="MATH-US-00027" nb-file="US07299251-20071120-M00027.NB">
<img id="EMI-M00027" he="4.23mm" wi="76.20mm" file="US07299251-20071120-M00027.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-math idrefs="MATH-US-00028" nb-file="US07299251-20071120-M00028.NB">
<img id="EMI-M00028" he="10.58mm" wi="76.20mm" file="US07299251-20071120-M00028.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-math idrefs="MATH-US-00029" nb-file="US07299251-20071120-M00029.NB">
<img id="EMI-M00029" he="6.69mm" wi="76.20mm" file="US07299251-20071120-M00029.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-math idrefs="MATH-US-00030" nb-file="US07299251-20071120-M00030.NB">
<img id="EMI-M00030" he="3.89mm" wi="76.20mm" file="US07299251-20071120-M00030.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-math idrefs="MATH-US-00031" nb-file="US07299251-20071120-M00031.NB">
<img id="EMI-M00031" he="8.81mm" wi="76.20mm" file="US07299251-20071120-M00031.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-claim-statement>The invention claimed is:</us-claim-statement>
<claims id="claims">
<claim id="CLM-00001" num="00001">
<claim-text>1. An adaptive filter implemented by means of filter weights, the filter including means for providing a sequence of signal samples for application to the filter and means for updating the filter weights by means of a gain vector derived from interpolation residuals of the sequence of signal samples applied to the filter.</claim-text>
</claim>
<claim id="CLM-00002" num="00002">
<claim-text>2. A method for adaptive filtering with filter weights comprising the steps of:
<claim-text>a) deriving interpolation residuals from a sequence of signal samples provided as data for filtering, and</claim-text>
<claim-text>b) updating the filter weights with a gain vector derived from the interpolation residuals.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00003" num="00003">
<claim-text>3. A method according to <claim-ref idref="CLM-00002">claim 2</claim-ref> wherein the gain vector is a Kalman gain vector.</claim-text>
</claim>
<claim id="CLM-00004" num="00004">
<claim-text>4. A method for adaptive filtering including:
<claim-text>a) processing an input sequence of signal samples to derive prediction residuals;</claim-text>
<claim-text>b) converting prediction residuals to interpolation residuals;</claim-text>
<claim-text>c) deriving elements of a gain vector from the interpolation residuals; and</claim-text>
<claim-text>d) combining the gain vector with input and reference signals and update the filter coefficients or weights as required to provide adaptive filtering.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00005" num="00005">
<claim-text>5. A method according <b>4</b> claim wherein the prediction residuals are least squares prediction residuals.</claim-text>
</claim>
<claim id="CLM-00006" num="00006">
<claim-text>6. A method according to <claim-ref idref="CLM-00005">claim 5</claim-ref> for obtaining prediction residuals by processing a sequence of signal samples using a recursive least squares lattice (RLSL) algorithm.</claim-text>
</claim>
<claim id="CLM-00007" num="00007">
<claim-text>7. A method according to <claim-ref idref="CLM-00004">claim 4</claim-ref> including converting prediction residuals to interpolation residuals corresponding to gain vector elements by an iterative approach in which each iteration changes an index of a residual or of an intermediate quantity derived therefrom.</claim-text>
</claim>
<claim id="CLM-00008" num="00008">
<claim-text>8. A method according to <claim-ref idref="CLM-00007">claim 7</claim-ref> wherein the iterative approach is a divide and conquer approach which treats prediction residuals as interpolation residuals and changes the index appropriately to convert the prediction residual to an interpolation residual providing an clement of the gain vector in un-normalised form.</claim-text>
</claim>
<claim id="CLM-00009" num="00009">
<claim-text>9. A method according to <claim-ref idref="CLM-00008">claim 8</claim-ref> wherein the prediction residuals are least squares prediction residuals and the iterative approach treats the prediction residuals as interpolation residuals with zero values for one of two indices corresponding to absence of succeeding and preceding time series signal samples respectively and changes the zero index in each case sufficiently to convert the forward or backward residual to an interpolation residual which is also an element of the gain vector in un-normalised form.</claim-text>
</claim>
<claim id="CLM-00010" num="00010">
<claim-text>10. A method according to <claim-ref idref="CLM-00007">claim 7</claim-ref> wherein the iterative approach also treats as an intermediate result an iteration in a sequence thereof leading to an element of the gain vector and changes an index of the intermediate result sufficiently to convert such result to an interpolation residual corresponding to an element of the gain vector.</claim-text>
</claim>
<claim id="CLM-00011" num="00011">
<claim-text>11. A method according to <claim-ref idref="CLM-00010">claim 10</claim-ref> implementing a digital filter of order N where N is equal to 2<sup>x </sup>and x is an integer, including iteration in a sequence to generate an un-normalised element of the gain vector, the iteration beginning with use of one of two residuals immediately adjacent and on opposite sides of a halfway point, the residual of use being that having a relatively higher value of an index not to be altered in the iteration sequence, and the relevant halfway point being halfway between two sequence, and the relevant halfway point being halfway between two quantities:
<claim-text>e) one of which is an interpolation residual and the other a member of the relevant time series for which the gain vector is generated, or</claim-text>
<claim-text>f) one of which is an interpolation residual and the other a starting point for an earlier iteration, or</claim-text>
<claim-text>g) which are respectively starting and end points for an earlier iteration.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00012" num="00012">
<claim-text>12. A method according to <claim-ref idref="CLM-00010">claim 10</claim-ref> implementing a digital filter of order N where N is equal to a sum of integers each of which is a power of two, wherein the iterative approach involves treating the filter as a combination of filters each of order a respective power of two.</claim-text>
</claim>
<claim id="CLM-00013" num="00013">
<claim-text>13. A method according to <claim-ref idref="CLM-00012">claim 12</claim-ref> including iteration in a sequence to generate an un-normalised element of the gain vector, the iteration beginning with use of one of two residuals immediately adjacent and on opposite sides of a halfway point the residual of use being that having a relatively higher value of an index not to be altered in the iteration sequence, and the relevant halfway point being halfway between two sequence, and the relevant halfway point being halfway between two quantities:
<claim-text>h) one of which is an interpolation residual and the other a member of the relevant time series for which the gain vector is generated, or</claim-text>
<claim-text>i) one of which is an interpolation residual and the other a starting point for an earlier iteration, or</claim-text>
<claim-text>j) which are respectively starting and end points for an earlier iteration.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00014" num="00014">
<claim-text>14. A method according to Claim <b>4</b> including converting prediction residuals to interpolation residuals corresponding to elements of a gain vector by a QR decomposition approach in which equations of the form:
<claim-text>k) ζ(i)=ψ(i)+k(i)ξ(i) are solved for ζ(i) and k(i) given ψ(i) and ξ(i) subject to a constraint that in so doing there is minimisation of a sum of squares of ψ(j)+k(i)ξ(j) obtained for different sample indexes j, where ζ(i) is an a posteriori interpolation residual, ψ(i) and ξ(i) are a posteriori modified prediction and interpolation residuals respectively, and k(i) is a coefficient to be determined;</claim-text>
<claim-text>l) ψ(i)=ζ(i) −k(i)ξ(i) are solved for ψ(i) and ξ(i) k(i) given ζ(i) and ξ(i) subject to a constraint that in so doing the value of k(i) that is obtained is substantially that which would be obtained in solving ζ(i)=ψ(i)+k(i)ξ(i) for ζand k(i).</claim-text>
</claim-text>
</claim>
<claim id="CLM-00015" num="00015">
<claim-text>15. A method according to <claim-ref idref="CLM-00014">claim 14</claim-ref> wherein the QR decomposition employs square root free equivalents of sine and cosine rotation parameters.</claim-text>
</claim>
<claim id="CLM-00016" num="00016">
<claim-text>16. A computer software product comprising a computer readable medium containing computer readable instructions for controlling operation of computer apparatus to implement an adaptive filter, wherein the computer readable instructions provide a means for controlling the computer apparatus to input a sequence of signal samples as data for filtering, and the computer readable instructions also provide a means for controlling the computer apparatus to generate updated filter weights by means of a gain vector derived from interpolation residuals of sequence of signal samples.</claim-text>
</claim>
<claim id="CLM-00017" num="00017">
<claim-text>17. A computer software product according to <claim-ref idref="CLM-00016">claim 16</claim-ref> wherein the gain vector is a Kalman gain vector.</claim-text>
</claim>
<claim id="CLM-00018" num="00018">
<claim-text>18. A computer software product comprising readable medium containing computer readable instructions for controlling operation of computer apparatus to implement an adaptive filter wherein the computer readable instructions provide a means for controlling the computer apparatus to:
<claim-text>m) process an input sequence of signal samples to derive prediction residuals;</claim-text>
<claim-text>n) convert prediction residuals to interpolation residuals corresponding to elements of a gain vector; and</claim-text>
<claim-text>o) derive elements of a gain vector from the interpolation residuals; and</claim-text>
<claim-text>p) combine the gain vector with input and reference signals and update the filter coefficients or weights as required to provide adaptive filtering.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00019" num="00019">
<claim-text>19. A computer software product according to <claim-ref idref="CLM-00018">claim 18</claim-ref> wherein the prediction residuals are least squares prediction residuals.</claim-text>
</claim>
<claim id="CLM-00020" num="00020">
<claim-text>20. A computer software product according to <claim-ref idref="CLM-00018">claim 18</claim-ref> wherein the computer readable instructions provide a means for controlling the computer apparatus to obtain prediction residuals by processing a sequence of signal samples using a recursive least squares lattice (RLSL) algorithm.</claim-text>
</claim>
<claim id="CLM-00021" num="00021">
<claim-text>21. A computer software product according to <claim-ref idref="CLM-00018">claim 18</claim-ref> wherein the computer readable instructions provide a means for controlling the computer apparatus to convert prediction residuals to interpolation residuals corresponding to gain vector elements by an iterative approach in which each iteration changes an index of a residual or of an intermediate quantity derived therefrom.</claim-text>
</claim>
<claim id="CLM-00022" num="00022">
<claim-text>22. A computer software product according to <claim-ref idref="CLM-00021">claim 21</claim-ref> wherein the computer readable instructions provide a means for controlling the computer apparatus to implement the iterative approach by a divide and conquer approach which treats prediction residuals as interpolation residuals, and by proceeding with iteration until the index is changed appropriately to convert the prediction residual to an interpolation residual providing an element of the gain vector in un-normalised form.</claim-text>
</claim>
<claim id="CLM-00023" num="00023">
<claim-text>23. A computer software product according to <claim-ref idref="CLM-00022">claim 22</claim-ref> wherein the prediction residuals are least squares prediction residuals and the computer readable instructions provide a means for controlling the computer apparatus to implement the iterative approach by treating the prediction residuals as interpolation residuals with zero values for one of two indices corresponding to absence of succeeding and preceding time series signal samples respectively, and by proceeding with iteration until the zero index in each ease is changed sufficiently to convert the forward or backward residual to an interpolation residual which is also an element of the gain vector in un-normalised form.</claim-text>
</claim>
<claim id="CLM-00024" num="00024">
<claim-text>24. A computer software program according to <claim-ref idref="CLM-00021">claim 21</claim-ref> wherein the computer readable instructions provide a means for controlling the computer apparatus to implement the iterative approach by treating as an intermediate result an iteration in a sequence thereof leading to an element of the gain vector, and by proceeding with iteration until an index of the intermediate result is changed sufficiently to convert such result to an interpolation residual corresponding to an element of the gain vector.</claim-text>
</claim>
<claim id="CLM-00025" num="00025">
<claim-text>25. A computer software product according to <claim-ref idref="CLM-00024">claim 24</claim-ref> for use in implementing a digital filter of order N where N is equal to 2<sup>x </sup>and x is an integer, wherein the computer readable instruction provide a means for controlling the computer apparatus to implement iteration in a sequence to generate an un-normalised element of the gain vector beginning with use of one of two residuals immediately adjacent and on opposite sides of a halfway point, the residual of use being that having a relatively higher value of an index not to be altered in the iteration sequence, and the relevant halfway point being halfway between two quantities:
<claim-text>q) one of which is an interpolation residual and the other a member of the relevant time series for which the gain vector is generated, or</claim-text>
<claim-text>r) one of which is an interpolation residual and the other a starting point for an earlier iteration, or</claim-text>
<claim-text>s) which are respectively starting and end points for an earlier iteration.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00026" num="00026">
<claim-text>26. A computer software product according to <claim-ref idref="CLM-00024">claim 24</claim-ref> for use in implementing a digital filter of order N where N is equal to a sum of integers each of which is a power of two, wherein the computer readable instructions provide a means for controlling the computer apparatus to implement the iterative approach by treating the filter as a combination of filters each of order a respective power of two.</claim-text>
</claim>
<claim id="CLM-00027" num="00027">
<claim-text>27. A computer software product according to <claim-ref idref="CLM-00026">claim 26</claim-ref> wherein the computer readable instructions provide a means for controlling the computer apparatus to implement iteration in a sequence to generate an un-normalised element of the gain vector beginning with use of one of two residuals immediately adjacent and on opposite sides of a halfway point, the residual of use being that having a relatively higher value of an index not to be altered in the iteration sequence, and the relevant halfway point being halfway between two quantities:
<claim-text>t) one of which is an interpolation residual and the other a member of the relevant time series for which the gain vector is generated, or</claim-text>
<claim-text>u) one of which is an interpolation residual and the other a starting point for an earlier iteration, or</claim-text>
<claim-text>v) which are respectively staring and end points for an earlier iteration.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00028" num="00028">
<claim-text>28. A computer software product according to <claim-ref idref="CLM-00018">claim 18</claim-ref> wherein the computer readable instructions provide a means for controlling the computer apparatus to convert prediction residuals to interpolation residuals corresponding to elements of a gain vector by a QR decomposition approach in which equations of the form:
<claim-text>w) ζ(i)=ψ(i)+k(i)ξ(i) are solved for ζ(i) and k(i) given ψ(i) and ξ(i) subject to a constraint that in so doing there is minimization of a sum of squares of ψ(j)+k(i)ξ(j) obtained for different sample indexes j, where ζ(i) is an a posteriori interpolation residual, ψ(i) and ξ(i) are a posteriori modified prediction and interpolation residuals respectively, and k(i) is a coefficient to be determined; and</claim-text>
<claim-text>x) ψ(i)=ζ(i) −k(i)ξ(i) are solved for ψ(i) and ξ(i) k(i) given ζ(i) and ξ(i) subject to a constraint that in so doing the value of k(i) that is obtained is substantially that which would be obtained in solving ζ(i)=ψ(i)+k(i)ξ(i) for ζand k(i).</claim-text>
</claim-text>
</claim>
<claim id="CLM-00029" num="00029">
<claim-text>29. A computer software product according to <claim-ref idref="CLM-00028">claim 28</claim-ref> wherein the QR decomposition employs square root free equivalents of sine and cosine rotation parameters.</claim-text>
</claim>
</claims>
</us-patent-grant>
