<us-patent-grant lang="EN" dtd-version="v4.2 2006-08-23" file="US07297860-20071120.XML" status="PRODUCTION" id="us-patent-grant" country="US" date-produced="20071106" date-publ="20071120">
<us-bibliographic-data-grant>
<publication-reference>
<document-id>
<country>US</country>
<doc-number>07297860</doc-number>
<kind>B2</kind>
<date>20071120</date>
</document-id>
</publication-reference>
<application-reference appl-type="utility">
<document-id>
<country>US</country>
<doc-number>10987966</doc-number>
<date>20041112</date>
</document-id>
</application-reference>
<us-application-series-code>10</us-application-series-code>
<us-term-of-grant>
<us-term-extension>286</us-term-extension>
</us-term-of-grant>
<classifications-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>G</section>
<class>10</class>
<subclass>H</subclass>
<main-group>1</main-group>
<subgroup>00</subgroup>
<symbol-position>F</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20071120</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>G</section>
<class>10</class>
<subclass>H</subclass>
<main-group>1</main-group>
<subgroup>18</subgroup>
<symbol-position>L</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20071120</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>G</section>
<class>10</class>
<subclass>H</subclass>
<main-group>7</main-group>
<subgroup>00</subgroup>
<symbol-position>L</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20071120</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>G</section>
<class>06</class>
<subclass>Q</subclass>
<main-group>30</main-group>
<subgroup>00</subgroup>
<symbol-position>L</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20071120</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>H</section>
<class>04</class>
<subclass>K</subclass>
<main-group>1</main-group>
<subgroup>00</subgroup>
<symbol-position>L</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20071120</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
</classifications-ipcr>
<classification-national>
<country>US</country>
<main-classification> 84615</main-classification>
<further-classification> 84464 R</further-classification>
<further-classification> 84477 R</further-classification>
<further-classification> 84600</further-classification>
<further-classification> 84609</further-classification>
<further-classification> 84634</further-classification>
<further-classification> 84645</further-classification>
<further-classification>700245</further-classification>
<further-classification>705  1</further-classification>
<further-classification>705 26</further-classification>
<further-classification>705 50</further-classification>
<further-classification>705 51</further-classification>
</classification-national>
<invention-title id="d0e53">System and method for determining genre of audio</invention-title>
<references-cited>
<citation>
<patcit num="00001">
<document-id>
<country>US</country>
<doc-number>5270480</doc-number>
<kind>A</kind>
<name>Hikawa</name>
<date>19931200</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification> 84645</main-classification></classification-national>
</citation>
<citation>
<patcit num="00002">
<document-id>
<country>US</country>
<doc-number>5508470</doc-number>
<kind>A</kind>
<name>Tajima et al.</name>
<date>19960400</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification> 84609</main-classification></classification-national>
</citation>
<citation>
<patcit num="00003">
<document-id>
<country>US</country>
<doc-number>5636994</doc-number>
<kind>A</kind>
<name>Tong</name>
<date>19970600</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>434308</main-classification></classification-national>
</citation>
<citation>
<patcit num="00004">
<document-id>
<country>US</country>
<doc-number>5870842</doc-number>
<kind>A</kind>
<name>Ogden et al.</name>
<date>19990200</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification> 40411</main-classification></classification-national>
</citation>
<citation>
<patcit num="00005">
<document-id>
<country>US</country>
<doc-number>6001013</doc-number>
<kind>A</kind>
<name>Ota</name>
<date>19991200</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>463  7</main-classification></classification-national>
</citation>
<citation>
<patcit num="00006">
<document-id>
<country>US</country>
<doc-number>6034315</doc-number>
<kind>A</kind>
<name>Takenaka et al.</name>
<date>20000300</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification> 84615</main-classification></classification-national>
</citation>
<citation>
<patcit num="00007">
<document-id>
<country>US</country>
<doc-number>6140565</doc-number>
<kind>A</kind>
<name>Yamauchi et al.</name>
<date>20001000</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification> 84600</main-classification></classification-national>
</citation>
<citation>
<patcit num="00008">
<document-id>
<country>US</country>
<doc-number>6177623</doc-number>
<kind>B1</kind>
<name>Ooseki</name>
<date>20010100</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification> 84477 R</main-classification></classification-national>
</citation>
<citation>
<patcit num="00009">
<document-id>
<country>US</country>
<doc-number>6253246</doc-number>
<kind>B1</kind>
<name>Nakatsuyama</name>
<date>20010600</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>709233</main-classification></classification-national>
</citation>
<citation>
<patcit num="00010">
<document-id>
<country>US</country>
<doc-number>6263313</doc-number>
<kind>B1</kind>
<name>Milsted et al.</name>
<date>20010700</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>705  1</main-classification></classification-national>
</citation>
<citation>
<patcit num="00011">
<document-id>
<country>US</country>
<doc-number>6345256</doc-number>
<kind>B1</kind>
<name>Milsted et al.</name>
<date>20020200</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>705  1</main-classification></classification-national>
</citation>
<citation>
<patcit num="00012">
<document-id>
<country>US</country>
<doc-number>6389403</doc-number>
<kind>B1</kind>
<name>Dorak, Jr.</name>
<date>20020500</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>705 52</main-classification></classification-national>
</citation>
<citation>
<patcit num="00013">
<document-id>
<country>US</country>
<doc-number>6389538</doc-number>
<kind>B1</kind>
<name>Gruse et al.</name>
<date>20020500</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>713194</main-classification></classification-national>
</citation>
<citation>
<patcit num="00014">
<document-id>
<country>US</country>
<doc-number>6398245</doc-number>
<kind>B1</kind>
<name>Gruse et al.</name>
<date>20020600</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>280228</main-classification></classification-national>
</citation>
<citation>
<patcit num="00015">
<document-id>
<country>US</country>
<doc-number>6418421</doc-number>
<kind>B1</kind>
<name>Hurtado et al.</name>
<date>20020700</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>705 54</main-classification></classification-national>
</citation>
<citation>
<patcit num="00016">
<document-id>
<country>US</country>
<doc-number>6573444</doc-number>
<kind>B1</kind>
<name>Yamamoto et al.</name>
<date>20030600</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification> 84604</main-classification></classification-national>
</citation>
<citation>
<patcit num="00017">
<document-id>
<country>US</country>
<doc-number>6574609</doc-number>
<kind>B1</kind>
<name>Downs et al.</name>
<date>20030600</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>705 50</main-classification></classification-national>
</citation>
<citation>
<patcit num="00018">
<document-id>
<country>US</country>
<doc-number>6587837</doc-number>
<kind>B1</kind>
<name>Spagna et al.</name>
<date>20030700</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>705 26</main-classification></classification-national>
</citation>
<citation>
<patcit num="00019">
<document-id>
<country>US</country>
<doc-number>6611812</doc-number>
<kind>B2</kind>
<name>Hurtado et al.</name>
<date>20030800</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>705 26</main-classification></classification-national>
</citation>
<citation>
<patcit num="00020">
<document-id>
<country>US</country>
<doc-number>6717042</doc-number>
<kind>B2</kind>
<name>Loo et al.</name>
<date>20040400</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification> 84464 R</main-classification></classification-national>
</citation>
<citation>
<patcit num="00021">
<document-id>
<country>US</country>
<doc-number>6822154</doc-number>
<kind>B1</kind>
<name>Thai</name>
<date>20041100</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification> 84609</main-classification></classification-national>
</citation>
<citation>
<patcit num="00022">
<document-id>
<country>US</country>
<doc-number>6834110</doc-number>
<kind>B1</kind>
<name>Marconcini et al.</name>
<date>20041200</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>380239</main-classification></classification-national>
</citation>
<citation>
<patcit num="00023">
<document-id>
<country>US</country>
<doc-number>6859791</doc-number>
<kind>B1</kind>
<name>Spagna et al.</name>
<date>20050200</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>705 51</main-classification></classification-national>
</citation>
<citation>
<patcit num="00024">
<document-id>
<country>US</country>
<doc-number>6959288</doc-number>
<kind>B1</kind>
<name>Medina et al.</name>
<date>20051000</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>705 51</main-classification></classification-national>
</citation>
<citation>
<patcit num="00025">
<document-id>
<country>US</country>
<doc-number>6983371</doc-number>
<kind>B1</kind>
<name>Hurtado et al.</name>
<date>20060100</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>713189</main-classification></classification-national>
</citation>
<citation>
<patcit num="00026">
<document-id>
<country>US</country>
<doc-number>7110984</doc-number>
<kind>B1</kind>
<name>Spagna et al.</name>
<date>20060900</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>705 57</main-classification></classification-national>
</citation>
<citation>
<patcit num="00027">
<document-id>
<country>US</country>
<doc-number>7188085</doc-number>
<kind>B2</kind>
<name>Pelletier</name>
<date>20070300</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>705 50</main-classification></classification-national>
</citation>
<citation>
<patcit num="00028">
<document-id>
<country>US</country>
<doc-number>7206748</doc-number>
<kind>B1</kind>
<name>Gruse et al.</name>
<date>20070400</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>705  1</main-classification></classification-national>
</citation>
<citation>
<patcit num="00029">
<document-id>
<country>US</country>
<doc-number>7213005</doc-number>
<kind>B2</kind>
<name>Mourad et al.</name>
<date>20070500</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>705 64</main-classification></classification-national>
</citation>
<citation>
<patcit num="00030">
<document-id>
<country>US</country>
<doc-number>7228437</doc-number>
<kind>B2</kind>
<name>Spagna et al.</name>
<date>20070600</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>713193</main-classification></classification-national>
</citation>
<citation>
<patcit num="00031">
<document-id>
<country>US</country>
<doc-number>2002/0002468</doc-number>
<kind>A1</kind>
<name>Spagna et al.</name>
<date>20020100</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>705  1</main-classification></classification-national>
</citation>
<citation>
<patcit num="00032">
<document-id>
<country>US</country>
<doc-number>2002/0009289</doc-number>
<kind>A1</kind>
<name>Morishita et al.</name>
<date>20020100</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>386 83</main-classification></classification-national>
</citation>
<citation>
<patcit num="00033">
<document-id>
<country>US</country>
<doc-number>2002/0107803</doc-number>
<kind>A1</kind>
<name>Lisanke et al.</name>
<date>20020800</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>705 51</main-classification></classification-national>
</citation>
<citation>
<patcit num="00034">
<document-id>
<country>US</country>
<doc-number>2003/0040904</doc-number>
<kind>A1</kind>
<name>Whitman et al.</name>
<date>20030200</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>704212</main-classification></classification-national>
</citation>
<citation>
<patcit num="00035">
<document-id>
<country>US</country>
<doc-number>2003/0069669</doc-number>
<kind>A1</kind>
<name>Yamaura</name>
<date>20030400</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>700245</main-classification></classification-national>
</citation>
<citation>
<patcit num="00036">
<document-id>
<country>US</country>
<doc-number>2003/0105718</doc-number>
<kind>A1</kind>
<name>Hurtado et al.</name>
<date>20030600</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>705 51</main-classification></classification-national>
</citation>
<citation>
<patcit num="00037">
<document-id>
<country>US</country>
<doc-number>2003/0110130</doc-number>
<kind>A1</kind>
<name>Pelletier</name>
<date>20030600</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>705 50</main-classification></classification-national>
</citation>
<citation>
<patcit num="00038">
<document-id>
<country>US</country>
<doc-number>2003/0135464</doc-number>
<kind>A1</kind>
<name>Mourad et al.</name>
<date>20030700</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>705 50</main-classification></classification-national>
</citation>
<citation>
<patcit num="00039">
<document-id>
<country>US</country>
<doc-number>2004/0037183</doc-number>
<kind>A1</kind>
<name>Tanaka et al.</name>
<date>20040200</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>369 4716</main-classification></classification-national>
</citation>
<citation>
<patcit num="00040">
<document-id>
<country>US</country>
<doc-number>2004/0193322</doc-number>
<kind>A1</kind>
<name>Pirjanian et al.</name>
<date>20040900</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>700259</main-classification></classification-national>
</citation>
<citation>
<patcit num="00041">
<document-id>
<country>US</country>
<doc-number>2004/0231498</doc-number>
<kind>A1</kind>
<name>Li et al.</name>
<date>20041100</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification> 84634</main-classification></classification-national>
</citation>
<citation>
<patcit num="00042">
<document-id>
<country>US</country>
<doc-number>2006/0065102</doc-number>
<kind>A1</kind>
<name>Xu</name>
<date>20060300</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification> 84600</main-classification></classification-national>
</citation>
</references-cited>
<number-of-claims>10</number-of-claims>
<us-exemplary-claim>1</us-exemplary-claim>
<us-field-of-classification-search>
<classification-national>
<country>US</country>
<main-classification> 84615</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification> 84464 R</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification> 84600</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification> 84477 R</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification> 84645</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification> 84609</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification> 84634</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>700245</main-classification>
</classification-national>
</us-field-of-classification-search>
<figures>
<number-of-drawing-sheets>4</number-of-drawing-sheets>
<number-of-figures>5</number-of-figures>
</figures>
<us-related-documents>
<related-publication>
<document-id>
<country>US</country>
<doc-number>20060101985</doc-number>
<kind>A1</kind>
<date>20060518</date>
</document-id>
</related-publication>
</us-related-documents>
<parties>
<applicants>
<applicant sequence="001" app-type="applicant-inventor" designation="us-only">
<addressbook>
<last-name>Decuir</last-name>
<first-name>John David</first-name>
<address>
<city>San Diego</city>
<state>CA</state>
<country>US</country>
</address>
</addressbook>
<nationality>
<country>omitted</country>
</nationality>
<residence>
<country>US</country>
</residence>
</applicant>
</applicants>
<agents>
<agent sequence="01" rep-type="attorney">
<addressbook>
<last-name>Rogitz</last-name>
<first-name>John L.</first-name>
<address>
<country>unknown</country>
</address>
</addressbook>
</agent>
</agents>
</parties>
<assignees>
<assignee>
<addressbook>
<orgname>Sony Corporation</orgname>
<role>03</role>
<address>
<city>Tokyo</city>
<country>JP</country>
</address>
</addressbook>
</assignee>
<assignee>
<addressbook>
<orgname>Sony Electronics Inc.</orgname>
<role>02</role>
<address>
<city>Park Ridge</city>
<state>NJ</state>
<country>US</country>
</address>
</addressbook>
</assignee>
</assignees>
<examiners>
<primary-examiner>
<last-name>Donovan</last-name>
<first-name>Lincoln</first-name>
<department>2837</department>
</primary-examiner>
<assistant-examiner>
<last-name>Russell</last-name>
<first-name>Christina</first-name>
</assistant-examiner>
</examiners>
</us-bibliographic-data-grant>
<abstract id="abstract">
<p id="p-0001" num="0000">Both the beat and genre of music are used to select or generate a dance robot object for display of a robot icon on a computer monitor or for establishing the movements of a three-dimensional robot. The detected genre can define the type of dance performed by the robot. Also, the detected genre can be used to sort music. The genre can be detected using a neural network or by correlating the compressibility of the music to a genre.</p>
</abstract>
<drawings id="DRAWINGS">
<figure id="Fig-EMI-D00000" num="00000">
<img id="EMI-D00000" he="141.22mm" wi="146.30mm" file="US07297860-20071120-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00001" num="00001">
<img id="EMI-D00001" he="224.20mm" wi="146.64mm" file="US07297860-20071120-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00002" num="00002">
<img id="EMI-D00002" he="153.08mm" wi="144.27mm" file="US07297860-20071120-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00003" num="00003">
<img id="EMI-D00003" he="228.35mm" wi="165.44mm" file="US07297860-20071120-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00004" num="00004">
<img id="EMI-D00004" he="72.98mm" wi="58.59mm" file="US07297860-20071120-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
</drawings>
<description id="description">
<?BRFSUM description="Brief Summary" end="lead"?>
<heading id="h-0001" level="1">I. FIELD OF THE INVENTION</heading>
<p id="p-0002" num="0001">The present invention relates generally to music genre detection systems.</p>
<heading id="h-0002" level="1">II. BACKGROUND OF THE INVENTION</heading>
<p id="p-0003" num="0002">Entertainment robots have been provided that essentially are figurines that can be three dimensional or that can be icons displayed on a computer monitor. In either case, the software governing robot movement is the same. The robots can move in keeping with the beat of a piece of music or other audio that is accessed by the computer. As recognized herein, however, while the robot moves with the beat, the particular dance poses adopted by the robot are independent of the particular genre of music being played, thus resulting in inappropriate motions for some music. For example, a robot that exhibits motions appropriate for rap music can slow down the motions for a slower, classical music beat but cannot change the motion patterns to reflect a classical music dance form. Having made this recognition and the broader recognition that it would be advantageous to detect music genre not just for altering the dance style of an entertainment robot but also to perform other useful functions such as automatically sorting a user's music by genre, the invention set forth herein is provided.</p>
<heading id="h-0003" level="1">SUMMARY OF THE INVENTION</heading>
<p id="p-0004" num="0003">A computer-implemented method for detecting and using music genre includes receiving an audio file representing music. The method also includes determining a genre of the music for output thereof. The genre can be determined using one or both of a neural network, and a determination of the compressibility of the music, which compressibility can be correlated to a music genre. Based on the genre the motion of a robot can be established, and/or the music can be stored in a genre-sorted data storage. As used herein, a “genre” of music is independent of the particular beat of the music.</p>
<p id="p-0005" num="0004">When compressibility is used, if the music is relatively incompressible, the detected genre is indicated as being rock music, and if the music is relatively compressible, the detected genre is indicated as being classical music. The compressibility may be determined using a lossless compression algorithm.</p>
<p id="p-0006" num="0005">In another aspect, a computer system includes a processor that receives data which represents music. The processor determines a genre of the music. Based thereon, the processor establishes motions of a dance robot associated with the processor.</p>
<p id="p-0007" num="0006">In yet another aspect, a computer system includes a processor that receives data which represents music. The processor determines a genre of the music and based thereon stores the music in a genre-sorted data storage.</p>
<p id="p-0008" num="0007">In still another aspect, a computer system includes a processor that receives data which represents music. Means are available to the processor for determining a compressibility of the music. Also, means are available to the processor for correlating the compressibility to a genre.</p>
<p id="p-0009" num="0008">In another aspect, a computer system includes a processor that receives data which represents music. Means are available to the processor for processing the music using a neural network to determine a genre of the music.</p>
<?BRFSUM description="Brief Summary" end="tail"?>
<?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?>
<description-of-drawings>
<heading id="h-0004" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading>
<p id="p-0010" num="0009">The details of the present invention, both as to its structure and operation, can best be understood in reference to the accompanying drawings, in which like reference numerals refer to like parts, and in which:</p>
<p id="p-0011" num="0010"><figref idref="DRAWINGS">FIG. 1</figref> is a block diagram of the present invention;</p>
<p id="p-0012" num="0011"><figref idref="DRAWINGS">FIG. 2</figref> is a flow chart of the logic for detecting beat and genre of a music piece;</p>
<p id="p-0013" num="0012"><figref idref="DRAWINGS">FIG. 3</figref> is a flow chart of the logic for constructing a dance robot using genre;</p>
<p id="p-0014" num="0013"><figref idref="DRAWINGS">FIG. 4</figref> is a static object class diagram; and</p>
<p id="p-0015" num="0014"><figref idref="DRAWINGS">FIG. 5</figref> is a schematic representation of a three-dimensional robot programmed to move in accordance with music genre detection.</p>
</description-of-drawings>
<?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?>
<?DETDESC description="Detailed Description" end="lead"?>
<heading id="h-0005" level="1">DETAILED DESCRIPTION OF THE PREFERRED EMBODIMENTS</heading>
<p id="p-0016" num="0015">In the preferred non-limiting embodiment shown, the processors described herein may access one or more software or hardware elements to undertake the present logic. The flow charts herein illustrate the structure of the logic modules of the present invention as embodied in computer program software, in logic flow chart format, it being understood that the logic could also be represented using a state diagram or other convention. Those skilled in the art will appreciate that the flow charts illustrate the structures of logic elements, such as computer program code elements or electronic logic circuits, that function according to this invention. Manifestly, the invention is practiced in its essential embodiment by a machine component that renders the logic elements in a form that instructs a digital processing apparatus (that is, a computer or microprocessor) to perform a sequence of function steps corresponding to those shown. Internal logic could be as simple as a state machine.</p>
<p id="p-0017" num="0016">In other words, the present logic may be established as a computer program that is executed by a processor within, e.g., the present microprocessors/servers as a series of computer-executable instructions. In addition to residing on hard disk drives, these instructions may reside, for example, in RAM of the appropriate computer, or the instructions may be stored on magnetic tape, electronic read-only memory, or other appropriate data storage device.</p>
<p id="p-0018" num="0017">Referring initially to <figref idref="DRAWINGS">FIG. 1</figref>, a system is shown, generally designated <b>10</b>, which includes any appropriate computer <b>12</b> such as, for instance, a Sony VAIO® computer, having a processor <b>14</b> and a logic module <b>16</b> that stores software executable by the processor <b>14</b> in accordance with present principles. The logic module <b>16</b> can be any suitable data storage device. The processor <b>14</b> may receive manual or audible data entry from one or more input devices <b>18</b>, such as keyboards, mice, voice recognition devices, etc. and may output information on one or more output devices, such as the monitor <b>20</b> shown or onto printers, networks, other computers, etc. In addition to the logic module <b>16</b>, the processor <b>14</b> may also access a data storage <b>22</b> such as a hard disk drive, optical disk drive, and the like.</p>
<p id="p-0019" num="0018">As set forth further below, the processor <b>14</b> executes logic in the logic module <b>16</b> to process digital audio files and particularly music files for determining music genre in accordance with present principles. The files may come from a source <b>24</b> of audio, the details of which are not limiting. The audio files may come from the source already digitized or the music may be in analog format and be processed by a digitizer in accordance with principles known in the art, for further processing by the processor <b>14</b>. In one exemplary non-limiting embodiment, the source <b>24</b> may be a DVD player.</p>
<p id="p-0020" num="0019">Now referring to <figref idref="DRAWINGS">FIG. 2</figref> for an exemplary non-limiting illustration of the present logic, commencing at decision diamond <b>26</b> it is determined whether the user has indicated overriding one or both of beat and genre automatic determination. If such is the case, the logic ends for the overridden parameter, or it may proceed to block .<b>28</b> to access a user-defined beat and genre.</p>
<p id="p-0021" num="0020">When the user has not indicated a desire to override automatic beat/genre determination, the logic moves to block <b>30</b> to receive an audio file and then to decision diamond <b>32</b> to determine whether a beat has been detected in the music. Beat detection techniques are well known in the art. If a beat has not been detected, a default beat is output at block <b>34</b>; otherwise, the detected beat is output at block <b>36</b>.</p>
<p id="p-0022" num="0021">After beat detection it is determined at decision diamond <b>38</b> whether a genre has been or can be determined for the music, it being understood that the order of beat and genre determination is not critical. In accordance with the present invention, music genre may be determined by processing the music with a neural network that has been trained by the user with example pieces of music. Neural networks and the training therefor are known in the art, e.g., U.S. Pat. No. 5,619,616, incorporated herein by reference, sets forth one non-limiting exemplary neural network that may be used.</p>
<p id="p-0023" num="0022">Alternatively, as recognized by the present invention the amount by which a music details of which are not limiting. The audio files may come from the source already digitized or the music may be in analog format and be processed by a digitizer in accordance with principles known in the art, for further processing by the processor <b>14</b>. In one exemplary non-limiting embodiment, the source <b>24</b> may be a DVD player.</p>
<p id="p-0024" num="0023">Now referring to <figref idref="DRAWINGS">FIG. 2</figref> for an exemplary non-limiting illustration of the present logic, commencing at decision diamond <b>26</b> it is determined whether the user has indicated overriding one or both of beat and genre automatic determination. If such is the case, the logic ends for the overridden parameter, or it may proceed to block .<b>28</b> to access a user-defined beat and genre.</p>
<p id="p-0025" num="0024">When the user has not indicated a desire to override automatic beat/genre determination, the logic moves to block <b>30</b> to receive an audio file and then to decision diamond <b>32</b> to determine whether a beat has been detected in the music. Beat detection techniques are well known in the art. If a beat has not been detected, a default beat is output at block <b>34</b>; otherwise, the detected beat is output at block <b>36</b>.</p>
<p id="p-0026" num="0025">After beat detection it is determined at decision diamond <b>38</b> whether a genre has been or can be determined for the music, it being understood that the order of beat and genre determination is not critical. In accordance with the present invention, music genre may be determined by processing the music with a neural network that has been trained by the user with example pieces of music. Neural networks and the training therefor are known in the art, e.g., U.S. Pat. No. 5,619,616, incorporated herein by reference, sets forth one non-limiting exemplary neural network that may be used.</p>
<p id="p-0027" num="0026">Alternatively, as recognized by the present invention the amount by which a music details of which are not limiting. The audio files may come from the source already digitized or the music may be in analog format and be processed by a digitizer in accordance with principles known in the art, for further processing by the processor <b>14</b>. In one exemplary non-limiting embodiment, the source <b>24</b> may be a DVD player.</p>
<p id="p-0028" num="0027">Now referring to <figref idref="DRAWINGS">FIG. 2</figref> for an exemplary non-limiting illustration of the present logic, commencing at decision diamond <b>26</b> it is determined whether the user has indicated overriding one or both of beat and genre automatic determination. If such is the case, the logic ends for the overridden parameter, or it may proceed to block .<b>28</b> to access a user-defined beat and genre.</p>
<p id="p-0029" num="0028">When the user has not indicated a desire to override automatic beat/genre determination, the logic moves to block <b>30</b> to receive an audio file and then to decision diamond <b>32</b> to determine whether a beat has been detected in the music. Beat detection techniques are well known in the art. If a beat has not been detected, a default beat is output at block <b>34</b>; otherwise, the detected beat is output at block <b>36</b>.</p>
<p id="p-0030" num="0029">After beat detection it is determined at decision diamond <b>38</b> whether a genre has been or can be determined for the music, it being understood that the order of beat and genre determination is not critical. In accordance with the present invention, music genre may be determined by processing the music with a neural network that has been trained by the user with example pieces of music. Neural networks and the training therefor are known in the art, e.g., U.S. Pat. No. 5,619,616, incorporated herein by reference, sets forth one non-limiting exemplary neural network that may be used.</p>
<p id="p-0031" num="0030">Alternatively, as recognized by the present invention the amount by which a music details of which are not limiting. The audio files may come from the source already digitized or the music may be in analog format and be processed by a digitizer in accordance with principles known in the art, for further processing by the processor <b>14</b>. In one exemplary non-limiting embodiment, the source <b>24</b> may be a DVD player.</p>
<p id="p-0032" num="0031">Now referring to <figref idref="DRAWINGS">FIG. 2</figref> for an exemplary non-limiting illustration of the present logic, commencing at decision diamond <b>26</b> it is determined whether the user has indicated overriding one or both of beat and genre automatic determination. If such is the case, the logic ends for the overridden parameter, or it may proceed to block .<b>28</b> to access a user-defined beat and genre.</p>
<p id="p-0033" num="0032">When the user has not indicated a desire to override automatic beat/genre determination, the logic moves to block <b>30</b> to receive an audio file and then to decision diamond <b>32</b> to determine whether a beat has been detected in the music. Beat detection techniques are well known in the art. If a beat has not been detected, a default beat is output at block <b>34</b>; otherwise, the detected beat is output at block <b>36</b>.</p>
<p id="p-0034" num="0033">After beat detection it is determined at decision diamond <b>38</b> whether a genre has been or can be determined for the music, it being understood that the order of beat and genre determination is not critical. In accordance with the present invention, music genre may be determined by processing the music with a neural network that has been trained by the user with example pieces of music. Neural networks and the training therefor are known in the art, e.g., U.S. Pat. No. 5,619,616, incorporated herein by reference, sets forth one non-limiting exemplary neural network that may be used.</p>
<p id="p-0035" num="0034">Alternatively, as recognized by the present invention the amount by which a music details of which are not limiting. The audio files may come from the source already digitized or the music may be in analog format and be processed by a digitizer in accordance with principles known in the art, for further processing by the processor <b>14</b>. In one exemplary non-limiting embodiment, the source <b>24</b> may be a DVD player.</p>
<p id="p-0036" num="0035">Now referring to <figref idref="DRAWINGS">FIG. 2</figref> for an exemplary non-limiting illustration of the present logic, commencing at decision diamond <b>26</b> it is determined whether the user has indicated overriding one or both of beat and genre automatic determination. If such is the case, the logic ends for the overridden parameter, or it may proceed to block .<b>28</b> to access a user-defined beat and genre.</p>
<p id="p-0037" num="0036">When the user has not indicated a desire to override automatic beat/genre determination, the logic moves to block <b>30</b> to receive an audio file and then to decision diamond <b>32</b> to determine whether a beat has been detected in the music. Beat detection techniques are well known in the art. If a beat has not been detected, a default beat is output at block <b>34</b>; otherwise, the detected beat is output at block <b>36</b>.</p>
<p id="p-0038" num="0037">After beat detection it is determined at decision diamond <b>38</b> whether a genre has been or can be determined for the music, it being understood that the order of beat and genre determination is not critical. In accordance with the present invention, music genre may be determined by processing the music with a neural network that has been trained by the user with example pieces of music. Neural networks and the training therefor are known in the art, e.g., U.S. Pat. No. 5,619,616, incorporated herein by reference, sets forth one non-limiting exemplary neural network that may be used.</p>
<p id="p-0039" num="0038">Alternatively, as recognized by the present invention the amount by which a music details of which are not limiting. The audio files may come from the source already digitized or the music may be in analog format and be processed by a digitizer in accordance with principles known in the art, for further processing by the processor <b>14</b>. In one exemplary non-limiting embodiment, the source <b>24</b> may be a DVD player.</p>
<p id="p-0040" num="0039">Now referring to <figref idref="DRAWINGS">FIG. 2</figref> for an exemplary non-limiting illustration of the present logic, commencing at decision diamond <b>26</b> it is determined whether the user has indicated overriding one or both of beat and genre automatic determination. If such is the case, the logic ends for the overridden parameter, or it may proceed to block .<b>28</b> to access a user-defined beat and genre.</p>
<p id="p-0041" num="0040">When the user has not indicated a desire to override automatic beat/genre determination, the logic moves to block <b>30</b> to receive an audio file and then to decision diamond <b>32</b> to determine whether a beat has been detected in the music. Beat detection techniques are well known in the art. If a beat has not been detected, a default beat is output at block <b>34</b>; otherwise, the detected beat is output at block <b>36</b>.</p>
<p id="p-0042" num="0041">After beat detection it is determined at decision diamond <b>38</b> whether a genre has been or can be determined for the music, it being understood that the order of beat and genre determination is not critical. In accordance with the present invention, music genre may be determined by processing the music with a neural network that has been trained by the user with example pieces of music. Neural networks and the training therefor are known in the art, e.g., U.S. Pat. No. 5,619,616, incorporated herein by reference, sets forth one non-limiting exemplary neural network that may be used.</p>
<p id="p-0043" num="0042">Alternatively, as recognized by the present invention the amount by which a music details of which are not limiting. The audio files may come from the source already digitized or the music may be in analog format and be processed by a digitizer in accordance with principles known in the art, for further processing by the processor <b>14</b>. In one exemplary non-limiting embodiment, the source <b>24</b> may be a DVD player.</p>
<p id="p-0044" num="0043">Now referring to <figref idref="DRAWINGS">FIG. 2</figref> for an exemplary non-limiting illustration of the present logic, commencing at decision diamond <b>26</b> it is determined whether the user has indicated overriding one or both of beat and genre automatic determination. If such is the case, the logic ends for the overridden parameter, or it may proceed to block .<b>28</b> to access a user-defined beat and genre.</p>
<p id="p-0045" num="0044">When the user has not indicated a desire to override automatic beat/genre determination, the logic moves to block <b>30</b> to receive an audio file and then to decision diamond <b>32</b> to determine whether a beat has been detected in the music. Beat detection techniques are well known in the art. If a beat has not been detected, a default beat is output at block <b>34</b>; otherwise, the detected beat is output at block <b>36</b>.</p>
<p id="p-0046" num="0045">After beat detection it is determined at decision diamond <b>38</b> whether a genre has been or can be determined for the music, it being understood that the order of beat and genre determination is not critical. In accordance with the present invention, music genre may be determined by processing the music with a neural network that has been trained by the user with example pieces of music. Neural networks and the training therefor are known in the art, e.g., U.S. Pat. No. 5,619,616, incorporated herein by reference, sets forth one non-limiting exemplary neural network that may be used.</p>
<p id="p-0047" num="0046">Alternatively, as recognized by the present invention the amount by which a music details of which are not limiting. The audio files may come from the source already digitized or the music may be in analog format and be processed by a digitizer in accordance with principles known in the art, for further processing by the processor <b>14</b>. In one exemplary non-limiting embodiment, the source <b>24</b> may be a DVD player.</p>
<p id="p-0048" num="0047">Now referring to <figref idref="DRAWINGS">FIG. 2</figref> for an exemplary non-limiting illustration of the present logic, commencing at decision diamond <b>26</b> it is determined whether the user has indicated overriding one or both of beat and genre automatic determination. If such is the case, the logic ends for the overridden parameter, or it may proceed to block .<b>28</b> to access a user-defined beat and genre.</p>
<p id="p-0049" num="0048">When the user has not indicated a desire to override automatic beat/genre determination, the logic moves to block <b>30</b> to receive an audio file and then to decision diamond <b>32</b> to determine whether a beat has been detected in the music. Beat detection techniques are well known in the art. If a beat has not been detected, a default beat is output at block <b>34</b>; otherwise, the detected beat is output at block <b>36</b>.</p>
<p id="p-0050" num="0049">After beat detection it is determined at decision diamond <b>38</b> whether a genre has been or can be determined for the music, it being understood that the order of beat and genre determination is not critical. In accordance with the present invention, music genre may be determined by processing the music with a neural network that has been trained by the user with example pieces of music. Neural networks and the training therefor are known in the art, e.g., U.S. Pat. No. 5,619,616, incorporated herein by reference, sets forth one non-limiting exemplary neural network that may be used.</p>
<p id="p-0051" num="0050">Alternatively, as recognized by the present invention the amount by which a music details of which are not limiting. The audio files may come from the source already digitized or the music may be in analog format and be processed by a digitizer in accordance with principles known in the art, for further processing by the processor <b>14</b>. In one exemplary non-limiting embodiment, the source <b>24</b> may be a DVD player.</p>
<p id="p-0052" num="0051">Now referring to <figref idref="DRAWINGS">FIG. 2</figref> for an exemplary non-limiting illustration of the present logic, commencing at decision diamond <b>26</b> it is determined whether the user has indicated overriding one or both of beat and genre automatic determination. If such is the case, the logic ends for the overridden parameter, or it may proceed to block .<b>28</b> to access a user-defined beat and genre.</p>
<p id="p-0053" num="0052">When the user has not indicated a desire to override automatic beat/genre determination, the logic moves to block <b>30</b> to receive an audio file and then to decision diamond <b>32</b> to determine whether a beat has been detected in the music. Beat detection techniques are well known in the art. If a beat has not been detected, a default beat is output at block <b>34</b>; otherwise, the detected beat is output at block <b>36</b>.</p>
<p id="p-0054" num="0053">After beat detection it is determined at decision diamond <b>38</b> whether a genre has been or can be determined for the music, it being understood that the order of beat and genre determination is not critical. In accordance with the present invention, music genre may be determined by processing the music with a neural network that has been trained by the user with example pieces of music. Neural networks and the training therefor are known in the art, e.g., U.S. Pat. No. 5,619,616, incorporated herein by reference, sets forth one non-limiting exemplary neural network that may be used.</p>
<p id="p-0055" num="0054">Alternatively, as recognized by the present invention the amount by which a music details of which are not limiting. The audio files may come from the source already digitized or the music may be in analog format and be processed by a digitizer in accordance with principles known in the art, for further processing by the processor <b>14</b>. In one exemplary non-limiting embodiment, the source <b>24</b> may be a DVD player.</p>
<p id="p-0056" num="0055">Now referring to <figref idref="DRAWINGS">FIG. 2</figref> for an exemplary non-limiting illustration of the present logic, commencing at decision diamond <b>26</b> it is determined whether the user has indicated overriding one or both of beat and genre automatic determination. If such is the case, the logic ends for the overridden parameter, or it may proceed to block .<b>28</b> to access a user-defined beat and genre.</p>
<p id="p-0057" num="0056">When the user has not indicated a desire to override automatic beat/genre determination, the logic moves to block <b>30</b> to receive an audio file and then to decision diamond <b>32</b> to determine whether a beat has been detected in the music. Beat detection techniques are well known in the art. If a beat has not been detected, a default beat is output at block <b>34</b>; otherwise, the detected beat is output at block <b>36</b>.</p>
<p id="p-0058" num="0057">After beat detection it is determined at decision diamond <b>38</b> whether a genre has been or can be determined for the music, it being understood that the order of beat and genre determination is not critical. In accordance with the present invention, music genre may be determined by processing the music with a neural network that has been trained by the user with example pieces of music. Neural networks and the training therefor are known in the art, e.g., U.S. Pat. No. 5,619,616, incorporated herein by reference, sets forth one non-limiting exemplary neural network that may be used.</p>
<p id="p-0059" num="0058">Alternatively, as recognized by the present invention the amount by which a music details of which are not limiting. The audio files may come from the source already digitized or the music may be in analog format and be processed by a digitizer in accordance with principles known in the art, for further processing by the processor <b>14</b>. In one exemplary non-limiting embodiment, the source <b>24</b> may be a DVD player.</p>
<p id="p-0060" num="0059">Now referring to <figref idref="DRAWINGS">FIG. 2</figref> for an exemplary non-limiting illustration of the present logic, commencing at decision diamond <b>26</b> it is determined whether the user has indicated overriding one or both of beat and genre automatic determination. If such is the case, the logic ends for the overridden parameter, or it may proceed to block .<b>28</b> to access a user-defined beat and genre.</p>
<p id="p-0061" num="0060">When the user has not indicated a desire to override automatic beat/genre determination, the logic moves to block <b>30</b> to receive an audio file and then to decision diamond <b>32</b> to determine whether a beat has been detected in the music. Beat detection techniques are well known in the art. If a beat has not been detected, a default beat is output at block <b>34</b>; otherwise, the detected beat is output at block <b>36</b>.</p>
<p id="p-0062" num="0061">After beat detection it is determined at decision diamond <b>38</b> whether a genre has been or can be determined for the music, it being understood that the order of beat and genre determination is not critical. In accordance with the present invention, music genre may be determined by processing the music with a neural network that has been trained by the user with example pieces of music. Neural networks and the training therefor are known in the art, e.g., U.S. Pat. No. 5,619,616, incorporated herein by reference, sets forth one non-limiting exemplary neural network that may be used.</p>
<p id="p-0063" num="0062">Alternatively, as recognized by the present invention the amount by which a music file may be compressed can be correlated to a genre of music. The compression algorithm used for the compression test may be a lossless algorithm such as but not limited to the Free Lossless Audio Codec (FLAG) compression algorithm. In any case, the present invention has critically observed that if the music is relatively incompressible, e.g., can be compressed by only around thirty percent or less, the detected genre can be indicated as being rock music, and if the music is relatively compressible, e.g., can be compressed by eighty percent or so, the detected genre may be indicated as being classical music. That is, the amount by which a piece of music is compressible can be correlated to its genre. Compression amounts corresponding to other music genres can be similarly determined empirically.</p>
<p id="p-0064" num="0063">If no genre is detected, a default genre can be output at block <b>40</b>. Otherwise, the detected genre can be output at block <b>42</b>. Then, at block <b>44</b> the genre is used for, e.g., selecting a particular dance robot routine as set forth further below in reference to <figref idref="DRAWINGS">FIG. 3</figref>, or to sort the music, i.e., to store the music in a sorted list by genre, so that all classical music, is stored together on a disk that might be burned, or all rock music is stored together, etc. Or, if the user has indicated a desire to hear a particular genre of music and the piece under test is in that genre, it can be played for the user at block <b>44</b>.</p>
<p id="p-0065" num="0064">The logic of <figref idref="DRAWINGS">FIG. 3</figref> can be invoked to select or construct a moving video dancer <b>45</b> (<figref idref="DRAWINGS">FIG. 1</figref>) for display of the dancer on the monitor <b>20</b>. The same logic can be implemented in a processor <b>100</b> of a three-dimensional metal or plastic robot <b>102</b> (<figref idref="DRAWINGS">FIG. 5</figref>) to cause the robot to move in accordance with the genre of the music.</p>
<p id="p-0066" num="0065">Commencing at decision diamond <b>46</b>, if a set of generic dancers is available, e.g., a classical dancer instance of a dancer object, a rap dancer instance of a dancer object, and a country-western dancer instance of a dancer object, the logic simply selects, at block <b>48</b>, the instance matching the determined genre for display. Note that all dance robots may move to the detected beat, and what separates one dance robot from another is the pattern of dance motions exhibited by the robot, which patterns are established as set forth herein to be appropriate for the particular musical genre with which the particular dance robot is associated.</p>
<p id="p-0067" num="0066">If no set of dancer instances are available, the logic can flow from decision diamond <b>46</b> to block <b>50</b> to display a non-generic instance of a dancer object. The user can then input various desired dance movements for the displayed figure. Or, the processor can present sample dance movements so that the dance moves of the figure are altered from a default dance move at block <b>52</b>. The logic can then proceed to decision diamond <b>54</b> to receive an input from the user indicating whether the displayed dance moves meet with the user's approval, or whether the user wishes further modifications. If the user indicates dissatisfaction the logic can move to block <b>56</b> to allow the user to reset the dance movements to default, or to eliminate genre processing altogether, or to receive further user instruction as to desired movements. The logic at block <b>56</b> may be performed without user interaction. If the user indicates satisfaction with the particular movements, however, the logic moves to block <b>58</b> to save the instance of the dancer object and correlate it to the associate music genre. In the same way, the robot <b>102</b> in <figref idref="DRAWINGS">FIG. 5</figref> can be programmed (using voice recognition input, or keyboard/keypad input) to move in accordance with music genre.</p>
<p id="p-0068" num="0067"><figref idref="DRAWINGS">FIG. 4</figref> shows one non-limiting illustration of the above-described invention implemented using object oriented programming. At the system level, a single instance of an emergency manager object <b>60</b> may be provided for recovering from faults, warning of low computer battery voltage, and other emergency actions. Also, a single instance of a beat detector object <b>62</b> can be provided that includes an integer representing the detected beat of the music. And, a single instance of a sensor proxy object <b>64</b> can be provided that represents the source <b>24</b> of audio shown in <figref idref="DRAWINGS">FIG. 1</figref>.</p>
<p id="p-0069" num="0068">At the application level, a single instance of a genre detector object <b>66</b> can be provided that represents the identification of the current genre being processed. As shown, the genre detector object <b>66</b> is logically connected to the beat detector object <b>62</b>, and to one or more instances of a personality object <b>68</b> and genre object <b>70</b>. The personality object <b>68</b> represents dance personalities as might be reflected in particular dance movements of the generic dance robots discussed above, and can include an identifier and weighting factors useful for defining dance movement patterns. An instance of the genre object <b>70</b> also includes an identifier and weighting factors useful for defining dance movements, with the weighting factors being changeable in accordance with principles set forth above.</p>
<p id="p-0070" num="0069">The personality and genre objects <b>68</b>, <b>70</b> are logically connected together as shown, and the genre object <b>70</b> furthermore is logically connected to one or more instances of a dance object <b>72</b>, which is the object used to display the dance robots set forth above. An instance of a dance object <b>72</b> can include an identifier, keyframe data, and weight bounds that are used to define dance motions, and can be played and can further accept user input in accordance with logic discussed above.</p>
<p id="p-0071" num="0070"><figref idref="DRAWINGS">FIG. 4</figref> shows that a baby personality object <b>74</b>, essentially a genericless dance robot object, can feed information to the personality object <b>68</b> instances, as can an expansion pack personality object <b>76</b>, which contains the predetermined generic dance robots discussed above in relation to block <b>48</b> of <figref idref="DRAWINGS">FIG. 3</figref>. If desired, instances of an audio commentary object <b>78</b> may feed information to the genre object <b>70</b> instances as shown. An audio commentary object <b>78</b> may include an identifier and a music file in, e.g., .wav format, and can be played when an associated dance robot is displayed or when a three-dimensional robot moves. A single instance of a genre dance object <b>80</b> and one or more instances of a movement object <b>82</b> feed information to the dance object <b>72</b> and logically connect to the genre object <b>70</b>. The software objects discussed above may be accessed by the processor <b>100</b> of the robot <b>102</b> in <figref idref="DRAWINGS">FIG. 5</figref> to cause the robot <b>102</b> to move accordingly.</p>
<p id="p-0072" num="0071">While the particular SYSTEM AND METHOD FOR DETERMINING GENRE OF AUDIO as herein shown and described in detail is fully capable of attaining the above-described objects of the invention, it is to be understood that it is the presently preferred embodiment of the present invention and is thus representative of the subject matter which is broadly contemplated by the present invention, that the scope of the present invention fully encompasses other embodiments which may become obvious to those skilled in the art, and that the scope of the present invention is accordingly to be limited by nothing other than the appended claims, in which reference to an element in the singular means “at least one”. All structural and functional equivalents to the elements of the above-described preferred embodiment that are known or later come to be known to those of ordinary skill in the art are expressly incorporated herein by reference and are intended to be encompassed by the present claims. Moreover, it is not necessary for a device or method to address each and every problem sought to be solved by the present invention, for it to be encompassed by the present claims. Furthermore, no element, component, or method step in the present disclosure is intended to be dedicated to the public regardless of whether the element, component, or method step is explicitly recited in the claims. No claim element herein is to be construed under the provisions of 35 U.S.C. §112, sixth paragraph, unless the element is expressly recited using the phrase “means for”.</p>
<?DETDESC description="Detailed Description" end="tail"?>
</description>
<us-claim-statement>What is claimed is:</us-claim-statement>
<claims id="claims">
<claim id="CLM-00001" num="00001">
<claim-text>1. A computer-implemented method for detecting and using music genre, comprising:
<claim-text>receiving at a three dimensional movable dance robot an analog version of an audio signal representing music;</claim-text>
<claim-text>digitizing the signal;</claim-text>
<claim-text>determining a genre of the music for output thereof based on determining a compressibility of the music, the compressibility being correlatable to a music genre;</claim-text>
<claim-text>based on the genre, establishing movements of the dance robot.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00002" num="00002">
<claim-text>2. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein if the music is relatively incompressible, the detected genre is indicated as being rock music, and if the music is relatively compressible, the detected genre is indicated as being classical music.</claim-text>
</claim>
<claim id="CLM-00003" num="00003">
<claim-text>3. The method of <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein the compressibility is determined using a lossless compression algorithm.</claim-text>
</claim>
<claim id="CLM-00004" num="00004">
<claim-text>4. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, comprising selecting one of plural dance robot objects based on the genre.</claim-text>
</claim>
<claim id="CLM-00005" num="00005">
<claim-text>5. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, comprising storing the music based on the genre.</claim-text>
</claim>
<claim id="CLM-00006" num="00006">
<claim-text>6. A computer system, comprising:
<claim-text>at least one processor in a three dimensional dance robot aurally receiving music which is digitized and sent to the processor;</claim-text>
<claim-text>the processor executing logic for determining a genre of the music by determining how compressible the music is and correlating how compressible the music is to a genre, and based thereon establishing movement of the robot.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00007" num="00007">
<claim-text>7. The system of <claim-ref idref="CLM-00006">claim 6</claim-ref>, wherein the logic further includes storing the music in a genre-sorted data storage.</claim-text>
</claim>
<claim id="CLM-00008" num="00008">
<claim-text>8. The system of <claim-ref idref="CLM-00006">claim 6</claim-ref>, wherein if the music is relatively incompressible, the detected genre is indicated as being rock music, and if the music is relatively compressible, the detected genre is indicated as being classical music.</claim-text>
</claim>
<claim id="CLM-00009" num="00009">
<claim-text>9. A computer system, comprising:
<claim-text>a three dimensional dance robot;</claim-text>
<claim-text>at least one processor in the dance robot aurally receiving aural music, the robot digitizing the aurally received music and providing the digitized music to the processor;</claim-text>
<claim-text>means available to the processor for determining at least a beat of music and how much the music can be compressed;</claim-text>
<claim-text>means available to the processor for determining a genre of the music based on the amount the music can be compressed; and</claim-text>
<claim-text>means for establishing a dance motion of the robot based at least in part on the beat and genre.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00010" num="00010">
<claim-text>10. The system of <claim-ref idref="CLM-00009">claim 9</claim-ref>, comprising means available to the processor for storing the music in a genre-sorted data storage.</claim-text>
</claim>
</claims>
</us-patent-grant>
