<us-patent-grant lang="EN" dtd-version="v4.2 2006-08-23" file="US07298781-20071120.XML" status="PRODUCTION" id="us-patent-grant" country="US" date-produced="20071106" date-publ="20071120">
<us-bibliographic-data-grant>
<publication-reference>
<document-id>
<country>US</country>
<doc-number>07298781</doc-number>
<kind>B2</kind>
<date>20071120</date>
</document-id>
</publication-reference>
<application-reference appl-type="utility">
<document-id>
<country>US</country>
<doc-number>11147421</doc-number>
<date>20050608</date>
</document-id>
</application-reference>
<us-application-series-code>11</us-application-series-code>
<priority-claims>
<priority-claim sequence="01" kind="national">
<country>JP</country>
<doc-number>8-060572</doc-number>
<date>19960318</date>
</priority-claim>
</priority-claims>
<us-term-of-grant>
<us-term-extension>112</us-term-extension>
<disclaimer>
<text>This patent is subject to a terminal disclaimer.</text>
</disclaimer>
</us-term-of-grant>
<classifications-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>H</section>
<class>04</class>
<subclass>B</subclass>
<main-group>1</main-group>
<subgroup>66</subgroup>
<symbol-position>F</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20071120</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>H</section>
<class>04</class>
<subclass>N</subclass>
<main-group>7</main-group>
<subgroup>12</subgroup>
<symbol-position>L</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20071120</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>H</section>
<class>04</class>
<subclass>N</subclass>
<main-group>11</main-group>
<subgroup>02</subgroup>
<symbol-position>L</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20071120</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>H</section>
<class>04</class>
<subclass>N</subclass>
<main-group>11</main-group>
<subgroup>04</subgroup>
<symbol-position>L</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20071120</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
</classifications-ipcr>
<classification-national>
<country>US</country>
<main-classification>37524016</main-classification>
</classification-national>
<invention-title id="d0e73">Method of coding and decoding image</invention-title>
<references-cited>
<citation>
<patcit num="00001">
<document-id>
<country>US</country>
<doc-number>4727506</doc-number>
<kind>A</kind>
<name>Fling</name>
<date>19880200</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<patcit num="00002">
<document-id>
<country>US</country>
<doc-number>4924310</doc-number>
<kind>A</kind>
<name>Brandt</name>
<date>19900500</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<patcit num="00003">
<document-id>
<country>US</country>
<doc-number>5408269</doc-number>
<kind>A</kind>
<name>Tsukagoshi</name>
<date>19950400</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<patcit num="00004">
<document-id>
<country>US</country>
<doc-number>5461423</doc-number>
<kind>A</kind>
<name>Tsukagoshi</name>
<date>19951000</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<patcit num="00005">
<document-id>
<country>US</country>
<doc-number>5617482</doc-number>
<kind>A</kind>
<name>Brusewitz</name>
<date>19970400</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<patcit num="00006">
<document-id>
<country>US</country>
<doc-number>5646691</doc-number>
<kind>A</kind>
<name>Yokoyama</name>
<date>19970700</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<patcit num="00007">
<document-id>
<country>US</country>
<doc-number>5647049</doc-number>
<kind>A</kind>
<name>Odaka et al.</name>
<date>19970700</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<patcit num="00008">
<document-id>
<country>US</country>
<doc-number>5657087</doc-number>
<kind>A</kind>
<name>Jeong et al.</name>
<date>19970800</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<patcit num="00009">
<document-id>
<country>US</country>
<doc-number>5684538</doc-number>
<kind>A</kind>
<name>Nakaya et al.</name>
<date>19971100</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<patcit num="00010">
<document-id>
<country>US</country>
<doc-number>5757670</doc-number>
<kind>A</kind>
<name>Ti et al.</name>
<date>19980500</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<patcit num="00011">
<document-id>
<country>US</country>
<doc-number>5822007</doc-number>
<kind>A</kind>
<name>Knee et al.</name>
<date>19981000</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<patcit num="00012">
<document-id>
<country>US</country>
<doc-number>6008852</doc-number>
<kind>A</kind>
<name>Nakaya</name>
<date>19991200</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<patcit num="00013">
<document-id>
<country>US</country>
<doc-number>6178202</doc-number>
<kind>B1</kind>
<name>Nakaya</name>
<date>20010100</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<patcit num="00014">
<document-id>
<country>JP</country>
<doc-number>4-180371</doc-number>
<date>19920600</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<patcit num="00015">
<document-id>
<country>JP</country>
<doc-number>6-193970</doc-number>
<date>19940800</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<patcit num="00016">
<document-id>
<country>JP</country>
<doc-number>0632657</doc-number>
<kind>A1</kind>
<date>19950100</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<nplcit num="00017">
<othercit>Y. Nakaya et al, “Three Quanitzation Parameters of Motion Vectors in Motion Compensation,” Television Society Technical Reports, vol. 18, No. 80, BCS94-54, pp. 1-8, Dec. 1994, plus partial translation.</othercit>
</nplcit>
<category>cited by other</category>
</citation>
<citation>
<nplcit num="00018">
<othercit>M. Hotter, “Differential Estimation of the Global Motion Parameters Zoom and Pan”, Signal Processing, vol. 16, No. 3, pp. 249-265, Mar. 1989.</othercit>
</nplcit>
<category>cited by other</category>
</citation>
<citation>
<nplcit num="00019">
<othercit>International Telecommunication Union Standardization Document H. 261, Line Transmission of Non-Telephone Signals, Video Codec for Audio Visual Services at px64kbits, Mar. 1993.</othercit>
</nplcit>
<category>cited by other</category>
</citation>
<citation>
<nplcit num="00020">
<othercit>Y. Nakaya et al., “Fast Warping Algorithms for Software Codecs”, The Proceedings of the 11th Picture Coding Symposium of Japan (PCS96), Oct. 1996.</othercit>
</nplcit>
<category>cited by other</category>
</citation>
</references-cited>
<number-of-claims>4</number-of-claims>
<us-exemplary-claim>1</us-exemplary-claim>
<us-field-of-classification-search>
<classification-national>
<country>US</country>
<main-classification>37524016</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>37524025</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>37524017</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>348699</main-classification>
</classification-national>
<us-classifications-ipcr>H04N 7/12</us-classifications-ipcr>
<us-classifications-ipcr>H04B 1/66</us-classifications-ipcr>
</us-field-of-classification-search>
<figures>
<number-of-drawing-sheets>16</number-of-drawing-sheets>
<number-of-figures>22</number-of-figures>
</figures>
<us-related-documents>
<continuation>
<relation>
<parent-doc>
<document-id>
<country>US</country>
<doc-number>10714627</doc-number>
<kind>00</kind>
<date>20031118</date>
</document-id>
<parent-grant-document>
<document-id>
<country>US</country>
<doc-number>6987810</doc-number>
<kind>A </kind>
</document-id>
</parent-grant-document>
</parent-doc>
<child-doc>
<document-id>
<country>US</country>
<doc-number>11147421</doc-number>
</document-id>
</child-doc>
</relation>
</continuation>
<continuation>
<relation>
<parent-doc>
<document-id>
<country>US</country>
<doc-number>10262934</doc-number>
<kind>00</kind>
<date>20021003</date>
</document-id>
<parent-grant-document>
<document-id>
<country>US</country>
<doc-number>6687302</doc-number>
<kind>A </kind>
</document-id>
</parent-grant-document>
</parent-doc>
<child-doc>
<document-id>
<country>US</country>
<doc-number>10714627</doc-number>
</document-id>
</child-doc>
</relation>
</continuation>
<continuation>
<relation>
<parent-doc>
<document-id>
<country>US</country>
<doc-number>09987212</doc-number>
<kind>00</kind>
<date>20011113</date>
</document-id>
<parent-grant-document>
<document-id>
<country>US</country>
<doc-number>6483877</doc-number>
<kind>A </kind>
</document-id>
</parent-grant-document>
</parent-doc>
<child-doc>
<document-id>
<country>US</country>
<doc-number>10262934</doc-number>
</document-id>
</child-doc>
</relation>
</continuation>
<continuation>
<relation>
<parent-doc>
<document-id>
<country>US</country>
<doc-number>09694686</doc-number>
<kind>00</kind>
<date>20001024</date>
</document-id>
<parent-grant-document>
<document-id>
<country>US</country>
<doc-number>6442205</doc-number>
<kind>A </kind>
</document-id>
</parent-grant-document>
</parent-doc>
<child-doc>
<document-id>
<country>US</country>
<doc-number>09987212</doc-number>
</document-id>
</child-doc>
</relation>
</continuation>
<continuation>
<relation>
<parent-doc>
<document-id>
<country>US</country>
<doc-number>09438528</doc-number>
<kind>00</kind>
<date>19991112</date>
</document-id>
<parent-grant-document>
<document-id>
<country>US</country>
<doc-number>6178202</doc-number>
<kind>A </kind>
</document-id>
</parent-grant-document>
</parent-doc>
<child-doc>
<document-id>
<country>US</country>
<doc-number>09694686</doc-number>
</document-id>
</child-doc>
</relation>
</continuation>
<continuation>
<relation>
<parent-doc>
<document-id>
<country>US</country>
<doc-number>08819628</doc-number>
<kind>00</kind>
<date>19970317</date>
</document-id>
<parent-grant-document>
<document-id>
<country>US</country>
<doc-number>6008852</doc-number>
<kind>A </kind>
</document-id>
</parent-grant-document>
</parent-doc>
<child-doc>
<document-id>
<country>US</country>
<doc-number>09438528</doc-number>
</document-id>
</child-doc>
</relation>
</continuation>
<related-publication>
<document-id>
<country>US</country>
<doc-number>20050226336</doc-number>
<kind>A1</kind>
<date>20051013</date>
</document-id>
</related-publication>
</us-related-documents>
<parties>
<applicants>
<applicant sequence="001" app-type="applicant-inventor" designation="us-only">
<addressbook>
<last-name>Nakaya</last-name>
<first-name>Yuichiro</first-name>
<address>
<city>Tokyo</city>
<country>JP</country>
</address>
</addressbook>
<nationality>
<country>JP</country>
</nationality>
<residence>
<country>JP</country>
</residence>
</applicant>
</applicants>
<agents>
<agent sequence="01" rep-type="attorney">
<addressbook>
<orgname>Mattingly, Stanger, Malur &amp; Brundidge PC</orgname>
<address>
<country>unknown</country>
</address>
</addressbook>
</agent>
</agents>
</parties>
<assignees>
<assignee>
<addressbook>
<orgname>Renesas Technology Corp.</orgname>
<role>03</role>
<address>
<city>Tokyo</city>
<country>JP</country>
</address>
</addressbook>
</assignee>
</assignees>
<examiners>
<primary-examiner>
<last-name>Diep</last-name>
<first-name>Nhon</first-name>
<department>2621</department>
</primary-examiner>
</examiners>
</us-bibliographic-data-grant>
<abstract id="abstract">
<p id="p-0001" num="0000">A method of simplifying the arithmetic operation in a global motion compensation process approximates the motion vector field of the whole image without using many parameters. Motion vectors in the global motion compensation are found by the interpolation and/or extrapolation of the motion vectors of a plurality of representative points <b>602, 603</b> and <b>604</b> having particular features in the spatial distance thereof. Since the shift operation can be substituted for the division for synthesizing a predicted image of global motion compensation, the processing using a computer or a dedicated hardware is simplified.</p>
</abstract>
<drawings id="DRAWINGS">
<figure id="Fig-EMI-D00000" num="00000">
<img id="EMI-D00000" he="118.53mm" wi="124.38mm" file="US07298781-20071120-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00001" num="00001">
<img id="EMI-D00001" he="262.89mm" wi="163.41mm" orientation="landscape" file="US07298781-20071120-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00002" num="00002">
<img id="EMI-D00002" he="252.73mm" wi="167.64mm" orientation="landscape" file="US07298781-20071120-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00003" num="00003">
<img id="EMI-D00003" he="240.62mm" wi="173.06mm" file="US07298781-20071120-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00004" num="00004">
<img id="EMI-D00004" he="262.55mm" wi="160.61mm" file="US07298781-20071120-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00005" num="00005">
<img id="EMI-D00005" he="256.46mm" wi="152.65mm" file="US07298781-20071120-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00006" num="00006">
<img id="EMI-D00006" he="172.55mm" wi="159.17mm" file="US07298781-20071120-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00007" num="00007">
<img id="EMI-D00007" he="253.15mm" wi="124.12mm" file="US07298781-20071120-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00008" num="00008">
<img id="EMI-D00008" he="230.38mm" wi="128.78mm" file="US07298781-20071120-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00009" num="00009">
<img id="EMI-D00009" he="170.18mm" wi="124.97mm" file="US07298781-20071120-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00010" num="00010">
<img id="EMI-D00010" he="157.23mm" wi="124.21mm" file="US07298781-20071120-D00010.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00011" num="00011">
<img id="EMI-D00011" he="253.07mm" wi="168.83mm" file="US07298781-20071120-D00011.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00012" num="00012">
<img id="EMI-D00012" he="265.43mm" wi="166.54mm" orientation="landscape" file="US07298781-20071120-D00012.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00013" num="00013">
<img id="EMI-D00013" he="157.40mm" wi="161.88mm" file="US07298781-20071120-D00013.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00014" num="00014">
<img id="EMI-D00014" he="158.67mm" wi="169.59mm" file="US07298781-20071120-D00014.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00015" num="00015">
<img id="EMI-D00015" he="248.07mm" wi="158.67mm" orientation="landscape" file="US07298781-20071120-D00015.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00016" num="00016">
<img id="EMI-D00016" he="245.45mm" wi="151.55mm" file="US07298781-20071120-D00016.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
</drawings>
<description id="description">
<?RELAPP description="Other Patent Relations" end="lead"?>
<p id="p-0002" num="0001">The above-referenced patent application is a continuation application of U.S. Ser. No. 10/714,627, filed Nov. 18, 2003, now U.S. Pat. No. 6,987,810 which is a continuation of application of U.S. Ser. No. 10/262,934, filed Oct. 3, 2002, now U.S. Pat. No. 6,687,302 which is a continuation application of U.S. Ser. No. 09/987,212, filed Nov. 13, 2001, now U.S. Pat. No. 6,483,877, which is a continuation application of U.S. Ser. No. 09/694,686, filed Oct. 24, 2000, now U.S. Pat. No. 6,442,205, which is a continuation application of U.S. Ser. No. 09/438,528, filed Nov. 12, 1999, now U.S. Pat. No. 6,178,202, which is a continuation application of U.S. Ser. No. 08/819,628, filed Mar. 17,1997, now U.S. Pat. No. 6,008,852, from which priority is claimed under 35 U.S.C. §120. This application is related to U.S. Ser. No. 10/262,936, filed Oct. 3, 2002.</p>
<?RELAPP description="Other Patent Relations" end="tail"?>
<?BRFSUM description="Brief Summary" end="lead"?>
<heading id="h-0001" level="1">FIELD OF THE INVENTION</heading>
<p id="p-0003" num="0002">The present invention relates to a method of coding and decoding an image by applying global motion compensation to the whole image based on linear interpolation and/or extrapolation or bilinear interpolation and/or extrapolation.</p>
<heading id="h-0002" level="1">BACKGROUND OF THE INVENTION</heading>
<p id="p-0004" num="0003">In the highly efficient coding of a dynamic image, it has been known, in recognition of the similarity of the frames that are close to each other in regard to time, to use motion compensation in compressing the data. The most widely used motion compensation system at present image coding technology is block matching, employed in Standards H.261, MPEG1 and MPEG2 which are international standards for a dynamic image coding system. According to this system, the image to be coded is divided into a number of blocks, and a motion vector is found for each of the blocks.</p>
<p id="p-0005" num="0004"><figref idref="DRAWINGS">FIG. 1</figref> illustrates the constitution of a coder <b>100</b> of the H.261 Standard which employs a hybrid coding system (adaptive interframe/intraframe coding method) which is a combination of block matching and DCT (discrete cosine transform). A subtractor <b>102</b> calculates the difference between an input image (original image of present frame) <b>101</b> and an output image <b>113</b> (that will be described later) of an interframe/intraframe switching unit <b>119</b>, and outputs an error image <b>103</b>. The error image is transformed into a DCT coefficient through a DCT processor <b>104</b> and is quantized through a quantizer <b>105</b> to obtain a quantized DCT coefficient <b>106</b>. The quantized DCT coefficient is output as transfer data onto a communication line and is, at the same time, used in the coder to synthesize an interframe predicted image. A procedure for synthesizing the predicted image will be described below. The quantized DCT coefficient <b>106</b> passes through a dequantizer <b>108</b> and an inverse DCT processor <b>109</b> to form a reconstructed error image <b>110</b> (the same image as the error image reproduced on the receiving side).</p>
<p id="p-0006" num="0005">An output image <b>113</b> (that will be described later) of the interframe/intraframe switching unit <b>119</b> is added thereto through an adder <b>111</b>, thereby to obtain a reconstructed image <b>112</b> of the present frame (the same image as the reconstructed image of the present frame reproduced on the receiving side). The image is temporarily stored in a frame memory <b>114</b> and is delayed in time by one frame. At the present moment, therefore, the frame memory <b>114</b> is outputting a reconstructed image <b>115</b> of the preceding frame. The reconstructed image of the preceding frame and the input image <b>101</b> of the present frame are input to a block matching unit <b>116</b> where block matching is executed.</p>
<p id="p-0007" num="0006">In the block matching, an image is divided into a plurality of blocks, and a portion most resembling the original image of the present frame is taken out for each of the blocks from the reconstructed image of the preceding frame, thereby synthesizing a predicted image <b>117</b> of the present frame. At this moment, it is necessary to execute a processing (local motion estimation) for detecting how much the blocks have moved from the preceding frame to the present frame. The motion vectors of the blocks detected by the motion estimation are transmitted to the receiving side as motion data <b>120</b>. From the motion data and the reconstructed image of the preceding frame, the receiving side can synthesize an estimated image which is the same as the one that is obtained independently on the transmitting side.</p>
<p id="p-0008" num="0007">Referring again to <figref idref="DRAWINGS">FIG. 1</figref>, the estimated image <b>117</b> is input together with a “0” signal <b>118</b> to the interframe/intraframe switching unit <b>119</b>. Upon selecting either of the two inputs, the switching unit switches the coding either the interframe coding or the intraframe coding. When the predicted image <b>117</b> is selected (<figref idref="DRAWINGS">FIG. 2</figref> illustrates this case), the interframe coding is executed. When the “0” signal is selected, on the other hand, the input image is directly DCT-coded and is output to the communication line. Therefore, the intraframe coding is executed.</p>
<p id="p-0009" num="0008">In order to properly obtain the reconstructed image on the receiving side, it becomes necessary to know whether the interframe coding is executed or the intraframe coding is executed on the transmitting side. For this purpose, a distinction flag <b>121</b> is output to the communication line. The final H.261 coded bit stream <b>123</b> is obtained by multiplexing the quantized DCT coefficient, motion vector, and interframe/intraframe distinction flag into multiplexed data in a multiplexer <b>122</b>.</p>
<p id="p-0010" num="0009"><figref idref="DRAWINGS">FIG. 2</figref> illustrates the constitution of a decoder <b>200</b> for receiving a coded bit stream output from the coder of <figref idref="DRAWINGS">FIG. 1</figref>. The H.261 bit stream <b>217</b> that is received is separated through a separator <b>216</b> into a quantized DCT coefficient <b>201</b>, a motion vector <b>202</b>, and an intraframe/interframe distinction flag <b>203</b>. The quantized DCT coefficient <b>201</b> is decoded into an error image <b>206</b> through a dequantizer <b>204</b> and an inverse DCT processor <b>205</b>. To the error image is added an output image <b>215</b> of an interframe/intraframe switching unit <b>214</b> through an adder <b>207</b> to form a reconstructed image <b>208</b>.</p>
<p id="p-0011" num="0010">The interframe/intraframe switching unit switches the output according to the interframe/intraframe coding distinction flag <b>203</b>. A predicted image <b>212</b> that is used for executing the interframe coding is synthesized by a predicted image synthesizer <b>211</b>. Here, the decoded image <b>210</b> of the preceding frame stored in the frame memory <b>209</b> is subjected to a processing of moving the position of each of the blocks according to the motion vector <b>202</b> that is received. In the case of intraframe coding, on the other hand, the interframe/intraframe switching unit outputs the “0” signal <b>213</b>.</p>
<p id="p-0012" num="0011">Block matching is a motion compensation system that is now most widely utilized. When the whole image is expanding, contracting, or turning, however, the motion vectors of all of the blocks must be transmitted, causing a problem of low coding efficiency. To solve this problem, global motion compensation (e.g., M. Hotter, “Differential Estimation of the Global Motion Parameters Zoom and Pan”, Signal Processing, Vol. 16, No. 3, pp. 249-265, March, 1989) has been proposed to express the motion vector field of the whole image while not using many parameters. According to this motion compensation system, the motion vector (ug(x, Y), vg(x, y)) of a pixel (x, y) in an image is expressed in the form of:
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>u</i><sub>g</sub>(<i>x, y</i>)=<i>a</i><sub>0</sub><i>x+a</i><sub>1</sub><i>y+a</i><sub>2</sub><?in-line-formulae description="In-line Formulae" end="tail"?>
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>v</i><sub>g</sub>(<i>x, y</i>)=<i>a</i><sub>3</sub><i>x+a</i><sub>4</sub><i>y=a</i><sub>5</sub>  Equation 1<?in-line-formulae description="In-line Formulae" end="tail"?>
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?>or<?in-line-formulae description="In-line Formulae" end="tail"?>
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>u</i><sub>g</sub>(<i>x, y</i>)=<i>b</i><sub>0</sub><i>xy+b</i><sub>1</sub><i>x=b</i><sub>2</sub><i>y+b</i><sub>3</sub><?in-line-formulae description="In-line Formulae" end="tail"?>
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>v</i><sub>g</sub>(<i>x, y</i>)=<i>b</i><sub>4</sub><i>xy+b</i><sub>5</sub><i>x+b</i><sub>6</sub><i>y+b</i><sub>7</sub>  Equation 2<?in-line-formulae description="In-line Formulae" end="tail"?>
<br/>
and the motion compensation is executed using the motion vectors. In these equations, a0 to a5 and b0 to b7 are motion parameters. In executing the motion compensation, the same predicted image must be generated both on the transmitting side and on the receiving side. For this purpose, the transmitting side may directly transmit values of a0 to a5 or b0 to b7 to the receiving side or may instead transmit motion vectors of several representative points.
</p>
<p id="p-0013" num="0012">As shown in <figref idref="DRAWINGS">FIG. 3A</figref>, assume that the coordinates of the pixels at the left upper, right upper, left lower and right lower corners of an image <b>301</b> are expressed by (0, 0), (r, 0), (0, s) and (r, s) (where r and s are positive integers). Here, letting the horizontal and vertical components of the motion vectors of the representative points (0, 0), (r, 0) and (0, s) be (ua, va), (ub, vb) and (uc, vc), respectively, Equation 1 is rewritten as:</p>
<p id="p-0014" num="0013">
<maths id="MATH-US-00001" num="00001">
<math overflow="scroll">
<mtable>
  <mtr>
    <mtd>
      <mrow>
        <mrow>
          <mrow>
            <msub>
              <mi>u</mi>
              <mi>g</mi>
            </msub>
            <mo>⁡</mo>
            <mrow>
              <mo>(</mo>
              <mrow>
                <mi>x</mi>
                <mo>,</mo>
                <mi>y</mi>
              </mrow>
              <mo>)</mo>
            </mrow>
          </mrow>
          <mo>=</mo>
          <mrow>
            <mrow>
              <mfrac>
                <mrow>
                  <msub>
                    <mi>u</mi>
                    <mi>b</mi>
                  </msub>
                  <mo>-</mo>
                  <msub>
                    <mi>u</mi>
                    <mi>a</mi>
                  </msub>
                </mrow>
                <mi>r</mi>
              </mfrac>
              <mo>⁢</mo>
              <mi>x</mi>
            </mrow>
            <mo>+</mo>
            <mrow>
              <mfrac>
                <mrow>
                  <msub>
                    <mi>u</mi>
                    <mi>c</mi>
                  </msub>
                  <mo>-</mo>
                  <msub>
                    <mi>u</mi>
                    <mi>a</mi>
                  </msub>
                </mrow>
                <mi>s</mi>
              </mfrac>
              <mo>⁢</mo>
              <mi>y</mi>
            </mrow>
            <mo>+</mo>
            <msub>
              <mi>u</mi>
              <mi>a</mi>
            </msub>
          </mrow>
        </mrow>
        <mo>⁢</mo>
        <mstyle>
          <mtext>
</mtext>
        </mstyle>
        <mo>⁢</mo>
        <mrow>
          <mrow>
            <msub>
              <mi>v</mi>
              <mi>g</mi>
            </msub>
            <mo>⁡</mo>
            <mrow>
              <mo>(</mo>
              <mrow>
                <mi>x</mi>
                <mo>,</mo>
                <mi>y</mi>
              </mrow>
              <mo>)</mo>
            </mrow>
          </mrow>
          <mo>=</mo>
          <mrow>
            <mrow>
              <mfrac>
                <mrow>
                  <msub>
                    <mi>v</mi>
                    <mi>b</mi>
                  </msub>
                  <mo>-</mo>
                  <msub>
                    <mi>v</mi>
                    <mi>a</mi>
                  </msub>
                </mrow>
                <mi>r</mi>
              </mfrac>
              <mo>⁢</mo>
              <mi>x</mi>
            </mrow>
            <mo>+</mo>
            <mrow>
              <mfrac>
                <mrow>
                  <msub>
                    <mi>v</mi>
                    <mi>c</mi>
                  </msub>
                  <mo>-</mo>
                  <msub>
                    <mi>v</mi>
                    <mi>a</mi>
                  </msub>
                </mrow>
                <mi>s</mi>
              </mfrac>
              <mo>⁢</mo>
              <mi>y</mi>
            </mrow>
            <mo>+</mo>
            <msub>
              <mi>v</mi>
              <mi>a</mi>
            </msub>
          </mrow>
        </mrow>
      </mrow>
    </mtd>
    <mtd>
      <mrow>
        <mi>Equation</mi>
        <mo>⁢</mo>
        <mstyle>
          <mspace width="1.1em" height="1.1ex"/>
        </mstyle>
        <mo>⁢</mo>
        <mn>3</mn>
      </mrow>
    </mtd>
  </mtr>
</mtable>
</math>
</maths>
</p>
<p id="p-0015" num="0014">This means that the same function can be fulfilled even when ua, va, ub, vb, uc and vc are transmitted instead of transmitting a0 to a5. This state is shown in <figref idref="DRAWINGS">FIGS. 3A and 3B</figref>. The motion vectors <b>306</b>, <b>307</b> and <b>308</b> (the motion vectors are defined to start from points of the original image of the present frame and ends at the corresponding points in the reference image) of the representative points <b>303</b>, <b>304</b> and <b>305</b> may be transmitted instead of the motion parameters based on the assumption that global motion compensation between the original image <b>302</b> of the present frame shown in <figref idref="DRAWINGS">FIG. 3B</figref> and the reference image <b>301</b> shown in <figref idref="DRAWINGS">FIG. 3A</figref> is effected. Similarly, by using the horizontal and vertical components (ua, va), (ub, vb), (uc, vc) and (ud, vd) of four representative points (0, 0), (r, 0), (0, s) and (r, s), Equation 2 can be rewritten as:</p>
<p id="p-0016" num="0015">
<maths id="MATH-US-00002" num="00002">
<math overflow="scroll">
<mtable>
  <mtr>
    <mtd>
      <mtable>
        <mtr>
          <mtd>
            <mrow>
              <mrow>
                <msub>
                  <mi>u</mi>
                  <mi>g</mi>
                </msub>
                <mo>⁡</mo>
                <mrow>
                  <mo>(</mo>
                  <mrow>
                    <mi>x</mi>
                    <mo>,</mo>
                    <mi>y</mi>
                  </mrow>
                  <mo>)</mo>
                </mrow>
              </mrow>
              <mo>=</mo>
              <mi/>
              <mo>⁢</mo>
              <mrow>
                <mrow>
                  <mfrac>
                    <mrow>
                      <mi>s</mi>
                      <mo>-</mo>
                      <mi>y</mi>
                    </mrow>
                    <mi>s</mi>
                  </mfrac>
                  <mo>⁢</mo>
                  <mrow>
                    <mo>(</mo>
                    <mrow>
                      <mrow>
                        <mfrac>
                          <mrow>
                            <mi>r</mi>
                            <mo>-</mo>
                            <mi>x</mi>
                          </mrow>
                          <mi>r</mi>
                        </mfrac>
                        <mo>⁢</mo>
                        <msub>
                          <mi>u</mi>
                          <mi>a</mi>
                        </msub>
                      </mrow>
                      <mo>+</mo>
                      <mrow>
                        <mfrac>
                          <mi>x</mi>
                          <mi>r</mi>
                        </mfrac>
                        <mo>⁢</mo>
                        <msub>
                          <mi>u</mi>
                          <mi>b</mi>
                        </msub>
                      </mrow>
                    </mrow>
                    <mo>)</mo>
                  </mrow>
                </mrow>
                <mo>+</mo>
              </mrow>
            </mrow>
          </mtd>
        </mtr>
        <mtr>
          <mtd>
            <mrow>
              <mi/>
              <mo>⁢</mo>
              <mrow>
                <mfrac>
                  <mi>y</mi>
                  <mi>s</mi>
                </mfrac>
                <mo>⁢</mo>
                <mrow>
                  <mo>(</mo>
                  <mrow>
                    <mrow>
                      <mfrac>
                        <mrow>
                          <mi>r</mi>
                          <mo>-</mo>
                          <mi>x</mi>
                        </mrow>
                        <mi>r</mi>
                      </mfrac>
                      <mo>⁢</mo>
                      <msub>
                        <mi>u</mi>
                        <mi>c</mi>
                      </msub>
                    </mrow>
                    <mo>+</mo>
                    <mrow>
                      <mfrac>
                        <mi>x</mi>
                        <mi>r</mi>
                      </mfrac>
                      <mo>⁢</mo>
                      <msub>
                        <mi>u</mi>
                        <mi>d</mi>
                      </msub>
                    </mrow>
                  </mrow>
                  <mo>)</mo>
                </mrow>
              </mrow>
            </mrow>
          </mtd>
        </mtr>
        <mtr>
          <mtd>
            <mrow>
              <mo>=</mo>
              <mi/>
              <mo>⁢</mo>
              <mrow>
                <mrow>
                  <mfrac>
                    <mrow>
                      <msub>
                        <mi>u</mi>
                        <mi>a</mi>
                      </msub>
                      <mo>-</mo>
                      <msub>
                        <mi>u</mi>
                        <mi>b</mi>
                      </msub>
                      <mo>-</mo>
                      <msub>
                        <mi>u</mi>
                        <mi>c</mi>
                      </msub>
                      <mo>+</mo>
                      <msub>
                        <mi>u</mi>
                        <mi>d</mi>
                      </msub>
                    </mrow>
                    <mi>rs</mi>
                  </mfrac>
                  <mo>⁢</mo>
                  <mi>xy</mi>
                </mrow>
                <mo>+</mo>
                <mrow>
                  <mfrac>
                    <mrow>
                      <mrow>
                        <mo>-</mo>
                        <msub>
                          <mi>u</mi>
                          <mi>a</mi>
                        </msub>
                      </mrow>
                      <mo>+</mo>
                      <msub>
                        <mi>u</mi>
                        <mi>b</mi>
                      </msub>
                    </mrow>
                    <mi>r</mi>
                  </mfrac>
                  <mo>⁢</mo>
                  <mi>x</mi>
                </mrow>
                <mo>+</mo>
              </mrow>
            </mrow>
          </mtd>
        </mtr>
        <mtr>
          <mtd>
            <mrow>
              <mi/>
              <mo>⁢</mo>
              <mrow>
                <mrow>
                  <mfrac>
                    <mrow>
                      <mrow>
                        <mo>-</mo>
                        <msub>
                          <mi>u</mi>
                          <mi>a</mi>
                        </msub>
                      </mrow>
                      <mo>+</mo>
                      <msub>
                        <mi>u</mi>
                        <mi>c</mi>
                      </msub>
                    </mrow>
                    <mi>s</mi>
                  </mfrac>
                  <mo>⁢</mo>
                  <mi>y</mi>
                </mrow>
                <mo>+</mo>
                <msub>
                  <mi>u</mi>
                  <mi>a</mi>
                </msub>
              </mrow>
            </mrow>
          </mtd>
        </mtr>
        <mtr>
          <mtd>
            <mrow>
              <mrow>
                <msub>
                  <mi>v</mi>
                  <mi>g</mi>
                </msub>
                <mo>⁡</mo>
                <mrow>
                  <mo>(</mo>
                  <mrow>
                    <mi>x</mi>
                    <mo>,</mo>
                    <mi>y</mi>
                  </mrow>
                  <mo>)</mo>
                </mrow>
              </mrow>
              <mo>=</mo>
              <mi/>
              <mo>⁢</mo>
              <mrow>
                <mrow>
                  <mfrac>
                    <mrow>
                      <msub>
                        <mi>v</mi>
                        <mi>a</mi>
                      </msub>
                      <mo>-</mo>
                      <msub>
                        <mi>v</mi>
                        <mi>b</mi>
                      </msub>
                      <mo>-</mo>
                      <msub>
                        <mi>v</mi>
                        <mi>c</mi>
                      </msub>
                      <mo>+</mo>
                      <msub>
                        <mi>v</mi>
                        <mi>d</mi>
                      </msub>
                    </mrow>
                    <mi>rs</mi>
                  </mfrac>
                  <mo>⁢</mo>
                  <mi>xy</mi>
                </mrow>
                <mo>+</mo>
                <mrow>
                  <mfrac>
                    <mrow>
                      <mrow>
                        <mo>-</mo>
                        <msub>
                          <mi>v</mi>
                          <mi>a</mi>
                        </msub>
                      </mrow>
                      <mo>+</mo>
                      <msub>
                        <mi>v</mi>
                        <mi>b</mi>
                      </msub>
                    </mrow>
                    <mi>r</mi>
                  </mfrac>
                  <mo>⁢</mo>
                  <mi>x</mi>
                </mrow>
                <mo>+</mo>
              </mrow>
            </mrow>
          </mtd>
        </mtr>
        <mtr>
          <mtd>
            <mrow>
              <mi/>
              <mo>⁢</mo>
              <mrow>
                <mrow>
                  <mfrac>
                    <mrow>
                      <mrow>
                        <mo>-</mo>
                        <msub>
                          <mi>v</mi>
                          <mi>a</mi>
                        </msub>
                      </mrow>
                      <mo>+</mo>
                      <msub>
                        <mi>v</mi>
                        <mi>c</mi>
                      </msub>
                    </mrow>
                    <mi>s</mi>
                  </mfrac>
                  <mo>⁢</mo>
                  <mi>y</mi>
                </mrow>
                <mo>+</mo>
                <msub>
                  <mi>v</mi>
                  <mi>a</mi>
                </msub>
              </mrow>
            </mrow>
          </mtd>
        </mtr>
      </mtable>
    </mtd>
    <mtd>
      <mrow>
        <mi>Equation</mi>
        <mo>⁢</mo>
        <mstyle>
          <mspace width="1.1em" height="1.1ex"/>
        </mstyle>
        <mo>⁢</mo>
        <mn>4</mn>
      </mrow>
    </mtd>
  </mtr>
</mtable>
</math>
</maths>
</p>
<p id="p-0017" num="0016">Therefore, a similar function is fulfilled even when ua, va, ub, vb, uc, vc, ud and vd are transmitted instead of b0 to b7. In this specification, the system using Equation 1 is referred to as global motion compensation based upon linear interpolation and/or extrapolation, and the system using Equation 2 is referred to as global motion compensation based upon the bilinear interpolation and/or extrapolation.</p>
<p id="p-0018" num="0017"><figref idref="DRAWINGS">FIG. 4</figref> illustrates the constitution of a motion compensation section <b>401</b> of an image coder employing the global motion compensation system based upon linear interpolation and/or extrapolation for transmitting motion vectors of the representative points. The same components as those of <figref idref="DRAWINGS">FIG. 1</figref> are denoted by the same reference numerals. A video coder that executes global motion compensation can be constituted by substituting a motion compensation section <b>401</b> for the block matching unit <b>116</b> of <figref idref="DRAWINGS">FIG. 1</figref>.</p>
<p id="p-0019" num="0018">A global motion compensation unit <b>402</b> performs motion estimation related to the global motion compensation between the decoded image <b>115</b> of the preceding frame and the original image <b>101</b> of the present frame, and estimates the values ua, va, ub, vb, uc and vc. The data <b>403</b> related to these values are transmitted as part of the motion data <b>120</b>. A predicted image <b>404</b> of global motion compensation is synthesized using Equation 3, and is fed to a block matching unit <b>405</b>. The motion is compensated by block matching between the predicted image of global motion compensation and the original image of the present frame, thereby generating motion vector data <b>406</b> of blocks and a final predicted image <b>117</b>. The motion vector data and the motion parameter data are multiplexed through a multiplexing unit <b>407</b> and are output as motion data <b>120</b>.</p>
<p id="p-0020" num="0019"><figref idref="DRAWINGS">FIG. 5</figref> illustrates the constitution of a motion compensation section <b>501</b> which is different from that of <figref idref="DRAWINGS">FIG. 4</figref>. A video coder that executes global motion compensation can be constituted by substituting a motion compensation section <b>501</b> for the block matching unit <b>116</b> of <figref idref="DRAWINGS">FIG. 1</figref>. In this embodiment, block matching is not adopted for the predicted image of global motion compensation but either global motion compensation or block matching is adopted for each of the blocks. Global motion compensation and block matching are executed in parallel by the global motion compensation unit <b>502</b> and the block matching unit <b>505</b> between the decoded image <b>115</b> of the preceding frame and the original image <b>101</b> of the present frame. A selection switch <b>508</b> selects an optimum system for each of the blocks between the predicted image <b>503</b> of global motion compensation and the predicted image <b>506</b> of block matching. The motion vectors <b>504</b> of the representative points, motion vectors <b>507</b> of the blocks and selection data <b>509</b> of global motion compensation/block matching are multiplexed by the multiplexing unit <b>510</b> and are output as motion data <b>120</b>.</p>
<p id="p-0021" num="0020">By introducing the above-mentioned global motion compensation, it becomes possible to express the general motion of the image using few parameters and to accomplish a high data compression ratio. However, the amounts of coding processing and decoding processing become larger than those of the conventional systems. In particular, the division in Equations 3 and 4 is a major factor of complexity in the processing.</p>
<heading id="h-0003" level="1">SUMMARY OF THE INVENTION</heading>
<p id="p-0022" num="0021">The global motion compensation in which the motion vector field of the whole image is approximated by a few parameters, involves a problem of increasing the amount of processing involved in synthesizing a predicted image. The object of the present invention is to decrease the amount of the processing by substituting a binary number shift operation for the division in the global motion compensation.</p>
<p id="p-0023" num="0022">The division is realized by the shift operation by suitably selecting the coordinates of representative points at the time of executing the global motion compensation.</p>
<?BRFSUM description="Brief Summary" end="tail"?>
<?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?>
<description-of-drawings>
<heading id="h-0004" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading>
<p id="p-0024" num="0023"><figref idref="DRAWINGS">FIG. 1</figref> is a diagram illustrating the constitution of an H.261 video coder.</p>
<p id="p-0025" num="0024"><figref idref="DRAWINGS">FIG. 2</figref> is a diagram illustrating the constitution of an H.261 video decoder.</p>
<p id="p-0026" num="0025"><figref idref="DRAWINGS">FIGS. 3A and 3B</figref> are diagrams illustrating an example of global motion compensation for transmitting the motion vectors of representative points.</p>
<p id="p-0027" num="0026"><figref idref="DRAWINGS">FIG. 4</figref> is a diagram illustrating a motion compensation section of the video decoder for effecting the block matching of a predicted image of global motion compensation.</p>
<p id="p-0028" num="0027"><figref idref="DRAWINGS">FIG. 5</figref> is a diagram illustrating a motion compensation section of the video coder for selecting either the global motion compensation or the block matching for each of the blocks.</p>
<p id="p-0029" num="0028"><figref idref="DRAWINGS">FIG. 6</figref> is a diagram illustrating the arrangement of representative points for executing high-speed processing.</p>
<p id="p-0030" num="0029"><figref idref="DRAWINGS">FIG. 7</figref> is a diagram illustrating a region for finding the motion vectors by extrapolation within the image.</p>
<p id="p-0031" num="0030"><figref idref="DRAWINGS">FIG. 8</figref> is a diagram illustrating an example which the motion vectors of all pixels in the image are found by interpolation from the motion vectors of the representative points.</p>
<p id="p-0032" num="0031"><figref idref="DRAWINGS">FIG. 9</figref> is a diagram in which the image is divided into two right triangles, and the global motion compensation is executed for each of them based on interpolation from the motion vectors of the representative points.</p>
<p id="p-0033" num="0032"><figref idref="DRAWINGS">FIG. 10</figref> shows a flow chart for performing video coding according to an embodiment of the invention.</p>
<p id="p-0034" num="0033"><figref idref="DRAWINGS">FIG. 11</figref> shows a flow chart for performing video coding according to another embodiment of the invention.</p>
<p id="p-0035" num="0034"><figref idref="DRAWINGS">FIG. 12</figref> shows a flow chart for video decoding according to an embodiment of the invention.</p>
<p id="p-0036" num="0035"><figref idref="DRAWINGS">FIG. 13</figref> shows a flow chart for video decoding according to another embodiment of the invention.</p>
<p id="p-0037" num="0036"><figref idref="DRAWINGS">FIG. 14</figref> is a diagram of a software encoder for a video coding method according to an embodiment of the invention.</p>
<p id="p-0038" num="0037"><figref idref="DRAWINGS">FIG. 15</figref> is a diagram of a software decoder for a video decoding method according to the present invention.</p>
<p id="p-0039" num="0038"><figref idref="DRAWINGS">FIG. 16</figref> is an overall diagram of a video encoder of the present invention.</p>
<p id="p-0040" num="0039"><figref idref="DRAWINGS">FIG. 17</figref> is a diagram of a motion compensation unit used in the encoder of <figref idref="DRAWINGS">FIG. 16</figref>, according to one embodiment of the invention.</p>
<p id="p-0041" num="0040"><figref idref="DRAWINGS">FIG. 18</figref> is a diagram of a motion compensation unit used in the encoder of the invention shown in <figref idref="DRAWINGS">FIG. 16</figref> according to another embodiment of the invention.</p>
<p id="p-0042" num="0041"><figref idref="DRAWINGS">FIG. 19</figref> is a diagram of a video decoder for decoding a video signal according to the present invention.</p>
<p id="p-0043" num="0042"><figref idref="DRAWINGS">FIG. 20</figref> is a diagram of a predicted image synthesizer used in the video decoder of <figref idref="DRAWINGS">FIG. 19</figref> according to one embodiment of the invention.</p>
<p id="p-0044" num="0043"><figref idref="DRAWINGS">FIG. 21</figref> is a diagram of a predicted image synthesizer used in the video decoder of <figref idref="DRAWINGS">FIG. 19</figref> according to another embodiment of the present invention.</p>
</description-of-drawings>
<?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?>
<?DETDESC description="Detailed Description" end="lead"?>
<heading id="h-0005" level="1">DETAILED DESCRIPTION OF THE PREFERRED EMBODIMENT</heading>
<p id="p-0045" num="0044">In the following description, the sampling intervals for the pixels are 1 in both the horizontal and vertical directions, and the coordinates of the pixels at the left upper, right upper, left lower and right lower corners are expressed by (0, 0), (r, 0), (0, s) and (r, s) (where r and s are positive integers), respectively, as shown in <figref idref="DRAWINGS">FIG. 3A</figref>.</p>
<p id="p-0046" num="0045">If the motion vector is quantized for each of the pixels in compensating the motion based upon the linear interpolation and/or extrapolation (affine transform) or the bilinear interpolation and/or extrapolation (bilinear transform), mismatching can be prevented and the operation can be simplified (Japanese Patent Application No. 193970/1994). In the following description, it is assumed that the horizontal component and vertical component of a motion vector of a pixel are integral multiples of 1/m (where m is a positive integer). Further, it is assumed that the global motion compensation is executed using the motion vectors of representative points explained in the “Prior Art” and that the motion vectors of the representative points are integral multiple of 1/k (where k is a positive integer). In this specification, the “motion vectors of pixels” are the motion vectors that are actually used for synthesizing a predicted image. On the other hand, the “motion vectors of representative points” are the parameters used for calculating the motion vectors of pixels. Because of the difference in the quantization step sizes, therefore, the motion vectors of pixels often may not be in agreement with the motion vectors of representative points even though they have the same coordinates.</p>
<p id="p-0047" num="0046">With reference to <figref idref="DRAWINGS">FIG. 6</figref>, a case based upon linear interpolation and/or extrapolation will be explained. Here, as described in the “Prior Art”, the representative points are not those located at the corners of the image <b>601</b>, but are the points <b>602</b>, <b>603</b> and <b>604</b> having the coordinates (i, j), (i+p, j) and (i, j+q) (i, j, p and q are integers), respectively. At this moment, the points <b>602</b>, <b>603</b> and <b>604</b> may exist inside or outside the image. Letting the coordinates whose values are given by multiplying the horizontal and vertical components of the motion vectors of the representative points by k be respectively (u<b>0</b>, v<b>0</b>), (ul, vl) and (u<b>2</b>, v<b>2</b>) (u<b>0</b>, v<b>0</b>, u<b>1</b>, v<b>1</b>, u<b>2</b>, and v<b>2</b> are integers), coordinates (u(x, y) and v(x, y) (where x, y, u(x, y) and v(x, y) are integers) which are m times the horizontal and vertical components of the motion vector of a pixel (x, y) are expressed by the following equations:
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>u</i>(<i>x, y</i>) =(((<i>u</i>1<i>−u</i>0) (<i>x−i</i>)<i>q</i>+(<i>u</i>2<i>−u</i>0) (<i>y−j</i>)<i>p+u</i>0<i>pq</i>)<i>m</i>)//(<i>pqk</i>)<?in-line-formulae description="In-line Formulae" end="tail"?>
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>v</i>(<i>x, y</i>)=(((<i>v</i>1<i>−v</i>0) (<i>x−i</i>)<i>q</i>+(<i>v</i>2<i>−v</i>0) (<i>y−j</i>)<i>p+v</i>0<i>pq</i>)<i>m</i>)//(<i>pqk</i>)  Equation 5<?in-line-formulae description="In-line Formulae" end="tail"?>
<br/>
where “ ” represents division for rounding the quotient of ordinary division into an adjacent integer when the quotient is not an integer, and its priority as an operator is the same as that of multiplication and division. To decrease the operation error, it is desirable that a value which is not an integer is rounded to the most adjacent integer. In this case, the methods for rounding a value of the sum of an integer and 1/2  are:
<ul id="ul0001" list-style="none">
    <li id="ul0001-0001" num="0047">(1) Rounding the value toward 0;</li>
    <li id="ul0001-0002" num="0048">(2) Rounding the value away from 0;</li>
    <li id="ul0001-0003" num="0049">(3) Rounding the value toward 0 when the dividend is negative, and rounding the value away from 0 when the dividend is positive (assuming that the divisor is positive at all times); and</li>
    <li id="ul0001-0004" num="0050">(4) Rounding the value away from 0 when the dividend is negative, and rounding the value toward 0 when the dividend is positive (assuming that the divisor is positive at all times)</li>
</ul>
</p>
<p id="p-0048" num="0051">Among them, (3) and (4) are advantageous from the standpoint of processing quantity since the rounding direction does not change irrespective of whether the dividend is positive or negative and there is no need to judge whether the sign is positive or negative. High-speed processing according to method (3) can be realized by the following equation:
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>u</i>(<i>x, y</i>) =(<i>Lpqk</i>+((<i>u</i>1<i>−u</i>0) (<i>x−i</i>)<i>q</i>+(<i>u</i>2<i>−u</i>0) (<i>y−j</i>)<i>p+u</i>0<i>pq</i>)<i>m</i>+(<i>pqk</i>#2))#(<i>pqk</i>)−<i>L</i><?in-line-formulae description="In-line Formulae" end="tail"?>
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>v</i>(<i>x, y</i>)=(<i>Mpqk</i>+((<i>v</i>1<i>−v</i>0) (<i>x−i</i>)q+(<i>v</i>2<i>−v</i>0) (<i>y−j</i>)<i>p+v</i>0<i>pq</i>)<i>m</i>+(<i>pqk</i>#2) )#(<i>pqk</i>)−<i>M</i>  Equation 6<?in-line-formulae description="In-line Formulae" end="tail"?>
<br/>
where “#” represents division of an integer for rounding off the decimal part toward 0, which is, usually, most easily realized using a computer. L and M are sufficiently large positive integers for maintaining the dividend of division to be positive at all times. The term (pqk #2) is used for rounding the quotient of division to the most adjacent integer.
</p>
<p id="p-0049" num="0052">Integer processing contributes to decreasing the amount of processing. Here, assuming that p, q and k are 2, 2 and 2<sup>h0</sup>, respectively wherein and are positive integers, and h<b>0</b> is an integer which is not negative. The division of Equation 5 can be realized by the shift operation of ++h<b>0</b> bits, making it possible to greatly decrease the amount of processing using a computer or dedicated hardware. Furthermore, assuming that m is 2<sup>h1 </sup>(h<b>1</b> is an integer which is not negative, and h<b>1</b> &lt;++h<b>0</b>), Equation 6 can be rewritten as:
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>u</i>(<i>x, y</i>)=((2<i>L</i>+1)&lt;&lt;(++<i>h</i>0<i>−h</i>1−1)+(<i>u</i>1<i>−u</i>0) (<i>x−i</i>)&lt;&lt;+(<i>u</i>2<i>−u</i>0) (<i>y−j</i>)&lt;&lt;+<i>u</i>0&lt;&lt;(+))&gt;&gt;(++<i>h</i>0<i>−h</i>1)−<i>L</i><?in-line-formulae description="In-line Formulae" end="tail"?>
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>v</i>(<i>x, y</i>)=((2<i>M</i>+1)&lt;&lt;(++<i>h</i>0<i>−h</i>1−1)+(<i>v</i>1<i>−v</i>0) (<i>x−i</i>)&lt;&lt;+(<i>v</i>2<i>−v</i>0) (<i>y−j</i>)&lt;&lt;+<i>v</i>0(+))&gt;&gt;(++<i>h</i>0<i>−h</i>1)−<i>M</i>  Equation 7<?in-line-formulae description="In-line Formulae" end="tail"?>
<br/>
where “x&lt;&lt;” means that x is shifted left by bits and 0 is substituted for the low-order bits, “x&gt;&gt;” means that x is shifted right by bits and 0 or 1 is substituted for the high-order bits (when x is a number of complement representation of 2, 1 is substituted when the most significant bit of x is 1 and 0 is substituted when it is 0), and the priority of these operators lies between addition/subtraction and multiplication/division, making it possible to further simplify the operation.
</p>
<p id="p-0050" num="0053">When the linear interpolation and/or extrapolation is used, letting (u<b>3</b>, v<b>3</b>) be the coordinates determined by multiplying the horizontal and vertical components of a motion vector of a representative point at (i+p, j+q) by k, Equation 5 is rewritten as Equation 8 or Equation 9, as follows:
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>u</i>(<i>x, y</i>)=(((<i>u</i>1<i>−u</i>0) (<i>x−i</i>)<i>q</i>+(<i>u</i>3<i>−u</i>1) (<i>y−j</i>)<i>p+u</i>0<i>pq</i>)<i>m</i>)//(<i>pqk</i>)<?in-line-formulae description="In-line Formulae" end="tail"?>
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>v</i>(<i>x, y</i>)=(((<i>v</i>1<i>−v</i>0) (<i>x−i</i>)<i>q</i>+(<i>v</i>3<i>−v</i>1) (<i>y−j</i>)<i>p+v</i>0<i>pq</i>)<i>m</i>)//(<i>pqk</i>)  Equation 8<?in-line-formulae description="In-line Formulae" end="tail"?>
<br/>
where the representative points are:
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?>(i, j) (i+p, j) and (i+p, j+q).<?in-line-formulae description="In-line Formulae" end="tail"?>
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>u</i>(<i>x, y</i>)=(((<i>u</i>3<i>−u</i>2) (<i>x−i</i>)<i>q</i>+(<i>u</i>2<i>−u</i>0) (<i>y−j</i>)<i>p+u</i>0<i>pq</i>)<i>m</i>)//(<i>pqk</i>)<?in-line-formulae description="In-line Formulae" end="tail"?>
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>v</i>(<i>x, y</i>)=(((<i>v</i>3<i>−v</i>2) (<i>x−i</i>)<i>q</i>+(<i>v</i>2<i>−v</i>0) (<i>y−j</i>)<i>p+v</i>0<i>pq</i>)<i>m</i>)//(<i>pqk</i>)  Equation 9<?in-line-formulae description="In-line Formulae" end="tail"?>
<br/>
where the representative points are:
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?>(i, j), (i, j+q) and (i+p, j+q),<?in-line-formulae description="In-line Formulae" end="tail"?>
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?>or<?in-line-formulae description="In-line Formulae" end="tail"?>
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>u</i>(<i>x, y</i>)=(((<i>u</i>2<i>−u</i>3) (<i>i+p−x</i>)<i>q</i>+(<i>u</i>1<i>−u</i>3) (<i>j+q−y</i>)<i>p+u</i>3<i>pq</i>)<i>m</i>)//(<i>pqk</i>)<?in-line-formulae description="In-line Formulae" end="tail"?>
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>v</i>(<i>x, y</i>)=(((<i>v</i>2<i>−v</i>3) (<i>i+p−x</i>)<i>q</i>+(<i>v</i>1<i>−v</i>3) (<i>j+q−y</i>)<i>p+v</i>3<i>pq</i>)<i>m</i>)//(<i>pqk</i>)  Equation 10<?in-line-formulae description="In-line Formulae" end="tail"?>
<br/>
where the representative points are:
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?>(i+p, j), (i, j+q) and (i+p, j+q),<?in-line-formulae description="In-line Formulae" end="tail"?>
<br/>
making it possible to decrease the amount of processing by using p, q, k and m which are numbers of 2<sup>n </sup>(where n is a positive integer)
</p>
<p id="p-0051" num="0054">When the bilinear interpolation and/or extrapolation are used, letting (u<b>0</b>, v<b>0</b>), (u<b>1</b>, v<b>1</b>), (u<b>2</b>, v<b>2</b>) and (u<b>3</b>, v<b>3</b>) be the coordinates determined by multiplying the horizontal and vertical components of the motion vectors of the representative points (i, j), (i+p, j), (i, j+q) and (i+p, j+q) by k, u(x, y) and v(x, y) are represented by the following equation:
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>u</i>(<i>x, y</i>)=(((<i>j+q−y</i>) ((<i>i+p−x</i>)<i>u</i>0+(<i>x−i</i>)<i>u</i>1)+(<i>y−j</i>) ((<i>i+p−x</i>)<i>u</i>2+(<i>x−i</i>)<i>u</i>3))<i>m</i>)//(<i>pqk</i>)<?in-line-formulae description="In-line Formulae" end="tail"?>
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>v</i>(<i>x, y</i>)=(((<i>j+q−y</i>) ((<i>i+p−x</i>)<i>v</i>0+(<i>x−i</i>)<i>v</i>1)+(<i>y−j</i>) ((<i>i+p−x</i>)<i>v</i>2+(<i>x−i</i>)<i>v</i>3))<i>m</i>)//(<i>pqk</i>)  Equation 11<?in-line-formulae description="In-line Formulae" end="tail"?>
</p>
<p id="p-0052" num="0055">Equation 11 can be rewritten as:
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>u</i>(<i>x, y</i>)=((2<i>L</i>+1)&lt;&lt;(++h0<i>−h</i>1−1)+(<i>j+q−y</i>) ((<i>i+p−x</i>)u0+(<i>x−i</i>)<i>u</i>1)+(<i>y−j</i>) ((<i>i+p−x</i>)<i>u</i>2+(<i>x−i</i>)<i>u</i>3))&gt;&gt;(++h0<i>−h</i>1)−<i>L</i><?in-line-formulae description="In-line Formulae" end="tail"?>
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>v</i>(<i>x, y</i>)=((2<i>M</i>+1)&lt;&lt;(++h0<i>−h</i>1−1)+(<i>j+q−y</i>) ((<i>i+p−x</i>)<i>v</i>0+(<i>x−i</i>)<i>v</i>1)+(<i>y−j</i>) ((<i>i+p−x</i>)<i>v</i>2+(<i>x−i</i>)<i>v</i>3))&gt;&gt;(++h0<i>−h</i>1)−M  Equation 12<?in-line-formulae description="In-line Formulae" end="tail"?>
<br/>
by using p, q, k and m which are numbers of 2 , 2 , 2<sup>ho</sup>, and 2<sup>h1</sup>, respectively, making it possible to decrease the amount of processing as in the above-mentioned processing.
</p>
<p id="p-0053" num="0056">In order to obtain the same predicted image of global motion compensation on the transmitting side and on the receiving side, the data related to the motion vectors of the representative points must be transmitted in a certain form to the receiving side. The motion vectors of the representative points may be directly transmitted. It is, however, also possible to transmit the motion vectors of the corner points of the image and to calculate the motion vectors of the representative points therefrom. This method will now be described.</p>
<p id="p-0054" num="0057">First a case where the linear interpolation and/or extrapolation is employed will be described. It is assumed that the motion vectors of three corner points (0, 0), (r, 0) and (0, s) of the image take only those values which are integral multiples of 1/n, and that the coordinates (u<b>00</b>, v<b>00</b>), Cu<b>01</b>, v<b>01</b>) and (u<b>02</b>, v<b>02</b>) which are determined by multiplying the horizontal and vertical components by n are transmitted. In this case, the coordinates (u<b>0</b>, v<b>0</b>), (u<b>1</b>, v<b>1</b>), (u<b>2</b>, v<b>2</b>) and (u<b>3</b>, v<b>3</b>) which are determined by multiplying the horizontal and vertical components of the motion vectors by k are defined as follows:
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>u</i>0<i>=u</i>′(<i>i, j</i>)<?in-line-formulae description="In-line Formulae" end="tail"?>
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>v</i>0<i>=v</i>′(<i>i, j</i>)<?in-line-formulae description="In-line Formulae" end="tail"?>
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>u</i>1<i>=u</i>′(<i>i+p, j</i>)<?in-line-formulae description="In-line Formulae" end="tail"?>
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>v</i>1<i>=v</i>′(<i>i+p, j</i>)<?in-line-formulae description="In-line Formulae" end="tail"?>
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>u</i>2<i>=u</i>′(<i>i, j+q</i>)<?in-line-formulae description="In-line Formulae" end="tail"?>
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>v</i>2<i>=v</i>′(<i>i, j+q</i>)<?in-line-formulae description="In-line Formulae" end="tail"?>
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>u</i>3<i>=u</i>′(<i>i+p, j+q</i>)<?in-line-formulae description="In-line Formulae" end="tail"?>
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>v</i>3<i>=v</i>′(<i>i+p, j+q</i>)  Equation 13<?in-line-formulae description="In-line Formulae" end="tail"?>
<br/>
where u′ (x, y) and v′ (x, y) are defined by the following equation, which is a modification of Equation 5:
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>u</i>′(<i>x, y</i>)=(((<i>u</i>01<i>−u</i>00)<i>xs</i>+(<i>u</i>02<i>−u</i>00)<i>yr+u</i>00<i>rs</i>)<i>k</i>)///(<i>rsn</i>)<?in-line-formulae description="In-line Formulae" end="tail"?>
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>v</i>′(<i>x, y</i>)=(((<i>v</i>01<i>−v</i>00)<i>xs</i>+(<i>v</i>02<i>−v</i>00)<i>yr+v</i>00<i>rs</i>)<i>k</i>)///(<i>rsn</i>)  Equation 14<?in-line-formulae description="In-line Formulae" end="tail"?>
<br/>
Here, “ ” represents division f or rounding the quotient of an ordinary division into an adjacent integer when the quotient is not an integer, and its priority as an operator is the same as that of multiplication and division. Three points are selected out of (u<b>0</b>, v<b>0</b>), (u<b>1</b>, v<b>1</b>), (u<b>2</b>, v<b>2</b>) and (u<b>3</b>, v<b>3</b>), and the global motion compensation is executed using such points as representative points. Then, the global motion compensation can be approximated by using (0, 0), (r, 0) and (0, s) as the representative points. Here, by using p and q which are 2<sup>n </sup>(n is positive integer), the processing can be simplified as described earlier. In order to decrease the operation errors, it is desirable that “///” rounds a value which is not an integer into the most adjacent integer. In this case, methods for rounding a value of the sum of an integer and 1/2  include the above-mentioned methods (1) to (4). Compared to the case using Equation 5 (calculation for each pixel), however, the operation of Equation 14 (only three calculations for one image) does not require many calculations. Even if methods (1) or (2) are selected, therefore, the total amount of calculation is not greatly affected.
</p>
<p id="p-0055" num="0058">When three points different from those of the case using Equation 13 are selected as corner points of the image, the same processing can be realized by modifying Equations 8 to 10. In addition to the above-mentioned examples, by letting (u<b>03</b>, v<b>03</b>) be the coordinates determined by multiplying the horizontal and vertical components of a motion vector at a corner point (r, s) of the image by n, Equation 14 can be rewritten as:
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>u</i>′(<i>x, y</i>)=(((<i>u</i>01<i>−u</i>00)<i>xs</i>+(<i>u</i>03<i>−u</i>01)<i>yr+u</i>00<i>rs</i>)<i>k</i>)///(<i>rsn</i>)<?in-line-formulae description="In-line Formulae" end="tail"?>
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>v</i>′(<i>x, y</i>)=(((<i>v</i>01<i>−v</i>00)<i>xs</i>+(<i>v</i>03<i>−v</i>01)<i>yr+v</i>00<i>rs</i>)<i>k</i>)///(<i>rsn</i>)  Equation 15<?in-line-formulae description="In-line Formulae" end="tail"?>
<br/>
when (u<b>00</b>, v<b>00</b>), (u<b>01</b>, v<b>01</b>) and (u<b>03</b>, v<b>03</b>) are transmitted; can be rewritten as:
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>u</i>′(<i>x, y</i>)=(((<i>u</i>03<i>−u</i>02)<i>xs</i>+(<i>u</i>02<i>−u</i>00)<i>yr+u</i>00<i>rs</i>)<i>k</i>)///(<i>rsn</i>)<?in-line-formulae description="In-line Formulae" end="tail"?>
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>v</i>′(<i>x, y</i>)=(((<i>v</i>03<i>−v</i>02)<i>xs</i>+(<i>v</i>02<i>−v</i>00)<i>yr+v</i>00<i>rs</i>)<i>k</i>)///(<i>rsn</i>)  Equation 16<?in-line-formulae description="In-line Formulae" end="tail"?>
<br/>
when (u<b>00</b>, v<b>00</b>), (u<b>02</b>, v<b>02</b>) and (u<b>03</b>, v<b>03</b>) are transmitted; and can be rewritten as:
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>u</i>′(<i>x, y</i>)=(((<i>u</i>02<i>−u</i>03) (<i>r−x</i>)<i>s</i>+(<i>u</i>01<i>−u</i>03) (<i>s−y</i>)<i>r+u</i>03<i>rs</i>)<i>k</i>)///(<i>rsn</i>)<?in-line-formulae description="In-line Formulae" end="tail"?>
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>v</i>′(<i>x, y</i>)=(((<i>v</i>02<i>−v</i>03) (<i>r−x</i>)<i>s</i>+(<i>v</i>01<i>−v</i>03) (<i>s−y</i>)<i>r+v</i>03<i>rs</i>)<i>k</i>)///(<i>rsn</i>)  Equation 17<?in-line-formulae description="In-line Formulae" end="tail"?>
<br/>
when (u<b>01</b>, v<b>01</b>), (u<b>02</b>, v<b>02</b>) and (u<b>03</b>, v<b>03</b>) are transmitted.
</p>
<p id="p-0056" num="0059">The same also holds even when the bilinear interpolation and/or extrapolation are executed. As in the above-mentioned case, assume that the motion vectors of the four corner representative points (0, 0), (r, 0), (0, s) and (r, s) of the image take only those values which are integral multiples of 1/n, and that (u<b>00</b>, v<b>00</b>), (u<b>01</b>, v<b>01</b>), (u<b>02</b>, v<b>02</b>) and (u<b>03</b>, v<b>03</b>) which are n times the horizontal and vertical components of the representative points are transmitted. In this case, (u<b>0</b>, v<b>0</b>), (u<b>1</b>, v<b>1</b>), (u<b>2</b>, v<b>2</b>) and (u<b>3</b>, v<b>3</b>) which are k times the horizontal and vertical components of the motion vectors of the representative points (i, j), (i+p, j), (i, j+q) and (i+p, j+q) are given by Equation 13 as described above. Here, however, by modifying Equation 11,
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?>u′(x, y) and v′(x, y) are defined by:<?in-line-formulae description="In-line Formulae" end="tail"?>
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>u</i>′(<i>x, y</i>)=(((<i>s−y</i>) ((<i>r−x</i>)<i>u</i>00<i>+xu</i>01)+<i>y</i>((<i>r−x</i>)<i>u</i>02<i>+xu</i>03))<i>k</i>)///(<i>rsn</i>)<?in-line-formulae description="In-line Formulae" end="tail"?>
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>v</i>′(<i>x, y</i>)=(((<i>s−y</i>) ((<i>r−x</i>)<i>v</i>00<i>+xv</i>01)+<i>y</i>((<i>r−x</i>)<i>v</i>02<i>+xv</i>03))<i>k</i>)///(<i>rsn</i>)  Equation 18<?in-line-formulae description="In-line Formulae" end="tail"?>
</p>
<p id="p-0057" num="0060">The advantage of the method in which the motion vectors of corner points of the image are transmitted and are interpolated and/or extrapolated to find motion vectors of representative points, is that the ranges of the motion vector for each of the pixels can be easily limited. For example, in the case of the bilinear interpolation and/or extrapolation given by Equation 4, the value ug(x, y) is not greater than the maximum value of ua, ub, uc and ud and not smaller than the minimum value thereof when the point (x, y) is inside the image. Therefore, if a limiting condition is added so that the values ua, ub, uc and ud lie within a limited range (e.g., rahge within ±32 pixels) at the time of estimating the global motion, the value ug(x, y) can be confined within the same range for all pixels (this also holds even for vg(x, y), as a matter of course). This makes it possible to definitely determine the number of digits necessary for the calculation, which is convenient from the standpoint of designing software or hardware.</p>
<p id="p-0058" num="0061">The foregoing description, however, is based upon the case that the calculations are all carried out based upon using floating-point arithmetic operations and, hence, care must be given in practice. The arithmetic operation (Equation 18) for finding the motion vectors of representative points from the motion vectors of corner points of the image involves rounding a value into an integer. Therefore, consideration must be taken to the probability that the motion vectors found by Equation 12 may deviate out of the above-mentioned limited range due to the calculation error. In particular, care must be taken when the representative points are located inside the image. This is because, the motion vectors are found by the extrapolation for the pixels outside a rectangle defined by representative points and, hence, the rounding error may be amplified.</p>
<p id="p-0059" num="0062"><figref idref="DRAWINGS">FIG. 7</figref> illustrates an example in which the motion vectors are found by extrapolation. When the global motion compensation is executed for the image <b>701</b> by using representative points <b>702</b>, <b>703</b>, <b>704</b> and <b>705</b>, the motion vectors are calculated by the extrapolation for the hatched portions inside the image. This is because, the hatched portions exist outside a rectangle <b>706</b> defined by the representative points.</p>
<p id="p-0060" num="0063">This problem can be effectively solved by so arranging the four representative points that a rectangle defined by the representative points includes the whole image. This is shown in <figref idref="DRAWINGS">FIG. 8</figref>. A rectangle <b>806</b> defined by representative points <b>802</b>, <b>803</b>, <b>804</b> and <b>805</b> includes an image <b>801</b>. Then the motion vectors of all pixels can be found by the interpolation from the representative points, and the effect of the rounding error at the representative points is not amplified inside the image. Accordingly, an error larger than the rounding error at representative points never occurs inside the image, and the upper limit of error is definite. When the rectangle defined by the representative points is too large, however, the range of values that the motion vectors of representative points take is so wide that a number of digits necessary for the arithmetic operation increases, causing a disadvantage from the standpoint of mounting.</p>
<p id="p-0061" num="0064">From the foregoing description, it is desirable that the value p is larger than r and the value q is larger than s in order to decrease the effect of the rounding error. It is also desirable that p and q assume values that are as large as possible even when they are smaller than r and s. It is further desirable that the values i and j are such that a portion which is as wide as possible inside the image is in an area that is defined by the representative points.</p>
<p id="p-0062" num="0065">When the bilinear interpolation and/or extrapolation are used for the global motion compensation as described above, the components of motion vectors of pixels in the rectangle defined by the two representative points can take only values that lie between maximum values and minimum values of the components of the motion vectors of the representative points. When linear interpolation and/or extrapolation is used, on the other hand, the motion vectors of pixels in a triangle defined by three representative points have the same property. When the global motion compensation is executed by using the linear interpolation and/or extrapolation, therefore, it is effective to transmit the motion vectors of the four corner points of the image and to carry out the global motion compensation independently for the two right triangles divided by a diagonal of the image. Then, the limitation on the range of the motion vectors of the four corner points can be directly applied to the motion vectors of all pixels inside the image. In this case, the values i, j, p and q may not be the same between the two right triangles. From the standpoint of operation error, furthermore, it is desirable that triangles defined by the representative points include right triangles of which the global motion compensation is to be executed, respectively, in order to avoid the calculation of motion vectors of pixels by the extrapolation. This is shown in <figref idref="DRAWINGS">FIG. 9</figref>. The motion vectors of points <b>909</b>, <b>903</b>, <b>908</b> and <b>910</b> which are the four corners of an image <b>901</b> are transmitted, and the global motion compensation is independently executed for each of a right triangle defined by the points <b>909</b>, <b>903</b> and <b>910</b> and a right triangle defined by the points <b>909</b>, <b>910</b> and <b>908</b>. Therefore, if a limitation is imposed on the range of motion vectors of vertexes, the motion vectors of all pixels within the image are included in this limited range. The right triangle defined by the points <b>909</b>, <b>903</b> and <b>910</b> uses points <b>902</b>, <b>903</b> and <b>904</b> as representative points, and the right triangle defined by the points <b>909</b>, <b>910</b> and <b>908</b> uses points <b>906</b>, <b>907</b> and <b>908</b> as representative points. The triangles defined by the representative points include therein right triangles to which the global motion compensation is to be executed, respectively. Therefore, the effect of the rounding error of the motion vectors of representative points is not amplified at points inside the image. In this example, the two triangles defined by the representative points are similar to each other. However, the triangles may not necessarily be similar to each other.</p>
<p id="p-0063" num="0066">The present invention makes it possible to substitute the shift operation for the division for synthesizing a predicted image of global motion compensation, and to simplify the processing using either software or dedicated hardware or a combination of both.</p>
<p id="p-0064" num="0067"><figref idref="DRAWINGS">FIG. 10</figref> shows the steps followed in performing video coding of video image data using fast global motion compensation according tO an embodiment of the present invention. In step <b>150</b>, a video signal is input and in step <b>151</b>, global motion estimation is performed between an input image and the decoded image of a previous frame. Then, the motion vectors are derived from the representative points of the input image in step <b>152</b>.</p>
<p id="p-0065" num="0068">In the next step, step <b>153</b>, a predicted image of global motion compensation is synthesized using the fast algorithm. The fast algorithm is a general expression for algorithms disclosed herein, such as the bilinear algorithm and affine algorithm. For example, equation 1 is an affine algorithm whereas equation 2 is a bilinear algorithm. Further, equations 3, 5, 6, 7-10, and 14-17 are affine whereas equations 4, 11 and 18 are bilinear.</p>
<p id="p-0066" num="0069">In step <b>154</b>, the local motion estimation is performed between the input image and the decoded image of the previous frame. The predicted image of local motion compensation is synthesized in step <b>155</b> and the global or local motion compensation for each block is selected in step <b>156</b>. The selection step is necessary since the global motion compensation and local motion compensation steps are performed in parallel in this embodiment.</p>
<p id="p-0067" num="0070">Then, in step <b>157</b>, the error image is synthesized by calculating the difference between the predicted image and the input image and the error image is subject to a discrete cosine transform for quantizing the DCT coefficients in step <b>158</b>. Finally, in step <b>159</b>, the compressed video data is output.</p>
<p id="p-0068" num="0071">In <figref idref="DRAWINGS">FIG. 11</figref>, an alternative embodiment is disclosed for performing video coding, which is similar to the video coding disclosed in <figref idref="DRAWINGS">FIG. 10</figref>. specifically, steps <b>150</b>-<b>153</b> are the same, but the remainder of the steps shown in the flow chart are different. The reason for this is that the steps performed in <figref idref="DRAWINGS">FIG. 11</figref> are for performing the local motion compensation and global motion compensation serially, rather than in parallel as in <figref idref="DRAWINGS">FIG. 10</figref>. Accordingly, in step <b>254</b>, local motion estimation is performed between the input image and the predicted image of global motion compensation. Then, in step <b>255</b>, the predicted image of local motion compensation is synthesized. Finally, the error image is synthesized by calculating the difference between the predicted image and the input image, just as in step <b>157</b> in <figref idref="DRAWINGS">FIG. 10</figref>, and steps <b>257</b> and <b>258</b> are the same as steps <b>158</b> and <b>159</b>, explained above.</p>
<p id="p-0069" num="0072"><figref idref="DRAWINGS">FIG. 12</figref> shows a flow chart of the video decoding according to the present invention. In step <b>160</b>, an input bit stream, such as a h.261 bit stream is received as the compressed video data. The motion vectors of the representative points are derived and in step <b>161</b> and in step <b>162</b>, the predicted image for blocks which selected global motion compensation using the fast algorithm are selected. In step <b>164</b>, the predicted image for blocks which selected local motion compensation are synthesized. The error image with respect to the predicted image is synthesized in step <b>165</b> and the error image is added to the predicted image in <b>166</b>. In step <b>167</b>, the reconstructed video signal is output to complete the decoding of the encoded video data.</p>
<p id="p-0070" num="0073">According to the embodiment of <figref idref="DRAWINGS">FIG. 12</figref>, the synthesizing of the predicted image for blocks which selected global motion compensation using the fast algorithm and also for blocks which selected local motion compensation is performed in parallel. On the other hand, in <figref idref="DRAWINGS">FIG. 13</figref>, the flow chart shows an alternative embodiment in which these steps are performed serially.</p>
<p id="p-0071" num="0074">In <figref idref="DRAWINGS">FIG. 13</figref>, steps <b>160</b> and <b>161</b> are the same as those in <figref idref="DRAWINGS">FIG. 12</figref>. In step <b>262</b>, the predicted image of global motion compensation using the fast algorithm is synthesized and in step <b>263</b> the predicted image of local motion compensation is synthesized. These steps are performed serially and followed by the step of synthesizing the error image by applying inverse DCT to the DCT coefficients, which is the same as step <b>165</b> in <figref idref="DRAWINGS">FIG. 12</figref>. Steps <b>265</b> and <b>266</b> which follow are also the same as steps <b>166</b> and <b>167</b> discussed with respect to <figref idref="DRAWINGS">FIG. 12</figref>, and which result in the output of the reconstructed video signal.</p>
<p id="p-0072" num="0075"><figref idref="DRAWINGS">FIGS. 14 and 15</figref> are block diagrams of the components of the encoder and decoder of the invention for storing and executing software operating as disclosed in the flowcharts of <figref idref="DRAWINGS">FIGS. 10-13</figref>. The components in common for both diagrams have the same reference numbers and include the data bus <b>140</b>, CPU <b>142</b> and storage device <b>143</b>. The encoder program for executing the video encoding is shown in <figref idref="DRAWINGS">FIG. 14</figref>, and is stored in storage device <b>143</b>. The decoder program for executing the video decoding is shown in <figref idref="DRAWINGS">FIG. 15</figref>, and is stored in storage device <b>143</b>. Storage devices <b>143</b> are storage media, such as hard disk drives, floppy disks or optical disks, for example.</p>
<p id="p-0073" num="0076">With reference to <figref idref="DRAWINGS">FIG. 14</figref>, an input video signal is A/D converted by A/D converter <b>141</b> and sent to CPU <b>142</b> over bus <b>140</b>. CPU <b>142</b> retrieves and executes the encoder program <b>144</b> stored in storage device <b>143</b> and then encodes and compresses the video data received from the A/D converter <b>141</b>. After the video data is encoded, it is stored in an output buffer <b>145</b> and output as output data. Control data and timing signals are also output with the compressed video data.</p>
<p id="p-0074" num="0077"><figref idref="DRAWINGS">FIG. 15</figref> shows the processing of coded video signal, which is received at input buffer <b>148</b> and then read by CPU <b>142</b>. CPU <b>142</b>, which retrieves the decoder program <b>147</b> from the storage device <b>143</b>, executes the decoding of the coded video data. The decoded video data is then sent over bus <b>140</b> to D/A converter <b>146</b> for outputting an analog video signal.</p>
<p id="p-0075" num="0078"><figref idref="DRAWINGS">FIG. 16</figref> shows the overall block diagram of a video coder according to the invention that is similar to <figref idref="DRAWINGS">FIG. 1</figref> of the prior art. Accordingly, the components in common for both diagrams have the same reference numbers. In the diagram of <figref idref="DRAWINGS">FIG. 16</figref>, block <b>116</b> of <figref idref="DRAWINGS">FIG. 1</figref>, which is a block matching unit for local motion compensation, is replaced with a block <b>1002</b> that uses global motion compensation and local motion compensation. Otherwise, the remaining components in the <figref idref="DRAWINGS">FIG. 16</figref> diagram are the same as those in the diagram of <figref idref="DRAWINGS">FIG. 1</figref>.</p>
<p id="p-0076" num="0079">In <figref idref="DRAWINGS">FIG. 17</figref>, a motion estimation and compensation unit <b>1003</b> that performs serial processing is shown. Unit <b>1003</b> can be used as the motion estimation and compensation unit <b>1002</b> of <figref idref="DRAWINGS">FIG. 16</figref>. Further, unit <b>1003</b> is a hardware embodiment performing functions nearly equivalent to the steps performed in the software processing shown in <figref idref="DRAWINGS">FIG. 11</figref>.</p>
<p id="p-0077" num="0080">As shown in <figref idref="DRAWINGS">FIG. 17</figref>, an input video signal <b>101</b> is received by the global motion estimation unit <b>1004</b> and also by the block matching unit <b>405</b>. Global motion estimation is performed between an input image and the decoded image of a previous frame by the global motion estimation unit <b>1004</b>. Unit <b>1004</b> also derives the motion vectors from the representative points of the input image. The data <b>403</b> related to these values is transmitted to the global motion compensation (GMC) image synthesizer <b>1005</b> which synthesizes the predicted image of the global motion compensation using the fast algorithm. A predicted image <b>404</b> of global motion compensation is then output to block matching unit <b>405</b> in which local motion estimation between the input image and the predicted image of global motion compensation is performed. Then, the motion vector data <b>406</b> is output to the multiplexer <b>407</b> and the predicted images of the present frame <b>117</b> is output to the adder <b>102</b> for synthesizing the error image by calculating the difference between the predicted image and the input image. The motion compensation unit shown in <figref idref="DRAWINGS">FIG. 17</figref> uses serial global motion estimation and local motion estimation.</p>
<p id="p-0078" num="0081">In <figref idref="DRAWINGS">FIG. 18</figref>, a motion compensation unit <b>1006</b> which can be used as the motion compensation unit <b>1002</b> in <figref idref="DRAWINGS">FIG. 16</figref> is disclosed in which parallel processing is performed for the global motion estimation unit and the local motion estimation, as follows. First, a video signal <b>101</b> is input and received by both global motion estimation unit <b>1008</b> and block matching unit <b>505</b>. Then, global motion estimation is performed between the input image and the decoded image of the previous frame by the global motion estimation unit <b>1008</b>. The motion parameters <b>504</b>, such as the motion vectors of representative points are input to the multiplexer <b>510</b> and the global motion compensation (GMC) image synthesizer <b>1007</b>. A predicted image of global motion compensation using the fact algorithm is synthesized and output to a block matching/global motion compensation changeover switch <b>508</b> for outputting the predicted image of the present frame <b>117</b>, obtained by one of global or local motion compensation for each block. The selection data <b>509</b> of the changeover switch selection is output to the multiplexer <b>510</b>. The multiplexer also receives the output of <b>507</b> of block matching unit <b>505</b>, which is the motion vector data. A signal <b>120</b> is output from the mutiplexer that includes signals <b>504</b>, <b>507</b> and <b>509</b>.</p>
<p id="p-0079" num="0082"><figref idref="DRAWINGS">FIG. 19</figref> shows a block diagram of a video decoder that is similar to the prior art decoder of <figref idref="DRAWINGS">FIG. 2</figref>, but that includes the addition of a predicted image synthesizer <b>1010</b> which synthesizes the predicted image in accordance with an embodiment of the present invention. Otherwise, the remaining components of the decoder <b>1009</b> are the same as shown in <figref idref="DRAWINGS">FIG. 2</figref>.</p>
<p id="p-0080" num="0083">In <figref idref="DRAWINGS">FIG. 20</figref>, a predicted image synthesizer according to one embodiment of the invention <b>1011</b> is shown, which can be used for the predicted image synthesizer <b>1010</b> shown in <figref idref="DRAWINGS">FIG. 19</figref>. Serial processing is shown in <figref idref="DRAWINGS">FIG. 20</figref>, in which the motion vector data <b>202</b> is received by the multiplexer <b>1013</b>, which provides the motion parameters <b>403</b> and motion vector data <b>406</b> to the global motion compensation (GMC) image synthesizer <b>1005</b> and the block matching image synthesizer <b>1012</b>, respectively. The GMC image synthesizer <b>1005</b> derives the motion vectors of representatives points and synthesizes a predicted image of global motion compensation using the fast algorithm. Then, it outputs the predicted image of global motion compensation <b>404</b> to the BM image synthesizer <b>1012</b>, which synthesizes the predicted image of local motion compensation. The predicted image of the present frame <b>212</b> is then output to the switching unit <b>214</b>, as shown in <figref idref="DRAWINGS">FIG. 19</figref>.</p>
<p id="p-0081" num="0084"><figref idref="DRAWINGS">FIG. 21</figref> shows a predicted image synthesizer <b>1014</b>, which operates to process the global motion compensation image synthesizing and block matching image synthesizing in parallel, as follows.</p>
<p id="p-0082" num="0085">The motion vector data <b>202</b> is input to the multiplexer <b>1016</b>, which provides separated motion parameter data <b>504</b>, motion vector data <b>507</b> and selection data of block matching/global motion compensation <b>509</b> to the GMC image synthesizer <b>1007</b>, BM image synthesizer <b>1015</b> and switch <b>508</b>, respectively, as shown. The BM image synthesizer <b>1015</b> synthesizes the predicted image for blocks which selected the local motion compensation and the GMC image synthesizer <b>1007</b> synthesizes the predicted image for blocks which selected the global motion compensation using the fast algorithm. The respective data <b>503</b> and <b>506</b> is output to the switch <b>508</b>, which selects one of these signals according to the selection data <b>509</b>, received from the demultiplexer. The predicted image of a present frame <b>212</b> is then output and received by switching unit <b>214</b>, as shown in <figref idref="DRAWINGS">FIG. 19</figref>.</p>
<p id="p-0083" num="0086">According to the embodiments of the invention, the video coding and decoding can be performed either by software operating as shown in the flowcharts of <figref idref="DRAWINGS">FIGS. 10-13</figref> using the software encoder or software decoder shown in <figref idref="DRAWINGS">FIGS. 14 and 15</figref> or by dedicated hardware, as shown in the embodiments of the invention in <figref idref="DRAWINGS">FIGS. 16-21</figref>.</p>
<?DETDESC description="Detailed Description" end="tail"?>
</description>
<us-math idrefs="MATH-US-00001" nb-file="US07298781-20071120-M00001.NB">
<img id="EMI-M00001" he="12.02mm" wi="76.20mm" file="US07298781-20071120-M00001.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-math idrefs="MATH-US-00002" nb-file="US07298781-20071120-M00002.NB">
<img id="EMI-M00002" he="37.76mm" wi="76.20mm" file="US07298781-20071120-M00002.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-claim-statement>I claim:</us-claim-statement>
<claims id="claims">
<claim id="CLM-00001" num="00001">
<claim-text>1. A video coding method comprising:
<claim-text>storing a decoded image of a reference frame; synthesizing a predicted image of a present frame by performing motion estimation between an input image of said present frame and said decoded image, wherein said synthesizing step comprises: calculating motion vectors of 3 representative points having coordinates (i), (i+p,j) and (i,j+p) using motion vectors of corner points of said predicted image having coordinates (0,0), (r,<b>0</b>), and (<b>0</b>,s), wherein r, s, i, and j are integers, p and q are integer powers of 2, p is greater than or equal to r, p/2 is less than r, q is greater than or equal to s, q/2 is less than s, and the sampling intervals of pixels are 1 in both horizontal and vertical directions;</claim-text>
<claim-text>calculating the motion vector of each pixel in said predicted image from said motion vectors of said representative points; and</claim-text>
<claim-text>synthesizing said predicted image from said motion vector of each pixel and said decoded image.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00002" num="00002">
<claim-text>2. A video coding method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein calculating motion vectors of 3 representative points and calculating the motion vector of each pixel performs liner interpolation or extrapolation.</claim-text>
</claim>
<claim id="CLM-00003" num="00003">
<claim-text>3. A video coding method comprising:
<claim-text>storing a decoded image of a reference frame;</claim-text>
<claim-text>synthesizing a predicted image of a present frame by performing motion estimation between an input image of said present frame and said decoded image,</claim-text>
<claim-text>wherein said synthesizine step comprises:</claim-text>
<claim-text>calculating motion vectors of 3 representative points having coordinates (i), (i+p, j) and (i, j+p) using motion vectors of corner points of said predicted image having coordinates (0, 0), (r, 0) and (0, s), where r, s, i, and j are integers, p and q are integer powers of 2, p is greater than or equal to r, p/2 is less than r, q is greater than or equal to s, q/2 is less than s, and the sampling intervals of pixels are 1 in both horizontal and vertical directions;</claim-text>
<claim-text>calculating the motion vector of each pixel in said predicted image from said motion vectors of said representative points; and</claim-text>
<claim-text>synthesizing said predicted image from said motion vector of each pixel and said decoded image,</claim-text>
<claim-text>wherein: the horizontal and vertical components of said motion vectors of said corner points are integer multiples of 1/n; the horizontal and vertical components of said motion vector of each pixel in said predicted image are integer multiples 1/m; the horizontal and vertical components of said motion vectors of said representative points are integer multiples of 1/k in said means for calculating motion vectors of 3 representative points, said motion vectors of 3 representative points are calculated using equations:
<claim-text>
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>u</i>′(<i>x, y</i>)=((<i>u</i>00<i>rs</i>+(<i>u</i>01<i>−u</i>00)<i>xs</i>+(<i>u</i>02<i>−u</i>00)<i>yr</i>)<i>k</i>)//(<i>rsn</i>),<?in-line-formulae description="In-line Formulae" end="tail"?>
</claim-text>
<claim-text>
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>y</i>′(<i>x, y</i>)=((<i>v</i>00<i>rs</i>+(<i>v</i>01<i>−v</i>00))<i>xs</i>+(<i>v</i>02<i>−v</i>00)<i>yr</i>)<i>k</i>)//(<i>rsn</i>),<?in-line-formulae description="In-line Formulae" end="tail"?>
</claim-text>
<claim-text>
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>u</i>0<i>=u</i>′(<i>i, j</i>), <i>v</i>0<i>+v</i>′(<i>i, j</i>), <i>u</i>1<i>=u</i>′(<i>i+p, i</i>), <i>v</i>1<i>=v</i>′(<i>i+p, j</i>),<?in-line-formulae description="In-line Formulae" end="tail"?>
</claim-text>
<claim-text>
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>u</i>2<i>=u</i>′(<i>i, j+q</i>), and <i>v</i>2<i>=y</i>′(<i>i, j+q</i>);<?in-line-formulae description="In-line Formulae" end="tail"?>
</claim-text>
</claim-text>
<claim-text>and in calculating the motion vector of each pixel in said predicted image, a motion vector of a pixel in said predicted image is calculated using equations:
<claim-text>
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>u</i>(<i>x, y</i>)=((<i>u</i>0<i>pq</i>+(<i>u</i>1<i>−u</i>0))<i>xq</i>+(<i>u</i>2<i>−u</i>0)<i>yp</i>)<i>m</i>)//(<i>pqk</i>), and<?in-line-formulae description="In-line Formulae" end="tail"?>
</claim-text>
<claim-text>
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>v</i>(<i>x, y</i>)=((<i>v</i>0<i>pq</i>+(<i>y</i>1<i>−y</i>0)<i>xq</i>+(<i>y</i>2<i>−v</i>0)<i>yp</i>)<i>m</i>)//(<i>pqk</i>),<?in-line-formulae description="In-line Formulae" end="tail"?>
</claim-text>
</claim-text>
</claim-text>
<claim-text>where (u<b>00</b>, v<b>00</b>), (u<b>01</b>, v<b>01</b>), and (u<b>02</b>, v<b>02</b>) are n times said motion vectors of said corner points having coordinates (0,0), (r, 0), and (0, s) (u(x, y), v(x, y)) is n times the horizontal and vertical components of the motion vector of a pixel having coordinate (x, y) in said predicted image, (u<b>0</b>, v<b>0</b>), (u<b>1</b>, v<b>1</b>), and (u<b>2</b>, v<b>2</b>) are k times said motion vectors of said representative points having coordinates (i,j), (i+p,j), and (i,j+p), u<b>00</b>, v<b>00</b>, u<b>01</b>, v<b>01</b>, u<b>02</b>, v<b>02</b>, u(x,y), u<b>0</b>, v<b>0</b>, u<b>1</b>, v<b>1</b>, u<b>2</b>, and v<b>2</b> are integers, k, m, and n are integer powers of 2, and “///” and “//” represent integer divisions that round the quotient of ordinary division into an adjacent integer when said quotient of said ordinary division is not an integer, and their priority as an operator is the same as that of ordinary multiplication and division.</claim-text>
</claim>
<claim id="CLM-00004" num="00004">
<claim-text>4. A video coding method according to <claim-ref idref="CLM-00003">claim 3</claim-ref>, wherein said “///” and “//” round said quotient of ordinary division, when the result of said quotient of ordinary division is the sum of ½ and an integer, either into the nearest integer: (1) away from 0; (2) toward 0; (3) away from 0 when the dividend is negative and toward 0 when the dividend is positive; or (4) away from 0 when the dividend is positive and toward 0 when the dividend is negative.</claim-text>
</claim>
</claims>
</us-patent-grant>
