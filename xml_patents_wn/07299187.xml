<us-patent-grant lang="EN" dtd-version="v4.2 2006-08-23" file="US07299187-20071120.XML" status="PRODUCTION" id="us-patent-grant" country="US" date-produced="20071106" date-publ="20071120">
<us-bibliographic-data-grant>
<publication-reference>
<document-id>
<country>US</country>
<doc-number>07299187</doc-number>
<kind>B2</kind>
<date>20071120</date>
</document-id>
</publication-reference>
<application-reference appl-type="utility">
<document-id>
<country>US</country>
<doc-number>10361547</doc-number>
<date>20030210</date>
</document-id>
</application-reference>
<us-application-series-code>10</us-application-series-code>
<priority-claims>
<priority-claim sequence="01" kind="national">
<country>JP</country>
<doc-number>2002-034973</doc-number>
<date>20020213</date>
</priority-claim>
</priority-claims>
<us-term-of-grant>
<us-term-extension>843</us-term-extension>
</us-term-of-grant>
<classifications-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>G</section>
<class>06</class>
<subclass>F</subclass>
<main-group>17</main-group>
<subgroup>27</subgroup>
<symbol-position>F</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20071120</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>G</section>
<class>10</class>
<subclass>L</subclass>
<main-group>15</main-group>
<subgroup>00</subgroup>
<symbol-position>L</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20071120</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>G</section>
<class>10</class>
<subclass>L</subclass>
<main-group>15</main-group>
<subgroup>18</subgroup>
<symbol-position>L</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20071120</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>G</section>
<class>10</class>
<subclass>L</subclass>
<main-group>21</main-group>
<subgroup>00</subgroup>
<symbol-position>L</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20071120</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
</classifications-ipcr>
<classification-national>
<country>US</country>
<main-classification>704275</main-classification>
<further-classification>704  9</further-classification>
<further-classification>704239</further-classification>
<further-classification>704240</further-classification>
</classification-national>
<invention-title id="d0e71">Voice command processing system and computer therefor, and voice command processing method</invention-title>
<references-cited>
<citation>
<patcit num="00001">
<document-id>
<country>US</country>
<doc-number>5230023</doc-number>
<kind>A</kind>
<name>Nakano</name>
<date>19930700</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>381110</main-classification></classification-national>
</citation>
<citation>
<patcit num="00002">
<document-id>
<country>US</country>
<doc-number>5255341</doc-number>
<kind>A</kind>
<name>Nakajima</name>
<date>19931000</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>704200</main-classification></classification-national>
</citation>
<citation>
<patcit num="00003">
<document-id>
<country>US</country>
<doc-number>5452397</doc-number>
<kind>A</kind>
<name>Ittycheriah et al.</name>
<date>19950900</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>704240</main-classification></classification-national>
</citation>
<citation>
<patcit num="00004">
<document-id>
<country>US</country>
<doc-number>5566272</doc-number>
<kind>A</kind>
<name>Brems et al.</name>
<date>19961000</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>704231</main-classification></classification-national>
</citation>
<citation>
<patcit num="00005">
<document-id>
<country>US</country>
<doc-number>5684925</doc-number>
<kind>A</kind>
<name>Morin et al.</name>
<date>19971100</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>704254</main-classification></classification-national>
</citation>
<citation>
<patcit num="00006">
<document-id>
<country>US</country>
<doc-number>5852801</doc-number>
<kind>A</kind>
<name>Hon et al.</name>
<date>19981200</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>704244</main-classification></classification-national>
</citation>
<citation>
<patcit num="00007">
<document-id>
<country>US</country>
<doc-number>5899972</doc-number>
<kind>A</kind>
<name>Miyazawa et al.</name>
<date>19990500</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>704249</main-classification></classification-national>
</citation>
<citation>
<patcit num="00008">
<document-id>
<country>US</country>
<doc-number>6064959</doc-number>
<kind>A</kind>
<name>Young et al.</name>
<date>20000500</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>704251</main-classification></classification-national>
</citation>
<citation>
<patcit num="00009">
<document-id>
<country>US</country>
<doc-number>6173266</doc-number>
<kind>B1</kind>
<name>Marx et al.</name>
<date>20010100</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>704270</main-classification></classification-national>
</citation>
<citation>
<patcit num="00010">
<document-id>
<country>US</country>
<doc-number>6185530</doc-number>
<kind>B1</kind>
<name>Ittycheriah et al.</name>
<date>20010200</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>704255</main-classification></classification-national>
</citation>
<citation>
<patcit num="00011">
<document-id>
<country>US</country>
<doc-number>6208972</doc-number>
<kind>B1</kind>
<name>Grant et al.</name>
<date>20010300</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>704275</main-classification></classification-national>
</citation>
<citation>
<patcit num="00012">
<document-id>
<country>US</country>
<doc-number>6587824</doc-number>
<kind>B1</kind>
<name>Everhart et al.</name>
<date>20030700</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>704275</main-classification></classification-national>
</citation>
<citation>
<patcit num="00013">
<document-id>
<country>US</country>
<doc-number>2001/0016813</doc-number>
<kind>A1</kind>
<name>Brown et al.</name>
<date>20010800</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>704231</main-classification></classification-national>
</citation>
<citation>
<patcit num="00014">
<document-id>
<country>US</country>
<doc-number>2002/0178009</doc-number>
<kind>A1</kind>
<name>Firman</name>
<date>20021100</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>704275</main-classification></classification-national>
</citation>
<citation>
<patcit num="00015">
<document-id>
<country>JP</country>
<doc-number>08-278794</doc-number>
<date>19961000</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<patcit num="00016">
<document-id>
<country>JP</country>
<doc-number>2000-029585</doc-number>
<date>20000100</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<patcit num="00017">
<document-id>
<country>JP</country>
<doc-number>2000-242464</doc-number>
<date>20000900</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<patcit num="00018">
<document-id>
<country>JP</country>
<doc-number>2000-242494</doc-number>
<date>20000900</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<patcit num="00019">
<document-id>
<country>WO</country>
<doc-number>WO 98/09228</doc-number>
<date>19980300</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<patcit num="00020">
<document-id>
<country>WO</country>
<doc-number>WO 02233583</doc-number>
<kind>A1</kind>
<date>20020400</date>
</document-id>
</patcit>
<category>cited by examiner</category>
</citation>
</references-cited>
<number-of-claims>13</number-of-claims>
<us-exemplary-claim>1</us-exemplary-claim>
<us-field-of-classification-search>
<classification-national>
<country>US</country>
<main-classification>704238</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>704239</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>704257</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>704275</main-classification>
</classification-national>
</us-field-of-classification-search>
<figures>
<number-of-drawing-sheets>9</number-of-drawing-sheets>
<number-of-figures>10</number-of-figures>
</figures>
<us-related-documents>
<related-publication>
<document-id>
<country>US</country>
<doc-number>20030154077</doc-number>
<kind>A1</kind>
<date>20030814</date>
</document-id>
</related-publication>
</us-related-documents>
<parties>
<applicants>
<applicant sequence="001" app-type="applicant-inventor" designation="us-only">
<addressbook>
<last-name>Tahara</last-name>
<first-name>Yoshinori</first-name>
<address>
<city>Yamato</city>
<country>JP</country>
</address>
</addressbook>
<nationality>
<country>JP</country>
</nationality>
<residence>
<country>JP</country>
</residence>
</applicant>
<applicant sequence="002" app-type="applicant-inventor" designation="us-only">
<addressbook>
<last-name>Tomoda</last-name>
<first-name>Daisuke</first-name>
<address>
<city>Yokohama</city>
<country>JP</country>
</address>
</addressbook>
<nationality>
<country>JP</country>
</nationality>
<residence>
<country>JP</country>
</residence>
</applicant>
<applicant sequence="003" app-type="applicant-inventor" designation="us-only">
<addressbook>
<last-name>Mitsubo</last-name>
<first-name>Kikuo</first-name>
<address>
<city>Yamato</city>
<country>JP</country>
</address>
</addressbook>
<nationality>
<country>JP</country>
</nationality>
<residence>
<country>JP</country>
</residence>
</applicant>
<applicant sequence="004" app-type="applicant-inventor" designation="us-only">
<addressbook>
<last-name>Atake</last-name>
<first-name>Yoshinori</first-name>
<address>
<city>Yamato</city>
<country>JP</country>
</address>
</addressbook>
<nationality>
<country>JP</country>
</nationality>
<residence>
<country>JP</country>
</residence>
</applicant>
</applicants>
<agents>
<agent sequence="01" rep-type="attorney">
<addressbook>
<orgname>Akerman Senterfitt</orgname>
<address>
<country>unknown</country>
</address>
</addressbook>
</agent>
</agents>
</parties>
<assignees>
<assignee>
<addressbook>
<orgname>International Business Machines Corporation</orgname>
<role>02</role>
<address>
<city>Armonk</city>
<state>NY</state>
<country>US</country>
</address>
</addressbook>
</assignee>
</assignees>
<examiners>
<primary-examiner>
<last-name>Hudspeth</last-name>
<first-name>David</first-name>
<department>2626</department>
</primary-examiner>
<assistant-examiner>
<last-name>Rider</last-name>
<first-name>Justin W.</first-name>
</assistant-examiner>
</examiners>
</us-bibliographic-data-grant>
<abstract id="abstract">
<p id="p-0001" num="0000">When a user issued voice command does not match grammars registered in advance, the voice command is identified as a sentence (step S<b>305</b>). This sentence is compared with the registered grammars to calculate a similarity (step S<b>307</b>). When the similarity is higher than a first threshold value (TH<b>1</b>), the voice command is executed (step S<b>315</b>). When the similarity is equal to or lower than the first threshold value (TH<b>1</b>) and higher than a second threshold value (TH<b>2</b>), command choices are displayed for the user and the user is permitted to select a command to be executed (step S<b>319</b>). When the similarity is equal to or lower than the second threshold value (TH<b>2</b>), the command is not executed (step S<b>321</b>). Furthermore, once a command has been executed it is added as a grammar, so that it can be identified when next it is used.</p>
</abstract>
<drawings id="DRAWINGS">
<figure id="Fig-EMI-D00000" num="00000">
<img id="EMI-D00000" he="207.86mm" wi="142.58mm" file="US07299187-20071120-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00001" num="00001">
<img id="EMI-D00001" he="186.35mm" wi="157.48mm" file="US07299187-20071120-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00002" num="00002">
<img id="EMI-D00002" he="147.83mm" wi="158.50mm" file="US07299187-20071120-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00003" num="00003">
<img id="EMI-D00003" he="226.14mm" wi="143.51mm" file="US07299187-20071120-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00004" num="00004">
<img id="EMI-D00004" he="199.73mm" wi="130.98mm" file="US07299187-20071120-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00005" num="00005">
<img id="EMI-D00005" he="216.24mm" wi="104.99mm" file="US07299187-20071120-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00006" num="00006">
<img id="EMI-D00006" he="218.19mm" wi="126.92mm" file="US07299187-20071120-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00007" num="00007">
<img id="EMI-D00007" he="202.44mm" wi="130.47mm" file="US07299187-20071120-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00008" num="00008">
<img id="EMI-D00008" he="153.50mm" wi="82.38mm" file="US07299187-20071120-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00009" num="00009">
<img id="EMI-D00009" he="141.73mm" wi="141.14mm" file="US07299187-20071120-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
</drawings>
<description id="description">
<?BRFSUM description="Brief Summary" end="lead"?>
<heading id="h-0001" level="1">CROSS-REFERENCE TO RELATED APPLICATIONS</heading>
<p id="p-0002" num="0001">This application claims the benefit of Japanese Application No. 2002-034973 filed Feb. 13, 2002 at the Japanese Patent Office.</p>
<heading id="h-0002" level="1">BACKGROUND OF THE INVENTION</heading>
<p id="p-0003" num="0002">1. Technical Field</p>
<p id="p-0004" num="0003">The present invention relates to a voice command processing system and a computer therefor, and to a voice command processing method and program that improves the recognition ratio for user issued voice commands.</p>
<p id="p-0005" num="0004">2. Description of the Related Art</p>
<p id="p-0006" num="0005">Currently, many computer types are employed in diverse locations, and accordingly, various data input means are employed for these computers. Such input means include, for example, keyboards equipped with multiple keys, mice for pointing to arbitrary locations on monitors, touch pads for entering data by running a pen across a photosensitive surface, and speech recognition means for using a microphone to collect and enter as speech data words uttered by a user. Of these input means, the speech input means not only can recognize speech as characters (dictation), but also can understand a voice command issued by a user to enable a predetermined operation of an application. That is, since this speech input means can be used to enter voice commands for a computer or an application, the attention of people is especially drawn to the convenience of its use as input means.</p>
<p id="p-0007" num="0006">For the recognition of a voice command, the speech of a user entered at a microphone connected to a computer is processed by a voice command recognition program executed by the computer. The voice command is defined using the BN method (Backus-Naur form), which is one of the context description representations for programs executed by the computer. When the voice command recognition program identifies a voice command that exactly matches the grammar of a voice command defined and registered in advance, a designated action is performed. That is, when a user exactly pronounces a voice command registered in advance, a desired action can be initiated.</p>
<p id="p-0008" num="0007">Since a voice command is defined for each action, however, the possible number of command types are so numerous that it is difficult for a user to exactly memorize and pronounce all voice commands. Further, a user who is not sure of the commands may issue a different, incorrect voice command instead of a correct, registered voice command. Since the word order in a sentence is comparatively changeable, especially in Japanese, a user tends to issue an incorrect voice command, and since the incorrect voice command is not identified as a voice command and is not executed, the user is displeased because even though instructed, a desired action is not initiated.</p>
<p id="p-0009" num="0008">There is another method used for the advance registration of voice commands that a user will be assumed to issue. According to this method, however, although the number of voice commands to be registered is increased and an extended period of time is allocated for the identification of a voice command, the recognition ratio still can be reduced.</p>
<p id="p-0010" num="0009">To resolve these technical problems, the primary objective of the present invention is the provision of a voice command processing system that can increase both the degree of freedom for the pronunciation of a voice command by a user, and the recognition ratio for a voice command.</p>
<heading id="h-0003" level="1">SUMMARY OF THE INVENTION</heading>
<p id="p-0011" num="0010">It is one object of the present invention to provide a voice command processing system that can improve the recognition ratio for a user issued voice command. To achieve this objective, a voice command processing system according to the present invention, for processing a user issued voice command can include registration means for the advance registration of multiple executable commands; acquisition means for obtaining a user issued voice command; determination means for determining which one of the commands registered in the registration means matches the voice command obtained by the acquisition means; and calculation means for, when the determination means ascertains that the voice command does not match any of the registered commands, analyzing the voice command as a sentence, and for calculating, for the registered commands and the sentence, a similarity. According to this invention, since a similarity is calculated, a registered command similar to a voice command can be identified.</p>
<p id="p-0012" num="0011">The voice command processing system further can include execution instruction means for instructing the execution of a registered command for which the similarity obtained by the calculation means falls within a predetermined range. The voice command processing system also can include request means for, when there are multiple registered commands for which the similarities fall within the first range or when there is one or multiple registered commands for which similarities fall within a second range set lower than the first range, notifying the user of the registered commands, and for requesting that the user determine whether the registered commands should be executed.</p>
<p id="p-0013" num="0012">The present invention can be implemented as a computer. A computer according to this invention, for executing a user issued voice command, can include a registration unit for registering a command executable by the computer; a voice command processor for accepting a user issued voice command, and for performing a process to execute the voice command; a speech recognition engine for analyzing, as a sentence, the voice command accepted by the voice command processor; and a similarity calculator for comparing the sentence analyzed by the speech recognition engine with the command registered in the registration unit, and for calculating a similarity for the voice command.</p>
<p id="p-0014" num="0013">The computer according to the invention also can include a data score registration unit for registering phrases similar to the phrases in the command registered in the registration unit, and scores set based on the phrases and the similar phrases. The similarity calculator may employ the scores when calculating the similarity. Further, the voice command processor can execute the registered command when the similarity exceeds a predetermined threshold value.</p>
<p id="p-0015" num="0014">Furthermore, the present invention can be implemented as a voice command processing method. A voice command processing method according to the invention that permits a computer to perform a user issued voice command can include the steps of accepting a user issued voice command; determining whether the voice command can be identified as a previously registered command; analyzing the voice command as a sentence when it is impossible for the voice command to be identified as a registered command; and comparing phrases in the analyzed sentence with phrases in the registered command in order to calculate a similarity.</p>
<p id="p-0016" num="0015">The voice command processing method further can include the steps of calculating scores based on matches obtained between phrases in the sentence and phrases in the registered command; and employing the scores to calculate a similarity for the sentence and the registered command. In this case, the voice command processing method further includes a step of registering, as a registered command and in correlation with the sentence, a voice command for which the similarity falls within a predetermined range. In addition, when the similarity falls within the predetermined range, the voice command processing method further includes a step of executing the registered command for which the similarity has been calculated.</p>
<p id="p-0017" num="0016">Also, the voice command processing method can include a step of at least either when there are multiple registered commands for which similarities fall within the first range, or when there is one or more registered commands for which the similarities fall within a second range, set lower than the first range, displaying for the user the registered commands for which similarities have been obtained, and requesting that the user select a registered command to be executed.</p>
<p id="p-0018" num="0017">Moreover, a voice command processing method, according to the invention, that permits a computer to execute a user issued voice command can include the steps of analyzing as a sentence a user issued voice command; and comparing the voice command (W<b>1</b>), analyzed as a sentence, with a command (W<b>2</b>) registered in advance to calculate a similarity S(W<b>1</b>,W<b>2</b>), whereby the similarity S(W<b>1</b>,W<b>2</b>) is represented by a value obtained by dividing, by the number (Vn) of applicable words for the calculation of the similarity S(W<b>1</b>,W<b>2</b>), the sum of scores (s) that is based on matching the i-th word (w<b>1</b>(i)) of the voice command and the j-th word (w<b>2</b>(j)) of the registered command. It should be noted that the similarity S(W<b>1</b>,W<b>2</b>) is represented as Σ s (w<b>1</b><sub>(i)</sub>,w<b>2</b><sub>(j)</sub>)/Vn.</p>
<p id="p-0019" num="0018">The voice command processing method can further include the steps of executing a registered command when the similarity S(W<b>1</b>,W<b>2</b>) is higher than a first threshold value (TH<b>1</b>); and when the similarity S(W<b>1</b>,W<b>2</b>) is lower than the first threshold value (TH<b>1</b>) and is higher than a second threshold value (TH<b>2</b>) lower than the first threshold value (TH<b>1</b>), displaying the registered command for the user and requesting the user determine whether the registered command should be executed.</p>
<p id="p-0020" num="0019">In addition, the present invention can also be implemented as a program that permits a computer to execute a voice command issued by a user.</p>
<?BRFSUM description="Brief Summary" end="tail"?>
<?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?>
<description-of-drawings>
<heading id="h-0004" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading>
<p id="p-0021" num="0020">There are shown in the drawings embodiments which are presently preferred, it being understood, however, that the invention is not limited to the precise arrangements and instrumentalities shown.</p>
<p id="p-0022" num="0021"><figref idref="DRAWINGS">FIG. 1</figref> is a diagram showing the configuration of a computer according to one embodiment of the invention.</p>
<p id="p-0023" num="0022"><figref idref="DRAWINGS">FIG. 2</figref> is a functional block diagram showing the components of a voice command processing system for the computer.</p>
<p id="p-0024" num="0023"><figref idref="DRAWINGS">FIG. 3</figref> is a diagram for explaining the processing performed by the voice command processing system.</p>
<p id="p-0025" num="0024"><figref idref="DRAWINGS">FIG. 4</figref> is a diagram showing example data registered in a dictionary.</p>
<p id="p-0026" num="0025"><figref idref="DRAWINGS">FIG. 5</figref> is a flowchart showing the similarity calculation processing.</p>
<p id="p-0027" num="0026"><figref idref="DRAWINGS">FIG. 6</figref> is a diagram showing an example grammar registered in a grammar registration unit.</p>
<p id="p-0028" num="0027"><figref idref="DRAWINGS">FIG. 7</figref> is a diagram showing example data entered in a score calculation table.</p>
<p id="p-0029" num="0028"><figref idref="DRAWINGS">FIG. 8</figref> is a diagram showing the similarities of the grammars relative to a voice command.</p>
<p id="p-0030" num="0029"><figref idref="DRAWINGS">FIG. 9</figref> is a flowchart showing the command selection processing.</p>
<p id="p-0031" num="0030"><figref idref="DRAWINGS">FIG. 10</figref> is a diagram showing an example screen for requesting a user grammar selection.</p>
</description-of-drawings>
<?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?>
<?DETDESC description="Detailed Description" end="lead"?>
<heading id="h-0005" level="1">DETAILED DESCRIPTION OF THE INVENTION</heading>
<p id="p-0032" num="0031">The preferred embodiment of the present invention will now be described in detail while referring to the accompanying drawings.</p>
<p id="p-0033" num="0032"><figref idref="DRAWINGS">FIG. 1</figref> is a diagram showing the configuration of a computer according to the embodiment of the present invention. A computer <b>100</b> in <figref idref="DRAWINGS">FIG. 1</figref> incorporates a predetermined OS (Operating System) conforming with the OADG (Open Architecture Developer's Group) specifications. The computer <b>100</b> includes a CPU (Central Processing Unit) <b>1</b>, a memory <b>2</b> and a graphic chip <b>4</b> for processing images, all of which are connected to a CPU bus <b>11</b> across a host bridge <b>3</b>. A CRT (display) <b>5</b> is also connected, through the graphics chip <b>4</b>, to the computer <b>100</b> as a data output means in order to present image data to a user.</p>
<p id="p-0034" num="0033">The computer <b>100</b> includes a keyboard/mouse controller <b>6</b> connected to the PCI bus <b>11</b>. And for entering screen location data, a mouse <b>7</b> and a keyboard <b>8</b> for supporting a key entry are connected as data input means to the computer <b>100</b> through the keyboard/mouse controller <b>6</b>.</p>
<p id="p-0035" num="0034">The computer <b>100</b> further includes: a modem/ethernet chip <b>12</b> for connecting with an external network; a card bus bridge <b>13</b>, for which slots <b>14</b> and <b>15</b> are provided for loading a MO (Magneto Optical) or a CD-ROM, for example; a USB (Universal Serial Bus) <b>17</b> for connecting an external device; and an IDE controller <b>19</b>, for which an HDD <b>18</b> is provided, all of which are connected to the PCI bus <b>11</b>. The computer <b>100</b> further includes an audio controller <b>21</b>, connected to the PCI bus <b>11</b>, for processing a speech signal, and an amplifier <b>22</b>, connected to the audio controller <b>21</b>. In addition, as one input means, a microphone <b>24</b> for collecting external sounds is connected to the computer <b>100</b> through the audio controller <b>21</b>. A loudspeaker <b>23</b> is also connected to the computer <b>100</b> to output as sound an audio signal received from the audio controller <b>21</b>.</p>
<p id="p-0036" num="0035">The computer <b>100</b> can be implemented by a common personal computer (PC), a workstation, a computer incorporated in an electric product, such as a television or a facsimile machine, a computer such as a navigation system mounted in a ground vehicle or in an airplane, or any combination thereof. Since the components shown in <figref idref="DRAWINGS">FIG. 1</figref> are merely examples, and since the present invention relates to the specification of a voice command and character data, not all components in <figref idref="DRAWINGS">FIG. 1</figref> are requisite, and components other than those shown may also be included.</p>
<p id="p-0037" num="0036">The operating system (OS) of the computer <b>100</b> can be one such as Windows (a trademark of Microsoft Corp.), OS/2 (a trademark of IBM Corp.) or MacOS (a trademark of Apple Inc.) that supports as a standard a GUI, multi-window environment, a character-based OS such as DOS, a real-time OS such as VxWorks (a trademark of Wind River Systems, Inc.) or another OS mounted in a network computer. In any case, the OS of computer <b>100</b> is not limited to a specific operating system.</p>
<p id="p-0038" num="0037"><figref idref="DRAWINGS">FIG. 2</figref> is a functional block diagram showing the components of a voice command processing system for the computer <b>100</b>. The voice command processing system in <figref idref="DRAWINGS">FIG. 2</figref> includes a command processor <b>201</b>, a voice command similarity calculator <b>203</b>, a score calculation table (score data registration unit) <b>205</b>, a voice recognition engine <b>207</b>, a grammar registration unit <b>209</b>, a dictionary <b>211</b>, and an application <b>213</b>.</p>
<p id="p-0039" num="0038">The command processor <b>201</b> obtains command data from the application <b>213</b>, and employs a similarity received from the voice command similarity calculator <b>203</b> to specify a command to be executed. The voice command similarity calculator <b>203</b> employs the score calculation table <b>205</b> to calculate a similarity for a sentence in a recognized voice command and the grammar of a command registered in the grammar registration unit <b>209</b>. The voice recognition engine <b>207</b> analyzes speech data using the dictionary <b>211</b>, and outputs a sentence as a speech recognition character string. The grammar registration unit <b>209</b> registers an executable command as a grammar, and words used for the voice command are registered in the dictionary <b>211</b>.</p>
<p id="p-0040" num="0039">The application <b>213</b> need only be software, such as a word processor, presentation software or a Web browser, that can handle character data, or software that can manage image data that can be converted into character data. The application <b>213</b> also executes a designated command upon receiving a request from the command processor <b>201</b>.</p>
<p id="p-0041" num="0040">The functional blocks in <figref idref="DRAWINGS">FIG. 2</figref> are logical functional blocks, and can be implemented by a combination of hardware and software blocks or by hardware and software blocks used in common, instead of by independent hardware or software blocks.</p>
<p id="p-0042" num="0041">The thus arranged computer <b>100</b> identifies a user issued voice command in order to perform a predetermined operation in the application <b>213</b>, and executes an action designated by the voice command. The voice command is a voice instruction to execute a predetermined OS operation or the application provided for the computer <b>100</b>. In this embodiment, even when the voice command issued by a user does not completely match the grammar registered in advance in the grammar registration unit <b>209</b>, the voice command can be executed by referring to how well the voice command matches the registered grammar (the similarity). This voice command recognition system will now be described in detail.</p>
<p id="p-0043" num="0042"><figref idref="DRAWINGS">FIG. 3</figref> is a flowchart for explaining the processing performed by the voice command processing system. First, the computer <b>100</b> determines whether a voice command issued by a user matches grammars registered as executable commands in the grammar registration unit <b>209</b> (step S<b>301</b>). Specifically, the user issued voice command is input at the microphone <b>24</b>, and is compared by the command processor <b>201</b> with the executable grammars registered in the grammar registration unit <b>209</b> to determine whether it matches one of the grammars.</p>
<p id="p-0044" num="0043">When it is ascertained at step <b>301</b> that the voice command matches one of the registered grammars, the command is executed by the application <b>213</b> (step S<b>303</b>) and the processing is terminated.</p>
<p id="p-0045" num="0044">When it is ascertained at step S<b>301</b> that the voice command issued by the user does not match any of the grammars, the recognition process for the sentence of the voice command is performed using a dictation process (step S<b>305</b>). The dictation process is one wherein the speech recognition engine <b>207</b> examines the dictionary <b>211</b> to extract words having readings that match those of words in the voice command, and identifies the speech as characters. Words, readings, and pronunciations are registered in the dictionary <b>211</b> in <figref idref="DRAWINGS">FIG. 4</figref>, and next, the command processor <b>201</b> transmits to the voice command similarity calculator <b>203</b> a sentence (W<b>1</b>) identified at step S<b>305</b>, and matches it with the executable grammars to calculate the similarity S (step S<b>307</b>). The calculation of the similarity S by the voice command similarity calculator <b>203</b> will now be specifically described.</p>
<p id="p-0046" num="0045"><figref idref="DRAWINGS">FIG. 5</figref> is a flowchart for explaining the processing for calculating a similarity. First, for the sentence (W<b>1</b>) obtained by recognition through dictation at step S<b>305</b>, the voice command similarity calculator <b>203</b> analyzes valid words that are applicable for calculation of the similarity (step S<b>401</b>). An explanation will be given by using an example wherein, as the recognition results obtained through dictation, the following sentence (voice command (W<b>1</b>)) is obtained as a user issued voice command: (W<b>1</b>) “Move the current window ten centimeters to the right.”</p>
<p id="p-0047" num="0046">At step S<b>401</b>, words are delimited for each phrase of this voice command (W<b>1</b>), and the types of parts are specified for these words. The analysis results obtained for the voice command (W<b>1</b>) are shown below. The underlined words (words other than postpositionals in this case) represent words that are determined to be applicable for calculation of the similarity.</p>
<p id="p-0048" num="0047">(W<b>1</b>); results of the analysis of “Move the current window ten centimeters to the right”:
<ul id="ul0001" list-style="none">
    <li id="ul0001-0001" num="0000">
    <ul id="ul0002" list-style="none">
        <li id="ul0002-0001" num="0048">Move . . . &lt;operation&gt;</li>
        <li id="ul0002-0002" num="0049">the . . . &lt;postpositional&gt;</li>
        <li id="ul0002-0003" num="0050">current . . . &lt;status&gt;</li>
        <li id="ul0002-0004" num="0051">window . . . &lt;entry&gt;</li>
        <li id="ul0002-0005" num="0052">ten . . . &lt;numeral&gt;</li>
        <li id="ul0002-0006" num="0053">centimeters . . . &lt;unit&gt;</li>
        <li id="ul0002-0007" num="0054">to . . . &lt;postpositional&gt;</li>
        <li id="ul0002-0008" num="0055">the . . . &lt;postpositional&gt;</li>
        <li id="ul0002-0009" num="0056">right . . . &lt;direction&gt;</li>
    </ul>
    </li>
</ul>
</p>
<p id="p-0049" num="0057">Following step S<b>401</b>, the voice command similarity calculator <b>203</b> employs the information received from the application <b>213</b> through the command processor <b>201</b> to obtain one of the grammars registered in the grammar registration unit <b>209</b> and executable by the application <b>213</b> (step S<b>403</b>). An example grammar registered in the grammar registration unit <b>209</b> is shown in <figref idref="DRAWINGS">FIG. 6</figref>. It should be noted, however, that the one shown in <figref idref="DRAWINGS">FIG. 6</figref> is merely one of the registered grammars, and many others can be registered. Further, while the grammar in <figref idref="DRAWINGS">FIG. 6</figref> is defined using the BN method, another method may be employed. An example grammar (W<b>2</b><sub>(1)</sub>) obtained at step S<b>403</b> is as follows: W<b>2</b><sub>(1)</sub>; Move the cursor &lt;number 1-9&gt; line &lt;forward, backward, upward, downward&gt;”.</p>
<p id="p-0050" num="0058">Next, as for the voice command (W<b>1</b>), the grammar W<b>2</b><sub>(1) </sub>obtained at step S<b>403</b> is analyzed to find applicable words for the similarity calculation (step S<b>405</b>). The analysis results for the grammar W<b>2</b><sub>(1) </sub>are shown below.</p>
<p id="p-0051" num="0059">The analysis results for W<b>2</b><sub>(1)</sub>; Move the cursor &lt;number 1-9&gt; line &lt;forward, backward, upward, downward&gt;”, are:
<ul id="ul0003" list-style="none">
    <li id="ul0003-0001" num="0000">
    <ul id="ul0004" list-style="none">
        <li id="ul0004-0001" num="0060">Move . . . &lt;operation&gt;</li>
        <li id="ul0004-0002" num="0061">the . . . &lt;postpositional&gt;</li>
        <li id="ul0004-0003" num="0062">cursor . . . &lt;entry&gt;</li>
        <li id="ul0004-0004" num="0063">&lt;number 1-9&gt;. . . &lt;numeral&gt;</li>
        <li id="ul0004-0005" num="0064">line . . . &lt;unit&gt;</li>
        <li id="ul0004-0006" num="0065">&lt;forward, backward, upward, downward&gt;. . . &lt;direction&gt;</li>
    </ul>
    </li>
</ul>
</p>
<p id="p-0052" num="0066">Following step S<b>405</b>, the words in the voice command (W<b>1</b>) issued by the user are compared with the words in the grammar (W<b>2</b><sub>(1)</sub>), and the score for each word combination is calculated (step S<b>407</b>). First, specifically for the similarity calculations, applicable words in the voice command (W<b>1</b>) are combined with the same type of words in accordance with the sentence word order. For example, the word pertinent to &lt;number&gt; in the voice command (W<b>1</b>) is combined with a word pertinent to &lt;number&gt; in the grammar (W<b>2</b><sub>(1)</sub>), and a score is calculated for the thus obtained combination. This process is repeated for all the words, and subsequently, the matching of words is performed without taking into account the word order in the voice command (W<b>1</b>) and the grammar (W<b>2</b><sub>(1)</sub>). The score is calculated.</p>
<p id="p-0053" num="0067">The calculation, at step S<b>407</b>, of the score for the matched words is performed by referring to data registered in a score calculation table <b>205</b>, and is based on a score calculation method that will be described below. For example, on the assumption 0≦s≦1, score (s) is specified to determine which of the following categories is pertinent to each word in the voice command (W<b>1</b>). In this case, since the word similarity is high, a high score is set.</p>
<p id="p-0054" num="0068">The following illustrates the score calculation method:
<ul id="ul0005" list-style="none">
    <li id="ul0005-0001" num="0000">
    <ul id="ul0006" list-style="none">
        <li id="ul0006-0001" num="0069">word completely matching a word designated by the grammar . . . 1.0;</li>
        <li id="ul0006-0002" num="0070">word defined as a variable by the grammar and matching the variable . . . 0.9;</li>
        <li id="ul0006-0003" num="0071">word matching “similar words” in the score calculation table <b>205</b> . . . score point designated in the score calculation table <b>205</b>;</li>
        <li id="ul0006-0004" num="0072">word for which the location, from the beginning of the sentence, matches the grammatical word order, even though the word, as a word, is not matched . . . 0.1; and</li>
        <li id="ul0006-0005" num="0073">word that does not correspond to any of the above categories . . . 0.</li>
    </ul>
    </li>
</ul>
</p>
<p id="p-0055" num="0074">Example data registered in the score calculation table <b>205</b> are shown in <figref idref="DRAWINGS">FIG. 7</figref>. As is shown in <figref idref="DRAWINGS">FIG. 7</figref>, scores are registered in the score calculation table <b>205</b> for words that are similar to predetermined words. Assuming that a word in the user issued voice command is related to “active” in the grammar, and that this word is related to one of the similar words in <figref idref="DRAWINGS">FIG. 7</figref>, the numerical value entered in the next right column is defined as a score. It should be noted that these scores can be changed as needed.</p>
<p id="p-0056" num="0075">An explanation will now be given for the score (s) that is obtained for each entry in the voice command (W<b>1</b>) and the grammar (W<b>2</b><sub>(1)</sub>) by referring to the score calculation table <b>205</b>. When multiple combinations are expected for a single word, the score finally obtained is the highest of those obtained for all the combinations. The results of the score (s) are shown in the order &lt;type&gt;: S(entry of W<b>1</b>; entry of W<b>2</b><sub>(1)</sub>).</p>
<p id="p-0057" num="0076">Score results (s) for the voice command (W<b>1</b>) and the grammar (W<b>2</b><sub>(1)</sub>);
<ul id="ul0007" list-style="none">
    <li id="ul0007-0001" num="0000">
    <ul id="ul0008" list-style="none">
        <li id="ul0008-0001" num="0077">&lt;numeral&gt;: s (10; &lt;number 1-9&gt;)=0.1</li>
        <li id="ul0008-0002" num="0078">&lt;unit&gt;: s (centimeters; line)=0.2</li>
        <li id="ul0008-0003" num="0079">&lt;direction&gt;: s (right; &lt;forward, backward, upward, backward&gt;)=0.4</li>
        <li id="ul0008-0004" num="0080">&lt;entry&gt;: s (window; cursor)=0.1</li>
        <li id="ul0008-0005" num="0081">&lt;operation&gt;: s (move; move)=1.0</li>
    </ul>
    </li>
</ul>
</p>
<p id="p-0058" num="0082">Then, as is shown in <figref idref="DRAWINGS">FIG. 5</figref>, based on these scores, the similarity S(W<b>1</b>,W<b>2</b>) is calculated using equation (1) (step S<b>409</b>).
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>S</i>(<i>W</i>1<i>,W</i>2)=Σ<i>s</i>(<i>w</i>1<sub>(i)</sub><i>, w</i>2<sub>(j)</sub>)/<i>Vn</i>  (1)<?in-line-formulae description="In-line Formulae" end="tail"?>
<br/>
wherein the terms in equation (1) are as follows:
<ul id="ul0009" list-style="none">
    <li id="ul0009-0001" num="0000">
    <ul id="ul0010" list-style="none">
        <li id="ul0010-0001" num="0083">w<b>1</b><sub>(i) </sub>. . . the i-th word of the issued voice command (1≦i≦m)</li>
        <li id="ul0010-0002" num="0084">w<b>2</b><sub>(j) </sub>. . . the j-th word of the issued voice command (1≦j≦n)</li>
        <li id="ul0010-0003" num="0085">W<b>1</b> . . . voice command that was issued</li>
        <li id="ul0010-0004" num="0086">W<b>2</b> . . . grammar to be compared</li>
        <li id="ul0010-0005" num="0087">Vn . . . the number of applicable words for similarity calculation</li>
    </ul>
    </li>
</ul>
</p>
<p id="p-0059" num="0088">As a result, between the voice command (W<b>1</b>) and the grammar (W<b>2</b><sub>(1)</sub>), the similarity S(W<b>1</b>,W<b>2</b><sub>(1)</sub>) is (0.1+0.2+0.4+0.1+1.0)/5=0.36.</p>
<p id="p-0060" num="0089">In this embodiment, the scores obtained at step S<b>407</b> are employed for the calculation of the similarity S; however, the weighting may be performed in accordance with the types of words used to calculate the similarity S. For example, since &lt;entry&gt;, which can serve as the subject, tends to affect the meaning of the command, the similarity S can also be calculated while weighting is performed to increase the &lt;entry&gt; score.</p>
<p id="p-0061" num="0090">Following step S<b>409</b>, a check is performed to determine whether all the executable grammars registered in the grammar registration unit <b>209</b> have been analyzed (step S<b>411</b>). Specifically, the voice command (W<b>1</b>) is combined with each executable grammar, and the process at steps S<b>405</b> to S<b>409</b> in <figref idref="DRAWINGS">FIG. 5</figref> is performed for each of these grammars. When all the grammars have been analyzed, this processing is terminated, and the process at step S<b>309</b> in <figref idref="DRAWINGS">FIG. 3</figref>, which will be described later, is performed.</p>
<p id="p-0062" num="0091">When at step S<b>411</b> not all the grammars have been analyzed, program control returns to step S<b>403</b>, one of the executable grammars is extracted, and the process is repeated. A specific explanation will now be given for an example wherein, following the completion of the processing performed for grammar (W<b>2</b><sub>(1)</sub>), the same processing is performed for grammars (W<b>2</b><sub>(2)</sub>) and (W<b>2</b><sub>(3)</sub>), which have been analyzed relative to the voice command W<b>1</b>, and the similarity S is calculated. The processes performed at steps S<b>403</b> to S<b>409</b> are the same as those performed for the grammar (W<b>2</b><sub>(1)</sub>), and no detailed explanation will be given for them.</p>
<p id="p-0063" num="0092">Analysis results obtained at step S<b>407</b> for (W<b>2</b><sub>(2)</sub>) “Move the cursor &lt;left, right, up, down&gt;”;
<ul id="ul0011" list-style="none">
    <li id="ul0011-0001" num="0000">
    <ul id="ul0012" list-style="none">
        <li id="ul0012-0001" num="0093">Move . . . &lt;operation&gt;</li>
        <li id="ul0012-0002" num="0094">the . . . &lt;postpositional&gt;</li>
        <li id="ul0012-0003" num="0095">cursor . . . &lt;entry&gt;</li>
        <li id="ul0012-0004" num="0096">&lt;left, right, up, down&gt;. . . &lt;direction&gt;
<br/>
Scores (s) between the voice command (W<b>1</b>) and the grammar (W<b>2</b><sub>(2)</sub>) obtained at step S<b>407</b>:
</li>
        <li id="ul0012-0005" num="0097">&lt;entry&gt;: s (window; cursor)=0.1</li>
        <li id="ul0012-0006" num="0098">&lt;direction&gt;: s (right, &lt;forward, backward, upward, downward&gt;)=0.9</li>
        <li id="ul0012-0007" num="0099">&lt;operation&gt;: s (move, move)=1.0
<br/>
Similarity S(W<b>1</b>,W<b>2</b><sub>(1)</sub>) between the voice command (W<b>1</b>) and the grammar (W<b>2</b><sub>(2)</sub>) obtained at step S<b>409</b>: (0.1+0.9+1.0)/3=0.67.
</li>
    </ul>
    </li>
</ul>
</p>
<p id="p-0064" num="0100">Analysis results at step S<b>405</b> for (W<b>2</b><sub>(3)</sub>) “Move the active window &lt;left, right, up, down&gt; by &lt;number&gt; &lt;unit&gt;;
<ul id="ul0013" list-style="none">
    <li id="ul0013-0001" num="0000">
    <ul id="ul0014" list-style="none">
        <li id="ul0014-0001" num="0101">Move . . . &lt;operation&gt;</li>
        <li id="ul0014-0002" num="0102">the . . . &lt;postpositional&gt;</li>
        <li id="ul0014-0003" num="0103">active . . . &lt;status&gt;</li>
        <li id="ul0014-0004" num="0104">window . . . &lt;entry&gt;</li>
        <li id="ul0014-0005" num="0105">&lt;left, right, up, down&gt;. . . &lt;direction&gt;</li>
        <li id="ul0014-0006" num="0106">by . . . &lt;postpositional&gt;</li>
        <li id="ul0014-0007" num="0107">&lt;number&gt;. . . &lt;numeral&gt;</li>
        <li id="ul0014-0008" num="0108">&lt;unit&gt;. . . &lt;unit&gt;
<br/>
Scores (s) obtained between the voice command (W<b>1</b>) and the grammar (W<b>2</b><sub>(3)</sub>) obtained at step S<b>407</b>:
</li>
        <li id="ul0014-0009" num="0109">&lt;status&gt;: s (current; active)=0.5</li>
        <li id="ul0014-0010" num="0110">&lt;entry&gt;: s (window; window)=1.0</li>
        <li id="ul0014-0011" num="0111">&lt;numeral&gt;: s (10; &lt;number&gt;)=0.9</li>
        <li id="ul0014-0012" num="0112">&lt;unit&gt;: s (centimeter; &lt;unit&gt;)=0.9</li>
        <li id="ul0014-0013" num="0113">&lt;direction&gt;: s (right; &lt;left, right, up, down&gt;)=0.9</li>
        <li id="ul0014-0014" num="0114">&lt;operation&gt;: s (move; move)=1.0
<br/>
Similarity S(W<b>1</b>,W<b>2</b><sub>(3)</sub>) between the voice command (W<b>1</b>) and the grammar (W<b>2</b><sub>(3)</sub>) obtained at step S<b>409</b>: (0.5+1.0+0.9+0.9+0.9+1.0)/6=0.87.
</li>
    </ul>
    </li>
</ul>
</p>
<p id="p-0065" num="0115">In this manner, the voice command similarity calculator <b>203</b> calculates the similarity S for the voice command relative to each executable grammar, and when at step S<b>411</b> all the executable grammars have been analyzed, the process at step S<b>309</b> in <figref idref="DRAWINGS">FIG. 3</figref> is performed. In order to simplify the explanation, analyzation is performed for only the three executable grammars (W<b>2</b><sub>(1)</sub>), (W<b>2</b><sub>(2)</sub>) and (W<b>2</b><sub>(3)</sub>); however, since actually all the executable grammars are analyzed, the similarity S is calculated for each of multiple grammars.</p>
<p id="p-0066" num="0116">Next, the command processor <b>201</b> determines whether, among the similarities S obtained at step S<b>307</b> between a single voice command (W<b>1</b>) and multiple grammars (W<b>2</b><sub>(1)</sub>), (W<b>2</b><sub>(2)</sub>), (W<b>2</b><sub>(3)</sub>), . . . there is a similarity higher than a first predetermined threshold value (TH<b>1</b>) (step S<b>309</b>). It should be noted that the first threshold value (TH<b>1</b>) is 0.9, for example. The similarities S for the grammars (W<b>2</b><sub>(1)</sub>), (W<b>2</b><sub>(2)</sub>), and (W<b>2</b><sub>(3)</sub>) are those shown in <figref idref="DRAWINGS">FIG. 8</figref>, none of which is higher than the first threshold value (TH<b>1</b>), 0.9. When no grammar has a similarity S higher than the first threshold value, the process at step S<b>317</b>, which will be described later, is performed.</p>
<p id="p-0067" num="0117">When it is ascertained at step S<b>309</b> that there is a similarity S higher than the first threshold value (TH<b>1</b>), a check is performed to determine whether there is only one (step S<b>311</b>). When there is only one, the execution of a command designated by the grammar (W<b>2</b><sub>(x)</sub>) having this similarity S is transmitted to the application <b>213</b>, which executes the command (step S<b>315</b>). Thereafter, the processing is terminated. But when it is ascertained at step S<b>311</b> that at least two similarities S are higher than the first threshold value (TH<b>1</b>), i.e., when there are multiple executable grammars having similarities S higher than the first threshold value (TH<b>1</b>), the process at step S<b>319</b>, which will be described later, is performed.</p>
<p id="p-0068" num="0118">When at step S<b>309</b> among the similarities S of the single voice command (W<b>1</b>) relative to multiple grammars, there is no similarity S higher than the first threshold value (TH<b>1</b>), a check is performed to determine whether there is a similarity S higher than a second threshold value (TH<b>2</b>), which is a lower limit value set lower than the first threshold value (TH<b>1</b>) (step S<b>317</b>). In this case, the second threshold value is, for example, 0.5. When there is no similarity S higher than the second threshold value (TH<b>2</b>), i.e., when the similarities S of all the grammars are equal to or lower than the second threshold value, the voice command is handled as a normally input sentence (step S<b>321</b>) and the processing is thereafter terminated. When at step S<b>317</b> there are similarities S higher than the second threshold value, e.g., when as is shown in <figref idref="DRAWINGS">FIG. 8</figref> there are grammars (W<b>2</b><sub>(2)</sub>) and (W<b>2</b><sub>(3)</sub>), the similarities S of which are higher than the 0.5 second threshold value (TH<b>2</b>), the command selection process is performed (step S<b>319</b>) and this processing is terminated.</p>
<p id="p-0069" num="0119">In this embodiment, only two threshold values are provided; however, the number is not limited to two, and the number of threshold values to be provided and their numerical values are variable in accordance with an arbitrary processing speed or accuracy. Further, an arbitrary determination reference, such as “a value higher than a predetermined threshold value” or “a value equal to or higher than a predetermined threshold value, can be employed. In short, at steps S<b>309</b>, S<b>311</b>, and S<b>317</b> of this invention, whether the similarity falls within a predetermined range, and different processes are preformed in accordance with the results.</p>
<p id="p-0070" num="0120"><figref idref="DRAWINGS">FIG. 9</figref> is a flowchart for explaining the command selection processing at step S<b>319</b>.</p>
<p id="p-0071" num="0121">First, the grammars (W<b>2</b><sub>(2)</sub>) and (W<b>2</b><sub>(3)</sub>), found at step S<b>317</b> in <figref idref="DRAWINGS">FIG. 3</figref> to have similarities S higher than the second threshold value (TH<b>2</b>), are displayed for the user, and a request is issued to the user to select a grammar as a command to be executed (step S<b>501</b>). In this embodiment, a screen shown in <figref idref="DRAWINGS">FIG. 10</figref> is displayed on the CRT <b>5</b> connected to the computer <b>100</b>. As is shown in <figref idref="DRAWINGS">FIG. 10</figref>, the grammars (W<b>2</b><sub>(2)</sub>) and (W<b>2</b><sub>(3)</sub>) having similarities S higher than the second threshold value (TH<b>2</b>) are displayed on the screen, so that the user can employ the mouse <b>7</b> or the keyboard <b>8</b> to select the grammar pertinent to the command that the user originally desired to execute. The command processor <b>201</b> accepts the grammar selected by the user (step S<b>503</b>).</p>
<p id="p-0072" num="0122">Then, the command processor <b>201</b> correlates the grammar accepted at step S<b>503</b> with the voice command that the user originally issued, and registers the accepted grammar as a new one in the grammar registration unit <b>209</b> (step S<b>505</b>). The newly registered grammar is then executed (step S<b>507</b>) and this processing is thereafter terminated.</p>
<p id="p-0073" num="0123">This new grammar registered in the grammar registration unit <b>209</b> is not the original voice command issued by the user, but a grammar that can also be identified even when the command is replaced with predetermined words by the BN method.</p>
<p id="p-0074" num="0124">An explanation will be given for an example wherein the voice command (W<b>1</b>) issued by the user is “Move the current window ten centimeters to the right”, while the user selects on the screen in <figref idref="DRAWINGS">FIG. 10</figref> “Move the active window ten centimeters to the right” (W<b>2</b><sub>(3)</sub>). At this time, &lt;command3&gt; shown in <figref idref="DRAWINGS">FIG. 6</figref> is already registered in the grammar (W<b>2</b><sub>(3)</sub>) in the grammar registration unit <b>209</b>. &lt;command3&gt; is changed to the following command. In this case, the already registered grammar is connected to a newly added grammar by “or”, and words “the current” is added as the &lt;status&gt; definition. “&lt;command3&gt;=Move the &lt;status&gt; window &lt;direction&gt; by &lt;number&gt; &lt;unit&gt;” and “&lt;status&gt;=active|. . . |current”.</p>
<p id="p-0075" num="0125">As is described above, the voice command processing system in this embodiment can execute the voice command issued by a user when the similarity of this command relative to the grammars already registered is high even when the command does not completely match them. Therefore, the user need not remember all the voice commands exactly, and those occasions when the user feels displeased because the voice command is not identified can be reduced.</p>
<p id="p-0076" num="0126">In addition, since the voice command of the user is registered in correlation with the already registered grammar, i.e., when the grammar is reconstructed, the variety of the voice commands that can be handled can be expanded. Furthermore, since by using this method the voice command of the user is not registered unchanged, the amount of registered grammars is not drastically increased, and the accuracy (recognition ratio) for the recognition of the voice command is not deteriorated much. Moreover, since the variation of the grammars registered in advance is reduced at the initial setup of the voice command processing system, and commands matching the user's tastes can be newly added, the usability is increased for each user.</p>
<p id="p-0077" num="0127">The program for performing the processing in this embodiment can be implemented as a storage medium or a program transmission apparatus as follows. That is, the program executed by the computer need only be stored on a computer-readable storage medium, such as a CD-ROM, a DVD, a memory, or a hard disk. Furthermore, the program transmission apparatus need only include storage means, such as a CD-ROM, a DVD, a memory or a hard disk, for storing the program; and transmission means for reading this program from the storage means and transmitting this program to a program execution apparatus via a connector or a network, such as a LAN.</p>
<p id="p-0078" num="0128">In addition, without departing from the scope of the invention, the configuration of this embodiment can be selectively changed or modified, as needed, to obtain another configuration.</p>
<p id="p-0079" num="0129">As is described above, according to the invention, the voice command recognition ratio can be improved by this voice command processing system.</p>
<p id="p-0080" num="0130">The present invention can be realized in hardware, software, or a combination of hardware and software. The present invention can be realized in a centralized fashion in one computer system, or in a distributed fashion where different elements are spread across several interconnected computer systems. Any kind of computer system or other apparatus adapted for carrying out the methods described herein is suited. A typical combination of hardware and software can be a general purpose computer system with a computer program that, when being loaded and executed, controls the computer system such that it carries out the methods described herein.</p>
<p id="p-0081" num="0131">The present invention also can be embedded in a computer program product, which comprises all the features enabling the implementation of the methods described herein, and which when loaded in a computer system is able to carry out these methods. Computer program in the present context means any expression, in any language, code or notation, of a set of instructions intended to cause a system having an information processing capability to perform a particular function either directly or after either or both of the following: a) conversion to another language, code or notation; b) reproduction in a different material form.</p>
<p id="p-0082" num="0132">This invention can be embodied in other forms without departing from the spirit or essential attributes thereof. Accordingly, reference should be made to the following claims, rather than to the foregoing specification, as indicating the scope of the invention.</p>
<?DETDESC description="Detailed Description" end="tail"?>
</description>
<us-claim-statement>What is claimed is:</us-claim-statement>
<claims id="claims">
<claim id="CLM-00001" num="00001">
<claim-text>1. A voice command processing system, for processing a user issued voice command, comprising:
<claim-text>registration means for the advance registration of multiple executable commands;</claim-text>
<claim-text>acquisition means for obtaining said user issued voice command;</claim-text>
<claim-text>determination means for determining whether one of said executable commands registered in said registration means matches said user issued voice command obtained by said acquisition means;</claim-text>
<claim-text>execution means for executing one of said registered commands if performing a process to execute said user issued voice command matches one of said grammars;</claim-text>
<claim-text>calculation means for, when said determination means ascertains that said voice command does not match any of said registered executable commands, analyzing said voice issued command as a dictated sentence, and calculating for said registered commands and said sentence a similarity, wherein said similarity is based on a comparison of a reading of words in said sentence analyzed with a reading of words in said registered executable command; and</claim-text>
<claim-text>request means for when there are multiple registered executable commands for which the similarities fall above a first threshold, notifying said user of said registered executable commands, and requesting that said user determine whether said registered commands for which the similarities fall above said first threshold should be executed.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00002" num="00002">
<claim-text>2. The voice command processing system according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising:
<claim-text>execution instruction means for instructing the execution of a registered executable command for which said similarity obtained by said calculation means falls within a predetermined range.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00003" num="00003">
<claim-text>3. The voice command processing system according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising:
<claim-text>request means for, when there is one or multiple registered commands for which similarities fall between the first threshold and a second threshold lower than the first threshold, notifying said user of said registered commands for which similarities fall between the first and the second threshold, and requesting that said user determine whether said registered commands for which similarities fall between the first and the second threshold should be executed.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00004" num="00004">
<claim-text>4. A computer, for executing user issued voice commands, comprising:
<claim-text>a grammar registration unit for registering one or more commands executable by said computer as grammars;</claim-text>
<claim-text>a command processor for accepting a user issued voice command and for executing one of said registered commands if said user issued voice command matches one of said grammars;</claim-text>
<claim-text>a voice recognition engine for analyzing, as a dictated sentence, said user issued voice command accepted by said command processor when it is impossible for said accepted voice command to be matched to one of said grammars;</claim-text>
<claim-text>a voice command similarity calculator for comparing a reading of words in said sentence analyzed by said voice recognition engine to a reading of words in each of said commands registered in said grammar registration unit, and calculating for said voice command a similarity to each of said registered commands; and</claim-text>
<claim-text>a score data registration unit for registering words having a reading similar to the reading of the words in each of said registered commands and a score for each of said similar reading words based on a similarity of the reading of said words in said registered command and said similar reading words;</claim-text>
<claim-text>wherein said voice command similarity calculator employs said scores when calculating said voice command similarity.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00005" num="00005">
<claim-text>5. The computer according to <claim-ref idref="CLM-00004">claim 4</claim-ref>, wherein said voice command processor executes said registered command when said similarity exceeds a predetermined threshold value.</claim-text>
</claim>
<claim id="CLM-00006" num="00006">
<claim-text>6. A voice command processing method that permits a computer to perform a user issued voice command comprising the steps of:
<claim-text>accepting a user issued voice command;</claim-text>
<claim-text>determining whether said voice command can be identified as a previously registered command for an operation;</claim-text>
<claim-text>analyzing said voice command as a dictated sentence when it is impossible for said accepted voice command to be identified as a previously registered command for an operation;</claim-text>
<claim-text>calculating a score for each word in said sentence based on a similarity of the reading of said each word in said sentence and words in a previously registered command for an operation;</claim-text>
<claim-text>employing said scores to calculate a similarity for said sentence relative to said previously registered command; and</claim-text>
<claim-text>registering, as a new registered command for said operation and in correlation with said sentence, said accepted voice command if said similarity falls within a predetermined range.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00007" num="00007">
<claim-text>7. The voice command processing method according to <claim-ref idref="CLM-00006">claim 6</claim-ref>, further comprising the step of:
<claim-text>when said similarity falls within said predetermined range, executing said registered command for which said similarity has been calculated.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00008" num="00008">
<claim-text>8. The voice command processing method according to <claim-ref idref="CLM-00006">claim 6</claim-ref>, further comprising the steps of:
<claim-text>at least either when there are two or more registered commands for which similarities fall above a first threshold, or when there are one or more registered commands for which said similarities fall between said first threshold and a second threshold, set lower than said first threshold, displaying for said user said registered commands for which similarities have been obtained, and requesting that said user select a registered command to be executed.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00009" num="00009">
<claim-text>9. A voice command processing method that permits a computer to execute a user issued voice command comprising the steps of:
<claim-text>determining that the user issued voice command cannot be identified as an executable command registered (W<b>2</b>) in advance;</claim-text>
<claim-text>analyzing as a dictated sentence said user issued voice command;</claim-text>
<claim-text>comparing said analyzed user issued voice command (W<b>1</b>) with the registered command (W<b>2</b>) to calculate a similarity S(W<b>1</b>,W<b>2</b>);</claim-text>
<claim-text>if said similarity S(W<b>1</b>,W<b>2</b>) is higher than a first threshold value (TH<b>1</b>), executing the registered command (W<b>2</b>); and</claim-text>
<claim-text>if said similarity S(W<b>1</b>,W<b>2</b>) is lower than said first threshold value (TH<b>1</b>) and higher than a second threshold value (TH<b>2</b>) lower than said first threshold value (TH<b>1</b>), displaying said registered command (W<b>2</b>) for said user and requesting said user determine whether said registered command (W<b>2</b>) should be executed,</claim-text>
<claim-text>whereby said similarity S(W<b>1</b>,W<b>2</b>) is represented by a value obtained by dividing, by the number (Vn) of applicable words for the calculation of said similarity S(W<b>1</b>,W<b>2</b>), a sum of scores (s) that is based on a similarity of a reading of the i-th word (w<b>1</b>(i)) of said voice command and the j-th word (w<b>2</b>(j)) of said registered command (W<b>2</b>).</claim-text>
</claim-text>
</claim>
<claim id="CLM-00010" num="00010">
<claim-text>10. A machine readable storage, having stored thereon a computer program having a plurality of code sections executable by a machine for causing the machine to perform the steps of:
<claim-text>accepting a user issued voice command;</claim-text>
<claim-text>determining whether said voice command can be identified as a previously registered command for an operation;</claim-text>
<claim-text>analyzing said voice command as a dictated sentence when it is impossible for said accepted voice command to be identified as a previously registered command for an operation;</claim-text>
<claim-text>calculating a score for each word in said sentence based on a similarity of the reading of said each word in said sentence and words in a previously registered command for an operation;</claim-text>
<claim-text>employing said scores to calculate a similarity for said sentence relative to said previously registered command; and</claim-text>
<claim-text>registering, as a new registered command for said operation and in correlation with said sentence, said accepted voice command if said similarity falls within a predetermined range.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00011" num="00011">
<claim-text>11. The machine readable storage of <claim-ref idref="CLM-00010">claim 10</claim-ref>, further causing the machine to perform the step of:
<claim-text>when said similarity falls within said predetermined range, executing said registered command for which said similarity has been calculated.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00012" num="00012">
<claim-text>12. The machine readable storage of <claim-ref idref="CLM-00010">claim 10</claim-ref>, further causing the machine to perform the steps of:
<claim-text>at least either when there are two or more registered commands for which similarities fall above a first threshold, or when there are one or more registered commands for which said similarities fall between said first threshold and a second threshold, set lower than said first threshold, displaying for said user said registered commands for which similarities have been obtained, and requesting that said user select a registered command to be executed.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00013" num="00013">
<claim-text>13. A machine readable storage, having stored thereon a computer program having a plurality of code sections executable by a machine for causing the machine to perform the steps of:
<claim-text>determining that the user issued voice command cannot be identified as an executable command registered (W<b>2</b>) in advance;</claim-text>
<claim-text>analyzing as a dictated sentence said user issued voice command; and</claim-text>
<claim-text>comparing said analyzed user issued voice command (W<b>1</b>) with the registered command (W<b>2</b>) to calculate a similarity S(W<b>1</b>,W<b>2</b>);</claim-text>
<claim-text>if said similarity S(W<b>1</b>,W<b>2</b>) is higher than a first threshold value (TH<b>1</b>), executing the registered command (W<b>2</b>); and</claim-text>
<claim-text>if said similarity S(W<b>1</b>,W<b>2</b>) is lower than said first threshold value (TH<b>1</b>) and higher than a second threshold value (TH<b>2</b>) lower than said first threshold value (TH<b>1</b>), displaying said registered command (W<b>2</b>) for said user and requesting said user determine whether said registered command (W<b>2</b>) should be executed,</claim-text>
<claim-text>whereby said similarity S(W<b>1</b>,W<b>2</b>) is represented by a value obtained by dividing, by the number (Vn) of applicable words for the calculation of said similarity S(W<b>1</b>,W<b>2</b>), the sum of scores (s) that is based on a similarity of a reading of the i-th word (w<b>1</b>(i)) of said voice command and the j-th word (w<b>2</b>(j)) of said registered command (W<b>2</b>).</claim-text>
</claim-text>
</claim>
</claims>
</us-patent-grant>
