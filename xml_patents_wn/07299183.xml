<us-patent-grant lang="EN" dtd-version="v4.2 2006-08-23" file="US07299183-20071120.XML" status="PRODUCTION" id="us-patent-grant" country="US" date-produced="20071106" date-publ="20071120">
<us-bibliographic-data-grant>
<publication-reference>
<document-id>
<country>US</country>
<doc-number>07299183</doc-number>
<kind>B2</kind>
<date>20071120</date>
</document-id>
</publication-reference>
<application-reference appl-type="utility">
<document-id>
<country>US</country>
<doc-number>10899007</doc-number>
<date>20040727</date>
</document-id>
</application-reference>
<us-application-series-code>10</us-application-series-code>
<priority-claims>
<priority-claim sequence="01" kind="national">
<country>JP</country>
<doc-number>2003-202554</doc-number>
<date>20030728</date>
</priority-claim>
</priority-claims>
<us-term-of-grant>
<us-term-extension>470</us-term-extension>
</us-term-of-grant>
<classifications-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>G</section>
<class>10</class>
<subclass>L</subclass>
<main-group>11</main-group>
<subgroup>00</subgroup>
<symbol-position>F</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20071120</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>H</section>
<class>04</class>
<subclass>N</subclass>
<main-group>5</main-group>
<subgroup>91</subgroup>
<symbol-position>L</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20071120</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
</classifications-ipcr>
<classification-national>
<country>US</country>
<main-classification>704270</main-classification>
<further-classification>386 95</further-classification>
</classification-national>
<invention-title id="d0e71">Closed caption signal processing apparatus and method</invention-title>
<references-cited>
<citation>
<patcit num="00001">
<document-id>
<country>US</country>
<doc-number>5649060</doc-number>
<kind>A</kind>
<name>Ellozy et al.</name>
<date>19970700</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>704278</main-classification></classification-national>
</citation>
<citation>
<patcit num="00002">
<document-id>
<country>US</country>
<doc-number>5703655</doc-number>
<kind>A</kind>
<name>Corey et al.</name>
<date>19971200</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>348468</main-classification></classification-national>
</citation>
<citation>
<patcit num="00003">
<document-id>
<country>US</country>
<doc-number>5741136</doc-number>
<kind>A</kind>
<name>Kirksey et al.</name>
<date>19980400</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>434169</main-classification></classification-national>
</citation>
<citation>
<patcit num="00004">
<document-id>
<country>US</country>
<doc-number>6076059</doc-number>
<kind>A</kind>
<name>Glickman et al.</name>
<date>20000600</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>704260</main-classification></classification-national>
</citation>
<citation>
<patcit num="00005">
<document-id>
<country>US</country>
<doc-number>6442518</doc-number>
<kind>B1</kind>
<name>Van Thong et al.</name>
<date>20020800</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>704235</main-classification></classification-national>
</citation>
<citation>
<patcit num="00006">
<document-id>
<country>US</country>
<doc-number>7054804</doc-number>
<kind>B2</kind>
<name>Gonzales et al.</name>
<date>20060500</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>704  8</main-classification></classification-national>
</citation>
<citation>
<patcit num="00007">
<document-id>
<country>US</country>
<doc-number>7117231</doc-number>
<kind>B2</kind>
<name>Fischer et al.</name>
<date>20061000</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>707203</main-classification></classification-national>
</citation>
<citation>
<patcit num="00008">
<document-id>
<country>US</country>
<doc-number>2004/0093220</doc-number>
<kind>A1</kind>
<name>Kirby et al.</name>
<date>20040500</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>704278</main-classification></classification-national>
</citation>
<citation>
<patcit num="00009">
<document-id>
<country>JP</country>
<doc-number>10-40260</doc-number>
<date>19980200</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<patcit num="00010">
<document-id>
<country>JP</country>
<doc-number>11-234611</doc-number>
<date>19990800</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<patcit num="00011">
<document-id>
<country>JP</country>
<doc-number>2000-324395</doc-number>
<date>20001100</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<patcit num="00012">
<document-id>
<country>JP</country>
<doc-number>2002-10222</doc-number>
<date>20020100</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<patcit num="00013">
<document-id>
<country>JP</country>
<doc-number>2002-351490</doc-number>
<date>20021200</date>
</document-id>
</patcit>
<category>cited by other</category>
</citation>
<citation>
<nplcit num="00014">
<othercit>Kim et al., “Content-Based News Video Retrieval with Closed Captions and Time Alignment”, Proceedings of the Second IEEE Pacific Rim Conference on Multimedia: Advances in Multimedia Information Processing, Lecture Notes In Computer Science, vol. 2195, pp. 879-884, 2001.</othercit>
</nplcit>
<category>cited by examiner</category>
</citation>
</references-cited>
<number-of-claims>18</number-of-claims>
<us-exemplary-claim>1</us-exemplary-claim>
<us-field-of-classification-search>
<classification-national>
<country>US</country>
<main-classification>None</main-classification>
</classification-national>
</us-field-of-classification-search>
<figures>
<number-of-drawing-sheets>5</number-of-drawing-sheets>
<number-of-figures>6</number-of-figures>
</figures>
<us-related-documents>
<related-publication>
<document-id>
<country>US</country>
<doc-number>20050060145</doc-number>
<kind>A1</kind>
<date>20050317</date>
</document-id>
</related-publication>
</us-related-documents>
<parties>
<applicants>
<applicant sequence="001" app-type="applicant-inventor" designation="us-only">
<addressbook>
<last-name>Abe</last-name>
<first-name>Kazuhiko</first-name>
<address>
<city>Yokohama</city>
<country>JP</country>
</address>
</addressbook>
<nationality>
<country>JP</country>
</nationality>
<residence>
<country>JP</country>
</residence>
</applicant>
<applicant sequence="002" app-type="applicant-inventor" designation="us-only">
<addressbook>
<last-name>Masai</last-name>
<first-name>Yasuyuki</first-name>
<address>
<city>Yokohama</city>
<country>JP</country>
</address>
</addressbook>
<nationality>
<country>JP</country>
</nationality>
<residence>
<country>JP</country>
</residence>
</applicant>
<applicant sequence="003" app-type="applicant-inventor" designation="us-only">
<addressbook>
<last-name>Yajima</last-name>
<first-name>Makoto</first-name>
<address>
<city>Tachikawa</city>
<country>JP</country>
</address>
</addressbook>
<nationality>
<country>JP</country>
</nationality>
<residence>
<country>JP</country>
</residence>
</applicant>
<applicant sequence="004" app-type="applicant-inventor" designation="us-only">
<addressbook>
<last-name>Momosaki</last-name>
<first-name>Kohei</first-name>
<address>
<city>Kawasaki</city>
<country>JP</country>
</address>
</addressbook>
<nationality>
<country>JP</country>
</nationality>
<residence>
<country>JP</country>
</residence>
</applicant>
<applicant sequence="005" app-type="applicant-inventor" designation="us-only">
<addressbook>
<last-name>Yamamoto</last-name>
<first-name>Koichi</first-name>
<address>
<city>Kawasaki</city>
<country>JP</country>
</address>
</addressbook>
<nationality>
<country>JP</country>
</nationality>
<residence>
<country>JP</country>
</residence>
</applicant>
<applicant sequence="006" app-type="applicant-inventor" designation="us-only">
<addressbook>
<last-name>Sasajima</last-name>
<first-name>Munehiko</first-name>
<address>
<city>Yokohama</city>
<country>JP</country>
</address>
</addressbook>
<nationality>
<country>JP</country>
</nationality>
<residence>
<country>JP</country>
</residence>
</applicant>
</applicants>
<agents>
<agent sequence="01" rep-type="attorney">
<addressbook>
<orgname>Oblon, Spivak, McClelland, Maier &amp; Neustadt, P.C.</orgname>
<address>
<country>unknown</country>
</address>
</addressbook>
</agent>
</agents>
</parties>
<assignees>
<assignee>
<addressbook>
<orgname>Kabushiki Kaisha Toshiba</orgname>
<role>03</role>
<address>
<city>Tokyo</city>
<country>JP</country>
</address>
</addressbook>
</assignee>
</assignees>
<examiners>
<primary-examiner>
<last-name>Hudspeth</last-name>
<first-name>David</first-name>
<department>2626</department>
</primary-examiner>
<assistant-examiner>
<last-name>Albertalli</last-name>
<first-name>Brian L.</first-name>
</assistant-examiner>
</examiners>
</us-bibliographic-data-grant>
<abstract id="abstract">
<p id="p-0001" num="0000">A closed caption signal processing apparatus comprises a speech recognition unit to recognize speech contents from the speech signal, a speech timing detecting unit to detect a speech timing of the speech contents, a closed caption contents acquisition unit to acquire closed caption contents from the closed caption signal associated with the speech signal, a presentation timing detecting unit to detect a presentation timing of the closed caption contents, an agreement detecting unit to detect an agreement between the speech contents and the closed caption contents, and a time difference calculating unit to calculate a time difference between the speech timing and the presentation timing that are detected on the speech contents and the closed caption contents that agree with each other, respectively.</p>
</abstract>
<drawings id="DRAWINGS">
<figure id="Fig-EMI-D00000" num="00000">
<img id="EMI-D00000" he="155.28mm" wi="253.83mm" file="US07299183-20071120-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00001" num="00001">
<img id="EMI-D00001" he="254.59mm" wi="163.32mm" orientation="landscape" file="US07299183-20071120-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00002" num="00002">
<img id="EMI-D00002" he="245.19mm" wi="168.83mm" file="US07299183-20071120-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00003" num="00003">
<img id="EMI-D00003" he="192.02mm" wi="127.00mm" file="US07299183-20071120-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00004" num="00004">
<img id="EMI-D00004" he="186.94mm" wi="131.91mm" file="US07299183-20071120-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00005" num="00005">
<img id="EMI-D00005" he="190.84mm" wi="136.99mm" file="US07299183-20071120-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
</drawings>
<description id="description">
<?BRFSUM description="Brief Summary" end="lead"?>
<heading id="h-0001" level="1">CROSS-REFERENCE TO RELATED APPLICATIONS</heading>
<p id="p-0002" num="0001">This application is based upon and claims the benefit of priority from prior Japanese Patent Application No. 2003-202554, filed Jul. 28, 2003, the entire contents of which are incorporated herein by reference.</p>
<heading id="h-0002" level="1">BACKGROUND OF THE INVENTION</heading>
<p id="p-0003" num="0002">1. Field of the Invention</p>
<p id="p-0004" num="0003">The present invention relates to a closed caption signal processing apparatus which outputs a speech and a closed caption based on a speech signal and a closed caption signal which express contents corresponding to each other, and a method therefor.</p>
<p id="p-0005" num="0004">2. Description of the Related Art</p>
<p id="p-0006" num="0005">Closed caption information is sent by a television broadcasting radio signal. However, the closed caption information does not always coincide with the speech signal. Particularly, when the closed caption is made up at a real time in the broadcasting station, the closed caption information may delay by 5 to 10 seconds with respect to the speech signal. Accordingly, when the television broadcasting radio signal is received with a television receiver, the closed caption is not displayed at the same timing as a speech. This makes a TV viewer to be hard to watch TV programming. It is desired that the closed caption is displayed at the same timing as the speech.</p>
<p id="p-0007" num="0006">The object of the present invention is to provide a closed caption signal processing apparatus to detect a timing deviation between utterance and display of the closed caption, and compensate the deviation, and a method therefor.</p>
<heading id="h-0003" level="1">BRIEF SUMMARY OF THE INVENTION</heading>
<p id="p-0008" num="0007">An aspect of the present invention provides a closed caption signal processing apparatus using a speech signal and a closed caption signal, the apparatus comprising: a speech recognition unit configured to recognize speech contents from the speech signal; a speech timing detecting unit configured to detect a speech timing of the speech contents; a closed caption contents acquisition unit configured to acquire closed caption contents from the closed caption signal associated with the speech signal; a presentation timing detecting unit configured to detect a presentation timing of the closed caption contents; an agreement detecting unit configured to detect an agreement between the speech contents and the closed caption contents; and a time difference calculating unit configured to calculate a time difference between the speech timing and the presentation timing that are detected on the speech contents and the closed caption contents that agree with each other, respectively.</p>
<p id="p-0009" num="0008">Another aspect of the present invention provides a method for processing a closed caption signal, comprising: recognizing speech contents from a speech signal; detecting a speech timing of the speech contents; deriving closed caption contents from a closed caption signal associated with the speech signal; detecting a presentation timing of the closed caption contents; detecting agreement between the speech contents and the closed caption contents; calculating a time difference between the speech timing and the presentation timing that are detected on the speech contents and the closed caption contents that agree with each other, respectively.</p>
<?BRFSUM description="Brief Summary" end="tail"?>
<?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?>
<description-of-drawings>
<heading id="h-0004" level="1">BRIEF DESCRIPTION OF THE SEVERAL VIEWS OF THE DRAWING</heading>
<p id="p-0010" num="0009"><figref idref="DRAWINGS">FIG. 1</figref> is a block diagram illustrating a configuration of a hard disk recorder related to an embodiment of the present invention.</p>
<p id="p-0011" num="0010"><figref idref="DRAWINGS">FIG. 2</figref> is a block diagram illustrating an internal structure of a speech recognition unit <b>141</b> during <figref idref="DRAWINGS">FIG. 1</figref>.</p>
<p id="p-0012" num="0011"><figref idref="DRAWINGS">FIG. 3</figref> is a flowchart of a process of a closed caption deviation quality determination unit <b>14</b> of <figref idref="DRAWINGS">FIG. 1</figref>.</p>
<p id="p-0013" num="0012"><figref idref="DRAWINGS">FIG. 4</figref> is a diagram of a concrete example for speech recognition and closed caption analysis.</p>
<p id="p-0014" num="0013"><figref idref="DRAWINGS">FIG. 5</figref> is a diagram showing an example of correction of a presentation timing of a closed caption.</p>
<p id="p-0015" num="0014"><figref idref="DRAWINGS">FIG. 6</figref> is a diagram showing an example of correction of a presentation timing of a closed caption.</p>
</description-of-drawings>
<?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?>
<?DETDESC description="Detailed Description" end="lead"?>
<heading id="h-0005" level="1">DETAILED DESCRIPTION OF THE INVENTION</heading>
<p id="p-0016" num="0015">There will now be described one embodiment of the present invention referring to the drawing.</p>
<p id="p-0017" num="0016"><figref idref="DRAWINGS">FIG. 1</figref> is a block diagram illustrating a configuration of a hard disk recorder (referred to as HDD recorder) <b>1</b> related to the present embodiment.</p>
<p id="p-0018" num="0017">The HDD recorder <b>1</b> includes a television receiver circuit <b>11</b>, a video recording controller <b>12</b>, a hard disk drive (referred to as HDD) <b>13</b>, a closed caption deviation quantity determination unit <b>14</b>, a deviation information storage unit <b>15</b>, a meta data memory <b>16</b>, a playback controller <b>17</b>, a closed caption signal correction unit <b>18</b> and a closed caption synthesizer <b>19</b>.</p>
<p id="p-0019" num="0018">The television receiver circuit <b>11</b> receives a television broadcasting through an antenna <b>2</b>. The television broadcasting to be received by the HDD recorder <b>1</b> may be any system, but assumes an NTSC system here. Accordingly, the television receiver circuit <b>11</b> outputs a signal based on the NTSC system (referred to as NTSC signal).</p>
<p id="p-0020" num="0019">The video recording controller <b>12</b> subjects video information and audio information of a baseband signal output from the television receiver circuit <b>11</b> to a compression process according to a given format and writes it in the HDD <b>13</b>. The video recording controller <b>12</b> extracts closed caption information multiplexed in the vertical blanking interval of the NTSC signal and writes it in the HDD <b>13</b>.</p>
<p id="p-0021" num="0020">The closed caption deviation quantity determination unit <b>14</b> includes a speech recognition module <b>141</b>, a closed caption analysis module <b>142</b>, an agreement search module <b>143</b>, a deviation quantity determination module <b>144</b>, a deviation information generation module <b>145</b>, and a meta data generation module <b>146</b>.</p>
<p id="p-0022" num="0021">This closed caption deviation quantity determination module <b>14</b> can be configured mainly by a processor, for example. The speech recognition module <b>141</b>, closed caption analysis module <b>142</b>, agreement search module <b>143</b>, deviation quantity determination module <b>144</b>, deviation information generation module <b>145</b>, and meta data generation module <b>146</b> can be realized by causing a processor to execute a program. In this time, the closed caption deviation quantity determination module <b>14</b> may be realized by a program installed in a built-in memory (not shown). Alternatively, it may be realized by a program stored in a removable recording medium such as a compact disk-read only memory, or a program distributed through a network and installed in the memory (not shown) included in the closed caption deviation quality determination module <b>14</b>.</p>
<p id="p-0023" num="0022">The speech recognition module <b>141</b> subjects the speech information stored in the HDD <b>13</b> to speech recognition to derive speech contents (notation data, pronunciation data and various morpheme information), and speech timing information indicating a timing at which the speech contents are uttered.</p>
<p id="p-0024" num="0023">The closed caption analysis module <b>142</b> decodes the closed caption information stored in the HDD <b>13</b> to derive text data indicating a character string of the closed caption. The closed caption analysis module <b>142</b> derives presentation timing information representing a presentation timing of the closed caption. The closed caption analysis module <b>142</b> subjects the text data to a morphological analysis to read each morpheme included in the text data and derive part-of-speech information.</p>
<p id="p-0025" num="0024">The agreement search module <b>143</b> searches for an agreement between speech contents derived by the speech recognition module <b>141</b> and the text data derived by the closed caption analysis module <b>142</b>. The agreement search module <b>143</b> outputs text data agreed with the speech contents, presentation timing information of the text data, and speech timing information of the speech contents agreed with the text data to the deviation quality determination unit <b>144</b>.</p>
<p id="p-0026" num="0025">The deviation quantity determination unit <b>144</b> determines a deviation quantity of a presentation timing with respect to the text data output from the agreement search module <b>143</b> as a time difference between the timings indicated respectively by the speech timing information and presentation timing information which are supplied at the same time. The deviation quantity determination module <b>144</b> outputs the determined deviation quantity along with the text data and presentation timing information to the deviation information generation module <b>145</b> and the meta data generation module <b>146</b>.</p>
<p id="p-0027" num="0026">The deviation information generation module <b>145</b> generates the deviation information that made it reflect the deviation quantity output from the deviation quantity determination module <b>144</b>, and write the deviation information in the information storage unit <b>15</b>.</p>
<p id="p-0028" num="0027">When the text data output from the deviation quantity determination module <b>144</b> matches a given meta registration condition, the meta data generation module <b>146</b> generates meta data which reflects the deviation quantity output from the deviation quantity determination module <b>144</b>, and write the meta data in the meta data memory <b>16</b>.</p>
<p id="p-0029" num="0028">It is desirable that the deviation information storage unit <b>15</b> has a high capacity and is easy to access. It comprises, for example, a semiconductor memory or HDD. The deviation information storage unit <b>15</b> stores deviation information.</p>
<p id="p-0030" num="0029">It is desirable that the meta data storage unit <b>16</b> has a high capacity and is easy to access. It comprises, for example, a semiconductor memory or HDD. The meta data storage unit <b>16</b> stores meta data.</p>
<p id="p-0031" num="0030">The playback controller <b>17</b> reads video information, speech information and closed caption information which are stored in the HDD <b>13</b>, and plays back an NTSC signal or AV signal which indicates these information. The playback controller <b>17</b> outputs a baseband signal to the closed caption signal correction unit <b>18</b>, and outputs a video signal and closed caption information of the AV signal to the closed caption synthesis unit <b>19</b>.</p>
<p id="p-0032" num="0031">The closed caption signal correction unit <b>18</b> corrects the closed caption information included in the NTSC signal output from the playback controller <b>17</b> referring to the deviation information stored in the information storage unit <b>15</b>. The NTSC signal from the closed caption signal correction unit <b>18</b> is sent via an NTSC terminal T<b>1</b> to the outside of the HDD recorder <b>1</b>. A television set (referred to as a closed caption broadcast compatible television) compatible with the closed-caption broadcasting, for example, is connected to the NTSC terminal T<b>1</b>.</p>
<p id="p-0033" num="0032">The closed caption synthesis unit <b>19</b> synthesizes the closed caption according to the closed caption information output from the playback controller <b>17</b> with the video signal output from the playback controller <b>17</b>. In this time, the closed caption synthesis unit <b>19</b> can shift a synthesis timing of closed caption information with respect to the video signal referring to the deviation information stored in the deviation information storage unit <b>15</b>. The video signal from the closed caption synthesis unit <b>19</b> is output via a video terminal T<b>2</b> to the outside of the HDD recorder <b>1</b>. For example, a television receiver <b>4</b> with an AV terminal is connected to the video terminal T<b>2</b>.</p>
<p id="p-0034" num="0033"><figref idref="DRAWINGS">FIG. 2</figref> is a block diagram illustrating an internal structure of the speech recognition module <b>141</b>. The speech recognition unit <b>141</b> includes a speech feature extraction module <b>141</b><i>a</i>, a pronunciation information estimation module <b>141</b><i>b</i>, and a speech contents estimation module <b>141</b><i>c. </i></p>
<p id="p-0035" num="0034">The speech recognition module <b>141</b> extracts a characteristic quality inherent to a speech from a speech signal with the speech feature extraction module <b>141</b><i>a</i>. The speech recognition module <b>141</b> compares the characteristic quantity with a model of the speech prepared beforehand by means of the pronunciation information estimation module <b>141</b><i>b </i>to estimate pronunciation information. Further, the speech recognition unit <b>141</b> estimates speech contents with linguistic meaning based on the pronunciation information by means of the speech contents estimation module <b>141</b><i>c</i>. A difference between kannji character notation and hiragana character notation, a homonym, and a recognition candidate are acquired at the same time, and output as a speech recognition result.</p>
<p id="p-0036" num="0035">The operation of the HDD recorder <b>1</b> configured as above will be described hereinafter. The operation for normal video recording and playback is omitted since it is similar to that of a conventional record-playback machine. The operation concerning a process of the closed caption will be described in detail hereinafter.</p>
<p id="p-0037" num="0036">This HDD recorder <b>1</b> records a broadcasted program on HDD <b>13</b>, once, and then plays back the recorded program at an arbitrary timing required by a user or at a substantially real time.</p>
<p id="p-0038" num="0037">In this time, if the program recorded on the HDD <b>13</b> has closed caption information, the closed caption deviation quantity determination unit <b>14</b> generates deviation information and meta data referring to the closed caption information and video information as follows.</p>
<p id="p-0039" num="0038"><figref idref="DRAWINGS">FIG. 3</figref> shows a flowchart of a process executed by the closed caption deviation quantity determination unit <b>14</b>. The speech recognition module <b>141</b> subjects the speech information stored in the HDD <b>13</b> to speech recognition to derive speech contents and speech timing information indicating a speech timing at which the speech contents are uttered. The speech recognition module <b>141</b> derives notation data and pronunciation data for the speech contents. The speech recognition module <b>141</b> takes a correlation time using, for example, the head of the program as a reference time to be speech timing information.</p>
<p id="p-0040" num="0039">If information such as a time code indicating the absolute position of a speech signal is added to the speech signal, this information can be used as speech timing information. The speech contents of one of a series of speeches may be delimited at an arbitrary level. However, in this embodiment, it is delimited every morpheme.</p>
<p id="p-0041" num="0040">Concretely, the speech recognition module <b>141</b> derives four speech contents of, for example, “ASU”, “WA”, “AME”, “DESU” as shown in <figref idref="DRAWINGS">FIG. 4</figref>, from speech data representing the speech “ASU WA AME DESU”. The speech recognition module <b>141</b> acquires times t<b>1</b>, t<b>2</b>, t<b>3</b>, t<b>4</b> shown in <figref idref="DRAWINGS">FIG. 4</figref> as speech timing information of the speech contents.</p>
<p id="p-0042" num="0041">The closed caption analysis module <b>142</b> analyzes the closed caption information stored in the HDD <b>13</b> in step ST<b>2</b>. The closed caption analysis module <b>142</b> decodes the closed caption information stored in the HDD <b>13</b> and acquires the text data representing a closed caption sentence. The closed caption analysis module <b>142</b> subjects the text data to a morphological analysis to derive reading information of each morpheme and part-of-speech information which are included in the text data.</p>
<p id="p-0043" num="0042">The closed caption analysis module <b>142</b> acquires presentation timing information representing a presentation timing of the closed caption sentence expressed by the text data. The presentation timing of the closed caption sentence assumes a timing at which the closed caption information is derived. Accordingly, the closed caption analysis module <b>142</b> sets a relative time defining, as a reference time, the position of the NTSC signal at which the closed caption information is multiplexed, for example, the head of the program, to presentation timing information.</p>
<p id="p-0044" num="0043">When the presentation timing of the closed caption sentence is designated by control information superposed on the NTSC signal, this control information can be used as presentation timing information.</p>
<p id="p-0045" num="0044"><figref idref="DRAWINGS">FIG. 4</figref> shows an example that the text data representing the closed caption sentence of “ASU WA AME DESU” is received after the speech of “ASU WA AME DESU”. In this case, the closed caption analysis module <b>142</b> acquires a time t<b>5</b> indicating an arrival timing of the text data as presentation timing information, as well as the text data of “ASU WA AME DESU”. The closed caption analysis module <b>142</b> derives reading information of four morphemes of “ASU”, “WA”, “AME”, “DESU” and part-of-speech information from the text data.</p>
<p id="p-0046" num="0045">In step ST<b>3</b>, the agreement search module <b>143</b> searches for an agreement between the character string expressing one or more speech contents which are acquired by the speech recognition module <b>141</b> and the character string represented by the text data acquired by the closed caption analysis module <b>142</b>.</p>
<p id="p-0047" num="0046">When the speech contents and the text data do not agree with each other due to a difference between hiragana notation and kanji notation and so on, the agreement search unit <b>143</b> compares data of the next candidate for speech recognition and pronunciation data with the text data and the reading data acquired by the morphological analysis. The agreement search module <b>143</b> performs not only comparison between the speech contents and the text data that are identical in the speech timing and the presentation timing, but also comparison between the speech contents and the text data that are deviated in timing from each other in a fixed range.</p>
<p id="p-0048" num="0047">In the example of <figref idref="DRAWINGS">FIG. 4</figref>, the character string representing the speech contents and the character string representing the text data include a character string of “ASU WA AME DESU” together. Such a character string is detected by the agreement search module <b>143</b>. In performing a character string agreement search, use of various morphological information, for example, part-of-speech information permits more precise agreement.</p>
<p id="p-0049" num="0048">In step ST<b>4</b>, the agreement search module <b>143</b> ensures whether an agreed character string is found by the search. If the agreed character string is found, the process advances to step ST<b>5</b>.</p>
<p id="p-0050" num="0049">In step ST<b>5</b>, the text data corresponding to the found character string, the morpheme information of this text data, the presentation timing information, and the speech timing information of each speech content including the found character string are input to the deviation quantity determination module <b>144</b> from the agreement search module <b>143</b>. In step ST<b>5</b>, the deviation quantity determination module <b>144</b> calculates a time difference between the speech time of the head indicated by the speech timing information and the presentation time indicated by the presentation timing information, and determines it as a deviation quantity of the closed caption sentence indicated by the text data with respect to the speech timing.</p>
<p id="p-0051" num="0050">The deviation quantity determination module <b>144</b> determines a deviation quantity based on a time difference between the speech time indicated by the speech timing information of agreed speech contents and the presentation time indicated by the presentation timing information in regard to each of morphemes included in the text data.</p>
<p id="p-0052" num="0051">In the example of <figref idref="DRAWINGS">FIG. 4</figref>, the speech times indicated by the speech timing information of speech contents agreeing with the closed caption sentence “ASU WA AME DESU” are t<b>1</b>, t<b>2</b>, t<b>3</b>, t<b>4</b>. Because the head is t<b>1</b>, the deviation quantity determination module <b>144</b> determines that the deviation quantity of the closed caption sentence expressing “ASU WA AME DESU” is [t<b>5</b>-t<b>1</b>]. The deviation quantity determination module <b>144</b> determines deviation quantities of the morphemes of “ASU”, “WA”, “AME”, “DESU” as [t<b>5</b>-t<b>1</b>], [t<b>5</b>-t<b>2</b>], [t<b>5</b>-t<b>3</b>], [t<b>5</b>-t<b>4</b>], respectively.</p>
<p id="p-0053" num="0052">In step ST<b>6</b>, the text data, the morpheme information, the presentation timing information, and the deviation quantity determined on the text data and morpheme information are input from the deviation quantity determination module <b>144</b> to the deviation information generation module <b>145</b>. The deviation information generation module <b>145</b> corresponds the text data, the presentation timing information and the deviation quantity to each other, and generates deviation information obtained by corresponding the morpheme information to the deviation quantity. This deviation information is stored in the deviation information storage unit <b>15</b>.</p>
<p id="p-0054" num="0053">In step ST<b>7</b>, the morpheme information, the presentation timing information, and the deviation quantity determined on the morpheme information are input to the meta data generation module <b>146</b> from the deviation quantity determination module <b>144</b>. The meta data generation module <b>146</b> ensures whether a morpheme according to a predetermined meta registration condition exists in the morphemes represented by the input morpheme information. The meta registration condition may be arbitrary, but it may be, for example, “nouns”. If an appropriate morpheme exists, the process advances to step ST<b>8</b>.</p>
<p id="p-0055" num="0054">In step ST<b>8</b>, the meta data generation module <b>146</b> generates the meta data by corresponding the morpheme matching the meta registration condition to the presentation timing information and the deviation quantity of the morpheme with respect to the presentation timing information, and stores the meta data in the meta data memory <b>16</b>.</p>
<p id="p-0056" num="0055">If the closed caption deviation quantity determination unit <b>14</b> completes step ST<b>8</b>, the process returns to step ST<b>3</b>. If there is no morpheme matching the meta registration condition, the closed caption deviation quantity determination unit <b>14</b> passes step ST<b>8</b>, and returns the process to step ST<b>3</b>. The closed caption deviation quantity determination unit <b>14</b> repeats steps ST<b>3</b>-ST<b>8</b> till it is determined that no agreed character string is found in step ST<b>4</b>. If it is determined that no agreed character string is found in step ST<b>4</b>, the closed caption deviation quantity determination unit <b>14</b> finishes the process.</p>
<p id="p-0057" num="0056">When the program recorded on the HDD <b>13</b> is played back, the closed caption signal correction unit <b>18</b> corrects the closed caption information included in the NTSC signal output from the playback controller <b>17</b> referring to the deviation information stored in the deviation information storage unit <b>15</b>.</p>
<p id="p-0058" num="0057">The closed caption synthesis unit <b>19</b> synthesizes the closed caption corresponding to the closed caption information output from the playback controller with the video signal output from the playback controller <b>17</b>. In this time, the closed caption synthesis unit <b>19</b> deviates a synthesis timing of the closed caption information with respect to the video signal, referring to the deviation information stored in the information storage unit <b>15</b>.</p>
<p id="p-0059" num="0058">The closed caption signal correction unit <b>18</b> and the closed caption synthesis unit <b>19</b> each have four compensation modes. In the first compensation mode, the closed caption signal correction unit <b>18</b> and the closed caption synthesis unit <b>19</b> decreases the deviation quantity of the closed caption sentence to correct the closed caption signal to approximate the presentation timing of the closed caption to the timing of the head of the agreed speech. For example, the presentation timing of the closed caption shown in <figref idref="DRAWINGS">FIG. 4</figref> is corrected to a presentation timing as shown in <figref idref="DRAWINGS">FIG. 5</figref>.</p>
<p id="p-0060" num="0059">In the second compensation mode, the closed caption signal correction unit <b>18</b> and the closed caption synthesis unit <b>19</b> decreases the deviation quantity included in the closed caption sentence every morpheme to correct the closed caption signal to approximate the speech timing of the agreed morpheme to the presentation timing of each morpheme. For example, the presentation timing of the closed caption shown in <figref idref="DRAWINGS">FIG. 4</figref> is corrected to the presentation timing as shown in <figref idref="DRAWINGS">FIG. 6</figref>.</p>
<p id="p-0061" num="0060">In the third compensation mode, the closed caption signal correction unit <b>18</b> and the closed caption synthesis unit <b>19</b> correct the closed caption signal to set the presentation timing of the closed caption to a timing deviated by a given time from the timing of the head of the agreed speech. For example, the presentation timing of the closed caption is again set to a timing further deviated by a give time from the state of <figref idref="DRAWINGS">FIG. 5</figref>. In this time, the given time is determined according to a predefined rule such as “present only a noun at three seconds late”.</p>
<p id="p-0062" num="0061">In the fourth compensation mode, the closed caption signal correction unit <b>18</b> and the closed caption synthesis unit <b>19</b> correct the closed caption signal to set the presentation timing of each morpheme of the closed caption to a timing deviated by a given time from the speaking timing of the agreed morpheme. For example, the presentation timing of the closed caption is again set to a timing further deviated by a given time from the status of <figref idref="DRAWINGS">FIG. 6</figref>. In this time, the given time is determined according to a predefined rule such as “present only a noun at three seconds late”.</p>
<p id="p-0063" num="0062">According to the present embodiment as described above, the deviation quantity of the presentation timing of the closed caption with respect to the speech timing can be determined.</p>
<p id="p-0064" num="0063">According to the present embodiment, the closed caption presentation can be realized at an appropriate timing by performing compensation of the closed caption signal and adjustment of a synthesis timing of the closed caption to decrease the deviation of the presentation timing of the closed caption with respect to the speech timing in consideration with the determined deviation quantity.</p>
<p id="p-0065" num="0064">According to the present embodiment, the closed caption presentation can be realized in a uniform deviation by performing compensation of the closed caption signal or adjustment of a synthesis timing of the closed caption to set the deviation of the presentation timing of the closed caption with respect to the speech timing at a constant time, in consideration with the determined deviation quality. This is convenient for cases, for example, to train a listening comprehension of a foreign language by a foreign language program.</p>
<p id="p-0066" num="0065">According to the present embodiment, the meta data which reflects the determined deviation quantity is generated. Consequently, it is possible to identify the part of an appropriate program that a closed caption including a certain word and phrase should be presented by referring to the meta data. For this reason, if, for example, the playback controller <b>17</b> uses this meta data for searching for a playback start point, it is possible to locate the start of the program appropriately.</p>
<p id="p-0067" num="0066">Selection and presentation of the word and phrase that may be more important than the meta data is a help to understand the program information. For example, providing such a keyword at the time of a fast-forward playback can provide various usages to make it possible to do a fast-forward while confirming the contents.</p>
<p id="p-0068" num="0067">According to the present embodiment, since a morpheme to register with meta data is only one according with a morpheme included in the speech contents, the important word and phrase shown in both of the speech and the closed caption are extracted to generate meta data. Therefore, it is possible to generate the appropriate meta data including no meaningless information.</p>
<p id="p-0069" num="0068">This embodiment permits the following various kinds of modifications.</p>
<p id="p-0070" num="0069">The closed caption deviation quantity determination unit <b>14</b> may be realized as an independent device, and circulated independently. The closed caption deviation quantity determination unit <b>14</b> does not need to include the deviation information generation module <b>145</b> and the meta data generation module <b>146</b>. In this case, the deviation quantity determined by the deviation quantity determination module <b>144</b> is output to an external computer to use the determined deviation quantity with the computer.</p>
<p id="p-0071" num="0070">The deviation information stored in the deviation information storage unit <b>15</b> or the meta data stored in the meta data memory <b>16</b> may be output to an external computer to use it with the computer.</p>
<p id="p-0072" num="0071">Additional advantages and modifications will readily occur to those skilled in the art. Therefore, the invention in its broader aspects is not limited to the specific details and representative embodiments shown and described herein. Accordingly, various modifications may be made without departing from the spirit or scope of the general inventive concept as defined by the appended claims and their equivalents.</p>
<?DETDESC description="Detailed Description" end="tail"?>
</description>
<us-claim-statement>What is claimed is:</us-claim-statement>
<claims id="claims">
<claim id="CLM-00001" num="00001">
<claim-text>1. A closed caption signal receiving apparatus to receive a television signal including a speech signal and a closed caption signal, the apparatus comprising:
<claim-text>a television receiver unit configured to receive a television signal including a speech signal and a closed caption signal;</claim-text>
<claim-text>a speech recognition unit configured to subject speech information to speech recognition to derive speech contents including notation data, pronunciation data and various morpheme information, the speech recognition unit including a pronunciation information estimation module to estimate the pronunciation information, and estimating the speech contents with linguistic meaning based on the pronunciation information;</claim-text>
<claim-text>a speech timing detecting unit configured to detect a speech timing of the speech contents;</claim-text>
<claim-text>a closed caption contents acquisition unit configured to acquire closed caption contents from the closed caption signal associated with the speech signal;</claim-text>
<claim-text>a presentation timing detecting unit configured to detect a presentation timing of the closed caption contents;</claim-text>
<claim-text>an agreement detecting unit configured to detect an agreement between the speech contents and the closed caption contents; and</claim-text>
<claim-text>a time difference calculating unit configured to calculate a time difference between the speech timing and the presentation timing that are detected on the speech contents and the closed caption contents that agree with each other, respectively.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00002" num="00002">
<claim-text>2. The apparatus according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, which further includes a word and phrase extraction unit configured to extract a word and phrase from the closed caption contents; and a meta data generating unit configured to generate meta data including the word and phrase, and timing information, which reflects the time difference and the presentation timing detected on the closed caption contents including the word and phrase.</claim-text>
</claim>
<claim id="CLM-00003" num="00003">
<claim-text>3. The apparatus according to <claim-ref idref="CLM-00002">claim 2</claim-ref> wherein the word and phrase extraction unit is configured to extract the word and phrase from the closed caption contents agreeing with the speech contents.</claim-text>
</claim>
<claim id="CLM-00004" num="00004">
<claim-text>4. The apparatus according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, which further includes a closed caption correction unit configured to correct the closed caption signal to decrease the time difference.</claim-text>
</claim>
<claim id="CLM-00005" num="00005">
<claim-text>5. The apparatus according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, which further includes a closed caption correction unit configured to correct the closed caption signal to match the time difference with a predefined time difference.</claim-text>
</claim>
<claim id="CLM-00006" num="00006">
<claim-text>6. The apparatus according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, which further includes a closed caption synthesis unit configured to synthesize the closed caption contents of the closed caption with a video image of the video signal synchronized with the speech signal, while shifting at least one of the speech timing and the presentation timing to decrease the time difference.</claim-text>
</claim>
<claim id="CLM-00007" num="00007">
<claim-text>7. The apparatus according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, which further includes a closed caption synthesis unit configured to synthesize the closed caption contents of the closed caption with a video image of the video signal synchronized with the speech signal, while shifting at least one of the speech timing and the presentation timing to match the time difference with a predefined time difference.</claim-text>
</claim>
<claim id="CLM-00008" num="00008">
<claim-text>8. The apparatus according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, which is built into a hard disk recorder including a television receiver to receive television broadcasting and a hard disk to store the speech signal and closed caption signal of the television broadcasting.</claim-text>
</claim>
<claim id="CLM-00009" num="00009">
<claim-text>9. A method for receiving a television signal including a closed caption signal, comprising:
<claim-text>receiving a television signal including a speech signal and a closed caption signal;</claim-text>
<claim-text>subjecting speech information to speech recognition to derive speech contents including notation data, pronunciation data and various morpheme information, the speech recognition including estimating the pronunciation data, and estimating the speech contents with linguistic meaning based on the pronunciation information;</claim-text>
<claim-text>detecting a speech timing of the speech contents;</claim-text>
<claim-text>deriving closed caption contents from a closed caption signal associated with the speech signal;</claim-text>
<claim-text>detecting a presentation timing of the closed caption contents;</claim-text>
<claim-text>detecting agreement between the speech contents and the closed caption contents;</claim-text>
<claim-text>calculating a time difference between the speech timing and the presentation timing that are detected on the speech contents and the closed caption contents that agree with each other, respectively.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00010" num="00010">
<claim-text>10. The method according to <claim-ref idref="CLM-00009">claim 9</claim-ref>, which further includes extracting a word and phrase from the closed caption contents, and generating meta data including the word and phrase, and timing information, which reflects the time difference and the presentation timing detected on the closed caption contents including the word and phrase.</claim-text>
</claim>
<claim id="CLM-00011" num="00011">
<claim-text>11. The method according to <claim-ref idref="CLM-00009">claim 9</claim-ref>, which further includes compensating the closed caption signal to decrease the time difference.</claim-text>
</claim>
<claim id="CLM-00012" num="00012">
<claim-text>12. The method according to <claim-ref idref="CLM-00009">claim 9</claim-ref>, which further includes compensating the closed caption signal to match the time difference with a predetermined normal time difference.</claim-text>
</claim>
<claim id="CLM-00013" num="00013">
<claim-text>13. The method according to <claim-ref idref="CLM-00009">claim 9</claim-ref>, which further includes synthesizing the closed caption contents of the closed caption with a video image of the video signal synchronized with the speech signal, while shifting at least one of the speech timing and the presentation timing to decrease the time difference.</claim-text>
</claim>
<claim id="CLM-00014" num="00014">
<claim-text>14. The method according to <claim-ref idref="CLM-00009">claim 9</claim-ref>, which further includes synthesizing the closed caption contents of the closed caption with a video image of the video signal synchronized with the speech signal, while shifting at least one of the speech timing and the presentation timing to match the time difference with a predefined time difference.</claim-text>
</claim>
<claim id="CLM-00015" num="00015">
<claim-text>15. The method according to <claim-ref idref="CLM-00009">claim 9</claim-ref>, using the speech signal and closed caption signal of the television broadcasting.</claim-text>
</claim>
<claim id="CLM-00016" num="00016">
<claim-text>16. A computer readable medium encoded with computer program instructions for receiving a television signal including a closed caption signal, which when executed by a computer results in performance of steps comprising:
<claim-text>receiving a television signal including a speech signal and a closed caption signal;</claim-text>
<claim-text>subjecting speech information in the speech signal to speech recognition to derive speech contents including notation data, pronunciation data and various morpheme information, the speech recognition including estimating the pronunciation data, and estimating the speech contents with linguistic meaning based on the pronunciation information;</claim-text>
<claim-text>detecting a speech timing of the speech contents;</claim-text>
<claim-text>acquiring closed caption contents from a closed caption signal associated with the speech signal;</claim-text>
<claim-text>detecting a presentation timing of the closed caption contents;</claim-text>
<claim-text>detecting agreement between the speech contents and the closed caption contents;</claim-text>
<claim-text>calculating a time difference between the speech timing and the presentation timing that are detected on the speech contents and the closed caption contents that agree with each other, respectively.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00017" num="00017">
<claim-text>17. The computer readable medium according to <claim-ref idref="CLM-00016">claim 16</claim-ref>, wherein said steps further comprise extracting a word and phrase from the closed caption contents, and generating meta data including the word and phrase, and timing information, which reflects the time difference and the presentation timing detected on the closed caption contents including the word and phrase.</claim-text>
</claim>
<claim id="CLM-00018" num="00018">
<claim-text>18. The computer readable medium according to <claim-ref idref="CLM-00016">claim 16</claim-ref>, wherein said steps further comprise using the speech signal and closed caption signal of the television broadcasting.</claim-text>
</claim>
</claims>
</us-patent-grant>
